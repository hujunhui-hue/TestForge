{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "parse_args", "parameters": [], "param_types": {}, "return_type": "Args", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Args", "argparse.ArgumentParser", "dataclasses.fields", "getattr", "parser.add_argument", "parser.parse_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_args() -> Args:\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-n', '--dry-run', action='store_true')\n    parser.add_argument('path', type=pathlib.Path)\n\n    args = parser.parse_args()\n\n    # Store arguments in a typed dataclass\n    fields = dataclasses.fields(Args)\n    kwargs = {field.name: getattr(args, field.name) for field in fields}\n\n    return Args(**kwargs)", "loc": 12}
{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "run", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["print", "shlex.join", "str", "subprocess.run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Log and run given command. The command is not actually executed if ``dry_run`` is truthy.", "source_code": "def run(\n    *args: str | pathlib.Path,\n    dry_run: bool = False,\n) -> None:\n    \"\"\"\n    Log and run given command.\n\n    The command is not actually executed if ``dry_run`` is truthy.\n    \"\"\"\n    cmd = [str(arg) for arg in args]\n\n    dry_prefix = '[would run] ' if dry_run else ''\n    print(f'==> {dry_prefix}{shlex.join(cmd)}', flush=True)\n\n    if not dry_run:\n        subprocess.run(cmd, check=True)", "loc": 16}
{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "install_codecov", "parameters": ["dest", "dry_run"], "param_types": {"dest": "pathlib.Path", "dry_run": "bool"}, "return_type": "pathlib.Path", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["requirement_file.with_suffix", "run", "venv.create"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Populate a transitively pinned venv with ``codecov-cli``.", "source_code": "def install_codecov(dest: pathlib.Path, dry_run: bool = False) -> pathlib.Path:\n    \"\"\"Populate a transitively pinned venv with ``codecov-cli``.\"\"\"\n    requirement_file = DEPS_DIR / 'codecov.in'\n    constraint_file = requirement_file.with_suffix('.txt')\n\n    venv_dir = dest / 'venv'\n    python_bin = venv_dir / 'bin' / 'python'\n    codecov_bin = venv_dir / 'bin' / 'codecovcli'\n\n    venv.create(venv_dir, with_pip=True)\n\n    run(\n        python_bin,\n        '-m',\n        'pip',\n        'install',\n        f'--constraint={constraint_file!s}',\n        f'--requirement={requirement_file!s}',\n        '--disable-pip-version-check',\n        dry_run=dry_run,\n    )\n\n    return codecov_bin", "loc": 23}
{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "process_files", "parameters": ["directory"], "param_types": {"directory": "pathlib.Path"}, "return_type": "t.Tuple[CoverageFile, ...]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CoverageFile", "directory.joinpath", "directory.joinpath('reports').glob", "file.stem.replace", "flag.split", "flag.startswith", "name.replace", "name.replace('-powershell', '').split", "processed.append", "tuple"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_files(directory: pathlib.Path) -> t.Tuple[CoverageFile, ...]:\n    processed = []\n    for file in directory.joinpath('reports').glob('coverage*.xml'):\n        name = file.stem.replace('coverage=', '')\n\n        # Get flags from name\n        flags = name.replace('-powershell', '').split('=')  # Drop '-powershell' suffix\n        flags = [flag if not flag.startswith('stub') else flag.split('-')[0] for flag in flags]  # Remove \"-01\" from stub files\n\n        processed.append(CoverageFile(name, file, flags))\n\n    return tuple(processed)", "loc": 12}
{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "upload_files", "parameters": ["codecov_bin", "config_file", "files", "dry_run"], "param_types": {"codecov_bin": "pathlib.Path", "config_file": "pathlib.Path", "files": "t.Tuple[CoverageFile, ...]", "dry_run": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "run"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def upload_files(codecov_bin: pathlib.Path, config_file: pathlib.Path, files: t.Tuple[CoverageFile, ...], dry_run: bool = False) -> None:\n    for file in files:\n        cmd = [\n            codecov_bin,\n            '--disable-telem',\n            '--codecov-yml-path',\n            config_file,\n            'upload-process',\n            '--disable-search',\n            '--disable-file-fixes',\n            '--plugin',\n            'noop',\n            '--name',\n            file.name,\n            '--file',\n            file.path,\n        ]\n\n        for flag in file.flags:\n            cmd.extend(['--flag', flag])\n\n        if dry_run:\n            cmd.append('--dry-run')\n\n        run(*cmd)", "loc": 25}
{"file": "ansible\\.azure-pipelines\\scripts\\publish-codecov.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["config_file.write_text", "install_codecov", "parse_args", "pathlib.Path", "process_files", "report_upload_completion", "tempfile.TemporaryDirectory", "upload_files"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main() -> None:\n    args = parse_args()\n\n    with tempfile.TemporaryDirectory(prefix='codecov-') as tmpdir:\n        config_file = pathlib.Path(tmpdir) / 'config.yml'\n        # Refs:\n        # * https://docs.codecov.com/docs/codecovyml-reference#codecovnotifymanual_trigger\n        # * https://docs.codecov.com/docs/notifications#preventing-notifications-until-youre-ready-to-send-notifications\n        config_file.write_text('codecov:\\n  notify:\\n    manual_trigger: true')\n\n        codecov_bin = install_codecov(\n            pathlib.Path(tmpdir),\n            dry_run=args.dry_run,\n        )\n        files = process_files(args.path)\n        upload_files(codecov_bin, config_file, files, args.dry_run)\n        # Ref: https://docs.codecov.com/docs/cli-options#send-notifications\n        report_upload_completion(codecov_bin, config_file, args.dry_run)", "loc": 18}
{"file": "ansible\\.azure-pipelines\\scripts\\time-command.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["sys.stdin.reconfigure", "sys.stdout.flush", "sys.stdout.reconfigure", "sys.stdout.write", "time.time"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Main program entry point.", "source_code": "def main():\n    \"\"\"Main program entry point.\"\"\"\n    start = time.time()\n\n    sys.stdin.reconfigure(errors='surrogateescape')\n    sys.stdout.reconfigure(errors='surrogateescape')\n\n    for line in sys.stdin:\n        seconds = time.time() - start\n        sys.stdout.write('%02d:%02d %s' % (seconds // 60, seconds % 60, line))\n        sys.stdout.flush()", "loc": 11}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "parse_args", "parameters": [], "param_types": {}, "return_type": "Args", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argparse.ArgumentParser", "create_common_arguments", "create_deprecation_parser", "create_feature_parser", "invoke_parser", "parser.add_subparsers"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_args() -> Args:\n    parser = argparse.ArgumentParser()\n\n    create_common_arguments(parser)\n\n    subparser = parser.add_subparsers(required=True)\n\n    create_deprecation_parser(subparser)\n    create_feature_parser(subparser)\n\n    args = invoke_parser(parser)\n\n    return args", "loc": 13}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "create_deprecation_parser", "parameters": ["subparser"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["create_common_arguments", "parser.add_argument", "parser.set_defaults", "subparser.add_parser", "tuple"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_deprecation_parser(subparser) -> None:\n    parser: argparse.ArgumentParser = subparser.add_parser('deprecation')\n    parser.set_defaults(type=DeprecationArgs)\n    parser.set_defaults(command=deprecated_command)\n\n    parser.add_argument(\n        '--test',\n        dest='tests',\n        choices=tuple(TEST_OPTIONS),\n        action='append',\n        help='sanity test name',\n    )\n\n    create_common_arguments(parser)", "loc": 14}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "create_feature_parser", "parameters": ["subparser"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["create_common_arguments", "parser.add_argument", "parser.set_defaults", "pathlib.Path", "subparser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_feature_parser(subparser) -> None:\n    epilog = \"\"\"\nExample source YAML:\n\ndefault:\n  component: ansible-test\n  labels:\n    - ansible-test\n    - feature\n  assignee: \"@me\"\nfeatures:\n  - title: Some title goes here\n    summary: A summary goes here.\n\"\"\"\n\n    parser: argparse.ArgumentParser = subparser.add_parser('feature', epilog=epilog, formatter_class=argparse.RawDescriptionHelpFormatter)\n    parser.set_defaults(type=FeatureArgs)\n    parser.set_defaults(command=feature_command)\n\n    parser.add_argument(\n        '--source',\n        type=pathlib.Path,\n        default=pathlib.Path('issues.yml'),\n        help='YAML file containing issue details (default: %(default)s)',\n    )\n\n    create_common_arguments(parser)", "loc": 27}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "invoke_parser", "parameters": ["parser"], "param_types": {"parser": "argparse.ArgumentParser"}, "return_type": "Args", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "args_type", "dataclasses.fields", "getattr", "parser.parse_args"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def invoke_parser(parser: argparse.ArgumentParser) -> Args:\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    parsed_args = parser.parse_args()\n\n    kvp = {}\n    args_type = parsed_args.type\n\n    for field in dataclasses.fields(args_type):\n        kvp[field.name] = getattr(parsed_args, field.name)\n\n    args = args_type(**kvp)\n\n    return args", "loc": 15}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "run_sanity_test", "parameters": ["test_name"], "param_types": {"test_name": "str"}, "return_type": "list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.rename", "process.stdout.decode", "process.stdout.decode().splitlines", "subprocess.run"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_sanity_test(test_name: str) -> list[str]:\n    cmd = ['ansible-test', 'sanity', '--test', test_name, '--lint', '--failure-ok']\n    skip_path = 'test/sanity/code-smell/skip.txt'\n    skip_temp_path = skip_path + '.tmp'\n\n    os.rename(skip_path, skip_temp_path)  # make sure ansible-test isn't configured to skip any tests\n\n    try:\n        process = subprocess.run(cmd, capture_output=True, check=True)\n    finally:\n        os.rename(skip_temp_path, skip_path)  # restore the skip entries\n\n    messages = process.stdout.decode().splitlines()\n\n    return messages", "loc": 15}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "create_issues_from_deprecation_messages", "parameters": ["test_type", "messages"], "param_types": {"test_type": "t.Type[Deprecation]", "messages": "list[str]"}, "return_type": "list[Issue]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bug_report.create_issue", "deprecation.create_bug_report", "test_type.parse"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_issues_from_deprecation_messages(test_type: t.Type[Deprecation], messages: list[str]) -> list[Issue]:\n    deprecations = [test_type.parse(message) for message in messages]\n    bug_reports = [deprecation.create_bug_report() for deprecation in deprecations]\n    issues = [bug_report.create_issue(PROJECT) for bug_report in bug_reports]\n    return issues", "loc": 5}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "deprecated_command", "parameters": ["args"], "param_types": {"args": "DeprecationArgs"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["create_issues", "create_issues_from_deprecation_messages", "info", "issues.extend", "list", "run_sanity_test"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deprecated_command(args: DeprecationArgs) -> None:\n    issues: list[Issue] = []\n\n    for test in args.tests or list(TEST_OPTIONS):\n        test_type = TEST_OPTIONS[test]\n        info(f'Running \"{test}\" sanity test...')\n        messages = run_sanity_test(test)\n        issues.extend(create_issues_from_deprecation_messages(test_type, messages))\n\n    create_issues(args, issues)", "loc": 10}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "feature_command", "parameters": ["args"], "param_types": {"args": "FeatureArgs"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Feature.from_dict", "RuntimeError", "args.source.open", "create_issues", "data.update", "default.copy", "feature.create_issue", "isinstance", "issues.append", "source.get", "yaml.safe_load"], "control_structures": ["For", "If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def feature_command(args: FeatureArgs) -> None:\n    with args.source.open() as source_file:\n        source = yaml.safe_load(source_file)\n\n    default: dict[str, t.Any] = source.get('default', {})\n    features: list[dict[str, t.Any]] = source.get('features', [])\n\n    if not isinstance(default, dict):\n        raise RuntimeError('`default` must be `dict[str, ...]`')\n\n    if not isinstance(features, list):\n        raise RuntimeError('`features` must be `list[dict[str, ...]]`')\n\n    issues: list[Issue] = []\n\n    for feature in features:\n        data = default.copy()\n        data.update(feature)\n\n        feature = Feature.from_dict(data)\n        issues.append(feature.create_issue(PROJECT))\n\n    create_issues(args, issues)", "loc": 23}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": null, "function_name": "create_issues", "parameters": ["args", "issues"], "param_types": {"args": "Args", "issues": "list[Issue]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info", "issue.create", "len"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_issues(args: Args, issues: list[Issue]) -> None:\n    if not issues:\n        info('No issues found.')\n        return\n\n    info(f'Found {len(issues)} issue(s) to report:')\n\n    for issue in issues:\n        info(f'[{issue.title}] {issue.summary}')\n\n        if args.verbose:\n            info('>>>')\n            info(issue.body)\n            info('>>>')\n\n        if args.create:\n            url = issue.create()\n            info(url)\n\n    if not args.create:\n        info('Pass the \"--create\" option to create these issues on GitHub.')", "loc": 21}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "Issue", "function_name": "create", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.extend", "ex.stderr.strip", "ex.stdout.strip", "print", "process.stdout.strip", "subprocess.run"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create(self) -> str:\n    cmd = ['gh', 'issue', 'create', '--title', self.title, '--body', self.body, '--project', self.project]\n\n    if self.labels:\n        for label in self.labels:\n            cmd.extend(('--label', label))\n\n    if self.assignee:\n        cmd.extend(('--assignee', self.assignee))\n\n    try:\n        process = subprocess.run(cmd, capture_output=True, check=True, text=True)\n    except subprocess.CalledProcessError as ex:\n        print('>>> Note')\n        print(f\"You may need to run 'gh auth refresh -s project' if 'gh' reports it cannot find the project {self.project!r} when it exists.\")\n        print(f'>>> Standard Output\\n{ex.stdout.strip()}\\n>>> Standard Error\\n{ex.stderr.strip()}\\n>>> Exception')\n        raise\n\n    url = process.stdout.strip()\n    return url", "loc": 20}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "Feature", "function_name": "from_dict", "parameters": ["data"], "param_types": {"data": "dict[str, t.Any]"}, "return_type": "Feature", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Feature", "RuntimeError", "all", "data.get", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_dict(data: dict[str, t.Any]) -> Feature:\n    title = data.get('title')\n    summary = data.get('summary')\n    component = data.get('component')\n    labels = data.get('labels')\n    assignee = data.get('assignee')\n\n    if not isinstance(title, str):\n        raise RuntimeError(f'`title` is not `str`: {title}')\n\n    if not isinstance(summary, str):\n        raise RuntimeError(f'`summary` is not `str`: {summary}')\n\n    if not isinstance(component, str):\n        raise RuntimeError(f'`component` is not `str`: {component}')\n\n    if not isinstance(assignee, (str, type(None))):\n        raise RuntimeError(f'`assignee` is not `str`: {assignee}')\n\n    if not isinstance(labels, list) or not all(isinstance(item, str) for item in labels):\n        raise RuntimeError(f'`labels` is not `list[str]`: {labels}')\n\n    return Feature(\n        title=title,\n        summary=summary,\n        component=component,\n        labels=labels,\n        assignee=assignee,\n    )", "loc": 29}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "Feature", "function_name": "create_issue", "parameters": ["self", "project"], "param_types": {"project": "str"}, "return_type": "Issue", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Issue", "body.strip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "    def create_issue(self, project: str) -> Issue:\n        body = f'''\n### Summary\n\n{self.summary}\n\n### Issue Type\n\nFeature Idea\n\n### Component Name\n\n`{self.component}`\n'''\n\n        return Issue(\n            title=self.title,\n            summary=self.summary,\n            body=body.strip(),\n            project=project,\n            labels=self.labels,\n            assignee=self.assignee,\n        )", "loc": 23}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "BugReport", "function_name": "create_issue", "parameters": ["self", "project"], "param_types": {"project": "str"}, "return_type": "Issue", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Issue", "body.strip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "    def create_issue(self, project: str) -> Issue:\n        body = f'''\n### Summary\n\n{self.summary}\n\n### Issue Type\n\nBug Report\n\n### Component Name\n\n`{self.component}`\n\n### Ansible Version\n\n{MAJOR_MINOR_VERSION}\n\n### Configuration\n\nN/A\n\n### OS / Environment\n\nN/A\n\n### Steps to Reproduce\n\nN/A\n\n### Expected Results\n\nN/A\n\n### Actual Results\n\nN/A\n'''\n\n        return Issue(\n            title=self.title,\n            summary=self.summary,\n            body=body.strip(),\n            project=project,\n            labels=self.labels,\n        )", "loc": 46}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "DeprecatedConfig", "function_name": "parse", "parameters": ["message"], "param_types": {"message": "str"}, "return_type": "DeprecatedConfig", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DeprecatedConfig", "Exception", "match.group", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(message: str) -> DeprecatedConfig:\n    match = re.search('^(?P<path>.*):[0-9]+:[0-9]+: (?P<config>.*) is scheduled for removal in (?P<version>[0-9.]+)$', message)\n\n    if not match:\n        raise Exception(f'Unable to parse: {message}')\n\n    return DeprecatedConfig(\n        path=match.group('path'),\n        config=match.group('config'),\n        version=match.group('version'),\n    )", "loc": 11}
{"file": "ansible\\hacking\\create-bulk-issues.py", "class_name": "UpdateBundled", "function_name": "parse", "parameters": ["message"], "param_types": {"message": "str"}, "return_type": "UpdateBundled", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "UpdateBundled", "match.group", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(message: str) -> UpdateBundled:\n    match = re.search('^(?P<path>.*):[0-9]+:[0-9]+: UPDATE (?P<package>.*) from (?P<old>[0-9.]+) to (?P<new>[0-9.]+) (?P<link>https://.*)$', message)\n\n    if not match:\n        raise Exception(f'Unable to parse: {message}')\n\n    return UpdateBundled(\n        path=match.group('path'),\n        package=match.group('package'),\n        old_version=match.group('old'),\n        new_version=match.group('new'),\n        json_link=match.group('link'),\n    )", "loc": 13}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["args.func", "os.chdir", "parse_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    os.chdir(BASE_PATH)\n\n    args = parse_args()\n    args.func()", "loc": 5}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "parse_args", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "argparse.ArgumentParser", "parser.add_subparsers", "parser.parse_args", "populate.set_defaults", "query.set_defaults", "subparsers.add_parser"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_args():\n    try:\n        import argcomplete\n    except ImportError:\n        argcomplete = None\n\n    parser = argparse.ArgumentParser()\n\n    subparsers = parser.add_subparsers(metavar='COMMAND')\n    subparsers.required = True  # work-around for python 3 bug which makes subparsers optional\n\n    populate = subparsers.add_parser('populate',\n                                     help='populate report database')\n\n    populate.set_defaults(func=populate_database)\n\n    query = subparsers.add_parser('query',\n                                  help='query report database')\n\n    query.set_defaults(func=query_database)\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    return args", "loc": 27}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "populate_coverage", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["coverage_rows.append", "dict", "files.items", "float", "json.load", "open_url", "populate_data"], "control_structures": ["For"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def populate_coverage():\n    response = open_url('https://codecov.io/api/gh/ansible/ansible/tree/devel/?src=extension')\n    data = json.load(response)\n    files = data['commit']['report']['files']\n    coverage_rows = []\n\n    for path, data in files.items():\n        report = data['t']\n        coverage_rows.append(dict(\n            path=path,\n            coverage=float(report['c']),\n            lines=report['n'],\n            hit=report['h'],\n            partial=report['p'],\n            missed=report['m'],\n        ))\n\n    populate_data(dict(\n        coverage=dict(\n            rows=coverage_rows,\n            schema=(\n                ('path', 'TEXT'),\n                ('coverage', 'REAL'),\n                ('lines', 'INTEGER'),\n                ('hit', 'INTEGER'),\n                ('partial', 'INTEGER'),\n                ('missed', 'INTEGER'),\n            )),\n    ))", "loc": 29}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "populate_integration_targets", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "list", "populate_data", "walk_integration_targets"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate_integration_targets():\n    targets = list(walk_integration_targets())\n\n    integration_targets_rows = [dict(\n        target=target.name,\n        type=target.type,\n        path=target.path,\n        script_path=target.script_path,\n    ) for target in targets]\n\n    integration_target_aliases_rows = [dict(\n        target=target.name,\n        alias=alias,\n    ) for target in targets for alias in target.aliases]\n\n    integration_target_modules_rows = [dict(\n        target=target.name,\n        module=module,\n    ) for target in targets for module in target.modules]\n\n    populate_data(dict(\n        integration_targets=dict(\n            rows=integration_targets_rows,\n            schema=(\n                ('target', 'TEXT'),\n                ('type', 'TEXT'),\n                ('path', 'TEXT'),\n                ('script_path', 'TEXT'),\n            )),\n        integration_target_aliases=dict(\n            rows=integration_target_aliases_rows,\n            schema=(\n                ('target', 'TEXT'),\n                ('alias', 'TEXT'),\n            )),\n        integration_target_modules=dict(\n            rows=integration_target_modules_rows,\n            schema=(\n                ('target', 'TEXT'),\n                ('module', 'TEXT'),\n            )),\n    ))", "loc": 42}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "create_table", "parameters": ["cursor", "name", "columns"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "cursor.execute"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_table(cursor, name, columns):\n    schema = ', '.join('%s %s' % column for column in columns)\n\n    cursor.execute('DROP TABLE IF EXISTS %s' % name)\n    cursor.execute('CREATE TABLE %s (%s)' % (name, schema))", "loc": 5}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "populate_table", "parameters": ["cursor", "rows", "name", "columns"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "create_table", "cursor.execute"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate_table(cursor, rows, name, columns):\n    create_table(cursor, name, columns)\n\n    values = ', '.join([':%s' % column[0] for column in columns])\n\n    for row in rows:\n        cursor.execute('INSERT INTO %s VALUES (%s)' % (name, values), row)", "loc": 7}
{"file": "ansible\\hacking\\report.py", "class_name": null, "function_name": "populate_data", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["connection.close", "connection.commit", "connection.cursor", "populate_table", "sqlite3.connect"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate_data(data):\n    connection = sqlite3.connect(DATABASE_PATH)\n    cursor = connection.cursor()\n\n    for table in data:\n        populate_table(cursor, data[table]['rows'], table, data[table]['schema'])\n\n    connection.commit()\n    connection.close()", "loc": 9}
{"file": "ansible\\hacking\\return_skeleton_generator.py", "class_name": null, "function_name": "represent_ordereddict", "parameters": ["dumper", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.items", "dumper.represent_data", "value.append", "yaml.nodes.MappingNode"], "control_structures": ["For"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def represent_ordereddict(dumper, data):\n    value = []\n\n    for item_key, item_value in data.items():\n        node_key = dumper.represent_data(item_key)\n        node_value = dumper.represent_data(item_value)\n\n        value.append((node_key, node_value))\n\n    return yaml.nodes.MappingNode(u'tag:yaml.org,2002:map', value)", "loc": 10}
{"file": "ansible\\hacking\\return_skeleton_generator.py", "class_name": null, "function_name": "get_return_data", "parameters": ["key", "value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OrderedDict", "get_all_items", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_return_data(key, value):\n    # The OrderedDict here is so that, for complex objects, the\n    # summary data is at the top before the contains information\n    returns_info = {key: OrderedDict()}\n    returns_info[key]['description'] = \"FIXME *** add description for %s\" % key\n    returns_info[key]['returned'] = \"always\"\n    if isinstance(value, dict):\n        returns_info[key]['type'] = 'complex'\n        returns_info[key]['contains'] = get_all_items(value)\n    elif isinstance(value, list) and value and isinstance(value[0], dict):\n        returns_info[key]['type'] = 'complex'\n        returns_info[key]['contains'] = get_all_items(value[0])\n    else:\n        returns_info[key]['type'] = type(value).__name__\n        returns_info[key]['sample'] = value\n        # override python unicode type to set to string for docs\n        if returns_info[key]['type'] == 'unicode':\n            returns_info[key]['type'] = 'str'\n    return returns_info", "loc": 19}
{"file": "ansible\\hacking\\return_skeleton_generator.py", "class_name": null, "function_name": "get_all_items", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OrderedDict", "data.items", "get_return_data", "item.items", "sorted"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_items(data):\n    items = sorted([get_return_data(key, value) for key, value in data.items()])\n    result = OrderedDict()\n    for item in items:\n        key, value = item.items()[0]\n        result[key] = value\n    return result", "loc": 7}
{"file": "ansible\\hacking\\return_skeleton_generator.py", "class_name": null, "function_name": "main", "parameters": ["args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_all_items", "json.load", "open", "print", "yaml.representer.SafeRepresenter.add_representer", "yaml.safe_dump"], "control_structures": ["If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def main(args):\n    yaml.representer.SafeRepresenter.add_representer(OrderedDict, represent_ordereddict)\n\n    if args:\n        src = open(args[0])\n    else:\n        src = sys.stdin\n\n    data = json.load(src, strict=False)\n    docs = get_all_items(data)\n    if 'invocation' in docs:\n        del docs['invocation']\n    print(yaml.safe_dump(docs, default_flow_style=False))", "loc": 13}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": null, "function_name": "pre_build_instructions", "parameters": ["requirements"], "param_types": {"requirements": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "instructions.append", "match.group", "match.group('package').lower", "package_versions.get", "packaging.specifiers.SpecifierSet", "packaging.version.Version", "re.search", "requirements.splitlines", "specifier_set.contains"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Parse the given requirements and return any applicable pre-build instructions.", "source_code": "def pre_build_instructions(requirements: str) -> str:\n    \"\"\"Parse the given requirements and return any applicable pre-build instructions.\"\"\"\n    parsed_requirements = requirements.splitlines()\n\n    package_versions = {\n        match.group('package').lower(): match.group('version') for match\n        in (re.search('^(?P<package>.*)==(?P<version>.*)$', requirement) for requirement in parsed_requirements)\n        if match\n    }\n\n    instructions: list[str] = []\n\n    build_constraints = (\n        ('pyyaml', '>= 5.4, <= 6.0', ('Cython < 3.0',)),\n    )\n\n    for package, specifier, constraints in build_constraints:\n        version_string = package_versions.get(package)\n\n        if version_string:\n            version = packaging.version.Version(version_string)\n            specifier_set = packaging.specifiers.SpecifierSet(specifier)\n\n            if specifier_set.contains(version):\n                instructions.append(f'# pre-build requirement: {package} == {version}\\n')\n\n                for constraint in constraints:\n                    instructions.append(f'# pre-build constraint: {constraint}\\n')\n\n    return ''.join(instructions)", "loc": 30}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "argparse.ArgumentParser", "find_tests", "parser.add_argument", "parser.parse_args", "print", "set", "test.freeze_requirements", "test.update_pre_build"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main() -> None:\n    tests = find_tests()\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '--test',\n        metavar='TEST',\n        dest='test_names',\n        action='append',\n        choices=[test.name for test in tests],\n        help='test requirements to update'\n    )\n\n    parser.add_argument(\n        '--pre-build-only',\n        action='store_true',\n        help='apply pre-build instructions to existing requirements',\n    )\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n    test_names: set[str] = set(args.test_names or [])\n\n    tests = [test for test in tests if test.name in test_names] if test_names else tests\n\n    for test in tests:\n        print(f'===[ {test.name} ]===', flush=True)\n\n        if args.pre_build_only:\n            test.update_pre_build()\n        else:\n            test.freeze_requirements()", "loc": 34}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": null, "function_name": "find_tests", "parameters": [], "param_types": {}, "return_type": "t.List[SanityTest]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_tests", "pathlib.Path", "sorted", "tests.extend"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_tests() -> t.List[SanityTest]:\n    globs = (\n        'test/lib/ansible_test/_data/requirements/sanity.*.txt',\n        'test/sanity/code-smell/*.requirements.txt',\n    )\n\n    tests: t.List[SanityTest] = []\n\n    for glob in globs:\n        tests.extend(get_tests(pathlib.Path(glob)))\n\n    return sorted(tests, key=lambda test: test.name)", "loc": 12}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": null, "function_name": "get_tests", "parameters": ["glob"], "param_types": {"glob": "pathlib.Path"}, "return_type": "t.List[SanityTest]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SanityTest.create", "path.glob", "pathlib.Path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_tests(glob: pathlib.Path) -> t.List[SanityTest]:\n    path = pathlib.Path(ROOT, glob.parent)\n    pattern = glob.name\n\n    return [SanityTest.create(item) for item in path.glob(pattern)]", "loc": 5}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": "SanityTest", "function_name": "freeze_requirements", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "dict", "freeze_options.extend", "packaging.requirements.Requirement", "pathlib.Path", "re.sub", "self.source_path.read_text", "self.source_path.read_text().splitlines", "self.write_requirements", "subprocess.run", "tempfile.TemporaryDirectory", "venv.create"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def freeze_requirements(self) -> None:\n    source_requirements = [packaging.requirements.Requirement(re.sub(' #.*$', '', line)) for line in self.source_path.read_text().splitlines()]\n\n    install_packages = {requirement.name for requirement in source_requirements}\n    exclude_packages = {'distribute', 'pip', 'setuptools', 'wheel'} - install_packages\n\n    with tempfile.TemporaryDirectory() as venv_dir:\n        venv.create(venv_dir, with_pip=True)\n\n        python = pathlib.Path(venv_dir, 'bin', 'python')\n        pip = [python, '-m', 'pip', '--disable-pip-version-check']\n        env = dict()\n\n        pip_freeze = subprocess.run(pip + ['freeze'], env=env, check=True, capture_output=True, text=True)\n\n        if pip_freeze.stdout:\n            raise Exception(f'Initial virtual environment is not empty:\\n{pip_freeze.stdout}')\n\n        subprocess.run(pip + ['install', '-r', self.source_path], env=env, check=True)\n\n        freeze_options = ['--all']\n\n        for exclude_package in exclude_packages:\n            freeze_options.extend(('--exclude', exclude_package))\n\n        pip_freeze = subprocess.run(pip + ['freeze'] + freeze_options, env=env, check=True, capture_output=True, text=True)\n\n    self.write_requirements(pip_freeze.stdout)", "loc": 28}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": "SanityTest", "function_name": "update_pre_build", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "line.startswith", "pathlib.Path", "pathlib.Path(self.requirements_path).read_text", "requirements.splitlines", "self.write_requirements"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Update requirements in place with current pre-build instructions.", "source_code": "def update_pre_build(self) -> None:\n    \"\"\"Update requirements in place with current pre-build instructions.\"\"\"\n    requirements = pathlib.Path(self.requirements_path).read_text()\n    lines = requirements.splitlines(keepends=True)\n    lines = [line for line in lines if not line.startswith('#')]\n    requirements = ''.join(lines)\n\n    self.write_requirements(requirements)", "loc": 8}
{"file": "ansible\\hacking\\update-sanity-requirements.py", "class_name": "SanityTest", "function_name": "create", "parameters": ["path"], "param_types": {"path": "pathlib.Path"}, "return_type": "SanityTest", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SanityTest", "path.stem.replace", "path.stem.replace('sanity.', '').replace", "path.with_suffix"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create(path: pathlib.Path) -> SanityTest:\n    return SanityTest(\n        name=path.stem.replace('sanity.', '').replace('.requirements', ''),\n        requirements_path=path,\n        source_path=path.with_suffix('.in'),\n    )", "loc": 6}
{"file": "ansible\\hacking\\azp\\download.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["download_run", "parse_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Main program body.", "source_code": "def main():\n    \"\"\"Main program body.\"\"\"\n\n    args = parse_args()\n    download_run(args)", "loc": 5}
{"file": "ansible\\hacking\\azp\\download.py", "class_name": null, "function_name": "run_id_arg", "parameters": ["arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "m.group", "re.fullmatch"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_id_arg(arg):\n    m = re.fullmatch(r\"(?:https:\\/\\/dev\\.azure\\.com\\/ansible\\/ansible\\/_build\\/results\\?buildId=)?(\\d+)\", arg)\n    if not m:\n        raise ValueError(\"run does not seems to be a URI or an ID\")\n    return m.group(1)", "loc": 5}
{"file": "ansible\\hacking\\azp\\download.py", "class_name": null, "function_name": "parse_args", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "argcomplete.autocomplete", "argparse.ArgumentParser", "parser.add_argument", "parser.error", "parser.parse_args", "re.compile"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse and return args.", "source_code": "def parse_args():\n    \"\"\"Parse and return args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='Download results from a CI run.')\n\n    parser.add_argument('run', metavar='RUN', type=run_id_arg, help='AZP run id or URI')\n\n    parser.add_argument('-v', '--verbose',\n                        dest='verbose',\n                        action='store_true',\n                        help='show what is being downloaded')\n\n    parser.add_argument('-t', '--test',\n                        dest='test',\n                        action='store_true',\n                        help='show what would be downloaded without downloading')\n\n    parser.add_argument('-p', '--pipeline-id', type=int, default=20, help='pipeline to download the job from')\n\n    parser.add_argument('--artifacts',\n                        action='store_true',\n                        help='download artifacts')\n\n    parser.add_argument('--console-logs',\n                        action='store_true',\n                        help='download console logs')\n\n    parser.add_argument('--run-metadata',\n                        action='store_true',\n                        help='download run metadata')\n\n    parser.add_argument('--all',\n                        action='store_true',\n                        help='download everything')\n\n    parser.add_argument('--match-artifact-name',\n                        default=re.compile('.*'),\n                        type=re.compile,\n                        help='only download artifacts which names match this regex')\n\n    parser.add_argument('--match-job-name',\n                        default=re.compile('.*'),\n                        type=re.compile,\n                        help='only download artifacts from jobs which names match this regex')\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    if args.all:\n        args.artifacts = True\n        args.run_metadata = True\n        args.console_logs = True\n\n    selections = (\n        args.artifacts,\n        args.run_metadata,\n        args.console_logs\n    )\n\n    if not any(selections):\n        parser.error('At least one download option is required.')\n\n    return args", "loc": 65}
{"file": "ansible\\hacking\\azp\\download.py", "class_name": null, "function_name": "download_run", "parameters": ["args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "allow_recursive", "allowed.add", "archive.extractall", "args.match_artifact_name.match", "args.match_job_name.match", "artifact_list_response.json", "artifact_list_response.raise_for_status", "children_of.get", "io.BytesIO", "json.dumps", "log.raise_for_status", "metadata_fd.write", "open", "open(log_path, 'wb').write", "os.makedirs", "os.path.exists", "os.path.join", "parent_of.get", "path.replace", "print", "requests.get", "response.raise_for_status", "roots.add", "run_info_response.json", "run_info_response.raise_for_status", "set", "timeline_response.json", "timeline_response.raise_for_status", "zipfile.ZipFile"], "control_structures": ["For", "If", "While"], "behavior_type": ["file_io", "network_io", "serialization"], "doc_summary": "Download a run.", "source_code": "def download_run(args):\n    \"\"\"Download a run.\"\"\"\n\n    output_dir = '%s' % args.run\n\n    if not args.test and not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n\n    if args.run_metadata:\n        run_url = 'https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs/%s?api-version=6.0-preview.1' % (args.pipeline_id, args.run)\n        run_info_response = requests.get(run_url)\n        run_info_response.raise_for_status()\n        run = run_info_response.json()\n\n        path = os.path.join(output_dir, 'run.json')\n        contents = json.dumps(run, sort_keys=True, indent=4)\n\n        if args.verbose:\n            print(path)\n\n        if not args.test:\n            with open(path, 'w') as metadata_fd:\n                metadata_fd.write(contents)\n\n    timeline_response = requests.get('https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/timeline?api-version=6.0' % args.run)\n    timeline_response.raise_for_status()\n    timeline = timeline_response.json()\n    roots = set()\n    by_id = {}\n    children_of = {}\n    parent_of = {}\n    for r in timeline['records']:\n        thisId = r['id']\n        parentId = r['parentId']\n\n        by_id[thisId] = r\n\n        if parentId is None:\n            roots.add(thisId)\n        else:\n            parent_of[thisId] = parentId\n            children_of[parentId] = children_of.get(parentId, []) + [thisId]\n\n    allowed = set()\n\n    def allow_recursive(ei):\n        allowed.add(ei)\n        for ci in children_of.get(ei, []):\n            allow_recursive(ci)\n\n    for ri in roots:\n        r = by_id[ri]\n        allowed.add(ri)\n        for ci in children_of.get(r['id'], []):\n            c = by_id[ci]\n            if not args.match_job_name.match(\"%s %s\" % (r['name'], c['name'])):\n                continue\n            allow_recursive(c['id'])\n\n    if args.artifacts:\n        artifact_list_url = 'https://dev.azure.com/ansible/ansible/_apis/build/builds/%s/artifacts?api-version=6.0' % args.run\n        artifact_list_response = requests.get(artifact_list_url)\n        artifact_list_response.raise_for_status()\n        for artifact in artifact_list_response.json()['value']:\n            if artifact['source'] not in allowed or not args.match_artifact_name.match(artifact['name']):\n                continue\n            if args.verbose:\n                print('%s/%s' % (output_dir, artifact['name']))\n            if not args.test:\n                response = requests.get(artifact['resource']['downloadUrl'])\n                response.raise_for_status()\n                archive = zipfile.ZipFile(io.BytesIO(response.content))\n                archive.extractall(path=output_dir)\n\n    if args.console_logs:\n        for r in timeline['records']:\n            if not r['log'] or r['id'] not in allowed or not args.match_artifact_name.match(r['name']):\n                continue\n            names = []\n            parent_id = r['id']\n            while parent_id is not None:\n                p = by_id[parent_id]\n                name = p['name']\n                if name not in names:\n                    names = [name] + names\n                parent_id = parent_of.get(p['id'], None)\n\n            path = \" \".join(names)\n\n            # Some job names have the separator in them.\n            path = path.replace(os.sep, '_')\n\n            log_path = os.path.join(output_dir, '%s.log' % path)\n            if args.verbose:\n                print(log_path)\n            if not args.test:\n                log = requests.get(r['log']['url'])\n                log.raise_for_status()\n                open(log_path, 'wb').write(log.content)", "loc": 99}
{"file": "ansible\\hacking\\azp\\get_recent_coverage_runs.py", "class_name": null, "function_name": "pretty_coverage_runs", "parameters": ["runs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ended.append", "in_progress.append", "print", "run.get", "sorted", "stringc"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pretty_coverage_runs(runs):\n    ended = []\n    in_progress = []\n    for run in runs:\n        if run.get('finishedDate'):\n            ended.append(run)\n        else:\n            in_progress.append(run)\n\n    for run in sorted(ended, key=lambda x: x['finishedDate']):\n        if run['result'] == \"succeeded\":\n            print('🙂 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s (%s)' % (\n                stringc('PASS', 'green'),\n                run['id'],\n                run['finishedDate']))\n        else:\n            print('😢 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s (%s)' % (\n                stringc('FAIL', 'red'),\n                run['id'],\n                run['finishedDate']))\n\n    if in_progress:\n        print('The following runs are ongoing:')\n        for run in in_progress:\n            print('🤔 [%s] https://dev.azure.com/ansible/ansible/_build/results?buildId=%s' % (\n                stringc('FATE', 'yellow'),\n                run['id']))", "loc": 27}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["incidental_report", "parse_args", "sys.exit"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Main program body.", "source_code": "def main():\n    \"\"\"Main program body.\"\"\"\n    args = parse_args()\n\n    try:\n        incidental_report(args)\n    except ApplicationError as ex:\n        sys.exit(ex)", "loc": 8}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": null, "function_name": "regex", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argparse.ArgumentTypeError", "re.compile"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def regex(value):\n    try:\n        return re.compile(value)\n    except Exception as ex:\n        raise argparse.ArgumentTypeError('\"%s\" is not a valid regex: %s' % (value, ex))", "loc": 5}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": null, "function_name": "collect_sources", "parameters": ["data_path", "git", "coverage_data", "result_sha"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SourceFile", "data.values", "git.show", "json.load", "open", "path_coverage.items", "sources.append"], "control_structures": ["For"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def collect_sources(data_path, git, coverage_data, result_sha):\n    with open(data_path) as data_file:\n        data = json.load(data_file)\n\n    sources = []\n\n    for path_coverage in data.values():\n        for path, path_data in path_coverage.items():\n            sources.append(SourceFile(path, git.show(['%s:%s' % (result_sha, path)]), coverage_data, path_data))\n\n    return sources", "loc": 11}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": null, "function_name": "check_failed", "parameters": ["args", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ApplicationError", "sys.stderr.write"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_failed(args, message):\n    if args.skip_checks:\n        sys.stderr.write('WARNING: %s\\n' % message)\n        return\n\n    raise ApplicationError(message)", "loc": 6}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": "CoverageTool", "function_name": "filter", "parameters": ["self", "input_path", "output_path", "include_targets", "exclude_targets", "include_path", "exclude_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["args.extend", "subprocess.check_call"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def filter(self, input_path, output_path, include_targets=None, exclude_targets=None, include_path=None, exclude_path=None):\n    args = []\n\n    if include_targets:\n        for target in include_targets:\n            args.extend(['--include-target', target])\n\n    if exclude_targets:\n        for target in exclude_targets:\n            args.extend(['--exclude-target', target])\n\n    if include_path:\n        args.extend(['--include-path', include_path])\n\n    if exclude_path:\n        args.extend(['--exclude-path', exclude_path])\n\n    subprocess.check_call(self.analyze_cmd + ['filter', input_path, output_path] + args)", "loc": 18}
{"file": "ansible\\hacking\\azp\\incidental.py", "class_name": "CoverageTool", "function_name": "missing", "parameters": ["self", "from_path", "to_path", "output_path", "only_gaps"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["args.append", "subprocess.check_call"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def missing(self, from_path, to_path, output_path, only_gaps=False):\n    args = []\n\n    if only_gaps:\n        args.append('--only-gaps')\n\n    subprocess.check_call(self.analyze_cmd + ['missing', from_path, to_path, output_path] + args)", "loc": 7}
{"file": "ansible\\hacking\\azp\\run.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.environ.get", "parse_args", "start_run", "sys.exit", "sys.stderr.write"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Main program body.", "source_code": "def main():\n    \"\"\"Main program body.\"\"\"\n\n    args = parse_args()\n\n    key = os.environ.get('AZP_TOKEN', None)\n    if not key:\n        sys.stderr.write(\"please set you AZP token in AZP_TOKEN\")\n        sys.exit(1)\n\n    start_run(args, key)", "loc": 11}
{"file": "ansible\\hacking\\azp\\run.py", "class_name": null, "function_name": "parse_args", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "argparse.ArgumentParser", "parser.add_argument", "parser.parse_args"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse and return args.", "source_code": "def parse_args():\n    \"\"\"Parse and return args.\"\"\"\n\n    parser = argparse.ArgumentParser(description='Start a new CI run.')\n\n    parser.add_argument('-p', '--pipeline-id', type=int, default=20, help='pipeline to download the job from')\n    parser.add_argument('--ref', help='git ref name to run on')\n\n    parser.add_argument('--env',\n                        nargs=2,\n                        metavar=('KEY', 'VALUE'),\n                        action='append',\n                        help='environment variable to pass')\n\n    if argcomplete:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n\n    return args", "loc": 20}
{"file": "ansible\\hacking\\azp\\run.py", "class_name": null, "function_name": "start_run", "parameters": ["args", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "print", "requests.auth.HTTPBasicAuth", "requests.post", "resp.json", "resp.raise_for_status"], "control_structures": [], "behavior_type": ["network_io", "serialization"], "doc_summary": "Start a new CI run.", "source_code": "def start_run(args, key):\n    \"\"\"Start a new CI run.\"\"\"\n\n    url = \"https://dev.azure.com/ansible/ansible/_apis/pipelines/%s/runs?api-version=6.0-preview.1\" % args.pipeline_id\n    payload = {\"resources\": {\"repositories\": {\"self\": {\"refName\": args.ref}}}}\n\n    resp = requests.post(url, auth=requests.auth.HTTPBasicAuth('user', key), data=payload)\n    resp.raise_for_status()\n\n    print(json.dumps(resp.json(), indent=4, sort_keys=True))", "loc": 10}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "url_to_org_repo", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}/{1}'.format", "PULL_HTTP_URL_RE.match", "match.group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given a full Github PR URL, extract the user/org and repo name. Return them in the form: \"user/repo\"", "source_code": "def url_to_org_repo(url):\n    \"\"\"\n    Given a full Github PR URL, extract the user/org and repo name.\n    Return them in the form: \"user/repo\"\n    \"\"\"\n    match = PULL_HTTP_URL_RE.match(url)\n    if not match:\n        return ''\n    return '{0}/{1}'.format(match.group('user'), match.group('repo'))", "loc": 9}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "generate_new_body", "parameters": ["pr", "source_pr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "'\\nBackport of {0}\\n'.format", "Exception", "line.startswith", "line.strip", "line.strip().endswith", "new_body_lines.append", "pr.body.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Given the new PR (the backport) and the originating (source) PR, construct the new body for the backport PR. If the backport follows the usual ansible/ansible template, we look for the", "source_code": "def generate_new_body(pr, source_pr):\n    \"\"\"\n    Given the new PR (the backport) and the originating (source) PR,\n    construct the new body for the backport PR.\n\n    If the backport follows the usual ansible/ansible template, we look for the\n    '##### SUMMARY'-type line and add our \"Backport of\" line right below that.\n\n    If we can't find the SUMMARY line, we add our line at the very bottom.\n\n    This function does not side-effect, it simply returns the new body as a\n    string.\n    \"\"\"\n    backport_text = '\\nBackport of {0}\\n'.format(source_pr)\n    body_lines = pr.body.split('\\n')\n    new_body_lines = []\n\n    added = False\n    for line in body_lines:\n        if 'Backport of http' in line:\n            raise Exception('Already has a backport line, aborting.')\n        new_body_lines.append(line)\n        if line.startswith('#') and line.strip().endswith('SUMMARY'):\n            # This would be a fine place to add it\n            new_body_lines.append(backport_text)\n            added = True\n    if not added:\n        # Otherwise, no '#### SUMMARY' line, so just add it at the bottom\n        new_body_lines.append(backport_text)\n\n    return '\\n'.join(new_body_lines)", "loc": 31}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "get_prs_for_commit", "parameters": ["g", "commit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'hash:{0} org:ansible org:ansible-collections is:public'.format", "commits[0].get_pulls", "commits[0].get_pulls().get_page", "g.search_commits", "g.search_commits('hash:{0} org:ansible org:ansible-collections is:public'.format(commit)).get_page", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given a commit hash, attempt to find the hash in any repo in the ansible orgs, and then use it to determine what, if any, PR it appeared in.", "source_code": "def get_prs_for_commit(g, commit):\n    \"\"\"\n    Given a commit hash, attempt to find the hash in any repo in the\n    ansible orgs, and then use it to determine what, if any, PR it appeared in.\n    \"\"\"\n\n    commits = g.search_commits(\n        'hash:{0} org:ansible org:ansible-collections is:public'.format(commit)\n    ).get_page(0)\n    if not commits or len(commits) == 0:\n        return []\n    pulls = commits[0].get_pulls().get_page(0)\n    if not pulls or len(pulls) == 0:\n        return []\n    return pulls", "loc": 15}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "search_backport", "parameters": ["pr", "g", "ansible_ansible"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}/{1}'.format", "PULL_BACKPORT_IN_TITLE.match", "PULL_CHERRY_PICKED_FROM.match", "PULL_HTTP_URL_RE.findall", "PULL_URL_RE.findall", "TICKET_NUMBER.findall", "ansible_ansible.get_pull", "cherrypick.group", "g.get_repo", "get_prs_for_commit", "int", "possibilities.append", "possibilities.extend", "pr.body.split", "repo.get_pull", "tickets.extend", "title_search.group"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Do magic. This is basically the \"brain\" of 'auto'. It will search the PR (the newest PR - the backport) and try to find where it originated.", "source_code": "def search_backport(pr, g, ansible_ansible):\n    \"\"\"\n    Do magic. This is basically the \"brain\" of 'auto'.\n    It will search the PR (the newest PR - the backport) and try to find where\n    it originated.\n\n    First it will search in the title. Some titles include things like\n    \"foo bar change (#12345)\" or \"foo bar change (backport of #54321)\"\n    so we search for those and pull them out.\n\n    Next it will scan the body of the PR and look for:\n      - cherry-pick reference lines (e.g. \"cherry-picked from commit XXXXX\")\n      - other PRs (#nnnnnn) and (foo/bar#nnnnnnn)\n      - full URLs to other PRs\n\n    It will take all of the above, and return a list of \"possibilities\",\n    which is a list of PullRequest objects.\n    \"\"\"\n\n    possibilities = []\n\n    # 1. Try searching for it in the title.\n    title_search = PULL_BACKPORT_IN_TITLE.match(pr.title)\n    if title_search:\n        ticket = title_search.group('ticket1')\n        if not ticket:\n            ticket = title_search.group('ticket2')\n        try:\n            possibilities.append(ansible_ansible.get_pull(int(ticket)))\n        except Exception:\n            pass\n\n    # 2. Search for clues in the body of the PR\n    body_lines = pr.body.split('\\n')\n    for line in body_lines:\n        # a. Try searching for a `git cherry-pick` line\n        cherrypick = PULL_CHERRY_PICKED_FROM.match(line)\n        if cherrypick:\n            prs = get_prs_for_commit(g, cherrypick.group('hash'))\n            possibilities.extend(prs)\n            continue\n\n        # b. Try searching for other referenced PRs (by #nnnnn or full URL)\n        tickets = [('ansible', 'ansible', ticket) for ticket in TICKET_NUMBER.findall(line)]\n        tickets.extend(PULL_HTTP_URL_RE.findall(line))\n        tickets.extend(PULL_URL_RE.findall(line))\n        if tickets:\n            for ticket in tickets:\n                # Is it a PR (even if not in ansible/ansible)?\n                # TODO: As a small optimization/to avoid extra calls to GitHub,\n                # we could limit this check to non-URL matches. If it's a URL,\n                # we know it's definitely a pull request.\n                try:\n                    repo_path = '{0}/{1}'.format(ticket[0], ticket[1])\n                    repo = ansible_ansible\n                    if repo_path != 'ansible/ansible':\n                        repo = g.get_repo(repo_path)\n                    ticket_pr = repo.get_pull(int(ticket))\n                    possibilities.append(ticket_pr)\n                except Exception:\n                    pass\n            continue  # Future-proofing\n\n    return possibilities", "loc": 64}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "prompt_add", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["input", "res.lower"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Prompt the user and return whether or not they agree.", "source_code": "def prompt_add():\n    \"\"\"\n    Prompt the user and return whether or not they agree.\n    \"\"\"\n    res = input('Shall I add the reference? [Y/n]: ')\n    return res.lower() in ('', 'y', 'yes')", "loc": 6}
{"file": "ansible\\hacking\\backport\\backport_of_line_adder.py", "class_name": null, "function_name": "commit_edit", "parameters": ["new_pr", "pr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["generate_new_body", "new_pr.edit", "print", "prompt_add"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given the new PR (the backport), and the \"possibility\" that we have decided on, prompt the user and then add the reference to the body of the new PR. This method does the actual \"destructive\" work of editing the PR body.", "source_code": "def commit_edit(new_pr, pr):\n    \"\"\"\n    Given the new PR (the backport), and the \"possibility\" that we have decided\n    on, prompt the user and then add the reference to the body of the new PR.\n\n    This method does the actual \"destructive\" work of editing the PR body.\n    \"\"\"\n    print('I think this PR might have come from:')\n    print(pr.title)\n    print('-' * 50)\n    print(pr.html_url)\n    if prompt_add():\n        new_body = generate_new_body(new_pr, pr.html_url)\n        new_pr.edit(body=new_body)\n        print('I probably added the reference successfully.')", "loc": 15}
{"file": "ansible\\lib\\ansible\\context.py", "class_name": null, "function_name": "cliargs_deferred_get", "parameters": ["key", "default", "shallowcopy"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CLIARGS.get", "is_sequence", "isinstance", "value.copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Closure over getting a key from CLIARGS with shallow copy functionality Primarily used in ``FieldAttribute`` where we need to defer setting the default until after the CLI arguments have been parsed", "source_code": "def cliargs_deferred_get(key, default=None, shallowcopy=False):\n    \"\"\"Closure over getting a key from CLIARGS with shallow copy functionality\n\n    Primarily used in ``FieldAttribute`` where we need to defer setting the default\n    until after the CLI arguments have been parsed\n\n    This function is not directly bound to ``CliArgs`` so that it works with\n    ``CLIARGS`` being replaced\n    \"\"\"\n    def inner():\n        value = CLIARGS.get(key, default=default)\n        if not shallowcopy:\n            return value\n        elif is_sequence(value):\n            return value[:]\n        elif isinstance(value, (Mapping, Set)):\n            return value.copy()\n        return value\n    return inner", "loc": 19}
{"file": "ansible\\lib\\ansible\\context.py", "class_name": null, "function_name": "inner", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CLIARGS.get", "is_sequence", "isinstance", "value.copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner():\n    value = CLIARGS.get(key, default=default)\n    if not shallowcopy:\n        return value\n    elif is_sequence(value):\n        return value[:]\n    elif isinstance(value, (Mapping, Set)):\n        return value.copy()\n    return value", "loc": 9}
{"file": "ansible\\lib\\ansible\\__main__.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_short_name", "argparse.ArgumentParser", "distribution", "ep_map[args.entry_point].load", "list", "main", "parser.add_argument", "parser.parse_known_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    dist = distribution('ansible-core')\n    ep_map = {_short_name(ep.name): ep for ep in dist.entry_points if ep.group == 'console_scripts'}\n\n    parser = argparse.ArgumentParser(prog='python -m ansible', add_help=False)\n    parser.add_argument('entry_point', choices=list(ep_map))\n    args, extra = parser.parse_known_args()\n\n    main = ep_map[args.entry_point].load()\n    main([args.entry_point] + extra)", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\adhoc.py", "class_name": "AdHocCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt_help.add_async_options", "opt_help.add_basedir_options", "opt_help.add_check_options", "opt_help.add_connect_options", "opt_help.add_fork_options", "opt_help.add_inventory_options", "opt_help.add_module_options", "opt_help.add_output_options", "opt_help.add_runas_options", "opt_help.add_runtask_options", "opt_help.add_tasknoplay_options", "opt_help.add_vault_options", "self.parser.add_argument", "super", "super(AdHocCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "create an options parser for bin/ansible", "source_code": "def init_parser(self):\n    \"\"\" create an options parser for bin/ansible \"\"\"\n    super(AdHocCLI, self).init_parser(desc=\"Define and run a single task 'playbook' against a set of hosts\",\n                                      epilog=\"Some actions do not make sense in Ad-Hoc (include, meta, etc)\")\n\n    opt_help.add_runas_options(self.parser)\n    opt_help.add_inventory_options(self.parser)\n    opt_help.add_async_options(self.parser)\n    opt_help.add_output_options(self.parser)\n    opt_help.add_connect_options(self.parser)\n    opt_help.add_check_options(self.parser)\n    opt_help.add_runtask_options(self.parser)\n    opt_help.add_vault_options(self.parser)\n    opt_help.add_fork_options(self.parser)\n    opt_help.add_module_options(self.parser)\n    opt_help.add_basedir_options(self.parser)\n    opt_help.add_tasknoplay_options(self.parser)\n\n    # options unique to ansible ad-hoc\n    self.parser.add_argument('-a', '--args', dest='module_args',\n                             help=\"The action's options in space separated k=v format: -a 'opt1=val1 opt2=val2' \"\n                                  \"or a json string: -a '{\\\"opt1\\\": \\\"val1\\\", \\\"opt2\\\": \\\"val2\\\"}'\",\n                             default=C.DEFAULT_MODULE_ARGS)\n    self.parser.add_argument('-m', '--module-name', dest='module_name',\n                             help=\"Name of the action to execute (default=%s)\" % C.DEFAULT_MODULE_NAME,\n                             default=C.DEFAULT_MODULE_NAME)\n    self.parser.add_argument('args', metavar='pattern', help='host pattern')", "loc": 27}
{"file": "ansible\\lib\\ansible\\cli\\adhoc.py", "class_name": "AdHocCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.validate_conflicts", "super", "super(AdHocCLI, self).post_process_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Post process and validate options for bin/ansible", "source_code": "def post_process_args(self, options):\n    \"\"\"Post process and validate options for bin/ansible \"\"\"\n\n    options = super(AdHocCLI, self).post_process_args(options)\n\n    display.verbosity = options.verbosity\n    self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n\n    return options", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\adhoc.py", "class_name": "AdHocCLI", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "C.CALLBACKS_ENABLED.append", "Play", "Play().load", "Playbook", "TaskQueueManager", "display.display", "display.warning", "len", "loader.cleanup_all_tmp_files", "pattern.endswith", "playbook._entries.append", "self._play_ds", "self._play_prereqs", "self._tqm.cleanup", "self._tqm.load_callbacks", "self._tqm.run", "self._tqm.send_callback", "self.ask_passwords", "self.get_host_list", "super", "super(AdHocCLI, self).run", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "create and execute the single task playbook", "source_code": "def run(self):\n    \"\"\" create and execute the single task playbook \"\"\"\n\n    super(AdHocCLI, self).run()\n\n    # only thing left should be host pattern\n    pattern = to_text(context.CLIARGS['args'], errors='surrogate_or_strict')\n\n    # handle password prompts\n    sshpass = None\n    becomepass = None\n\n    (sshpass, becomepass) = self.ask_passwords()\n    passwords = {'conn_pass': sshpass, 'become_pass': becomepass}\n\n    # get basic objects\n    loader, inventory, variable_manager = self._play_prereqs()\n\n    # get list of hosts to execute against\n    try:\n        hosts = self.get_host_list(inventory, context.CLIARGS['subset'], pattern)\n    except AnsibleError:\n        if context.CLIARGS['subset']:\n            raise\n        else:\n            hosts = []\n            display.warning(\"No hosts matched, nothing to do\")\n\n    # just listing hosts?\n    if context.CLIARGS['listhosts']:\n        display.display('  hosts (%d):' % len(hosts))\n        for host in hosts:\n            display.display('    %s' % host)\n        return 0\n\n    # verify we have arguments if we know we need em\n    if context.CLIARGS['module_name'] in C.MODULE_REQUIRE_ARGS and not context.CLIARGS['module_args']:\n        err = \"No argument passed to %s module\" % context.CLIARGS['module_name']\n        if pattern.endswith(\".yml\"):\n            err = err + ' (did you mean to run ansible-playbook?)'\n        raise AnsibleOptionsError(err)\n\n    # Avoid modules that don't work with ad-hoc\n    if context.CLIARGS['module_name'] in C._ACTION_IMPORT_PLAYBOOK:\n        raise AnsibleOptionsError(\"'%s' is not a valid action for ad-hoc commands\"\n                                  % context.CLIARGS['module_name'])\n\n    # construct playbook objects to wrap task\n    play_ds = self._play_ds(pattern, context.CLIARGS['seconds'], context.CLIARGS['poll_interval'])\n    play = Play().load(play_ds, variable_manager=variable_manager, loader=loader)\n\n    # used in start callback\n    playbook = Playbook(loader)\n    playbook._entries.append(play)\n    playbook._file_name = '__adhoc_playbook__'\n\n    if self.callback:\n        cb = self.callback\n    elif context.CLIARGS['one_line']:\n        cb = 'oneline'\n    # Respect custom 'stdout_callback' only with enabled 'bin_ansible_callbacks'\n    elif C.DEFAULT_LOAD_CALLBACK_PLUGINS and C.DEFAULT_STDOUT_CALLBACK != 'default':\n        cb = C.DEFAULT_STDOUT_CALLBACK\n    else:\n        cb = 'minimal'\n\n    run_tree = False\n    if context.CLIARGS['tree']:\n        C.CALLBACKS_ENABLED.append('tree')\n        C.TREE_DIR = context.CLIARGS['tree']\n        run_tree = True\n\n    # now create a task queue manager to execute the play\n    self._tqm = None\n    try:\n        self._tqm = TaskQueueManager(\n            inventory=inventory,\n            variable_manager=variable_manager,\n            loader=loader,\n            passwords=passwords,\n            stdout_callback_name=cb,\n            run_additional_callbacks=C.DEFAULT_LOAD_CALLBACK_PLUGINS,\n            run_tree=run_tree,\n            forks=context.CLIARGS['forks'],\n        )\n\n        self._tqm.load_callbacks()\n        self._tqm.send_callback('v2_playbook_on_start', playbook)\n\n        result = self._tqm.run(play)\n\n        self._tqm.send_callback('v2_playbook_on_stats', self._tqm._stats)\n    finally:\n        if self._tqm:\n            self._tqm.cleanup()\n        if loader:\n            loader.cleanup_all_tmp_files()\n\n    return result", "loc": 99}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": null, "function_name": "get_constants", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dir", "getattr", "hasattr", "k.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "helper method to ensure we can template based on existing constants", "source_code": "def get_constants():\n    \"\"\" helper method to ensure we can template based on existing constants \"\"\"\n    if not hasattr(get_constants, 'cvars'):\n        get_constants.cvars = {k: getattr(C, k) for k in dir(C) if not k.startswith('__')}\n    return get_constants.cvars", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["common.add_argument", "dump_parser.add_argument", "dump_parser.set_defaults", "init_parser.add_argument", "init_parser.set_defaults", "list", "list_parser.add_argument", "list_parser.set_defaults", "opt_help.ArgumentParser", "opt_help.add_verbosity_options", "self.parser.add_subparsers", "subparsers.add_parser", "super", "super(ConfigCLI, self).init_parser", "validate_parser.add_argument", "validate_parser.set_defaults", "view_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def init_parser(self):\n\n    super(ConfigCLI, self).init_parser(\n        desc=\"View ansible configuration.\",\n    )\n\n    common = opt_help.ArgumentParser(add_help=False)\n    opt_help.add_verbosity_options(common)\n    common.add_argument('-c', '--config', dest='config_file',\n                        help=\"path to configuration file, defaults to first file found in precedence.\")\n    common.add_argument(\"-t\", \"--type\", action=\"store\", default='base', dest='type', choices=['all', 'base'] + list(C.CONFIGURABLE_PLUGINS),\n                        help=\"Filter down to a specific plugin type.\")\n    common.add_argument('args', help='Specific plugin to target, requires type of plugin to be set', nargs='*')\n\n    subparsers = self.parser.add_subparsers(dest='action')\n    subparsers.required = True\n\n    list_parser = subparsers.add_parser('list', help='Print all config options', parents=[common])\n    list_parser.set_defaults(func=self.execute_list)\n    list_parser.add_argument('--format', '-f', dest='format', action='store', choices=['json', 'yaml'], default='yaml',\n                             help='Output format for list')\n\n    dump_parser = subparsers.add_parser('dump', help='Dump configuration', parents=[common])\n    dump_parser.set_defaults(func=self.execute_dump)\n    dump_parser.add_argument('--only-changed', '--changed-only', dest='only_changed', action='store_true',\n                             help=\"Only show configurations that have changed from the default\")\n    dump_parser.add_argument('--format', '-f', dest='format', action='store', choices=['json', 'yaml', 'display'], default='display',\n                             help='Output format for dump')\n\n    view_parser = subparsers.add_parser('view', help='View configuration file', parents=[common])\n    view_parser.set_defaults(func=self.execute_view)\n\n    init_parser = subparsers.add_parser('init', help='Create initial configuration', parents=[common])\n    init_parser.set_defaults(func=self.execute_init)\n    init_parser.add_argument('--format', '-f', dest='format', action='store', choices=['ini', 'env', 'vars'], default='ini',\n                             help='Output format for init')\n    init_parser.add_argument('--disabled', dest='commented', action='store_true', default=False,\n                             help='Prefixes all entries with a comment character to disable them')\n\n    validate_parser = subparsers.add_parser('validate',\n                                            help='Validate the configuration file and environment variables. '\n                                                 'By default it only checks the base settings without accounting for plugins (see -t).',\n                                            parents=[common])\n    validate_parser.set_defaults(func=self.execute_validate)\n    validate_parser.add_argument('--format', '-f', dest='format', action='store', choices=['ini', 'env'] , default='ini',\n                                 help='Output format for init')", "loc": 46}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super(ConfigCLI, self).post_process_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(ConfigCLI, self).post_process_args(options)\n    display.verbosity = options.verbosity\n\n    return options", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "execute_list", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_json.json_dumps_formatted", "self._list_entries_from_args", "self.pager", "to_text", "yaml_dump"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "list and output available configs", "source_code": "def execute_list(self):\n    \"\"\"\n    list and output available configs\n    \"\"\"\n\n    config_entries = self._list_entries_from_args()\n    if context.CLIARGS['format'] == 'yaml':\n        output = yaml_dump(config_entries)\n    elif context.CLIARGS['format'] == 'json':\n        output = _json.json_dumps_formatted(config_entries)\n\n    self.pager(to_text(output, errors='surrogate_or_strict'))", "loc": 12}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "execute_init", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "config_entries.pop", "data.append", "data.extend", "plugin_types[ptype].keys", "sections.keys", "sections[s].extend", "self._get_settings_ini", "self._get_settings_vars", "self._list_entries_from_args", "self.pager", "to_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Create initial configuration", "source_code": "def execute_init(self):\n    \"\"\"Create initial configuration\"\"\"\n\n    seen = {}\n    data = []\n    config_entries = self._list_entries_from_args()\n    plugin_types = config_entries.pop('PLUGINS', None)\n\n    if context.CLIARGS['format'] == 'ini':\n        sections = self._get_settings_ini(config_entries, seen)\n\n        if plugin_types:\n            for ptype in plugin_types:\n                plugin_sections = self._get_settings_ini(plugin_types[ptype], seen)\n                for s in plugin_sections:\n                    if s in sections:\n                        sections[s].extend(plugin_sections[s])\n                    else:\n                        sections[s] = plugin_sections[s]\n\n        if sections:\n            for section in sections.keys():\n                data.append('[%s]' % section)\n                for key in sections[section]:\n                    data.append(key)\n                    data.append('')\n                data.append('')\n\n    elif context.CLIARGS['format'] in ('env', 'vars'):  # TODO: add yaml once that config option is added\n        data = self._get_settings_vars(config_entries, context.CLIARGS['format'])\n        if plugin_types:\n            for ptype in plugin_types:\n                for plugin in plugin_types[ptype].keys():\n                    data.extend(self._get_settings_vars(plugin_types[ptype][plugin], context.CLIARGS['format']))\n\n    self.pager(to_text('\\n'.join(data), errors='surrogate_or_strict'))", "loc": 36}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "execute_dump", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "_json.json_dumps_formatted", "len", "list", "output.append", "output.extend", "ptype.upper", "self._get_galaxy_server_configs", "self._get_global_configs", "self._get_plugin_configs", "self.pager", "server_config.keys", "server_config.pop", "server_reduced_config.values", "to_text", "yaml_dump"], "control_structures": ["For", "If"], "behavior_type": ["serialization"], "doc_summary": "Shows the current settings, merges ansible.cfg if specified", "source_code": "def execute_dump(self):\n    \"\"\"\n    Shows the current settings, merges ansible.cfg if specified\n    \"\"\"\n    output = []\n    if context.CLIARGS['type'] in ('base', 'all'):\n        # deal with base\n        output = self._get_global_configs()\n\n        # add galaxy servers\n        server_config_list = self._get_galaxy_server_configs()\n        if context.CLIARGS['format'] == 'display':\n            output.append('\\nGALAXY_SERVERS:\\n')\n            output.extend(server_config_list)\n        else:\n            configs = {}\n            for server_config in server_config_list:\n                server = list(server_config.keys())[0]\n                server_reduced_config = server_config.pop(server)\n                configs[server] = list(server_reduced_config.values())\n            output.append({'GALAXY_SERVERS': configs})\n\n    if context.CLIARGS['type'] == 'all':\n        # add all plugins\n        for ptype in C.CONFIGURABLE_PLUGINS:\n            plugin_list = self._get_plugin_configs(ptype, context.CLIARGS['args'])\n            if context.CLIARGS['format'] == 'display':\n                if not context.CLIARGS['only_changed'] or plugin_list:\n                    output.append('\\n%s:\\n%s' % (ptype.upper(), '=' * len(ptype)))\n                    output.extend(plugin_list)\n            else:\n                if ptype in ('modules', 'doc_fragments'):\n                    pname = ptype.upper()\n                else:\n                    pname = '%s_PLUGINS' % ptype.upper()\n                output.append({pname: plugin_list})\n\n    elif context.CLIARGS['type'] != 'base':\n        # deal with specific plugin\n        output = self._get_plugin_configs(context.CLIARGS['type'], context.CLIARGS['args'])\n\n    if context.CLIARGS['format'] == 'display':\n        text = '\\n'.join(output)\n    if context.CLIARGS['format'] == 'yaml':\n        text = yaml_dump(output)\n    elif context.CLIARGS['format'] == 'json':\n        text = _json.json_dumps_formatted(output)\n\n    self.pager(to_text(text, errors='surrogate_or_strict'))", "loc": 49}
{"file": "ansible\\lib\\ansible\\cli\\config.py", "class_name": "ConfigCLI", "function_name": "execute_validate", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_ansible_env_vars", "_get_evar_list", "_get_ini_entries", "config_entries.pop", "data.extend", "display.display", "display.error", "os.environ.keys", "p.options", "p.sections", "plugin_types[ptype].keys", "sections[s].update", "self._list_entries_from_args", "sys.exit"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def execute_validate(self):\n\n    found = False\n    config_entries = self._list_entries_from_args()\n    plugin_types = config_entries.pop('PLUGINS', None)\n    galaxy_servers = config_entries.pop('GALAXY_SERVERS', None)\n\n    if context.CLIARGS['format'] == 'ini':\n        if C.CONFIG_FILE is not None:\n            # validate ini config since it is found\n\n            sections = _get_ini_entries(config_entries)\n            # Also from plugins\n            if plugin_types:\n                for ptype in plugin_types:\n                    for plugin in plugin_types[ptype].keys():\n                        plugin_sections = _get_ini_entries(plugin_types[ptype][plugin])\n                        for s in plugin_sections:\n                            if s in sections:\n                                sections[s].update(plugin_sections[s])\n                            else:\n                                sections[s] = plugin_sections[s]\n            if galaxy_servers:\n                for server in galaxy_servers:\n                    server_sections = _get_ini_entries(galaxy_servers[server])\n                    for s in server_sections:\n                        if s in sections:\n                            sections[s].update(server_sections[s])\n                        else:\n                            sections[s] = server_sections[s]\n            if sections:\n                p = C.config._parsers[C.CONFIG_FILE]\n                for s in p.sections():\n                    # check for valid sections\n                    if s not in sections:\n                        display.error(f\"Found unknown section '{s}' in '{C.CONFIG_FILE}.\")\n                        found = True\n                        continue\n\n                    # check keys in valid sections\n                    for k in p.options(s):\n                        if k not in sections[s]:\n                            display.error(f\"Found unknown key '{k}' in section '{s}' in '{C.CONFIG_FILE}.\")\n                            found = True\n\n    elif context.CLIARGS['format'] == 'env':\n        # validate any 'ANSIBLE_' env vars found\n        evars = [varname for varname in os.environ.keys() if _ansible_env_vars(varname)]\n        if evars:\n            data = _get_evar_list(config_entries)\n            if plugin_types:\n                for ptype in plugin_types:\n                    for plugin in plugin_types[ptype].keys():\n                        data.extend(_get_evar_list(plugin_types[ptype][plugin]))\n\n            for evar in evars:\n                if evar not in data:\n                    display.error(f\"Found unknown environment variable '{evar}'.\")\n                    found = True\n\n    # we found discrepancies!\n    if found:\n        sys.exit(1)\n\n    # allsgood\n    display.display(\"All configurations seem valid!\")", "loc": 66}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt_help.add_basedir_options", "opt_help.add_check_options", "opt_help.add_connect_options", "opt_help.add_fork_options", "opt_help.add_inventory_options", "opt_help.add_module_options", "opt_help.add_runas_options", "opt_help.add_runtask_options", "opt_help.add_tasknoplay_options", "opt_help.add_vault_options", "self.parser.add_argument", "super", "super(ConsoleCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def init_parser(self):\n    super(ConsoleCLI, self).init_parser(\n        desc=\"REPL console for executing Ansible tasks.\",\n        epilog=\"This is not a live session/connection: each task is executed in the background and returns its results.\"\n    )\n    opt_help.add_runas_options(self.parser)\n    opt_help.add_inventory_options(self.parser)\n    opt_help.add_connect_options(self.parser)\n    opt_help.add_check_options(self.parser)\n    opt_help.add_vault_options(self.parser)\n    opt_help.add_fork_options(self.parser)\n    opt_help.add_module_options(self.parser)\n    opt_help.add_basedir_options(self.parser)\n    opt_help.add_runtask_options(self.parser)\n    opt_help.add_tasknoplay_options(self.parser)\n\n    # options unique to shell\n    self.parser.add_argument('pattern', help='host pattern', metavar='pattern', default='all', nargs='?')\n    self.parser.add_argument('--step', dest='step', action='store_true',\n                             help=\"one-step-at-a-time: confirm each task before running\")", "loc": 20}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.validate_conflicts", "super", "super(ConsoleCLI, self).post_process_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(ConsoleCLI, self).post_process_args(options)\n    display.verbosity = options.verbosity\n    self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n    return options", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "cmdloop", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.Cmd.cmdloop", "self.cmdloop", "self.display", "self.do_exit"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cmdloop(self):\n    try:\n        cmd.Cmd.cmdloop(self)\n\n    except KeyboardInterrupt:\n        self.cmdloop()\n\n    except EOFError:\n        self.display(\"[Ansible-console was exited]\")\n        self.do_exit(self)", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "set_prompt", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getpass.getuser", "len", "self.inventory.list_hosts", "stringc"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_prompt(self):\n    login_user = self.remote_user or getpass.getuser()\n    self.selected = self.inventory.list_hosts(self.cwd)\n    prompt = \"%s@%s (%d)[f:%s]\" % (login_user, self.cwd, len(self.selected), self.forks)\n    if self.become and self.become_user in [None, 'root']:\n        prompt += \"# \"\n        color = C.COLOR_ERROR\n    else:\n        prompt += \"$ \"\n        color = self.NORMAL_PROMPT\n    self.prompt = stringc(prompt, color, wrap_nonvisible_chars=True)", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "default", "parameters": ["self", "line", "forceshell"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "Play", "Play().load", "TaskQueueManager", "TrustedAsTemplate", "TrustedAsTemplate().tag", "dict", "display.debug", "display.error", "line.split", "line.startswith", "module_loader.find_plugin", "parse_kv", "self._tqm.cleanup", "self._tqm.run", "self.loader.cleanup_all_tmp_files", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "actually runs modules", "source_code": "def default(self, line, forceshell=False):\n    \"\"\" actually runs modules \"\"\"\n    if line.startswith(\"#\"):\n        return False\n\n    if not self.cwd:\n        display.error(\"No host found\")\n        return False\n\n    # defaults\n    module = 'shell'\n    module_args = line\n\n    if forceshell is not True:\n        possible_module, *possible_args = line.split()\n        if module_loader.find_plugin(possible_module):\n            # we found module!\n            module = possible_module\n            if possible_args:\n                module_args = ' '.join(possible_args)\n            else:\n                module_args = ''\n\n    module_args = TrustedAsTemplate().tag(module_args)\n\n    if self.callback:\n        cb = self.callback\n    elif C.DEFAULT_LOAD_CALLBACK_PLUGINS and C.DEFAULT_STDOUT_CALLBACK != 'default':\n        cb = C.DEFAULT_STDOUT_CALLBACK\n    else:\n        cb = 'minimal'\n\n    result = None\n    try:\n        check_raw = module in C._ACTION_ALLOWS_RAW_ARGS\n        task = dict(action=module, args=parse_kv(module_args, check_raw=check_raw), timeout=self.task_timeout)\n        play_ds = dict(\n            name=\"Ansible Shell\",\n            hosts=self.cwd,\n            gather_facts='no',\n            tasks=[task],\n            remote_user=self.remote_user,\n            become=self.become,\n            become_user=self.become_user,\n            become_method=self.become_method,\n            check_mode=self.check_mode,\n            diff=self.diff,\n            collections=self.collections,\n        )\n        play = Play().load(play_ds, variable_manager=self.variable_manager, loader=self.loader)\n    except Exception as e:\n        display.error(u\"Unable to build command: %s\" % to_text(e))\n        return False\n\n    try:\n        # now create a task queue manager to execute the play\n        self._tqm = None\n        try:\n            self._tqm = TaskQueueManager(\n                inventory=self.inventory,\n                variable_manager=self.variable_manager,\n                loader=self.loader,\n                passwords=self.passwords,\n                stdout_callback_name=cb,\n                run_additional_callbacks=C.DEFAULT_LOAD_CALLBACK_PLUGINS,\n                run_tree=False,\n                forks=self.forks,\n            )\n\n            result = self._tqm.run(play)\n            display.debug(result)\n        finally:\n            if self._tqm:\n                self._tqm.cleanup()\n            if self.loader:\n                self.loader.cleanup_all_tmp_files()\n\n        if result is None:\n            display.error(\"No hosts found\")\n            return False\n    except KeyboardInterrupt:\n        display.error('User interrupted execution')\n        return False\n    except Exception as ex:\n        display.error(ex)\n        return False", "loc": 86}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_forks", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.error", "int", "self.set_prompt", "self.usage_forks"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set the number of forks", "source_code": "def do_forks(self, arg):\n    \"\"\"Set the number of forks\"\"\"\n    if arg:\n        try:\n            forks = int(arg)\n        except TypeError:\n            display.error('Invalid argument for \"forks\"')\n            self.usage_forks()\n\n        if forks > 0:\n            self.forks = forks\n            self.set_prompt()\n\n        else:\n            display.display('forks must be greater than or equal to 1')\n    else:\n        self.usage_forks()", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_collections", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "arg.split", "collection.strip", "display.v", "self.collections.append", "self.usage_collections"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Set list of collections for 'short name' usage", "source_code": "def do_collections(self, arg):\n    \"\"\"Set list of collections for 'short name' usage\"\"\"\n    if arg in ('', 'none'):\n        self.collections = None\n    elif not arg:\n        self.usage_collections()\n    else:\n        collections = arg.split(',')\n        for collection in collections:\n            if self.collections is None:\n                self.collections = []\n            self.collections.append(collection.strip())\n\n    if self.collections:\n        display.v('Collections name search is set to: %s' % ', '.join(self.collections))\n    else:\n        display.v('Collections name search is using defaults')", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_verbosity", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.error", "display.v", "int", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set verbosity level", "source_code": "def do_verbosity(self, arg):\n    \"\"\"Set verbosity level\"\"\"\n    if not arg:\n        display.display('Usage: verbosity <number>')\n    else:\n        try:\n            display.verbosity = int(arg)\n            display.v('verbosity level set to %s' % arg)\n        except (TypeError, ValueError) as e:\n            display.error('The verbosity must be a valid integer: %s' % to_text(e))", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_cd", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "self.inventory.get_hosts", "self.set_prompt"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Change active host/group. You can use hosts patterns as well eg.: cd webservers cd webservers:dbservers", "source_code": "def do_cd(self, arg):\n    \"\"\"\n        Change active host/group. You can use hosts patterns as well eg.:\n        cd webservers\n        cd webservers:dbservers\n        cd webservers:!phoenix\n        cd webservers:&staging\n        cd webservers:dbservers:&staging:!phoenix\n    \"\"\"\n    if not arg:\n        self.cwd = '*'\n    elif arg in '/*':\n        self.cwd = 'all'\n    elif self.inventory.get_hosts(arg):\n        self.cwd = arg\n    else:\n        display.display(\"no host matched\")\n\n    self.set_prompt()", "loc": 19}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_list", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.error", "self.help_list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "List the hosts in the current group", "source_code": "def do_list(self, arg):\n    \"\"\"List the hosts in the current group\"\"\"\n    if not arg:\n        for host in self.selected:\n            display.display(host.name)\n    elif arg == 'groups':\n        for group in self.groups:\n            display.display(group)\n    else:\n        display.error('Invalid option passed to \"list\"')\n        self.help_list()", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_become", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["boolean", "display.display", "display.v", "self.set_prompt"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Toggle whether plays run with become", "source_code": "def do_become(self, arg):\n    \"\"\"Toggle whether plays run with become\"\"\"\n    if arg:\n        self.become = boolean(arg, strict=False)\n        display.v(\"become changed to %s\" % self.become)\n        self.set_prompt()\n    else:\n        display.display(\"Please specify become value, e.g. `become yes`\")", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_remote_user", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "self.set_prompt"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given a username, set the remote user plays are run by", "source_code": "def do_remote_user(self, arg):\n    \"\"\"Given a username, set the remote user plays are run by\"\"\"\n    if arg:\n        self.remote_user = arg\n        self.set_prompt()\n    else:\n        display.display(\"Please specify a remote user, e.g. `remote_user root`\")", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_become_user", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.v", "self.set_prompt"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given a username, set the user that plays are run by when using become", "source_code": "def do_become_user(self, arg):\n    \"\"\"Given a username, set the user that plays are run by when using become\"\"\"\n    if arg:\n        self.become_user = arg\n    else:\n        display.display(\"Please specify a user, e.g. `become_user jenkins`\")\n        display.v(\"Current user is %s\" % self.become_user)\n    self.set_prompt()", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_become_method", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.v"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given a become_method, set the privilege escalation method when using become", "source_code": "def do_become_method(self, arg):\n    \"\"\"Given a become_method, set the privilege escalation method when using become\"\"\"\n    if arg:\n        self.become_method = arg\n        display.v(\"become_method changed to %s\" % self.become_method)\n    else:\n        display.display(\"Please specify a become_method, e.g. `become_method su`\")\n        display.v(\"Current become_method is %s\" % self.become_method)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_check", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["boolean", "display.display", "display.v"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Toggle whether plays run with check mode", "source_code": "def do_check(self, arg):\n    \"\"\"Toggle whether plays run with check mode\"\"\"\n    if arg:\n        self.check_mode = boolean(arg, strict=False)\n        display.display(\"check mode changed to %s\" % self.check_mode)\n    else:\n        display.display(\"Please specify check mode value, e.g. `check yes`\")\n        display.v(\"check mode is currently %s.\" % self.check_mode)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_diff", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["boolean", "display.display", "display.v"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Toggle whether plays run with diff", "source_code": "def do_diff(self, arg):\n    \"\"\"Toggle whether plays run with diff\"\"\"\n    if arg:\n        self.diff = boolean(arg, strict=False)\n        display.display(\"diff mode changed to %s\" % self.diff)\n    else:\n        display.display(\"Please specify a diff value , e.g. `diff yes`\")\n        display.v(\"diff mode is currently %s\" % self.diff)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "do_timeout", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.error", "int", "self.usage_timeout", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set the timeout", "source_code": "def do_timeout(self, arg):\n    \"\"\"Set the timeout\"\"\"\n    if arg:\n        try:\n            timeout = int(arg)\n            if timeout < 0:\n                display.error('The timeout must be greater than or equal to 1, use 0 to disable')\n            else:\n                self.task_timeout = timeout\n        except (TypeError, ValueError) as e:\n            display.error('The timeout must be a valid positive integer, or 0 to disable: %s' % to_text(e))\n    else:\n        self.usage_timeout()", "loc": 13}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "helpdefault", "parameters": ["self", "module_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.error", "module_loader.find_plugin", "oc['options'].keys", "plugin_docs.get_docstring", "stringc"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def helpdefault(self, module_name):\n    if module_name:\n        in_path = module_loader.find_plugin(module_name)\n        if in_path:\n            oc, a, _dummy1, _dummy2 = plugin_docs.get_docstring(in_path, fragment_loader)\n            if oc:\n                display.display(oc['short_description'])\n                display.display('Parameters:')\n                for opt in oc['options'].keys():\n                    display.display('  ' + stringc(opt, self.NORMAL_PROMPT) + ' ' + oc['options'][opt]['description'][0])\n            else:\n                display.error('No documentation found for %s.' % module_name)\n        else:\n            display.error('%s is not a valid command, use ? to list all valid commands.' % module_name)", "loc": 14}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "complete_cd", "parameters": ["self", "text", "line", "begidx", "endidx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.partition", "self.inventory.list_hosts", "to_native", "to_native(s).startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def complete_cd(self, text, line, begidx, endidx):\n    mline = line.partition(' ')[2]\n    offs = len(mline) - len(text)\n\n    if self.cwd in ('all', '*', '\\\\'):\n        completions = self.hosts + self.groups\n    else:\n        completions = [x.name for x in self.inventory.list_hosts(self.cwd)]\n\n    return [to_native(s)[offs:] for s in completions if to_native(s).startswith(to_native(mline))]", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\console.py", "class_name": "ConsoleCLI", "function_name": "completedefault", "parameters": ["self", "text", "line", "begidx", "endidx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "s.startswith", "self.list_modules", "self.module_args"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def completedefault(self, text, line, begidx, endidx):\n    if line.split()[0] in self.list_modules():\n        mline = line.split(' ')[-1]\n        offs = len(mline) - len(text)\n        completions = self.module_args(line.split()[0])\n\n        return [s[offs:] + '=' for s in completions if s.startswith(mline)]", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": null, "function_name": "jdump", "parameters": ["text"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_json.json_dumps_formatted", "display.display"], "control_structures": ["Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def jdump(text):\n    try:\n        display.display(_json.json_dumps_formatted(text))\n    except TypeError as ex:\n        raise AnsibleError('We could not convert all the documentation into JSON as there was a conversion issue.') from ex", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "tty_ify", "parameters": ["cls", "text"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n{0}\\n'.format", "_doclink", "_format", "cls._BOLD.sub", "cls._CONST.sub", "cls._ITALIC.sub", "cls._LINK.sub", "cls._MODULE.sub", "cls._PLUGIN.sub", "cls._REF.sub", "cls._RST_DIRECTIVES.sub", "cls._RST_NOTE.sub", "cls._RST_ROLES.sub", "cls._RST_SEEALSO.sub", "cls._RULER.sub", "cls._SEM_ENV_VARIABLE.sub", "cls._SEM_OPTION_NAME.sub", "cls._SEM_OPTION_VALUE.sub", "cls._SEM_RET_VALUE.sub", "cls._URL.sub", "m.group", "re.sub"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def tty_ify(cls, text):\n\n    # general formatting\n    t = cls._ITALIC.sub(_format(r\"\\1\", 'UNDERLINE'), text)  # no ascii code for this\n    t = cls._BOLD.sub(_format(r\"\\1\", 'BOLD'), t)\n    t = cls._MODULE.sub(_format(r\"\\1\", 'MODULE'), t)    # M(word) => [word]\n    t = cls._URL.sub(r\"\\1\", t)                      # U(word) => word\n    t = cls._LINK.sub(r\"\\1 <\\2>\", t)                # L(word, url) => word <url>\n\n    t = cls._PLUGIN.sub(_format(\"[\" + r\"\\1\" + \"]\", 'PLUGIN'), t)       # P(word#type) => [word]\n\n    t = cls._REF.sub(_format(r\"\\1\", 'REF'), t)      # R(word, sphinx-ref) => word\n    t = cls._CONST.sub(_format(r\"`\\1'\", 'CONSTANT'), t)\n    t = cls._SEM_OPTION_NAME.sub(cls._tty_ify_sem_complex, t)  # O(expr)\n    t = cls._SEM_OPTION_VALUE.sub(cls._tty_ify_sem_simle, t)  # V(expr)\n    t = cls._SEM_ENV_VARIABLE.sub(cls._tty_ify_sem_simle, t)  # E(expr)\n    t = cls._SEM_RET_VALUE.sub(cls._tty_ify_sem_complex, t)  # RV(expr)\n    t = cls._RULER.sub(\"\\n{0}\\n\".format(\"-\" * 13), t)   # HORIZONTALLINE => -------\n\n    # remove rst\n    t = cls._RST_SEEALSO.sub(r\"See also:\", t)   # seealso to See also:\n    t = cls._RST_NOTE.sub(_format(r\"Note:\", 'bold'), t)  # .. note:: to note:\n    t = cls._RST_ROLES.sub(r\"`\", t)             # remove :ref: and other tags, keep tilde to match ending one\n    t = cls._RST_DIRECTIVES.sub(r\"\", t)         # remove .. stuff:: in general\n\n    # handle docsite refs\n    # U(word) => word\n    t = re.sub(cls._URL, lambda m: _format(r\"%s\" % _doclink(m.group(1)), 'LINK'), t)\n    # L(word, url) => word <url>\n    t = re.sub(cls._LINK, lambda m: r\"%s <%s>\" % (m.group(1), _format(_doclink(m.group(2)), 'LINK')), t)\n\n    return t", "loc": 32}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "'Choose which plugin type (defaults to \"module\"). Available plugin types are : {0}'.format", "exclusive.add_argument", "opt_help.add_basedir_options", "opt_help.add_module_options", "opt_help.unfrack_path", "self.parser.add_argument", "self.parser.add_mutually_exclusive_group", "super", "super(DocCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def init_parser(self):\n\n    coll_filter = 'A supplied argument will be used for filtering, can be a namespace or full collection name.'\n\n    super(DocCLI, self).init_parser(\n        desc=\"plugin documentation tool\",\n        epilog=\"See man pages for Ansible CLI options or website for tutorials https://docs.ansible.com\"\n    )\n    opt_help.add_module_options(self.parser)\n    opt_help.add_basedir_options(self.parser)\n\n    # targets\n    self.parser.add_argument('args', nargs='*', help='Plugin', metavar='plugin')\n\n    self.parser.add_argument(\"-t\", \"--type\", action=\"store\", default='module', dest='type',\n                             help='Choose which plugin type (defaults to \"module\"). '\n                                  'Available plugin types are : {0}'.format(TARGET_OPTIONS),\n                             choices=TARGET_OPTIONS)\n\n    # formatting\n    self.parser.add_argument(\"-j\", \"--json\", action=\"store_true\", default=False, dest='json_format',\n                             help='Change output into json format.')\n\n    # TODO: warn if not used with -t roles\n    # role-specific options\n    self.parser.add_argument(\"-r\", \"--roles-path\", dest='roles_path', default=C.DEFAULT_ROLES_PATH,\n                             type=opt_help.unfrack_path(pathsep=True),\n                             action=opt_help.PrependListAction,\n                             help='The path to the directory containing your roles.')\n\n    # exclusive modifiers\n    exclusive = self.parser.add_mutually_exclusive_group()\n\n    # TODO: warn if not used with -t roles\n    exclusive.add_argument(\"-e\", \"--entry-point\", dest=\"entry_point\",\n                           help=\"Select the entry point for role(s).\")\n\n    # TODO: warn with --json as it is incompatible\n    exclusive.add_argument(\"-s\", \"--snippet\", action=\"store_true\", default=False, dest='show_snippet',\n                           help='Show playbook snippet for these plugin types: %s' % ', '.join(SNIPPETS))\n\n    # TODO: warn when arg/plugin is passed\n    exclusive.add_argument(\"-F\", \"--list_files\", action=\"store_true\", default=False, dest=\"list_files\",\n                           help='Show plugin names and their source files without summaries (implies --list). %s' % coll_filter)\n    exclusive.add_argument(\"-l\", \"--list\", action=\"store_true\", default=False, dest='list_dir',\n                           help='List available plugins. %s' % coll_filter)\n    exclusive.add_argument(\"--metadata-dump\", action=\"store_true\", default=False, dest='dump',\n                           help='**For internal use only** Dump json metadata for all entries, ignores other options.')\n\n    # generic again\n    self.parser.add_argument(\"--no-fail-on-errors\", action=\"store_true\", default=False, dest='no_fail_on_errors',\n                             help='**For internal use only** Only used for --metadata-dump. '\n                                  'Do not fail on errors. Report the error message in the JSON instead.')", "loc": 53}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super(DocCLI, self).post_process_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(DocCLI, self).post_process_args(options)\n\n    display.verbosity = options.verbosity\n\n    return options", "loc": 6}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "display_plugin_list", "parameters": ["self", "results"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "'\\n'.join", "DocCLI.pager", "DocCLI.tty_ify", "deprecated.append", "len", "max", "pbreak[-1].startswith", "plugin.split", "plugin.startswith", "results.keys", "sorted", "text.append", "text.extend", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display_plugin_list(self, results):\n\n    # format for user\n    displace = max(len(x) for x in results.keys())\n    linelimit = display.columns - displace - 5\n    text = []\n    deprecated = []\n\n    # format display per option\n    if context.CLIARGS['list_files']:\n        # list plugin file names\n        for plugin in sorted(results.keys()):\n            filename = to_native(results[plugin])\n\n            # handle deprecated for builtin/legacy\n            pbreak = plugin.split('.')\n            if pbreak[-1].startswith('_') and pbreak[0] == 'ansible' and pbreak[1] in ('builtin', 'legacy'):\n                pbreak[-1] = pbreak[-1][1:]\n                plugin = '.'.join(pbreak)\n                deprecated.append(\"%-*s %-*.*s\" % (displace, plugin, linelimit, len(filename), filename))\n            else:\n                text.append(\"%-*s %-*.*s\" % (displace, plugin, linelimit, len(filename), filename))\n    else:\n        # list plugin names and short desc\n        for plugin in sorted(results.keys()):\n            desc = DocCLI.tty_ify(results[plugin])\n\n            if len(desc) > linelimit:\n                desc = desc[:linelimit] + '...'\n\n            pbreak = plugin.split('.')\n            # TODO: add mark for deprecated collection plugins\n            if pbreak[-1].startswith('_') and plugin.startswith(('ansible.builtin.', 'ansible.legacy.')):\n                # Handle deprecated ansible.builtin plugins\n                pbreak[-1] = pbreak[-1][1:]\n                plugin = '.'.join(pbreak)\n                deprecated.append(\"%-*s %-*.*s\" % (displace, plugin, linelimit, len(desc), desc))\n            else:\n                text.append(\"%-*s %-*.*s\" % (displace, plugin, linelimit, len(desc), desc))\n\n    if len(deprecated) > 0:\n        text.append(\"\\nDEPRECATED:\")\n        text.extend(deprecated)\n\n    # display results\n    DocCLI.pager(\"\\n\".join(text))", "loc": 46}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "get_all_plugins_of_type", "parameters": ["plugin_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_list_plugins_with_info", "_list_plugins_with_info(plugin_type).keys", "getattr", "loader._get_paths_with_context", "sorted"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_plugins_of_type(plugin_type):\n    loader = getattr(plugin_loader, '%s_loader' % plugin_type)\n    paths = loader._get_paths_with_context()\n    plugins = []\n    for path_context in paths:\n        plugins += _list_plugins_with_info(plugin_type).keys()\n    return sorted(plugins)", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "get_plugin_metadata", "parameters": ["plugin_type", "plugin_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'unable to load {0} plugin named {1} '.format", "AnsibleError", "DocCLI.namespace_from_plugin_filepath", "dict", "doc.get", "get_docstring", "getattr", "loader.find_plugin_with_context"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_metadata(plugin_type, plugin_name):\n    # if the plugin lives in a non-python file (eg, win_X.ps1), require the corresponding python file for docs\n    loader = getattr(plugin_loader, '%s_loader' % plugin_type)\n    result = loader.find_plugin_with_context(plugin_name, mod_type='.py', ignore_deprecated=True, check_aliases=True)\n    if not result.resolved:\n        raise AnsibleError(\"unable to load {0} plugin named {1} \".format(plugin_type, plugin_name))\n    filename = result.plugin_resolved_path\n    collection_name = result.plugin_resolved_collection\n\n    try:\n        doc, __, __, __ = get_docstring(filename, fragment_loader, verbose=(context.CLIARGS['verbosity'] > 0),\n                                        collection_name=collection_name, plugin_type=plugin_type)\n    except Exception as ex:\n        raise AnsibleError(f\"{plugin_type} {plugin_name} at {filename!r} has a documentation formatting error or is missing documentation.\") from ex\n\n    if doc is None:\n        # Removed plugins don't have any documentation\n        return None\n\n    return dict(\n        name=plugin_name,\n        namespace=DocCLI.namespace_from_plugin_filepath(filename, plugin_name, loader.package_path),\n        description=doc.get('short_description', \"UNKNOWN\"),\n        version_added=doc.get('version_added', \"UNKNOWN\")\n    )", "loc": 25}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "format_snippet", "parameters": ["plugin", "plugin_type", "doc"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'The {0} inventory plugin does not take YAML type config source that can be used with the \"auto\" plugin so a snippet cannot be created.'.format", "'\\n'.join", "ValueError", "_do_lookup_snippet", "_do_yaml_snippet", "doc.get", "doc.get('options', {}).get", "text.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return heavily commented plugin use to insert into play", "source_code": "def format_snippet(plugin, plugin_type, doc):\n    \"\"\" return heavily commented plugin use to insert into play \"\"\"\n    if plugin_type == 'inventory' and doc.get('options', {}).get('plugin'):\n        # these do not take a yaml config that we can write a snippet for\n        raise ValueError('The {0} inventory plugin does not take YAML type config source'\n                         ' that can be used with the \"auto\" plugin so a snippet cannot be'\n                         ' created.'.format(plugin))\n\n    text = []\n\n    if plugin_type == 'lookup':\n        text = _do_lookup_snippet(doc)\n\n    elif 'options' in doc:\n        text = _do_yaml_snippet(doc)\n\n    text.append('')\n    return \"\\n\".join(text)", "loc": 18}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "format_plugin_doc", "parameters": ["plugin", "plugin_type", "doc", "plainexamples", "returndocs", "metadata"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "DocCLI.get_man_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_plugin_doc(plugin, plugin_type, doc, plainexamples, returndocs, metadata):\n    collection_name = doc['collection']\n\n    # TODO: do we really want this?\n    # add_collection_to_versions_and_dates(doc, '(unknown)', is_module=(plugin_type == 'module'))\n    # remove_current_collection_from_versions_and_dates(doc, collection_name, is_module=(plugin_type == 'module'))\n    # remove_current_collection_from_versions_and_dates(\n    #     returndocs, collection_name, is_module=(plugin_type == 'module'), return_docs=True)\n\n    # assign from other sections\n    doc['plainexamples'] = plainexamples\n    doc['returndocs'] = returndocs\n    doc['metadata'] = metadata\n\n    try:\n        text = DocCLI.get_man_text(doc, collection_name, plugin_type)\n    except Exception as ex:\n        raise AnsibleError(f\"Unable to retrieve documentation from {plugin!r}.\") from ex\n\n    return text", "loc": 20}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "warp_fill", "parameters": ["text", "limit", "initial_indent", "subsequent_indent", "initial_extra"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "result.append", "text.split", "textwrap.fill", "wrapped.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def warp_fill(text, limit, initial_indent='', subsequent_indent='', initial_extra=0, **kwargs):\n    result = []\n    for paragraph in text.split('\\n\\n'):\n        wrapped = textwrap.fill(paragraph, limit, initial_indent=initial_indent + ' ' * initial_extra, subsequent_indent=subsequent_indent,\n                                break_on_hyphens=False, break_long_words=False, drop_whitespace=True, **kwargs)\n        if initial_extra and wrapped.startswith(' ' * initial_extra):\n            wrapped = wrapped[initial_extra:]\n        result.append(wrapped)\n        initial_indent = subsequent_indent\n        initial_extra = 0\n    return '\\n'.join(result)", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\doc.py", "class_name": "DocCLI", "function_name": "get_role_man_text", "parameters": ["self", "role", "role_json"], "param_types": {}, "return_type": null, "param_doc": {"role": "The role name.", "role_json": "The JSON for the given role as returned from _create_role_doc()."}, "return_doc": "A array of text suitable for displaying to screen.", "raises_doc": [], "called_functions": ["', '.join", "AnsibleParserError", "DocCLI._add_seealso", "DocCLI._dump_yaml", "DocCLI._format_version_added", "DocCLI._indent_lines", "DocCLI.add_fields", "DocCLI.tty_ify", "DocCLI.warp_fill", "_format", "doc.get", "doc.pop", "doc.pop('examples').strip", "int", "isinstance", "k.upper", "len", "max", "role_json.get", "text.append", "yaml_dump"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Generate text for the supplied role suitable for display. This is similar to get_man_text(), but roles are different enough that we have a separate method for formatting their display.", "source_code": "def get_role_man_text(self, role, role_json):\n    \"\"\"Generate text for the supplied role suitable for display.\n\n    This is similar to get_man_text(), but roles are different enough that we have\n    a separate method for formatting their display.\n\n    :param role: The role name.\n    :param role_json: The JSON for the given role as returned from _create_role_doc().\n\n    :returns: A array of text suitable for displaying to screen.\n    \"\"\"\n    text = []\n    opt_indent = \"          \"\n    pad = display.columns * 0.20\n    limit = max(display.columns - int(pad), 70)\n\n    text.append(\"> ROLE: %s (%s)\" % (_format(role, 'BOLD'), role_json.get('path')))\n\n    for entry_point in role_json['entry_points']:\n        doc = role_json['entry_points'][entry_point]\n        desc = ''\n        if doc.get('short_description'):\n            desc = \"- %s\" % (doc.get('short_description'))\n        text.append('')\n        text.append(\"ENTRY POINT: %s %s\" % (_format(entry_point, \"BOLD\"), desc))\n        text.append('')\n\n        if version_added := doc.pop('version_added', None):\n            text.append(_format(\"ADDED IN:\", 'bold') + \" %s\\n\" % DocCLI._format_version_added(version_added))\n\n        if doc.get('description'):\n            if isinstance(doc['description'], list):\n                descs = doc['description']\n            else:\n                descs = [doc['description']]\n            for desc in descs:\n                text.append(\"%s\" % DocCLI.warp_fill(DocCLI.tty_ify(desc), limit, initial_indent=opt_indent, subsequent_indent=opt_indent))\n            text.append('')\n\n        if doc.get('options'):\n            text.append(_format(\"Options\", 'bold') + \" (%s indicates it is required):\" % (\"=\" if C.ANSIBLE_NOCOLOR else 'red'))\n            DocCLI.add_fields(text, doc.pop('options'), limit, opt_indent)\n\n        if notes := doc.pop('notes', False):\n            text.append(\"\")\n            text.append(_format(\"NOTES:\", 'bold'))\n            for note in notes:\n                text.append(DocCLI.warp_fill(DocCLI.tty_ify(note), limit - 6,\n                                             initial_indent=opt_indent[:-2] + \"* \", subsequent_indent=opt_indent))\n\n        if seealso := doc.pop('seealso', False):\n            text.append(\"\")\n            text.append(_format(\"SEE ALSO:\", 'bold'))\n            DocCLI._add_seealso(text, seealso, limit=limit, opt_indent=opt_indent)\n\n        # generic elements we will handle identically\n        for k in ('author',):\n            if k not in doc:\n                continue\n            text.append('')\n            if isinstance(doc[k], str):\n                text.append('%s: %s' % (k.upper(), DocCLI.warp_fill(DocCLI.tty_ify(doc[k]),\n                                        limit - (len(k) + 2), subsequent_indent=opt_indent)))\n            elif isinstance(doc[k], (list, tuple)):\n                text.append('%s: %s' % (k.upper(), ', '.join(doc[k])))\n            else:\n                # use empty indent since this affects the start of the yaml doc, not it's keys\n                text.append(DocCLI._indent_lines(DocCLI._dump_yaml({k.upper(): doc[k]}), ''))\n\n        if doc.get('examples', False):\n            text.append('')\n            text.append(_format(\"EXAMPLES:\", 'bold'))\n            if isinstance(doc['examples'], str):\n                text.append(doc.pop('examples').strip())\n            else:\n                try:\n                    text.append(yaml_dump(doc.pop('examples'), indent=2, default_flow_style=False))\n                except Exception as e:\n                    raise AnsibleParserError(\"Unable to parse examples section.\") from e\n\n    return text", "loc": 81}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": null, "function_name": "with_collection_artifacts_manager", "parameters": ["wrapped_method"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ConcreteArtifactsManager.under_tmpdir", "GalaxyCLI._resolve_path", "artifacts_manager_kwargs.update", "context.CLIARGS.get", "functools.wraps", "wrapped_method"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Inject an artifacts manager if not passed explicitly. This decorator constructs a ConcreteArtifactsManager and maintains the related temporary directory auto-cleanup around the target", "source_code": "def with_collection_artifacts_manager(wrapped_method):\n    \"\"\"Inject an artifacts manager if not passed explicitly.\n\n    This decorator constructs a ConcreteArtifactsManager and maintains\n    the related temporary directory auto-cleanup around the target\n    method invocation.\n    \"\"\"\n    @functools.wraps(wrapped_method)\n    def method_wrapper(*args, **kwargs):\n        if 'artifacts_manager' in kwargs:\n            return wrapped_method(*args, **kwargs)\n\n        # FIXME: use validate_certs context from Galaxy servers when downloading collections\n        # .get used here for when this is used in a non-CLI context\n        artifacts_manager_kwargs = {'validate_certs': context.CLIARGS.get('resolved_validate_certs', True)}\n\n        keyring = context.CLIARGS.get('keyring', None)\n        if keyring is not None:\n            artifacts_manager_kwargs.update({\n                'keyring': GalaxyCLI._resolve_path(keyring),\n                'required_signature_count': context.CLIARGS.get('required_valid_signature_count', None),\n                'ignore_signature_errors': context.CLIARGS.get('ignore_gpg_errors', None),\n            })\n\n        with ConcreteArtifactsManager.under_tmpdir(\n                C.DEFAULT_LOCAL_TMP,\n                **artifacts_manager_kwargs\n        ) as concrete_artifact_cm:\n            kwargs['artifacts_manager'] = concrete_artifact_cm\n            return wrapped_method(*args, **kwargs)\n    return method_wrapper", "loc": 31}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": null, "function_name": "validate_signature_count", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def validate_signature_count(value):\n    match = re.match(SIGNATURE_COUNT_RE, value)\n\n    if match is None:\n        raise ValueError(f\"{value} is not a valid signature count value\")\n\n    return value", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": null, "function_name": "method_wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ConcreteArtifactsManager.under_tmpdir", "GalaxyCLI._resolve_path", "artifacts_manager_kwargs.update", "context.CLIARGS.get", "functools.wraps", "wrapped_method"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def method_wrapper(*args, **kwargs):\n    if 'artifacts_manager' in kwargs:\n        return wrapped_method(*args, **kwargs)\n\n    # FIXME: use validate_certs context from Galaxy servers when downloading collections\n    # .get used here for when this is used in a non-CLI context\n    artifacts_manager_kwargs = {'validate_certs': context.CLIARGS.get('resolved_validate_certs', True)}\n\n    keyring = context.CLIARGS.get('keyring', None)\n    if keyring is not None:\n        artifacts_manager_kwargs.update({\n            'keyring': GalaxyCLI._resolve_path(keyring),\n            'required_signature_count': context.CLIARGS.get('required_valid_signature_count', None),\n            'ignore_signature_errors': context.CLIARGS.get('ignore_gpg_errors', None),\n        })\n\n    with ConcreteArtifactsManager.under_tmpdir(\n            C.DEFAULT_LOCAL_TMP,\n            **artifacts_manager_kwargs\n    ) as concrete_artifact_cm:\n        kwargs['artifacts_manager'] = concrete_artifact_cm\n        return wrapped_method(*args, **kwargs)", "loc": 22}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "RoleDistributionServer", "function_name": "api", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def api(self):\n    if self._api:\n        return self._api\n\n    for server in self.api_servers:\n        try:\n            if u'v1' in server.available_api_versions:\n                self._api = server\n                break\n        except Exception:\n            continue\n\n    if not self._api:\n        self._api = self.api_servers[0]\n\n    return self._api", "loc": 16}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"One or more directories to search for collections in addition to the default COLLECTIONS_PATHS. Separate multiple paths with '{0}'.\".format", "C.config.get_configuration_definition", "C.config.get_configuration_definition('DEFAULT_ROLES_PATH').get", "cache_options.add_argument", "collection.add_subparsers", "collection.set_defaults", "collections_path.add_argument", "common.add_argument", "force.add_argument", "github.add_argument", "offline.add_argument", "opt_help.ArgumentParser", "opt_help.add_verbosity_options", "opt_help.unfrack_path", "role.add_subparsers", "role.set_defaults", "roles_path.add_argument", "self.add_build_options", "self.add_delete_options", "self.add_download_options", "self.add_import_options", "self.add_info_options", "self.add_init_options", "self.add_install_options", "self.add_list_options", "self.add_publish_options", "self.add_remove_options", "self.add_search_options", "self.add_setup_options", "self.add_verify_options", "self.parser.add_subparsers", "super", "super(GalaxyCLI, self).init_parser", "type_parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "create an options parser for bin/ansible", "source_code": "def init_parser(self):\n    \"\"\" create an options parser for bin/ansible \"\"\"\n\n    super(GalaxyCLI, self).init_parser(\n        desc=\"Perform various Role and Collection related operations.\",\n    )\n\n    # Common arguments that apply to more than 1 action\n    common = opt_help.ArgumentParser(add_help=False)\n    common.add_argument('-s', '--server', dest='api_server', help='The Galaxy API server URL')\n    common.add_argument('--api-version', type=int, choices=[2, 3], help=argparse.SUPPRESS)  # Hidden argument that should only be used in our tests\n    common.add_argument('--token', '--api-key', dest='api_key',\n                        help='The Ansible Galaxy API key which can be found at '\n                             'https://galaxy.ansible.com/me/preferences.')\n    common.add_argument('-c', '--ignore-certs', action='store_true', dest='ignore_certs', help='Ignore SSL certificate validation errors.', default=None)\n\n    # --timeout uses the default None to handle two different scenarios.\n    # * --timeout > C.GALAXY_SERVER_TIMEOUT for non-configured servers\n    # * --timeout > server-specific timeout > C.GALAXY_SERVER_TIMEOUT for configured servers.\n    common.add_argument('--timeout', dest='timeout', type=int,\n                        help=\"The time to wait for operations against the galaxy server, defaults to 60s.\")\n\n    opt_help.add_verbosity_options(common)\n\n    force = opt_help.ArgumentParser(add_help=False)\n    force.add_argument('-f', '--force', dest='force', action='store_true', default=False,\n                       help='Force overwriting an existing role or collection')\n\n    github = opt_help.ArgumentParser(add_help=False)\n    github.add_argument('github_user', help='GitHub username')\n    github.add_argument('github_repo', help='GitHub repository')\n\n    offline = opt_help.ArgumentParser(add_help=False)\n    offline.add_argument('--offline', dest='offline', default=False, action='store_true',\n                         help=\"Don't query the galaxy API when creating roles\")\n\n    default_roles_path = C.config.get_configuration_definition('DEFAULT_ROLES_PATH').get('default', '')\n    roles_path = opt_help.ArgumentParser(add_help=False)\n    roles_path.add_argument('-p', '--roles-path', dest='roles_path', type=opt_help.unfrack_path(pathsep=True),\n                            default=C.DEFAULT_ROLES_PATH, action=opt_help.PrependListAction,\n                            help='The path to the directory containing your roles. The default is the first '\n                                 'writable one configured via DEFAULT_ROLES_PATH: %s ' % default_roles_path)\n\n    collections_path = opt_help.ArgumentParser(add_help=False)\n    collections_path.add_argument('-p', '--collections-path', dest='collections_path', type=opt_help.unfrack_path(pathsep=True),\n                                  action=opt_help.PrependListAction,\n                                  help=\"One or more directories to search for collections in addition \"\n                                  \"to the default COLLECTIONS_PATHS. Separate multiple paths \"\n                                  \"with '{0}'.\".format(os.path.pathsep))\n\n    cache_options = opt_help.ArgumentParser(add_help=False)\n    cache_options.add_argument('--clear-response-cache', dest='clear_response_cache', action='store_true',\n                               default=False, help='Clear the existing server response cache.')\n    cache_options.add_argument('--no-cache', dest='no_cache', action='store_true', default=False,\n                               help='Do not use the server response cache.')\n\n    # Add sub parser for the Galaxy role type (role or collection)\n    type_parser = self.parser.add_subparsers(metavar='TYPE', dest='type')\n    type_parser.required = True\n\n    # Add sub parser for the Galaxy collection actions\n    collection = type_parser.add_parser('collection', help='Manage an Ansible Galaxy collection.')\n    collection.set_defaults(func=self.execute_collection)  # to satisfy doc build\n    collection_parser = collection.add_subparsers(metavar='COLLECTION_ACTION', dest='action')\n    collection_parser.required = True\n    self.add_download_options(collection_parser, parents=[common, cache_options])\n    self.add_init_options(collection_parser, parents=[common, force])\n    self.add_build_options(collection_parser, parents=[common, force])\n    self.add_publish_options(collection_parser, parents=[common])\n    self.add_install_options(collection_parser, parents=[common, force, cache_options])\n    self.add_list_options(collection_parser, parents=[common, collections_path])\n    self.add_verify_options(collection_parser, parents=[common, collections_path])\n\n    # Add sub parser for the Galaxy role actions\n    role = type_parser.add_parser('role', help='Manage an Ansible Galaxy role.')\n    role.set_defaults(func=self.execute_role)  # to satisfy doc build\n    role_parser = role.add_subparsers(metavar='ROLE_ACTION', dest='action')\n    role_parser.required = True\n    self.add_init_options(role_parser, parents=[common, force, offline])\n    self.add_remove_options(role_parser, parents=[common, roles_path])\n    self.add_delete_options(role_parser, parents=[common, github])\n    self.add_list_options(role_parser, parents=[common, roles_path])\n    self.add_search_options(role_parser, parents=[common])\n    self.add_import_options(role_parser, parents=[common, github])\n    self.add_setup_options(role_parser, parents=[common, roles_path])\n\n    self.add_info_options(role_parser, parents=[common, roles_path, offline])\n    self.add_install_options(role_parser, parents=[common, force, roles_path])", "loc": 88}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_download_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["download_parser.add_argument", "download_parser.set_defaults", "parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_download_options(self, parser, parents=None):\n    download_parser = parser.add_parser('download', parents=parents,\n                                        help='Download collections and their dependencies as a tarball for an '\n                                             'offline install.')\n    download_parser.set_defaults(func=self.execute_download)\n\n    download_parser.add_argument('args', help='Collection(s)', metavar='collection', nargs='*')\n\n    download_parser.add_argument('-n', '--no-deps', dest='no_deps', action='store_true', default=False,\n                                 help=\"Don't download collection(s) listed as dependencies.\")\n\n    download_parser.add_argument('-p', '--download-path', dest='download_path',\n                                 default='./collections',\n                                 help='The directory to download the collections to.')\n    download_parser.add_argument('-r', '--requirements-file', dest='requirements',\n                                 help='A file containing a list of collections to be downloaded.')\n    download_parser.add_argument('--pre', dest='allow_pre_release', action='store_true',\n                                 help='Include pre-release versions. Semantic versioning pre-releases are ignored by default')", "loc": 18}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_init_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'--{0}-skeleton'.format", "'Initialize new {0} with the base structure of a {0}.'.format", "'The path in which the skeleton {0} will be created. The default is the current working directory.'.format", "'The path to a {0} skeleton that the new {0} should be based upon.'.format", "'{0} name'.format", "'{0}_name'.format", "'{0}_skeleton'.format", "galaxy_type.capitalize", "init_parser.add_argument", "init_parser.set_defaults", "opt_help.add_runtask_options", "parser.add_parser"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_init_options(self, parser, parents=None):\n    galaxy_type = 'collection' if parser.metavar == 'COLLECTION_ACTION' else 'role'\n\n    init_parser = parser.add_parser('init', parents=parents,\n                                    help='Initialize new {0} with the base structure of a '\n                                         '{0}.'.format(galaxy_type))\n    init_parser.set_defaults(func=self.execute_init)\n\n    init_parser.add_argument('--init-path', dest='init_path', default='./',\n                             help='The path in which the skeleton {0} will be created. The default is the '\n                                  'current working directory.'.format(galaxy_type))\n    init_parser.add_argument('--{0}-skeleton'.format(galaxy_type), dest='{0}_skeleton'.format(galaxy_type),\n                             default=C.GALAXY_COLLECTION_SKELETON if galaxy_type == 'collection' else C.GALAXY_ROLE_SKELETON,\n                             help='The path to a {0} skeleton that the new {0} should be based '\n                                  'upon.'.format(galaxy_type))\n\n    obj_name_kwargs = {}\n    if galaxy_type == 'collection':\n        obj_name_kwargs['type'] = validate_collection_name\n    init_parser.add_argument('{0}_name'.format(galaxy_type), help='{0} name'.format(galaxy_type.capitalize()),\n                             **obj_name_kwargs)\n\n    if galaxy_type == 'role':\n        init_parser.add_argument('--type', dest='role_type', action='store', default='default',\n                                 help=\"Initialize using an alternate role type. Valid types include: 'container', \"\n                                      \"'apb' and 'network'.\")\n    opt_help.add_runtask_options(init_parser)", "loc": 27}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_remove_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_parser", "remove_parser.add_argument", "remove_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_remove_options(self, parser, parents=None):\n    remove_parser = parser.add_parser('remove', parents=parents, help='Delete roles from roles_path.')\n    remove_parser.set_defaults(func=self.execute_remove)\n\n    remove_parser.add_argument('args', help='Role(s)', metavar='role', nargs='+')", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_delete_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["delete_parser.set_defaults", "parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_delete_options(self, parser, parents=None):\n    delete_parser = parser.add_parser('delete', parents=parents,\n                                      help='Removes the role from Galaxy. It does not remove or alter the actual '\n                                           'GitHub repository.')\n    delete_parser.set_defaults(func=self.execute_delete)", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_list_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Show the name and version of each {0} installed in the {0}s_path.'.format", "galaxy_type.capitalize", "list_parser.add_argument", "list_parser.set_defaults", "parser.add_parser"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_list_options(self, parser, parents=None):\n    galaxy_type = 'role'\n    if parser.metavar == 'COLLECTION_ACTION':\n        galaxy_type = 'collection'\n\n    list_parser = parser.add_parser('list', parents=parents,\n                                    help='Show the name and version of each {0} installed in the {0}s_path.'.format(galaxy_type))\n\n    list_parser.set_defaults(func=self.execute_list)\n\n    list_parser.add_argument(galaxy_type, help=galaxy_type.capitalize(), nargs='?', metavar=galaxy_type)\n\n    if galaxy_type == 'collection':\n        list_parser.add_argument('--format', dest='output_format', choices=('human', 'yaml', 'json'), default='human',\n                                 help=\"Format to display the list of collections in.\")", "loc": 15}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_search_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_parser", "search_parser.add_argument", "search_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_search_options(self, parser, parents=None):\n    search_parser = parser.add_parser('search', parents=parents,\n                                      help='Search the Galaxy database by tags, platforms, author and multiple '\n                                           'keywords.')\n    search_parser.set_defaults(func=self.execute_search)\n\n    search_parser.add_argument('--platforms', dest='platforms', help='list of OS platforms to filter by')\n    search_parser.add_argument('--galaxy-tags', dest='galaxy_tags', help='list of galaxy tags to filter by')\n    search_parser.add_argument('--author', dest='author', help='GitHub username')\n    search_parser.add_argument('args', help='Search terms', metavar='searchterm', nargs='*')", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_import_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["import_parser.add_argument", "import_parser.set_defaults", "parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_import_options(self, parser, parents=None):\n    import_parser = parser.add_parser('import', parents=parents, help='Import a role into a galaxy server')\n    import_parser.set_defaults(func=self.execute_import)\n\n    import_parser.add_argument('--no-wait', dest='wait', action='store_false', default=True,\n                               help=\"Don't wait for import results.\")\n    import_parser.add_argument('--branch', dest='reference',\n                               help='The name of a branch to import. Defaults to the repository\\'s default branch '\n                                    '(usually master)')\n    import_parser.add_argument('--role-name', dest='role_name',\n                               help='The name the role should have, if different than the repo name')\n    import_parser.add_argument('--status', dest='check_status', action='store_true', default=False,\n                               help='Check the status of the most recent import request for given github_'\n                                    'user/github_repo.')", "loc": 14}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_setup_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_parser", "setup_parser.add_argument", "setup_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_setup_options(self, parser, parents=None):\n    setup_parser = parser.add_parser('setup', parents=parents,\n                                     help='Manage the integration between Galaxy and the given source.')\n    setup_parser.set_defaults(func=self.execute_setup)\n\n    setup_parser.add_argument('--remove', dest='remove_id', default=None,\n                              help='Remove the integration matching the provided ID value. Use --list to see '\n                                   'ID values.')\n    setup_parser.add_argument('--list', dest=\"setup_list\", action='store_true', default=False,\n                              help='List all of your integrations.')\n    setup_parser.add_argument('source', help='Source')\n    setup_parser.add_argument('github_user', help='GitHub username')\n    setup_parser.add_argument('github_repo', help='GitHub repository')\n    setup_parser.add_argument('secret', help='Secret')", "loc": 14}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_info_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info_parser.add_argument", "info_parser.set_defaults", "parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_info_options(self, parser, parents=None):\n    info_parser = parser.add_parser('info', parents=parents, help='View more details about a specific role.')\n    info_parser.set_defaults(func=self.execute_info)\n\n    info_parser.add_argument('args', nargs='+', help='role', metavar='role_name[,version]')", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_verify_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}_name'.format", "GPG_ERROR_MAP.keys", "list", "parser.add_parser", "verify_parser.add_argument", "verify_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_verify_options(self, parser, parents=None):\n    galaxy_type = 'collection'\n    verify_parser = parser.add_parser('verify', parents=parents, help='Compare checksums with the collection(s) '\n                                      'found on the server and the installed copy. This does not verify dependencies.')\n    verify_parser.set_defaults(func=self.execute_verify)\n\n    verify_parser.add_argument('args', metavar='{0}_name'.format(galaxy_type), nargs='*', help='The installed collection(s) name. '\n                               'This is mutually exclusive with --requirements-file.')\n    verify_parser.add_argument('-i', '--ignore-errors', dest='ignore_errors', action='store_true', default=False,\n                               help='Ignore errors during verification and continue with the next specified collection.')\n    verify_parser.add_argument('--offline', dest='offline', action='store_true', default=False,\n                               help='Validate collection integrity locally without contacting server for '\n                                    'canonical manifest hash.')\n    verify_parser.add_argument('-r', '--requirements-file', dest='requirements',\n                               help='A file containing a list of collections to be verified.')\n    verify_parser.add_argument('--keyring', dest='keyring', default=C.GALAXY_GPG_KEYRING,\n                               help='The keyring used during signature verification')  # Eventually default to ~/.ansible/pubring.kbx?\n    verify_parser.add_argument('--signature', dest='signatures', action='append',\n                               help='An additional signature source to verify the authenticity of the MANIFEST.json before using '\n                                    'it to verify the rest of the contents of a collection from a Galaxy server. Use in '\n                                    'conjunction with a positional collection name (mutually exclusive with --requirements-file).')\n    valid_signature_count_help = 'The number of signatures that must successfully verify the collection. This should be a positive integer ' \\\n                                 'or all to signify that all signatures must be used to verify the collection. ' \\\n                                 'Prepend the value with + to fail if no valid signatures are found for the collection (e.g. +all).'\n    ignore_gpg_status_help = 'A space separated list of status codes to ignore during signature verification (for example, NO_PUBKEY FAILURE). ' \\\n                             'Descriptions for the choices can be seen at L(https://github.com/gpg/gnupg/blob/master/doc/DETAILS#general-status-codes).' \\\n                             'Note: specify these after positional arguments or use -- to separate them.'\n    verify_parser.add_argument('--required-valid-signature-count', dest='required_valid_signature_count', type=validate_signature_count,\n                               help=valid_signature_count_help, default=C.GALAXY_REQUIRED_VALID_SIGNATURE_COUNT)\n    verify_parser.add_argument('--ignore-signature-status-code', dest='ignore_gpg_errors', type=str, action='append',\n                               help=opt_help.argparse.SUPPRESS, default=C.GALAXY_IGNORE_INVALID_SIGNATURE_STATUS_CODES,\n                               choices=list(GPG_ERROR_MAP.keys()))\n    verify_parser.add_argument('--ignore-signature-status-codes', dest='ignore_gpg_errors', type=str, action='extend', nargs='+',\n                               help=ignore_gpg_status_help, default=C.GALAXY_IGNORE_INVALID_SIGNATURE_STATUS_CODES,\n                               choices=list(GPG_ERROR_MAP.keys()))", "loc": 35}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_build_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["build_parser.add_argument", "build_parser.set_defaults", "parser.add_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_build_options(self, parser, parents=None):\n    build_parser = parser.add_parser('build', parents=parents,\n                                     help='Build an Ansible collection artifact that can be published to Ansible '\n                                          'Galaxy.')\n    build_parser.set_defaults(func=self.execute_build)\n\n    build_parser.add_argument('args', metavar='collection', nargs='*', default=('.',),\n                              help='Path to the collection(s) directory to build. This should be the directory '\n                                   'that contains the galaxy.yml file. The default is the current working '\n                                   'directory.')\n    build_parser.add_argument('--output-path', dest='output_path', default='./',\n                              help='The path in which the collection is built to. The default is the current '\n                                   'working directory.')", "loc": 13}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "add_publish_options", "parameters": ["self", "parser", "parents"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_parser", "publish_parser.add_argument", "publish_parser.set_defaults"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_publish_options(self, parser, parents=None):\n    publish_parser = parser.add_parser('publish', parents=parents,\n                                       help='Publish a collection artifact to Ansible Galaxy.')\n    publish_parser.set_defaults(func=self.execute_publish)\n\n    publish_parser.add_argument('args', metavar='collection_path',\n                                help='The path to the collection tarball to publish.')\n    publish_parser.add_argument('--no-wait', dest='wait', action='store_false', default=True,\n                                help=\"Don't wait for import validation results.\")\n    publish_parser.add_argument('--import-timeout', dest='import_timeout', type=int, default=0,\n                                help=\"The time to wait for the collection import process to finish.\")", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["setattr", "super", "super(GalaxyCLI, self).post_process_args"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(GalaxyCLI, self).post_process_args(options)\n\n    # ensure we have 'usable' cli option\n    setattr(options, 'validate_certs', (None if options.ignore_certs is None else not options.ignore_certs))\n    # the default if validate_certs is None\n    setattr(options, 'resolved_validate_certs', (options.validate_certs if options.validate_certs is not None else not C.GALAXY_IGNORE_CERTS))\n\n    display.verbosity = options.verbosity\n    return options", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BasicAuthToken", "C.config.get_plugin_options", "C.config.load_galaxy_server_defs", "Galaxy", "GalaxyAPI", "GalaxyToken", "KeycloakToken", "RoleDistributionServer", "config_servers.append", "enumerate", "len", "next", "self.api_servers.append", "server_options.pop", "server_options.update", "super", "super(GalaxyCLI, self).run"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self):\n\n    super(GalaxyCLI, self).run()\n\n    self.galaxy = Galaxy()\n\n    # dynamically add per server config depending on declared servers\n    C.config.load_galaxy_server_defs(C.GALAXY_SERVER_LIST)\n\n    galaxy_options = {}\n    for optional_key in ['clear_response_cache', 'no_cache']:\n        if optional_key in context.CLIARGS:\n            galaxy_options[optional_key] = context.CLIARGS[optional_key]\n\n    config_servers = []\n    # Need to filter out empty strings or non truthy values as an empty server list env var is equal to [''].\n    server_list = [s for s in C.GALAXY_SERVER_LIST or [] if s]\n    for server_priority, server_key in enumerate(server_list, start=1):\n\n        # resolve the config created options above with existing config and user options\n        server_options = C.config.get_plugin_options(plugin_type='galaxy_server', name=server_key)\n\n        # auth_url is used to create the token, but not directly by GalaxyAPI, so\n        # it doesn't need to be passed as kwarg to GalaxyApi, same for others we pop here\n        auth_url = server_options.pop('auth_url')\n        client_id = server_options.pop('client_id')\n        client_secret = server_options.pop('client_secret')\n        token_val = server_options['token'] or NoTokenSentinel\n        username = server_options['username']\n        if server_options['validate_certs'] is None:\n            server_options['validate_certs'] = context.CLIARGS['resolved_validate_certs']\n        validate_certs = server_options['validate_certs']\n\n        # default case if no auth info is provided.\n        server_options['token'] = None\n\n        if username:\n            server_options['token'] = BasicAuthToken(username, server_options['password'])\n        else:\n            if auth_url:\n                server_options['token'] = KeycloakToken(\n                    access_token=token_val,\n                    auth_url=auth_url,\n                    validate_certs=validate_certs,\n                    client_id=client_id,\n                    client_secret=client_secret,\n                )\n            elif token_val:\n                # The galaxy v1 / github / django / 'Token'\n                server_options['token'] = GalaxyToken(token=token_val)\n\n        server_options.update(galaxy_options)\n        config_servers.append(GalaxyAPI(\n            self.galaxy, server_key,\n            priority=server_priority,\n            **server_options\n        ))\n\n    cmd_server = context.CLIARGS['api_server']\n\n    cmd_token = GalaxyToken(token=context.CLIARGS['api_key'])\n\n    validate_certs = context.CLIARGS['resolved_validate_certs']\n    default_server_timeout = context.CLIARGS['timeout'] if context.CLIARGS['timeout'] is not None else C.GALAXY_SERVER_TIMEOUT\n    if cmd_server:\n        # Cmd args take precedence over the config entry but fist check if the arg was a name and use that config\n        # entry, otherwise create a new API entry for the server specified.\n        config_server = next((s for s in config_servers if s.name == cmd_server), None)\n        if config_server:\n            self.api_servers.append(config_server)\n        else:\n            self.api_servers.append(GalaxyAPI(\n                self.galaxy, 'cmd_arg', cmd_server, token=cmd_token,\n                priority=len(config_servers) + 1,\n                validate_certs=validate_certs,\n                timeout=default_server_timeout,\n                **galaxy_options\n            ))\n    else:\n        self.api_servers = config_servers\n\n    # Default to C.GALAXY_SERVER if no servers were defined\n    if len(self.api_servers) == 0:\n        self.api_servers.append(GalaxyAPI(\n            self.galaxy, 'default', C.GALAXY_SERVER, token=cmd_token,\n            priority=0,\n            validate_certs=validate_certs,\n            timeout=default_server_timeout,\n            **galaxy_options\n        ))\n\n    # checks api versions once a GalaxyRole makes an api call\n    # self.api can be used to evaluate the best server immediately\n    self.lazy_role_api = RoleDistributionServer(None, self.api_servers)\n\n    return context.CLIARGS['func']()", "loc": 96}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "exit_without_ignore", "parameters": ["rc"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Exits with the specified return code unless the option --ignore-errors was specified", "source_code": "def exit_without_ignore(rc=1):\n    \"\"\"\n    Exits with the specified return code unless the\n    option --ignore-errors was specified\n    \"\"\"\n    if not context.CLIARGS['ignore_errors']:\n        raise AnsibleError('- you can use --ignore-errors to skip failed roles and finish processing the list.')", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "GalaxyRole", "RoleRequirement", "req.role_yaml_parse", "role_info.update", "self._display_role_info", "self.api.lookup_role_by_name", "self.pager"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "prints out detailed information about an installed role as well as info available from the galaxy API.", "source_code": "def execute_info(self):\n    \"\"\"\n    prints out detailed information about an installed role as well as info available from the galaxy API.\n    \"\"\"\n\n    roles_path = context.CLIARGS['roles_path']\n\n    data = ''\n    for role in context.CLIARGS['args']:\n\n        role_info = {'path': roles_path}\n        gr = GalaxyRole(self.galaxy, self.lazy_role_api, role)\n\n        install_info = gr.install_info\n        if install_info:\n            if 'version' in install_info:\n                install_info['installed_version'] = install_info['version']\n                del install_info['version']\n            role_info.update(install_info)\n\n        if not context.CLIARGS['offline']:\n            remote_data = None\n            try:\n                remote_data = self.api.lookup_role_by_name(role, False)\n            except GalaxyError as e:\n                if e.http_code == 400 and 'Bad Request' in e.message:\n                    # Role does not exist in Ansible Galaxy\n                    data = u\"- the role %s was not found\" % role\n                    break\n\n                raise AnsibleError(\"Unable to find info about '%s': %s\" % (role, e))\n\n            if remote_data:\n                role_info.update(remote_data)\n            else:\n                data = u\"- the role %s was not found\" % role\n                break\n\n        elif context.CLIARGS['offline'] and not gr._exists:\n            data = u\"- the role %s was not found\" % role\n            break\n\n        if gr.metadata:\n            role_info.update(gr.metadata)\n\n        req = RoleRequirement()\n        role_spec = req.role_yaml_parse({'role': role})\n        if role_spec:\n            role_info.update(role_spec)\n\n        data += self._display_role_info(role_info)\n\n    self.pager(data)", "loc": 53}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_verify", "parameters": ["self", "artifacts_manager"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GalaxyCLI._resolve_path", "any", "list", "self._require_one_of_collections_requirements", "validate_collection_path", "verify_collections"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Compare checksums with the collection(s) found on the server and the installed copy. This does not verify dependencies.", "source_code": "def execute_verify(self, artifacts_manager=None):\n    \"\"\"Compare checksums with the collection(s) found on the server and the installed copy. This does not verify dependencies.\"\"\"\n\n    collections = context.CLIARGS['args']\n    search_paths = self.collection_paths\n    ignore_errors = context.CLIARGS['ignore_errors']\n    local_verify_only = context.CLIARGS['offline']\n    requirements_file = context.CLIARGS['requirements']\n    signatures = context.CLIARGS['signatures']\n    if signatures is not None:\n        signatures = list(signatures)\n\n    requirements = self._require_one_of_collections_requirements(\n        collections, requirements_file,\n        signatures=signatures,\n        artifacts_manager=artifacts_manager,\n    )['collections']\n\n    resolved_paths = [validate_collection_path(GalaxyCLI._resolve_path(path)) for path in search_paths]\n\n    results = verify_collections(\n        requirements, resolved_paths,\n        self.api_servers, ignore_errors,\n        local_verify_only=local_verify_only,\n        artifacts_manager=artifacts_manager,\n    )\n\n    if any(result for result in results if not result.success):\n        return 1\n\n    return 0", "loc": 31}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_install", "parameters": ["self", "artifacts_manager"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleOptionsError", "GalaxyCLI._resolve_path", "GalaxyRole", "RoleRequirement.role_yaml_parse", "context.CLIARGS.get", "display.display", "display.vvv", "display_func", "list", "requirements_file.endswith", "rname.strip", "role_requirements.append", "self._execute_install_collection", "self._execute_install_role", "self._get_default_collection_path", "self._parse_requirements_file", "self._require_one_of_collections_requirements", "to_text", "two_type_warning.format"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Install one or more roles(``ansible-galaxy role install``), or one or more collections(``ansible-galaxy collection install``). You can pass in a list (roles or collections) or use the file option listed below (these are mutually exclusive). If you pass in a list, it", "source_code": "def execute_install(self, artifacts_manager=None):\n    \"\"\"\n    Install one or more roles(``ansible-galaxy role install``), or one or more collections(``ansible-galaxy collection install``).\n    You can pass in a list (roles or collections) or use the file\n    option listed below (these are mutually exclusive). If you pass in a list, it\n    can be a name (which will be downloaded via the galaxy API and github), or it can be a local tar archive file.\n    \"\"\"\n    install_items = context.CLIARGS['args']\n    requirements_file = context.CLIARGS['requirements']\n    collection_path = None\n    signatures = context.CLIARGS.get('signatures')\n    if signatures is not None:\n        signatures = list(signatures)\n\n    if requirements_file:\n        requirements_file = GalaxyCLI._resolve_path(requirements_file)\n\n    two_type_warning = \"The requirements file '%s' contains {0}s which will be ignored. To install these {0}s \" \\\n                       \"run 'ansible-galaxy {0} install -r' or to install both at the same time run \" \\\n                       \"'ansible-galaxy install -r' without a custom install path.\" % to_text(requirements_file)\n\n    # TODO: Would be nice to share the same behaviour with args and -r in collections and roles.\n    collection_requirements = []\n    role_requirements = []\n    if context.CLIARGS['type'] == 'collection':\n        collection_path = GalaxyCLI._resolve_path(context.CLIARGS['collections_path'])\n        requirements = self._require_one_of_collections_requirements(\n            install_items, requirements_file,\n            signatures=signatures,\n            artifacts_manager=artifacts_manager,\n        )\n\n        collection_requirements = requirements['collections']\n        if requirements['roles']:\n            display.vvv(two_type_warning.format('role'))\n    else:\n        if not install_items and requirements_file is None:\n            raise AnsibleOptionsError(\"- you must specify a user/role name or a roles file\")\n\n        if requirements_file:\n            if not (requirements_file.endswith('.yaml') or requirements_file.endswith('.yml')):\n                raise AnsibleError(\"Invalid role requirements file, it must end with a .yml or .yaml extension\")\n\n            galaxy_args = self._raw_args\n            will_install_collections = self._implicit_role and '-p' not in galaxy_args and '--roles-path' not in galaxy_args\n\n            requirements = self._parse_requirements_file(\n                requirements_file,\n                artifacts_manager=artifacts_manager,\n                validate_signature_options=will_install_collections,\n            )\n            role_requirements = requirements['roles']\n\n            # We can only install collections and roles at the same time if the type wasn't specified and the -p\n            # argument was not used. If collections are present in the requirements then at least display a msg.\n            if requirements['collections'] and (not self._implicit_role or '-p' in galaxy_args or\n                                                '--roles-path' in galaxy_args):\n\n                # We only want to display a warning if 'ansible-galaxy install -r ... -p ...'. Other cases the user\n                # was explicit about the type and shouldn't care that collections were skipped.\n                display_func = display.warning if self._implicit_role else display.vvv\n                display_func(two_type_warning.format('collection'))\n            else:\n                collection_path = self._get_default_collection_path()\n                collection_requirements = requirements['collections']\n        else:\n            # roles were specified directly, so we'll just go out grab them\n            # (and their dependencies, unless the user doesn't want us to).\n            for rname in context.CLIARGS['args']:\n                role = RoleRequirement.role_yaml_parse(rname.strip())\n                role_requirements.append(GalaxyRole(self.galaxy, self.lazy_role_api, **role))\n\n    if not role_requirements and not collection_requirements:\n        display.display(\"Skipping install, no requirements found\")\n        return\n\n    if role_requirements:\n        display.display(\"Starting galaxy role install process\")\n        self._execute_install_role(role_requirements)\n\n    if collection_requirements:\n        display.display(\"Starting galaxy collection install process\")\n        # Collections can technically be installed even when ansible-galaxy is in role mode so we need to pass in\n        # the install path as context.CLIARGS['collections_path'] won't be set (default is calculated above).\n        self._execute_install_collection(\n            collection_requirements, collection_path,\n            artifacts_manager=artifacts_manager,\n        )", "loc": 88}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_remove", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleOptionsError", "GalaxyRole", "display.display", "role.remove", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "removes the list of roles passed as arguments from the local system.", "source_code": "def execute_remove(self):\n    \"\"\"\n    removes the list of roles passed as arguments from the local system.\n    \"\"\"\n\n    if not context.CLIARGS['args']:\n        raise AnsibleOptionsError('- you must specify at least one role to remove.')\n\n    for role_name in context.CLIARGS['args']:\n        role = GalaxyRole(self.galaxy, self.api, role_name)\n        try:\n            if role.remove():\n                display.display('- successfully removed %s' % role_name)\n            else:\n                display.display('- %s is not installed, skipping.' % role_name)\n        except Exception as e:\n            raise AnsibleError(\"Failed to remove role %s: %s\" % (role_name, to_native(e)))\n\n    return 0", "loc": 19}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_list", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_list_collection", "self.execute_list_role"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "List installed collections or roles", "source_code": "def execute_list(self):\n    \"\"\"\n    List installed collections or roles\n    \"\"\"\n\n    if context.CLIARGS['type'] == 'role':\n        self.execute_list_role()\n    elif context.CLIARGS['type'] == 'collection':\n        self.execute_list_collection()", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_list_collection", "parameters": ["self", "artifacts_manager"], "param_types": {}, "return_type": null, "param_doc": {"artifacts_manager": "Artifacts manager."}, "return_doc": "", "raises_doc": [], "called_functions": ["'- None of the provided paths were usable. Please specify a valid path with --{0}s-path'.format", "'- the configured path {0} does not exist.'.format", "'- the configured path {0}, exists, but it is not a directory.'.format", "AnsibleOptionsError", "_display_collection", "_display_header", "_get_collection_widths", "collection_name.split", "collections_in_paths.setdefault", "display.display", "display.warning", "find_existing_collections", "json.dumps", "list", "os.path.exists", "os.path.isdir", "pathlib.Path", "pathlib.Path(to_text(collection.src)).parent.parent.as_posix", "seen.add", "set", "sorted", "to_text", "validate_collection_name", "warnings.append", "yaml_dump"], "control_structures": ["For", "If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "List all collections installed on the local system", "source_code": "def execute_list_collection(self, artifacts_manager=None):\n    \"\"\"\n    List all collections installed on the local system\n\n    :param artifacts_manager: Artifacts manager.\n    \"\"\"\n    if artifacts_manager is not None:\n        artifacts_manager.require_build_metadata = False\n\n    output_format = context.CLIARGS['output_format']\n    collection_name = context.CLIARGS['collection']\n    default_collections_path = set(C.COLLECTIONS_PATHS)\n    collections_search_paths = (\n        set(context.CLIARGS['collections_path'] or []) | default_collections_path | set(self.collection_paths)\n    )\n    collections_in_paths = {}\n\n    warnings = []\n    path_found = False\n    collection_found = False\n\n    namespace_filter = None\n    collection_filter = None\n    if collection_name:\n        # list a specific collection\n\n        validate_collection_name(collection_name)\n        namespace_filter, collection_filter = collection_name.split('.')\n\n    collections = list(find_existing_collections(\n        list(collections_search_paths),\n        artifacts_manager,\n        namespace_filter=namespace_filter,\n        collection_filter=collection_filter,\n        dedupe=False\n    ))\n\n    seen = set()\n    fqcn_width, version_width = _get_collection_widths(collections)\n    for collection in sorted(collections, key=lambda c: c.src):\n        collection_found = True\n        collection_path = pathlib.Path(to_text(collection.src)).parent.parent.as_posix()\n\n        if output_format in {'yaml', 'json'}:\n            collections_in_paths.setdefault(collection_path, {})\n            collections_in_paths[collection_path][collection.fqcn] = {'version': collection.ver}\n        else:\n            if collection_path not in seen:\n                _display_header(\n                    collection_path,\n                    'Collection',\n                    'Version',\n                    fqcn_width,\n                    version_width\n                )\n                seen.add(collection_path)\n            _display_collection(collection, fqcn_width, version_width)\n\n    path_found = False\n    for path in collections_search_paths:\n        if not os.path.exists(path):\n            if path in default_collections_path:\n                # don't warn for missing default paths\n                continue\n            warnings.append(\"- the configured path {0} does not exist.\".format(path))\n        elif os.path.exists(path) and not os.path.isdir(path):\n            warnings.append(\"- the configured path {0}, exists, but it is not a directory.\".format(path))\n        else:\n            path_found = True\n\n    # Do not warn if the specific collection was found in any of the search paths\n    if collection_found and collection_name:\n        warnings = []\n\n    for w in warnings:\n        display.warning(w)\n\n    if not collections and not path_found:\n        raise AnsibleOptionsError(\n            \"- None of the provided paths were usable. Please specify a valid path with --{0}s-path\".format(context.CLIARGS['type'])\n        )\n\n    if output_format == 'json':\n        display.display(json.dumps(collections_in_paths))\n    elif output_format == 'yaml':\n        display.display(yaml_dump(collections_in_paths))\n\n    return 0", "loc": 88}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_publish", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GalaxyCLI._resolve_path", "publish_collection"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Publish a collection into Ansible Galaxy. Requires the path to the collection tarball to publish.", "source_code": "def execute_publish(self):\n    \"\"\"\n    Publish a collection into Ansible Galaxy. Requires the path to the collection tarball to publish.\n    \"\"\"\n    collection_path = GalaxyCLI._resolve_path(context.CLIARGS['args'])\n    wait = context.CLIARGS['wait']\n    timeout = context.CLIARGS['import_timeout']\n\n    publish_collection(collection_path, self.api, wait, timeout)", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_search", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'+'.join", "AnsibleError", "data.append", "display.warning", "len", "max", "max_len.append", "self.api.search_roles", "self.pager", "u'\\n'.join"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "searches for roles on the Ansible Galaxy server", "source_code": "def execute_search(self):\n    \"\"\" searches for roles on the Ansible Galaxy server\"\"\"\n    page_size = 1000\n    search = None\n\n    if context.CLIARGS['args']:\n        search = '+'.join(context.CLIARGS['args'])\n\n    if not search and not context.CLIARGS['platforms'] and not context.CLIARGS['galaxy_tags'] and not context.CLIARGS['author']:\n        raise AnsibleError(\"Invalid query. At least one search term, platform, galaxy tag or author must be provided.\")\n\n    response = self.api.search_roles(search, platforms=context.CLIARGS['platforms'],\n                                     tags=context.CLIARGS['galaxy_tags'], author=context.CLIARGS['author'], page_size=page_size)\n\n    if response['count'] == 0:\n        display.warning(\"No roles match your search.\")\n        return 0\n\n    data = [u'']\n\n    if response['count'] > page_size:\n        data.append(u\"Found %d roles matching your search. Showing first %s.\" % (response['count'], page_size))\n    else:\n        data.append(u\"Found %d roles matching your search:\" % response['count'])\n\n    max_len = []\n    for role in response['results']:\n        max_len.append(len(role['username'] + '.' + role['name']))\n    name_len = max(max_len)\n    format_str = u\" %%-%ds %%s\" % name_len\n    data.append(u'')\n    data.append(format_str % (u\"Name\", u\"Description\"))\n    data.append(format_str % (u\"----\", u\"-----------\"))\n    for role in response['results']:\n        data.append(format_str % (u'%s.%s' % (role['username'], role['name']), role['description']))\n\n    data = u'\\n'.join(data)\n    self.pager(data)\n\n    return 0", "loc": 40}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_import", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["['SUCCESS', 'FAILED'].index", "display.display", "len", "msg_list.append", "self.api.create_import_task", "self.api.get_import_task", "time.sleep", "to_text"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "used to import a role into Ansible Galaxy", "source_code": "def execute_import(self):\n    \"\"\" used to import a role into Ansible Galaxy \"\"\"\n\n    colors = {\n        'INFO': 'normal',\n        'WARNING': C.COLOR_WARN,\n        'ERROR': C.COLOR_ERROR,\n        'SUCCESS': C.COLOR_OK,\n        'FAILED': C.COLOR_ERROR,\n    }\n\n    github_user = to_text(context.CLIARGS['github_user'], errors='surrogate_or_strict')\n    github_repo = to_text(context.CLIARGS['github_repo'], errors='surrogate_or_strict')\n\n    rc = 0\n    if context.CLIARGS['check_status']:\n        task = self.api.get_import_task(github_user=github_user, github_repo=github_repo)\n    else:\n        # Submit an import request\n        task = self.api.create_import_task(github_user, github_repo,\n                                           reference=context.CLIARGS['reference'],\n                                           role_name=context.CLIARGS['role_name'])\n\n        if len(task) > 1:\n            # found multiple roles associated with github_user/github_repo\n            display.display(\"WARNING: More than one Galaxy role associated with Github repo %s/%s.\" % (github_user, github_repo),\n                            color='yellow')\n            display.display(\"The following Galaxy roles are being updated:\" + u'\\n', color=C.COLOR_CHANGED)\n            for t in task:\n                display.display('%s.%s' % (t['summary_fields']['role']['namespace'], t['summary_fields']['role']['name']), color=C.COLOR_CHANGED)\n            display.display(u'\\nTo properly namespace this role, remove each of the above and re-import %s/%s from scratch' % (github_user, github_repo),\n                            color=C.COLOR_CHANGED)\n            return rc\n        # found a single role as expected\n        display.display(\"Successfully submitted import request %d\" % task[0]['id'])\n        if not context.CLIARGS['wait']:\n            display.display(\"Role name: %s\" % task[0]['summary_fields']['role']['name'])\n            display.display(\"Repo: %s/%s\" % (task[0]['github_user'], task[0]['github_repo']))\n\n    if context.CLIARGS['check_status'] or context.CLIARGS['wait']:\n        # Get the status of the import\n        msg_list = []\n        finished = False\n        while not finished:\n            task = self.api.get_import_task(task_id=task[0]['id'])\n            for msg in task[0]['summary_fields']['task_messages']:\n                if msg['id'] not in msg_list:\n                    display.display(msg['message_text'], color=colors[msg['message_type']])\n                    msg_list.append(msg['id'])\n            if (state := task[0]['state']) in ['SUCCESS', 'FAILED']:\n                rc = ['SUCCESS', 'FAILED'].index(state)\n                finished = True\n            else:\n                time.sleep(self._task_check_delay_sec)\n\n    return rc", "loc": 56}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_setup", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "len", "self.api.add_secret", "self.api.list_secrets", "self.api.remove_secret"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Setup an integration from Github or Travis for Ansible Galaxy roles", "source_code": "def execute_setup(self):\n    \"\"\" Setup an integration from Github or Travis for Ansible Galaxy roles\"\"\"\n\n    if context.CLIARGS['setup_list']:\n        # List existing integration secrets\n        secrets = self.api.list_secrets()\n        if len(secrets) == 0:\n            # None found\n            display.display(\"No integrations found.\")\n            return 0\n        display.display(u'\\n' + \"ID         Source     Repo\", color=C.COLOR_OK)\n        display.display(\"---------- ---------- ----------\", color=C.COLOR_OK)\n        for secret in secrets:\n            display.display(\"%-10s %-10s %s/%s\" % (secret['id'], secret['source'], secret['github_user'],\n                                                   secret['github_repo']), color=C.COLOR_OK)\n        return 0\n\n    if context.CLIARGS['remove_id']:\n        # Remove a secret\n        self.api.remove_secret(context.CLIARGS['remove_id'])\n        display.display(\"Secret removed. Integrations using this secret will not longer work.\", color=C.COLOR_OK)\n        return 0\n\n    source = context.CLIARGS['source']\n    github_user = context.CLIARGS['github_user']\n    github_repo = context.CLIARGS['github_repo']\n    secret = context.CLIARGS['secret']\n\n    resp = self.api.add_secret(source, github_user, github_repo, secret)\n    display.display(\"Added integration for %s %s/%s\" % (resp['source'], resp['github_user'], resp['github_repo']))\n\n    return 0", "loc": 32}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": "GalaxyCLI", "function_name": "execute_delete", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "len", "self.api.delete_role"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Delete a role from Ansible Galaxy.", "source_code": "def execute_delete(self):\n    \"\"\" Delete a role from Ansible Galaxy. \"\"\"\n\n    github_user = context.CLIARGS['github_user']\n    github_repo = context.CLIARGS['github_repo']\n    resp = self.api.delete_role(github_user, github_repo)\n\n    if len(resp['deleted_roles']) > 1:\n        display.display(\"Deleted the following roles:\")\n        display.display(\"ID     User            Name\")\n        display.display(\"------ --------------- ----------\")\n        for role in resp['deleted_roles']:\n            display.display(\"%-8s %-15s %s\" % (role.id, role.namespace, role.name))\n\n    display.display(resp['status'])\n\n    return 0", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\galaxy.py", "class_name": null, "function_name": "comment_ify", "parameters": ["v"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'. '.join", "const_pattern.sub", "isinstance", "l.rstrip", "link_pattern.sub", "textwrap.fill"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def comment_ify(v):\n    if isinstance(v, list):\n        v = \". \".join([l.rstrip('.') for l in v])\n\n    v = link_pattern.sub(r\"\\1 <\\2>\", v)\n    v = const_pattern.sub(r\"'\\1'\", v)\n\n    return textwrap.fill(v, width=117, initial_indent=\"# \", subsequent_indent=\"# \", break_on_hyphens=False)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": null, "function_name": "toml_dumps", "parameters": ["data"], "param_types": {"data": "t.Any"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleRuntimeError", "_tomli_w_dumps"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def toml_dumps(data: t.Any) -> str:\n    try:\n        from tomli_w import dumps as _tomli_w_dumps\n    except ImportError:\n        pass\n    else:\n        return _tomli_w_dumps(data)\n\n    raise AnsibleRuntimeError('The Python library \"tomli-w\" is required when using the TOML output format.')", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["action_group.add_argument", "opt_help.add_basedir_options", "opt_help.add_inventory_options", "opt_help.add_runtask_options", "opt_help.add_vault_options", "self.parser.add_argument", "self.parser.add_argument_group", "super", "super(InventoryCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def init_parser(self):\n    super(InventoryCLI, self).init_parser(\n        desc='Show Ansible inventory information, by default it uses the inventory script JSON format')\n\n    opt_help.add_inventory_options(self.parser)\n    opt_help.add_vault_options(self.parser)\n    opt_help.add_basedir_options(self.parser)\n    opt_help.add_runtask_options(self.parser)\n\n    # remove unused default options\n    self.parser.add_argument('--list-hosts', help=argparse.SUPPRESS, action=opt_help.UnrecognizedArgument)\n\n    self.parser.add_argument('args', metavar='group', nargs='?', help='The name of a group in the inventory, relevant when using --graph')\n\n    # Actions\n    action_group = self.parser.add_argument_group(\"Actions\", \"One of following must be used on invocation, ONLY ONE!\")\n    action_group.add_argument(\"--list\", action=\"store_true\", default=False, dest='list', help='Output all hosts info, works as inventory script')\n    action_group.add_argument(\"--host\", action=\"store\", default=None, dest='host',\n                              help='Output specific host info, works as inventory script. It will ignore limit')\n    action_group.add_argument(\"--graph\", action=\"store_true\", default=False, dest='graph',\n                              help='create inventory graph, if supplying pattern it must be a valid group name. It will ignore limit')\n    self.parser.add_argument_group(action_group)\n\n    # graph\n    self.parser.add_argument(\"-y\", \"--yaml\", action=\"store_true\", default=False, dest='yaml',\n                             help='Use YAML format instead of default JSON, ignored for --graph')\n    self.parser.add_argument('--toml', action='store_true', default=False, dest='toml',\n                             help='Use TOML format instead of default JSON, ignored for --graph')\n    self.parser.add_argument(\"--vars\", action=\"store_true\", default=False, dest='show_vars',\n                             help='Add vars to graph display, ignored unless used with --graph')\n\n    # list\n    self.parser.add_argument(\"--export\", action=\"store_true\", default=C.INVENTORY_EXPORT, dest='export',\n                             help=\"When doing --list, represent in a way that is optimized for export,\"\n                                  \"not as an accurate representation of how Ansible has processed it\")\n    self.parser.add_argument('--output', default=None, dest='output_file',\n                             help=\"When doing --list, send the inventory to a file instead of to the screen\")", "loc": 37}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "self.validate_conflicts", "super", "super(InventoryCLI, self).post_process_args"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(InventoryCLI, self).post_process_args(options)\n\n    display.verbosity = options.verbosity\n    self.validate_conflicts(options)\n\n    # there can be only one! and, at least, one!\n    used = 0\n    for opt in (options.list, options.host, options.graph):\n        if opt:\n            used += 1\n    if used == 0:\n        raise AnsibleOptionsError(\"No action selected, at least one of --host, --graph or --list needs to be specified.\")\n    elif used > 1:\n        raise AnsibleOptionsError(\"Conflicting options used, only one of --host, --graph or --list can be used at the same time.\")\n\n    # set host pattern to default if not supplied\n    if options.args:\n        options.pattern = options.args\n    else:\n        options.pattern = 'all'\n\n    return options", "loc": 23}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "dump", "parameters": ["stuff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "to_text", "toml_dumps", "yaml.dump"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def dump(stuff):\n    if context.CLIARGS['yaml']:\n        import yaml\n\n        from ansible.parsing.yaml.dumper import AnsibleDumper\n\n        results = to_text(yaml.dump(stuff, Dumper=AnsibleDumper, default_flow_style=False, allow_unicode=True))\n    elif context.CLIARGS['toml']:\n        results = toml_dumps(stuff)\n    else:\n        results = json.dumps(stuff, cls=_inventory_legacy.Encoder, sort_keys=True, indent=4)\n\n    return results", "loc": 13}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "inventory_graph", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "AnsibleOptionsError", "self._get_group", "self._graph_group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inventory_graph(self):\n\n    start_at = self._get_group(context.CLIARGS['pattern'])\n    if start_at:\n        return '\\n'.join(self._graph_group(start_at))\n    else:\n        raise AnsibleOptionsError(\"Pattern must be valid group name when using --graph\")", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "json_inventory", "parameters": ["self", "top"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_group", "frozenset", "results.update", "results[group.name]['children'].append", "seen_groups.add", "self._get_group_variables", "self._get_host_variables", "self._remove_empty_keys", "self.inventory.get_hosts", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def json_inventory(self, top):\n\n    seen_groups = set()\n\n    def format_group(group, available_hosts):\n        results = {}\n        results[group.name] = {}\n        if group.name != 'all':\n            results[group.name]['hosts'] = [h.name for h in group.hosts if h.name in available_hosts]\n        results[group.name]['children'] = []\n        for subgroup in group.child_groups:\n            results[group.name]['children'].append(subgroup.name)\n            if subgroup.name not in seen_groups:\n                results.update(format_group(subgroup, available_hosts))\n                seen_groups.add(subgroup.name)\n        if context.CLIARGS['export']:\n            results[group.name]['vars'] = self._get_group_variables(group)\n\n        self._remove_empty_keys(results[group.name])\n        # remove empty groups\n        if not results[group.name]:\n            del results[group.name]\n\n        return results\n\n    hosts = self.inventory.get_hosts(top.name)\n    results = format_group(top, frozenset(h.name for h in hosts))\n\n    # populate meta\n    results['_meta'] = {\n        'hostvars': {},\n        'profile': _inventory_legacy.Encoder.profile_name,\n    }\n\n    for host in hosts:\n        hvars = self._get_host_variables(host)\n        if hvars:\n            results['_meta']['hostvars'][host.name] = hvars\n\n    return results", "loc": 40}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "yaml_inventory", "parameters": ["self", "top"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_group", "frozenset", "results[group.name]['children'].update", "seen_groups.add", "seen_hosts.add", "self._get_group_variables", "self._get_host_variables", "self._remove_empty_keys", "self.inventory.get_hosts", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yaml_inventory(self, top):\n\n    seen_hosts = set()\n    seen_groups = set()\n\n    def format_group(group, available_hosts):\n        results = {}\n\n        # initialize group + vars\n        results[group.name] = {}\n\n        # subgroups\n        results[group.name]['children'] = {}\n        for subgroup in group.child_groups:\n            if subgroup.name != 'all':\n                if subgroup.name in seen_groups:\n                    results[group.name]['children'].update({subgroup.name: {}})\n                else:\n                    results[group.name]['children'].update(format_group(subgroup, available_hosts))\n                    seen_groups.add(subgroup.name)\n\n        # hosts for group\n        results[group.name]['hosts'] = {}\n        if group.name != 'all':\n            for h in group.hosts:\n                if h.name not in available_hosts:\n                    continue  # observe limit\n                myvars = {}\n                if h.name not in seen_hosts:  # avoid defining host vars more than once\n                    seen_hosts.add(h.name)\n                    myvars = self._get_host_variables(host=h)\n                results[group.name]['hosts'][h.name] = myvars\n\n        if context.CLIARGS['export']:\n            gvars = self._get_group_variables(group)\n            if gvars:\n                results[group.name]['vars'] = gvars\n\n        self._remove_empty_keys(results[group.name])\n        # remove empty groups\n        if not results[group.name]:\n            del results[group.name]\n\n        return results\n\n    available_hosts = frozenset(h.name for h in self.inventory.get_hosts(top.name))\n    return format_group(top, available_hosts)", "loc": 47}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": "InventoryCLI", "function_name": "toml_inventory", "parameters": ["self", "top"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "format_group", "frozenset", "next", "results.update", "results[group.name]['children'].append", "seen_hosts.add", "self._get_group_variables", "self._get_host_variables", "self._remove_empty_keys", "self.inventory.get_hosts", "set"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def toml_inventory(self, top):\n    seen_hosts = set()\n    seen_hosts = set()\n    has_ungrouped = bool(next(g.hosts for g in top.child_groups if g.name == 'ungrouped'))\n\n    def format_group(group, available_hosts):\n        results = {}\n        results[group.name] = {}\n\n        results[group.name]['children'] = []\n        for subgroup in group.child_groups:\n            if subgroup.name == 'ungrouped' and not has_ungrouped:\n                continue\n            if group.name != 'all':\n                results[group.name]['children'].append(subgroup.name)\n            results.update(format_group(subgroup, available_hosts))\n\n        if group.name != 'all':\n            for host in group.hosts:\n                if host.name not in available_hosts:\n                    continue\n                if host.name not in seen_hosts:\n                    seen_hosts.add(host.name)\n                    host_vars = self._get_host_variables(host=host)\n                else:\n                    host_vars = {}\n                try:\n                    results[group.name]['hosts'][host.name] = host_vars\n                except KeyError:\n                    results[group.name]['hosts'] = {host.name: host_vars}\n\n        if context.CLIARGS['export']:\n            results[group.name]['vars'] = self._get_group_variables(group)\n\n        self._remove_empty_keys(results[group.name])\n        # remove empty groups\n        if not results[group.name]:\n            del results[group.name]\n\n        return results\n\n    available_hosts = frozenset(h.name for h in self.inventory.get_hosts(top.name))\n    results = format_group(top, available_hosts)\n\n    return results", "loc": 45}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": null, "function_name": "format_group", "parameters": ["group", "available_hosts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_group", "results.update", "results[group.name]['children'].append", "seen_groups.add", "self._get_group_variables", "self._remove_empty_keys"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_group(group, available_hosts):\n    results = {}\n    results[group.name] = {}\n    if group.name != 'all':\n        results[group.name]['hosts'] = [h.name for h in group.hosts if h.name in available_hosts]\n    results[group.name]['children'] = []\n    for subgroup in group.child_groups:\n        results[group.name]['children'].append(subgroup.name)\n        if subgroup.name not in seen_groups:\n            results.update(format_group(subgroup, available_hosts))\n            seen_groups.add(subgroup.name)\n    if context.CLIARGS['export']:\n        results[group.name]['vars'] = self._get_group_variables(group)\n\n    self._remove_empty_keys(results[group.name])\n    # remove empty groups\n    if not results[group.name]:\n        del results[group.name]\n\n    return results", "loc": 20}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": null, "function_name": "format_group", "parameters": ["group", "available_hosts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_group", "results[group.name]['children'].update", "seen_groups.add", "seen_hosts.add", "self._get_group_variables", "self._get_host_variables", "self._remove_empty_keys"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_group(group, available_hosts):\n    results = {}\n\n    # initialize group + vars\n    results[group.name] = {}\n\n    # subgroups\n    results[group.name]['children'] = {}\n    for subgroup in group.child_groups:\n        if subgroup.name != 'all':\n            if subgroup.name in seen_groups:\n                results[group.name]['children'].update({subgroup.name: {}})\n            else:\n                results[group.name]['children'].update(format_group(subgroup, available_hosts))\n                seen_groups.add(subgroup.name)\n\n    # hosts for group\n    results[group.name]['hosts'] = {}\n    if group.name != 'all':\n        for h in group.hosts:\n            if h.name not in available_hosts:\n                continue  # observe limit\n            myvars = {}\n            if h.name not in seen_hosts:  # avoid defining host vars more than once\n                seen_hosts.add(h.name)\n                myvars = self._get_host_variables(host=h)\n            results[group.name]['hosts'][h.name] = myvars\n\n    if context.CLIARGS['export']:\n        gvars = self._get_group_variables(group)\n        if gvars:\n            results[group.name]['vars'] = gvars\n\n    self._remove_empty_keys(results[group.name])\n    # remove empty groups\n    if not results[group.name]:\n        del results[group.name]\n\n    return results", "loc": 39}
{"file": "ansible\\lib\\ansible\\cli\\inventory.py", "class_name": null, "function_name": "format_group", "parameters": ["group", "available_hosts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_group", "results.update", "results[group.name]['children'].append", "seen_hosts.add", "self._get_group_variables", "self._get_host_variables", "self._remove_empty_keys"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_group(group, available_hosts):\n    results = {}\n    results[group.name] = {}\n\n    results[group.name]['children'] = []\n    for subgroup in group.child_groups:\n        if subgroup.name == 'ungrouped' and not has_ungrouped:\n            continue\n        if group.name != 'all':\n            results[group.name]['children'].append(subgroup.name)\n        results.update(format_group(subgroup, available_hosts))\n\n    if group.name != 'all':\n        for host in group.hosts:\n            if host.name not in available_hosts:\n                continue\n            if host.name not in seen_hosts:\n                seen_hosts.add(host.name)\n                host_vars = self._get_host_variables(host=host)\n            else:\n                host_vars = {}\n            try:\n                results[group.name]['hosts'][host.name] = host_vars\n            except KeyError:\n                results[group.name]['hosts'] = {host.name: host_vars}\n\n    if context.CLIARGS['export']:\n        results[group.name]['vars'] = self._get_group_variables(group)\n\n    self._remove_empty_keys(results[group.name])\n    # remove empty groups\n    if not results[group.name]:\n        del results[group.name]\n\n    return results", "loc": 35}
{"file": "ansible\\lib\\ansible\\cli\\playbook.py", "class_name": "PlaybookCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt_help.add_check_options", "opt_help.add_connect_options", "opt_help.add_fork_options", "opt_help.add_inventory_options", "opt_help.add_meta_options", "opt_help.add_module_options", "opt_help.add_runas_options", "opt_help.add_runtask_options", "opt_help.add_subset_options", "opt_help.add_vault_options", "self.parser.add_argument", "super", "super(PlaybookCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def init_parser(self):\n\n    # create parser for CLI options\n    super(PlaybookCLI, self).init_parser(\n        desc=\"Runs Ansible playbooks, executing the defined tasks on the targeted hosts.\")\n\n    opt_help.add_connect_options(self.parser)\n    opt_help.add_meta_options(self.parser)\n    opt_help.add_runas_options(self.parser)\n    opt_help.add_subset_options(self.parser)\n    opt_help.add_check_options(self.parser)\n    opt_help.add_inventory_options(self.parser)\n    opt_help.add_runtask_options(self.parser)\n    opt_help.add_vault_options(self.parser)\n    opt_help.add_fork_options(self.parser)\n    opt_help.add_module_options(self.parser)\n\n    # ansible playbook specific opts\n    self.parser.add_argument('--syntax-check', dest='syntax', action='store_true',\n                             help=\"perform a syntax check on the playbook, but do not execute it\")\n    self.parser.add_argument('--list-tasks', dest='listtasks', action='store_true',\n                             help=\"list all tasks that would be executed\")\n    self.parser.add_argument('--list-tags', dest='listtags', action='store_true',\n                             help=\"list all available tags\")\n    self.parser.add_argument('--step', dest='step', action='store_true',\n                             help=\"one-step-at-a-time: confirm each task before running\")\n    self.parser.add_argument('--start-at-task', dest='start_at_task',\n                             help=\"start the playbook at the task matching this name\")\n    self.parser.add_argument('args', help='Playbook(s)', metavar='playbook', nargs='+')", "loc": 29}
{"file": "ansible\\lib\\ansible\\cli\\playbook.py", "class_name": "PlaybookCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "self.validate_conflicts", "super", "super(PlaybookCLI, self).post_process_args"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n\n    # for listing, we need to know if user had tag input\n    # capture here as parent function sets defaults for tags\n    havetags = bool(options.tags or options.skip_tags)\n\n    options = super(PlaybookCLI, self).post_process_args(options)\n\n    if options.listtags:\n        # default to all tags (including never), when listing tags\n        # unless user specified tags\n        if not havetags:\n            options.tags = ['never', 'all']\n\n    display.verbosity = options.verbosity\n    self.validate_conflicts(options, runas_opts=True, fork_opts=True)\n\n    return options", "loc": 18}
{"file": "ansible\\lib\\ansible\\cli\\pull.py", "class_name": null, "function_name": "safe_output_env", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SAFE_OUTPUT_ENV.items", "f", "orig.keys", "os.environ.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def safe_output_env(f):\n\n    def wrapper(*args, **kwargs):\n\n        orig = {}\n\n        for k, v in SAFE_OUTPUT_ENV.items():\n            orig[k] = os.environ.get(k, None)\n            os.environ[k] = v\n\n        result = f(*args, **kwargs)\n\n        for key in orig.keys():\n            if orig[key] is None:\n                del os.environ[key]\n            else:\n                os.environ[key] = orig[key]\n\n        return result\n\n    return wrapper", "loc": 21}
{"file": "ansible\\lib\\ansible\\cli\\pull.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SAFE_OUTPUT_ENV.items", "f", "orig.keys", "os.environ.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n\n    orig = {}\n\n    for k, v in SAFE_OUTPUT_ENV.items():\n        orig[k] = os.environ.get(k, None)\n        os.environ[k] = v\n\n    result = f(*args, **kwargs)\n\n    for key in orig.keys():\n        if orig[key] is None:\n            del os.environ[key]\n        else:\n            os.environ[key] = orig[key]\n\n    return result", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\pull.py", "class_name": "PullCLI", "function_name": "init_parser", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt_help.add_connect_options", "opt_help.add_inventory_options", "opt_help.add_module_options", "opt_help.add_runas_prompt_options", "opt_help.add_runtask_options", "opt_help.add_subset_options", "opt_help.add_vault_options", "opt_help.unfrack_path", "self.parser.add_argument", "super", "super(PullCLI, self).init_parser"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Specific args/option parser for pull", "source_code": "def init_parser(self):\n    \"\"\" Specific args/option parser for pull \"\"\"\n\n    # signature is different from parent as caller should not need to add usage/desc\n    super(PullCLI, self).init_parser(\n        desc=\"pulls playbooks from a VCS repo and executes them on target host\")\n\n    # Do not add check_options as there's a conflict with --checkout/-C\n    opt_help.add_connect_options(self.parser)\n    opt_help.add_vault_options(self.parser)\n    opt_help.add_runtask_options(self.parser)\n    opt_help.add_subset_options(self.parser)\n    opt_help.add_inventory_options(self.parser)\n    opt_help.add_module_options(self.parser)\n    opt_help.add_runas_prompt_options(self.parser)\n\n    self.parser.add_argument('args', help='Playbook(s)', metavar='playbook.yml', nargs='*')\n\n    # options unique to pull\n    self.parser.add_argument('--purge', default=False, action='store_true', help='purge checkout after playbook run')\n    self.parser.add_argument('-o', '--only-if-changed', dest='ifchanged', default=False, action='store_true',\n                             help='only run the playbook if the repository has been updated')\n    self.parser.add_argument('-s', '--sleep', dest='sleep', default=None,\n                             help='sleep for random interval (between 0 and n number of seconds) before starting. '\n                                  'This is a useful way to disperse git requests')\n    self.parser.add_argument('-f', '--force', dest='force', default=False, action='store_true',\n                             help='run the playbook even if the repository could not be updated')\n    self.parser.add_argument('-d', '--directory', dest='dest', default=None, type=opt_help.unfrack_path(),\n                             help='path to the directory to which Ansible will checkout the repository.')\n    self.parser.add_argument('-U', '--url', dest='url', default=None, help='URL of the playbook repository')\n    self.parser.add_argument('--full', dest='fullclone', action='store_true', help='Do a full clone, instead of a shallow one.')\n    # TODO: resolve conflict with check mode, added manually below\n    self.parser.add_argument('-C', '--checkout', dest='checkout',\n                             help='branch/tag/commit to checkout. Defaults to behavior of repository module.')\n    self.parser.add_argument('--accept-host-key', default=False, dest='accept_host_key', action='store_true',\n                             help='adds the hostkey for the repo url if not already added')\n    # Overloaded with adhoc ... but really passthrough to adhoc\n    self.parser.add_argument('-m', '--module-name', dest='module_name', default=self.DEFAULT_REPO_TYPE,\n                             help='Repository module name, which ansible will use to check out the repo. Choices are %s. Default is %s.'\n                                  % (self.REPO_CHOICES, self.DEFAULT_REPO_TYPE))\n    self.parser.add_argument('--verify-commit', dest='verify', default=False, action='store_true',\n                             help='verify GPG signature of checked out commit, if it fails abort running the playbook. '\n                                  'This needs the corresponding VCS module to support such an operation')\n    self.parser.add_argument('--clean', dest='clean', default=False, action='store_true',\n                             help='modified files in the working repository will be discarded')\n    self.parser.add_argument('--track-subs', dest='tracksubs', default=False, action='store_true',\n                             help='submodules will track the latest changes. This is equivalent to specifying the --remote flag to git submodule update')\n    # add a subset of the check_opts flag group manually, as the full set's\n    # shortcodes conflict with above --checkout/-C, see to-do above\n    self.parser.add_argument(\"--check\", default=False, dest='check', action='store_true',\n                             help=\"don't make any changes; instead, try to predict some of the changes that may occur\")\n    self.parser.add_argument(\"--diff\", default=C.DIFF_ALWAYS, dest='diff', action='store_true',\n                             help=\"when changing (small) files and templates, show the differences in those files; works great with --check\")", "loc": 53}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "getattr", "len", "super", "super(VaultCLI, self).post_process_args"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_process_args(self, options):\n    options = super(VaultCLI, self).post_process_args(options)\n\n    display.verbosity = options.verbosity\n\n    if options.vault_ids:\n        for vault_id in options.vault_ids:\n            if u';' in vault_id:\n                raise AnsibleOptionsError(\"'%s' is not a valid vault id. The character ';' is not allowed in vault ids\" % vault_id)\n\n    if getattr(options, 'output_file', None) and len(options.args) > 1:\n        raise AnsibleOptionsError(\"At most one input file may be used with the --output option\")\n\n    if options.action == 'encrypt_string':\n        if '-' in options.args or options.encrypt_string_stdin_name or (not options.args and not options.encrypt_string_prompt):\n            # prompting from stdin and reading from stdin are mutually exclusive, if stdin is still provided, it is ignored\n            self.encrypt_string_read_stdin = True\n\n        if options.encrypt_string_prompt and self.encrypt_string_read_stdin:\n            # should only trigger if prompt + either - or encrypt string stdin name were provided\n            raise AnsibleOptionsError('The --prompt option is not supported if also reading input from stdin')\n\n    return options", "loc": 23}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "execute_encrypt", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "self.editor.encrypt_file", "sys.stdin.isatty", "sys.stdout.isatty"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "encrypt the supplied file using the provided vault secret", "source_code": "def execute_encrypt(self):\n    \"\"\" encrypt the supplied file using the provided vault secret \"\"\"\n\n    if not context.CLIARGS['args'] and sys.stdin.isatty():\n        display.display(\"Reading plaintext input from stdin\", stderr=True)\n\n    for f in context.CLIARGS['args'] or ['-']:\n        # FIXME: use the correct vau\n        self.editor.encrypt_file(f, self.encrypt_secret,\n                                 vault_id=self.encrypt_vault_id,\n                                 output_file=context.CLIARGS['output_file'])\n\n    if sys.stdout.isatty():\n        display.display(\"Encryption successful\", stderr=True)", "loc": 14}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "format_ciphertext_yaml", "parameters": ["b_ciphertext", "indent", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "lines.append", "to_text", "vault_ciphertext.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_ciphertext_yaml(b_ciphertext, indent=None, name=None):\n    indent = indent or 10\n\n    block_format_var_name = \"\"\n    if name:\n        block_format_var_name = \"%s: \" % name\n\n    block_format_header = \"%s!vault |\" % block_format_var_name\n    lines = []\n    vault_ciphertext = to_text(b_ciphertext)\n\n    lines.append(block_format_header)\n    for line in vault_ciphertext.splitlines():\n        lines.append('%s%s' % (' ' * indent, line))\n\n    yaml_ciphertext = '\\n'.join(lines)\n    return yaml_ciphertext", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "execute_decrypt", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "self.editor.decrypt_file", "sys.stdin.isatty", "sys.stdout.isatty"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "decrypt the supplied file using the provided vault secret", "source_code": "def execute_decrypt(self):\n    \"\"\" decrypt the supplied file using the provided vault secret \"\"\"\n\n    if not context.CLIARGS['args'] and sys.stdin.isatty():\n        display.display(\"Reading ciphertext input from stdin\", stderr=True)\n\n    for f in context.CLIARGS['args'] or ['-']:\n        self.editor.decrypt_file(f, output_file=context.CLIARGS['output_file'])\n\n    if sys.stdout.isatty():\n        display.display(\"Decryption successful\", stderr=True)", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "execute_create", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "len", "self.editor.create_file", "sys.stdout.isatty"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "create and open a file in an editor that will be encrypted with the provided vault secret when closed", "source_code": "def execute_create(self):\n    \"\"\" create and open a file in an editor that will be encrypted with the provided vault secret when closed\"\"\"\n\n    if len(context.CLIARGS['args']) != 1:\n        raise AnsibleOptionsError(\"ansible-vault create can take only one filename argument\")\n\n    if sys.stdout.isatty() or context.CLIARGS['skip_tty_check']:\n        self.editor.create_file(context.CLIARGS['args'][0], self.encrypt_secret,\n                                vault_id=self.encrypt_vault_id)\n    else:\n        raise AnsibleOptionsError(\"not a tty, editor cannot be opened\")", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "execute_view", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.editor.plaintext", "self.pager", "to_text"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "open, decrypt and view an existing vaulted file using a pager using the supplied vault secret", "source_code": "def execute_view(self):\n    \"\"\" open, decrypt and view an existing vaulted file using a pager using the supplied vault secret \"\"\"\n\n    for f in context.CLIARGS['args']:\n        # Note: vault should return byte strings because it could encrypt\n        # and decrypt binary files.  We are responsible for changing it to\n        # unicode here because we are displaying it and therefore can make\n        # the decision that the display doesn't have to be precisely what\n        # the input was (leave that to decrypt instead)\n        plaintext = self.editor.plaintext(f)\n        self.pager(to_text(plaintext))", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\vault.py", "class_name": "VaultCLI", "function_name": "execute_rekey", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "self.editor.rekey_file"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "re-encrypt a vaulted file with a new secret, the previous secret is required", "source_code": "def execute_rekey(self):\n    \"\"\" re-encrypt a vaulted file with a new secret, the previous secret is required \"\"\"\n    for f in context.CLIARGS['args']:\n        # FIXME: plumb in vault_id, use the default new_vault_secret for now\n        self.editor.rekey_file(f, self.new_encrypt_secret,\n                               self.new_encrypt_vault_id)\n\n    display.display(\"Rekey successful\", stderr=True)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\_ssh_askpass.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "t.Never", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["handle_prompt", "len", "sys.exit"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main() -> t.Never:\n    if len(sys.argv) > 1:\n        exit_code = 0 if handle_prompt(sys.argv[1]) else 1\n    else:\n        exit_code = 1\n\n    sys.exit(exit_code)", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\_ssh_askpass.py", "class_name": null, "function_name": "handle_prompt", "parameters": ["prompt"], "param_types": {"prompt": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SharedMemory", "dict", "json.loads", "multiprocessing.resource_tracker.unregister", "re.search", "shm.buf.tobytes", "shm.buf.tobytes().rstrip", "sys.stdout.flush", "sys.stdout.write"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def handle_prompt(prompt: str) -> bool:\n    if re.search(r'(The authenticity of host |differs from the key for the IP address)', prompt):\n        sys.stdout.write('no')\n        sys.stdout.flush()\n        return True\n\n    # deprecated: description='Python 3.13 and later support track' python_version='3.12'\n    can_track = sys.version_info[:2] >= (3, 13)\n    kwargs = dict(track=False) if can_track else {}\n\n    # This SharedMemory instance is intentionally not closed or unlinked.\n    # Closing will occur naturally in the SharedMemory finalizer.\n    # Unlinking is the responsibility of the process which created it.\n    shm = SharedMemory(name=os.environ['_ANSIBLE_SSH_ASKPASS_SHM'], **kwargs)\n\n    if not can_track:\n        # When track=False is not available, we must unregister explicitly, since it otherwise only occurs during unlink.\n        # This avoids resource tracker noise on stderr during process exit.\n        multiprocessing.resource_tracker.unregister(shm._name, 'shared_memory')\n\n    cfg = json.loads(shm.buf.tobytes().rstrip(b'\\x00'))\n\n    if cfg['prompt'] not in prompt:\n        return False\n\n    # Report the password provided by the SharedMemory instance.\n    # The contents are left untouched after consumption to allow subsequent attempts to succeed.\n    # This can occur when multiple password prompting methods are enabled, such as password and keyboard-interactive, which is the default on macOS.\n    sys.stdout.write(cfg['password'])\n    sys.stdout.flush()\n    return True", "loc": 31}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": null, "function_name": "check_blocking_io", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "SystemExit", "getattr", "handle.fileno", "handles.append", "os.get_blocking"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Check stdin/stdout/stderr to make sure they are using blocking IO.", "source_code": "def check_blocking_io():\n    \"\"\"Check stdin/stdout/stderr to make sure they are using blocking IO.\"\"\"\n    handles = []\n\n    for handle in (sys.stdin, sys.stdout, sys.stderr):\n        # noinspection PyBroadException\n        try:\n            fd = handle.fileno()\n        except Exception:\n            continue  # not a real file handle, such as during the import sanity test\n\n        if not os.get_blocking(fd):\n            handles.append(getattr(handle, 'name', None) or '#%s' % fd)\n\n    if handles:\n        raise SystemExit('ERROR: Ansible requires blocking IO on stdin/stdout/stderr. '\n                         'Non-blocking file handles detected: %s' % ', '.join(_io for _io in handles))", "loc": 17}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": null, "function_name": "initialize_locale", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SystemExit", "encoding.lower", "fs_enc.lower", "locale.getlocale", "locale.setlocale", "sys.getfilesystemencoding"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set the locale to the users default setting and ensure the locale and filesystem encoding are UTF-8.", "source_code": "def initialize_locale():\n    \"\"\"Set the locale to the users default setting and ensure\n    the locale and filesystem encoding are UTF-8.\n    \"\"\"\n    try:\n        locale.setlocale(locale.LC_ALL, '')\n        dummy, encoding = locale.getlocale()\n    except (locale.Error, ValueError) as e:\n        raise SystemExit(\n            'ERROR: Ansible could not initialize the preferred locale: %s' % e\n        )\n\n    if not encoding or encoding.lower() not in ('utf-8', 'utf8'):\n        raise SystemExit('ERROR: Ansible requires the locale encoding to be UTF-8; Detected %s.' % encoding)\n\n    fs_enc = sys.getfilesystemencoding()\n    if fs_enc.lower() != 'utf-8':\n        raise SystemExit('ERROR: Ansible requires the filesystem encoding to be UTF-8; Detected %s.' % fs_enc)", "loc": 18}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "show_devel_warning", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["__version__.endswith", "display.warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def show_devel_warning(self) -> None:\n    if C.DEVEL_WARNING and __version__.endswith('dev0'):\n        display.warning(\n            'You are running the development version of Ansible. You should only run Ansible from \"devel\" if '\n            'you are modifying the Ansible engine, or trying out features under development. This is a rapidly '\n            'changing source of code and can become unstable at any point.'\n        )", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_display._report_config_warnings", "context.CLIARGS.get", "display.v", "display.vv", "init_plugin_loader", "is_sequence", "opt_help.version", "self.parse", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Run the ansible command Subclasses must implement this method.  It does the actual work of running an Ansible command.", "source_code": "def run(self):\n    \"\"\"Run the ansible command\n\n    Subclasses must implement this method.  It does the actual work of\n    running an Ansible command.\n    \"\"\"\n    self.parse()\n\n    # Initialize plugin loader after parse, so that the init code can utilize parsed arguments\n    cli_collections_path = context.CLIARGS.get('collections_path') or []\n    if not is_sequence(cli_collections_path):\n        # In some contexts ``collections_path`` is singular\n        cli_collections_path = [cli_collections_path]\n    init_plugin_loader(cli_collections_path)\n\n    display.vv(to_text(opt_help.version(self.parser.prog)))\n\n    if C.CONFIG_FILE:\n        display.v(u\"Using %s as config file\" % to_text(C.CONFIG_FILE))\n    else:\n        display.v(u\"No config file found; using defaults\")\n\n    _display._report_config_warnings(_deprecator.ANSIBLE_CORE_DEPRECATOR)", "loc": 23}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "split_vault_id", "parameters": ["vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["tuple", "vault_id.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def split_vault_id(vault_id):\n    # return (before_@, after_@)\n    # if no @, return whole string as after_\n    if '@' not in vault_id:\n        return (None, vault_id)\n\n    parts = vault_id.split('@', 1)\n    ret = tuple(parts)\n    return ret", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "build_vault_ids", "parameters": ["vault_ids", "vault_password_files", "ask_vault_pass", "auto_prompt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["vault_ids.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_vault_ids(vault_ids, vault_password_files=None,\n                    ask_vault_pass=None, auto_prompt=True):\n    vault_password_files = vault_password_files or []\n    vault_ids = vault_ids or []\n\n    # convert vault_password_files into vault_ids slugs\n    for password_file in vault_password_files:\n        id_slug = u'%s@%s' % (C.DEFAULT_VAULT_IDENTITY, password_file)\n\n        # note this makes --vault-id higher precedence than --vault-password-file\n        # if we want to intertwingle them in order probably need a cli callback to populate vault_ids\n        # used by --vault-id and --vault-password-file\n        vault_ids.append(id_slug)\n\n    # if an action needs an encrypt password (create_new_password=True) and we don't\n    # have other secrets setup, then automatically add a password prompt as well.\n    # prompts can't/shouldn't work without a tty, so don't add prompt secrets\n    if ask_vault_pass or (not vault_ids and auto_prompt):\n\n        id_slug = u'%s@%s' % (C.DEFAULT_VAULT_IDENTITY, u'prompt_ask_vault_pass')\n        vault_ids.append(id_slug)\n\n    return vault_ids", "loc": 23}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "ask_passwords", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CLI._get_secret", "CLI.get_password_from_file", "op['become_method'].upper"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "prompt for connection and become passwords if needed", "source_code": "def ask_passwords():\n    \"\"\" prompt for connection and become passwords if needed \"\"\"\n\n    op = context.CLIARGS\n    sshpass = None\n    becomepass = None\n\n    become_prompt_method = \"BECOME\" if C.AGNOSTIC_BECOME_PROMPT else op['become_method'].upper()\n\n    try:\n        become_prompt = \"%s password: \" % become_prompt_method\n        if op['ask_pass']:\n            sshpass = CLI._get_secret(\"SSH password: \")\n            become_prompt = \"%s password[defaults to SSH password]: \" % become_prompt_method\n        elif op['connection_password_file']:\n            sshpass = CLI.get_password_from_file(op['connection_password_file'])\n\n        if op['become_ask_pass']:\n            becomepass = CLI._get_secret(become_prompt)\n            if op['ask_pass'] and becomepass == '':\n                becomepass = sshpass\n        elif op['become_password_file']:\n            becomepass = CLI.get_password_from_file(op['become_password_file'])\n\n    except EOFError:\n        pass\n\n    return sshpass, becomepass", "loc": 28}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "validate_conflicts", "parameters": ["self", "op", "runas_opts", "fork_opts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.parser.error"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "check for conflicting options", "source_code": "def validate_conflicts(self, op, runas_opts=False, fork_opts=False):\n    \"\"\" check for conflicting options \"\"\"\n\n    if fork_opts:\n        if op.forks < 1:\n            self.parser.error(\"The number of processes (--forks) must be >= 1\")\n\n    return op", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "post_process_args", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "isinstance", "list", "path.rstrip", "set", "skip_tags.add", "tag.strip", "tag_set.split", "tags.add", "unfrackpath"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Process the command line args Subclasses need to implement this method.  This method validates and transforms the command line arguments.  It can be used to check whether conflicting values were given, whether filenames", "source_code": "def post_process_args(self, options):\n    \"\"\"Process the command line args\n\n    Subclasses need to implement this method.  This method validates and transforms the command\n    line arguments.  It can be used to check whether conflicting values were given, whether filenames\n    exist, etc.\n\n    An implementation will look something like this::\n\n        def post_process_args(self, options):\n            options = super(MyCLI, self).post_process_args(options)\n            if options.addition and options.subtraction:\n                raise AnsibleOptionsError('Only one of --addition and --subtraction can be specified')\n            if isinstance(options.listofhosts, str):\n                options.listofhosts = options.listofhosts.split(',')\n            return options\n    \"\"\"\n\n    # process tags\n    if hasattr(options, 'tags') and not options.tags:\n        # optparse defaults does not do what's expected\n        # More specifically, we want `--tags` to be additive. So we cannot\n        # simply change C.TAGS_RUN's default to [\"all\"] because then passing\n        # --tags foo would cause us to have ['all', 'foo']\n        options.tags = ['all']\n    if hasattr(options, 'tags') and options.tags:\n        tags = set()\n        for tag_set in options.tags:\n            for tag in tag_set.split(u','):\n                tags.add(tag.strip())\n        options.tags = list(tags)\n\n    # process skip_tags\n    if hasattr(options, 'skip_tags') and options.skip_tags:\n        skip_tags = set()\n        for tag_set in options.skip_tags:\n            for tag in tag_set.split(u','):\n                skip_tags.add(tag.strip())\n        options.skip_tags = list(skip_tags)\n\n    # Make sure path argument doesn't have a backslash\n    if hasattr(options, 'action') and options.action in ['install', 'download'] and hasattr(options, 'args'):\n        options.args = [path.rstrip(\"/\") for path in options.args]\n\n    # process inventory options except for CLIs that require their own processing\n    if hasattr(options, 'inventory') and not self.SKIP_INVENTORY_DEFAULTS:\n\n        if options.inventory:\n\n            # should always be list\n            if isinstance(options.inventory, str):\n                options.inventory = [options.inventory]\n\n            # Ensure full paths when needed\n            options.inventory = [unfrackpath(opt, follow=False) if ',' not in opt else opt for opt in options.inventory]\n        else:\n            options.inventory = C.DEFAULT_HOST_LIST\n\n    return options", "loc": 59}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "parse", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "context._init_global_context", "self.init_parser", "self.parser.exit", "self.parser.format_help", "self.parser.parse_args", "self.post_process_args"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Parse the command line args This method parses the command line arguments.  It uses the parser stored in the self.parser attribute and saves the args and options in", "source_code": "def parse(self):\n    \"\"\"Parse the command line args\n\n    This method parses the command line arguments.  It uses the parser\n    stored in the self.parser attribute and saves the args and options in\n    context.CLIARGS.\n\n    Subclasses need to implement two helper methods, init_parser() and post_process_args() which\n    are called from this function before and after parsing the arguments.\n    \"\"\"\n    self.init_parser()\n\n    if HAS_ARGCOMPLETE:\n        argcomplete.autocomplete(self.parser)\n\n    try:\n        options = self.parser.parse_args(self.args[1:])\n    except SystemExit as ex:\n        if ex.code != 0:\n            self.parser.exit(status=2, message=\" \\n%s\" % self.parser.format_help())\n        raise\n    options = self.post_process_args(options)\n    context._init_global_context(options)", "loc": 23}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "version_info", "parameters": ["gitinfo"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ansible_version.split", "ansible_version_string.split", "ansible_version_string.strip", "ansible_versions.append", "int", "len", "opt_help.version", "range"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "return full ansible version info", "source_code": "def version_info(gitinfo=False):\n    \"\"\" return full ansible version info \"\"\"\n    if gitinfo:\n        # expensive call, user with care\n        ansible_version_string = opt_help.version()\n    else:\n        ansible_version_string = __version__\n    ansible_version = ansible_version_string.split()[0]\n    ansible_versions = ansible_version.split('.')\n    for counter in range(len(ansible_versions)):\n        if ansible_versions[counter] == \"\":\n            ansible_versions[counter] = 0\n        try:\n            ansible_versions[counter] = int(ansible_versions[counter])\n        except Exception:\n            pass\n    if len(ansible_versions) < 3:\n        for counter in range(len(ansible_versions), 3):\n            ansible_versions.append(0)\n    return {'string': ansible_version_string.strip(),\n            'full': ansible_version,\n            'major': ansible_versions[0],\n            'minor': ansible_versions[1],\n            'revision': ansible_versions[2]}", "loc": 24}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "get_host_list", "parameters": ["inventory", "subset", "pattern"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.warning", "inventory.list_hosts", "inventory.subset", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_host_list(inventory, subset, pattern='all'):\n\n    no_hosts = False\n    if len(inventory.list_hosts()) == 0:\n        # Empty inventory\n        if C.LOCALHOST_WARNING and pattern not in C.LOCALHOST:\n            display.warning(\"provided hosts list is empty, only localhost is available. Note that the implicit localhost does not match 'all'\")\n        no_hosts = True\n\n    inventory.subset(subset)\n\n    hosts = inventory.list_hosts(pattern)\n    if not hosts and no_hosts is False:\n        raise AnsibleError(\"Specified inventory, host pattern and/or --limit leaves us with no hosts to target.\")\n\n    return hosts", "loc": 16}
{"file": "ansible\\lib\\ansible\\cli\\__init__.py", "class_name": "CLI", "function_name": "cli_executor", "parameters": ["cls", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Path", "Path(C.ANSIBLE_HOME).expanduser", "ansible_dir.mkdir", "cli.run", "cls", "display.debug", "display.error", "display.error_as_warning", "sys.exit"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cli_executor(cls, args=None):\n    if args is None:\n        args = sys.argv\n\n    try:\n        display.debug(\"starting run\")\n\n        ansible_dir = Path(C.ANSIBLE_HOME).expanduser()\n        try:\n            ansible_dir.mkdir(mode=0o700, exist_ok=True)\n        except OSError as ex:\n            display.error_as_warning(f\"Failed to create the directory {ansible_dir!r}.\", ex)\n        else:\n            display.debug(\"Created the '%s' directory\" % ansible_dir)\n\n        cli = cls(args)\n        exit_code = cli.run()\n    except AnsibleError as ex:\n        display.error(ex)\n        exit_code = ex._exit_code\n    except KeyboardInterrupt:\n        display.error(\"User interrupted execution\")\n        exit_code = ExitCode.KEYBOARD_INTERRUPT\n    except Exception as ex:\n        try:\n            raise AnsibleError(\"Unexpected Exception, this is probably a bug.\") from ex\n        except AnsibleError as ex2:\n            # DTFIX-FUTURE: clean this up so we're not hacking the internals- re-wrap in an AnsibleCLIUnhandledError that always shows TB, or?\n            from ansible.module_utils._internal import _traceback\n            _traceback._is_traceback_enabled = lambda *_args, **_kwargs: True\n            display.error(ex2)\n            exit_code = ExitCode.UNKNOWN_ERROR\n\n    sys.exit(exit_code)", "loc": 34}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "unfrack_path", "parameters": ["pathsep", "follow"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["unfrackpath", "value.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Turn an Option's data into a single path in Ansible locations", "source_code": "def unfrack_path(pathsep=False, follow=True):\n    \"\"\"Turn an Option's data into a single path in Ansible locations\"\"\"\n    def inner(value):\n        if pathsep:\n            return [unfrackpath(x, follow=follow) for x in value.split(os.pathsep) if x]\n\n        if value == '-':\n            return value\n\n        return unfrackpath(value, follow=follow)\n    return inner", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "maybe_unfrack_path", "parameters": ["beacon"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["unfrackpath", "value.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def maybe_unfrack_path(beacon):\n\n    def inner(value):\n        if value.startswith(beacon):\n            return beacon + unfrackpath(value[1:])\n        return value\n    return inner", "loc": 7}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "version", "parameters": ["prog"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "':'.join", "'\\n'.join", "'{0} [core {1}]'.format", "'{0} {1}'.format", "_gitinfo", "get_version_string", "result.append", "sys.version.splitlines", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "return ansible version", "source_code": "def version(prog=None):\n    \"\"\" return ansible version \"\"\"\n    if prog:\n        result = [\"{0} [core {1}]\".format(prog, __version__)]\n    else:\n        result = [__version__]\n\n    gitinfo = _gitinfo()\n    if gitinfo:\n        result[0] = \"{0} {1}\".format(result[0], gitinfo)\n    result.append(\"  config file = %s\" % C.CONFIG_FILE)\n    if C.DEFAULT_MODULE_PATH is None:\n        cpath = \"Default w/o overrides\"\n    else:\n        cpath = C.DEFAULT_MODULE_PATH\n\n    if HAS_LIBYAML:\n        libyaml_fragment = \"with libyaml\"\n\n        # noinspection PyBroadException\n        try:\n            from yaml._yaml import get_version_string\n\n            libyaml_fragment += f\" v{get_version_string()}\"\n        except Exception:  # pylint: disable=broad-except\n            libyaml_fragment += \", version unknown\"\n    else:\n        libyaml_fragment = \"without libyaml\"\n\n    result.append(\"  configured module search path = %s\" % cpath)\n    result.append(\"  ansible python module location = %s\" % ':'.join(ansible.__path__))\n    result.append(\"  ansible collection location = %s\" % ':'.join(C.COLLECTIONS_PATHS))\n    result.append(\"  executable location = %s\" % sys.argv[0])\n    result.append(\"  python version = %s (%s)\" % (''.join(sys.version.splitlines()), to_native(sys.executable)))\n    result.append(f\"  jinja version = {_templating.jinja2_version}\")\n    result.append(f\"  pyyaml version = {yaml.__version__} ({libyaml_fragment})\")\n\n    return \"\\n\".join(result)", "loc": 38}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "create_base_parser", "parameters": ["prog", "desc", "epilog"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ArgumentParser", "add_verbosity_options", "parser.add_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create an options parser for all ansible scripts", "source_code": "def create_base_parser(prog, desc=None, epilog=None):\n    \"\"\"\n    Create an options parser for all ansible scripts\n    \"\"\"\n    # base opts\n    parser = ArgumentParser(\n        prog=prog,\n        formatter_class=SortingHelpFormatter,\n        epilog=epilog,\n        description=desc,\n        conflict_handler='resolve',\n    )\n    version_help = \"show program's version number, config file location, configured module search path,\" \\\n                   \" module location, executable location and exit\"\n\n    parser.add_argument('--version', action=AnsibleVersion, nargs=0, help=version_help)\n    add_verbosity_options(parser)\n    return parser", "loc": 18}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_basedir_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_argument", "unfrack_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands which can set a playbook basedir", "source_code": "def add_basedir_options(parser):\n    \"\"\"Add options for commands which can set a playbook basedir\"\"\"\n    parser.add_argument('--playbook-dir', default=C.PLAYBOOK_DIR, dest='basedir', action='store',\n                        help=\"Since this tool does not use playbooks, use this as a substitute playbook directory. \"\n                             \"This sets the relative path for many features including roles/ group_vars/ etc.\",\n                        type=unfrack_path())", "loc": 6}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_connect_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["connect_group.add_argument", "connect_password_group.add_argument", "parser.add_argument_group", "parser.add_mutually_exclusive_group", "unfrack_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands which need to connection to other hosts", "source_code": "def add_connect_options(parser):\n    \"\"\"Add options for commands which need to connection to other hosts\"\"\"\n    connect_group = parser.add_argument_group(\"Connection Options\", \"control as whom and how to connect to hosts\")\n\n    connect_group.add_argument('--private-key', '--key-file', default=C.DEFAULT_PRIVATE_KEY_FILE, dest='private_key_file',\n                               help='use this file to authenticate the connection', type=unfrack_path())\n    connect_group.add_argument('-u', '--user', default=C.DEFAULT_REMOTE_USER, dest='remote_user',\n                               help='connect as this user (default=%s)' % C.DEFAULT_REMOTE_USER)\n    connect_group.add_argument('-c', '--connection', dest='connection', default=C.DEFAULT_TRANSPORT,\n                               help=\"connection type to use (default=%s)\" % C.DEFAULT_TRANSPORT)\n    connect_group.add_argument('-T', '--timeout', default=None, type=int, dest='timeout',\n                               help=\"override the connection timeout in seconds (default depends on connection)\")\n\n    # ssh only\n    connect_group.add_argument('--ssh-common-args', default=None, dest='ssh_common_args',\n                               help=\"specify common arguments to pass to sftp/scp/ssh (e.g. ProxyCommand)\")\n    connect_group.add_argument('--sftp-extra-args', default=None, dest='sftp_extra_args',\n                               help=\"specify extra arguments to pass to sftp only (e.g. -f, -l)\")\n    connect_group.add_argument('--scp-extra-args', default=None, dest='scp_extra_args',\n                               help=\"specify extra arguments to pass to scp only (e.g. -l)\")\n    connect_group.add_argument('--ssh-extra-args', default=None, dest='ssh_extra_args',\n                               help=\"specify extra arguments to pass to ssh only (e.g. -R)\")\n\n    parser.add_argument_group(connect_group)\n\n    connect_password_group = parser.add_mutually_exclusive_group()\n    connect_password_group.add_argument('-k', '--ask-pass', default=C.DEFAULT_ASK_PASS, dest='ask_pass', action='store_true',\n                                        help='ask for connection password')\n    connect_password_group.add_argument('--connection-password-file', '--conn-pass-file', default=C.CONNECTION_PASSWORD_FILE, dest='connection_password_file',\n                                        help=\"Connection password file\", type=unfrack_path(), action='store')\n\n    parser.add_argument_group(connect_password_group)", "loc": 32}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_inventory_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DeprecatedArgument", "parser.add_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands that utilize inventory", "source_code": "def add_inventory_options(parser):\n    \"\"\"Add options for commands that utilize inventory\"\"\"\n    parser.add_argument('-i', '--inventory', '--inventory-file', dest='inventory', action=\"append\",\n                        help=\"specify inventory host path or comma separated host list\",\n                        deprecated=DeprecatedArgument(version='2.23', option='--inventory-file'))\n    parser.add_argument('--list-hosts', dest='listhosts', action='store_true',\n                        help='outputs a list of matching hosts; does not execute anything else')\n    parser.add_argument('-l', '--limit', default=C.DEFAULT_SUBSET, dest='subset',\n                        help='further limit selected hosts to an additional pattern')\n    parser.add_argument('--flush-cache', dest='flush_cache', action='store_true',\n                        help=\"clear the fact cache for every host in inventory\")", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_module_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.config.get_configuration_definition", "C.config.get_configuration_definition('DEFAULT_MODULE_PATH').get", "parser.add_argument", "unfrack_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands that load modules", "source_code": "def add_module_options(parser):\n    \"\"\"Add options for commands that load modules\"\"\"\n    module_path = C.config.get_configuration_definition('DEFAULT_MODULE_PATH').get('default', '')\n    parser.add_argument('-M', '--module-path', dest='module_path', default=None,\n                        help=\"prepend colon-separated path(s) to module library (default=%s)\" % module_path,\n                        type=unfrack_path(pathsep=True), action=PrependListAction)", "loc": 6}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_output_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DeprecatedArgument", "parser.add_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands which can change their output", "source_code": "def add_output_options(parser):\n    \"\"\"Add options for commands which can change their output\"\"\"\n    parser.add_argument('-o', '--one-line', dest='one_line', action='store_true',\n                        help='condense output', deprecated=DeprecatedArgument(version='2.23'))\n    parser.add_argument('-t', '--tree', dest='tree', default=None,\n                        help='log output to this directory', deprecated=DeprecatedArgument(version='2.23'))", "loc": 6}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_runas_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_runas_prompt_options", "parser.add_argument_group", "runas_group.add_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for commands which can run tasks as another user Note that this includes the options from add_runas_prompt_options().  Only one of these functions should be used.", "source_code": "def add_runas_options(parser):\n    \"\"\"\n    Add options for commands which can run tasks as another user\n\n    Note that this includes the options from add_runas_prompt_options().  Only one of these\n    functions should be used.\n    \"\"\"\n    runas_group = parser.add_argument_group(\"Privilege Escalation Options\", \"control how and which user you become as on target hosts\")\n\n    # consolidated privilege escalation (become)\n    runas_group.add_argument(\"-b\", \"--become\", default=C.DEFAULT_BECOME, action=\"store_true\", dest='become',\n                             help=\"run operations with become (does not imply password prompting)\")\n    runas_group.add_argument('--become-method', dest='become_method', default=C.DEFAULT_BECOME_METHOD,\n                             help='privilege escalation method to use (default=%s)' % C.DEFAULT_BECOME_METHOD +\n                                  ', use `ansible-doc -t become -l` to list valid choices.')\n    runas_group.add_argument('--become-user', default=None, dest='become_user', type=str,\n                             help='run operations as this user (default=%s)' % C.DEFAULT_BECOME_USER)\n\n    parser.add_argument_group(runas_group)\n\n    add_runas_prompt_options(parser)", "loc": 21}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_runas_prompt_options", "parameters": ["parser", "runas_group"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parser.add_argument_group", "parser.add_mutually_exclusive_group", "runas_pass_group.add_argument", "unfrack_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add options for commands which need to prompt for privilege escalation credentials Note that add_runas_options() includes these options already.  Only one of the two functions should be used.", "source_code": "def add_runas_prompt_options(parser, runas_group=None):\n    \"\"\"\n    Add options for commands which need to prompt for privilege escalation credentials\n\n    Note that add_runas_options() includes these options already.  Only one of the two functions\n    should be used.\n    \"\"\"\n    if runas_group is not None:\n        parser.add_argument_group(runas_group)\n\n    runas_pass_group = parser.add_mutually_exclusive_group()\n\n    runas_pass_group.add_argument('-K', '--ask-become-pass', dest='become_ask_pass', action='store_true',\n                                  default=C.DEFAULT_BECOME_ASK_PASS,\n                                  help='ask for privilege escalation password')\n    runas_pass_group.add_argument('--become-password-file', '--become-pass-file', default=C.BECOME_PASSWORD_FILE, dest='become_password_file',\n                                  help=\"Become password file\", type=unfrack_path(), action='store')\n\n    parser.add_argument_group(runas_pass_group)", "loc": 19}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "add_vault_options", "parameters": ["parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["base_group.add_argument", "parser.add_argument", "parser.add_mutually_exclusive_group", "unfrack_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Add options for loading vault files", "source_code": "def add_vault_options(parser):\n    \"\"\"Add options for loading vault files\"\"\"\n    parser.add_argument('--vault-id', default=[], dest='vault_ids', action='append', type=str,\n                        help='the vault identity to use')\n    base_group = parser.add_mutually_exclusive_group()\n    base_group.add_argument('-J', '--ask-vault-password', '--ask-vault-pass', default=C.DEFAULT_ASK_VAULT_PASS, dest='ask_vault_pass', action='store_true',\n                            help='ask for vault password')\n    base_group.add_argument('--vault-password-file', '--vault-pass-file', default=[], dest='vault_password_files',\n                            help=\"vault password file\", type=unfrack_path(follow=False), action='append')", "loc": 9}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": "DeprecatedArgument", "function_name": "check", "parameters": ["self", "option"], "param_types": {"option": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Display", "Display().deprecated", "self.is_deprecated"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Display a deprecation warning if the given option is deprecated.", "source_code": "def check(self, option: str) -> None:\n    \"\"\"Display a deprecation warning if the given option is deprecated.\"\"\"\n    if not self.is_deprecated(option):\n        return\n\n    from ansible.utils.display import Display\n\n    Display().deprecated(  # pylint: disable=ansible-invalid-deprecated-version\n        msg=f'The {option!r} argument is deprecated.',\n        version=self.version,\n    )", "loc": 11}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": "ArgumentParser", "function_name": "register", "parameters": ["self", "registry_name", "value", "object"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().register"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Track registration of actions so that they can be resolved later by name, without depending on the internals of ArgumentParser.", "source_code": "def register(self, registry_name, value, object):\n    \"\"\"Track registration of actions so that they can be resolved later by name, without depending on the internals of ArgumentParser.\"\"\"\n    if registry_name == 'action':\n        self.__actions[value] = object\n\n    super().register(registry_name, value, object)", "loc": 6}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": "ArgumentParser", "function_name": "add_subparsers", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._patch_parser", "sub_add_parser", "super", "super().add_subparsers"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_subparsers(self, *args, **kwargs):\n    sub = super().add_subparsers(*args, **kwargs)\n    sub_add_parser = sub.add_parser\n\n    def add_parser(*sub_args, **sub_kwargs):\n        return self._patch_parser(sub_add_parser(*sub_args, **sub_kwargs))\n\n    sub.add_parser = add_parser\n\n    return sub", "loc": 10}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": "ArgumentParser", "function_name": "add_argument", "parameters": ["self"], "param_types": {}, "return_type": "argparse.Action", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["deprecated.check", "help.rstrip", "kwargs.get", "kwargs.pop", "self.__actions.get", "self._patch_argument", "super", "super().__call__", "super().add_argument"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_argument(self, *args, **kwargs) -> argparse.Action:\n    action = kwargs.get('action')\n    help = kwargs.get('help')\n    if help and action in {'append', 'append_const', 'count', 'extend', PrependListAction}:\n        help = f'{help.rstrip(\".\")}. This argument may be specified multiple times.'\n    kwargs['help'] = help\n\n    self._patch_argument(args, kwargs)\n\n    deprecated: DeprecatedArgument | None\n\n    if deprecated := kwargs.pop('deprecated', None):\n        action_type = self.__actions.get(action, action)\n\n        class DeprecatedAction(action_type):  # type: ignore[misc, valid-type]\n            \"\"\"A wrapper around an action which handles deprecation warnings.\"\"\"\n\n            def __call__(self, parser, namespace, values, option_string=None) -> t.Any:\n                deprecated.check(option_string)\n\n                return super().__call__(parser, namespace, values, option_string)\n\n        kwargs['action'] = DeprecatedAction\n\n    return super().add_argument(*args, **kwargs)", "loc": 25}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "inner", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["unfrackpath", "value.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner(value):\n    if pathsep:\n        return [unfrackpath(x, follow=follow) for x in value.split(os.pathsep) if x]\n\n    if value == '-':\n        return value\n\n    return unfrackpath(value, follow=follow)", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\arguments\\option_helpers.py", "class_name": null, "function_name": "tag_value", "parameters": ["value"], "param_types": {"value": "str"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Origin", "Origin.get_tag", "TrustedAsTemplate", "TrustedAsTemplate().tag", "func", "origin.tag"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def tag_value(value: str) -> object:\n    result = func(value)\n\n    if result is value or func is str:\n        # Values which are not mutated are automatically trusted for templating.\n        # The `is` reference equality is critically important, as other types may only alter the tags, so object equality is\n        # not sufficient to prevent them being tagged as trusted when they should not.\n        # Explicitly include all usages using the `str` type factory since it strips tags.\n        result = TrustedAsTemplate().tag(result)\n\n    if not (origin := Origin.get_tag(value)):\n        origin = Origin(description=f'<CLI option {name!r}>')\n\n    return origin.tag(result)", "loc": 14}
{"file": "ansible\\lib\\ansible\\cli\\scripts\\ansible_connection_cli_stub.py", "class_name": null, "function_name": "read_stream", "parameters": ["byte_stream"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "byte_stream.read", "byte_stream.readline", "byte_stream.readline().strip", "int", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_stream(byte_stream):\n    size = int(byte_stream.readline().strip())\n\n    data = byte_stream.read(size)\n    if len(data) < size:\n        raise Exception(\"EOF found before data was complete\")\n\n    return data", "loc": 8}
{"file": "ansible\\lib\\ansible\\cli\\scripts\\ansible_connection_cli_stub.py", "class_name": "ConnectionProcess", "function_name": "start", "parameters": ["self", "options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ConnectionError", "connection_loader.get", "json.dumps", "list", "messages.append", "messages.extend", "os.path.join", "self.connection.pop_messages", "self.connection.set_options", "self.fd.close", "self.fd.write", "self.sock.bind", "self.sock.listen", "self.srv.register", "socket.socket", "sys.stdout.getvalue", "sys.stdout.getvalue().splitlines", "to_text", "traceback.format_exc"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def start(self, options):\n    messages = list()\n    result = {}\n\n    try:\n        messages.append(('vvvv', 'control socket path is %s' % self.socket_path))\n\n        # If this is a relative path (~ gets expanded later) then plug the\n        # key's path on to the directory we originally came from, so we can\n        # find it now that our cwd is /\n        if self.play_context.private_key_file and self.play_context.private_key_file[0] not in '~/':\n            self.play_context.private_key_file = os.path.join(self.original_path, self.play_context.private_key_file)\n        self.connection = connection_loader.get(self.play_context.connection, self.play_context, '/dev/null',\n                                                task_uuid=self._task_uuid, ansible_playbook_pid=self._ansible_playbook_pid)\n        try:\n            self.connection.set_options(direct=options)\n        except ConnectionError as exc:\n            messages.append(('debug', to_text(exc)))\n            raise ConnectionError('Unable to decode JSON from response set_options. See the debug log for more information.')\n\n        self.connection._socket_path = self.socket_path\n        self.srv.register(self.connection)\n        messages.extend([('vvvv', msg) for msg in sys.stdout.getvalue().splitlines()])\n\n        self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        self.sock.bind(self.socket_path)\n        self.sock.listen(1)\n        messages.append(('vvvv', 'local domain socket listeners started successfully'))\n    except Exception as exc:\n        messages.extend(self.connection.pop_messages())\n        result['error'] = to_text(exc)\n        result['exception'] = traceback.format_exc()\n    finally:\n        result['messages'] = messages\n        self.fd.write(json.dumps(result, cls=_tagless.Encoder))\n        self.fd.close()", "loc": 36}
{"file": "ansible\\lib\\ansible\\cli\\scripts\\ansible_connection_cli_stub.py", "class_name": "ConnectionProcess", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "hasattr", "json.loads", "recv_data", "request.get", "s.close", "self.connection._connect", "self.connection.get_option", "self.shutdown", "self.sock.accept", "self.srv.handle_request", "send_data", "signal.alarm", "signal.signal", "time.sleep", "to_bytes", "to_text", "traceback.format_exc"], "control_structures": ["If", "Try", "While"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def run(self):\n    try:\n        log_messages = self.connection.get_option('persistent_log_messages')\n        while not self.connection._conn_closed:\n            signal.signal(signal.SIGALRM, self.connect_timeout)\n            signal.signal(signal.SIGTERM, self.handler)\n            signal.alarm(self.connection.get_option('persistent_connect_timeout'))\n\n            self.exception = None\n            (s, addr) = self.sock.accept()\n            signal.alarm(0)\n            signal.signal(signal.SIGALRM, self.command_timeout)\n            while True:\n                data = recv_data(s)\n                if not data:\n                    break\n\n                if log_messages:\n                    display.display(\"jsonrpc request: %s\" % data, log_only=True)\n\n                request = json.loads(to_text(data, errors='surrogate_or_strict'))\n                if request.get('method') == \"exec_command\" and not self.connection.connected:\n                    self.connection._connect()\n\n                signal.alarm(self.connection.get_option('persistent_command_timeout'))\n\n                resp = self.srv.handle_request(data)\n                signal.alarm(0)\n\n                if log_messages:\n                    display.display(\"jsonrpc response: %s\" % resp, log_only=True)\n\n                send_data(s, to_bytes(resp))\n\n            s.close()\n\n    except Exception as e:\n        # socket.accept() will raise EINTR if the socket.close() is called\n        if hasattr(e, 'errno'):\n            if e.errno != errno.EINTR:\n                self.exception = traceback.format_exc()\n        else:\n            self.exception = traceback.format_exc()\n\n    finally:\n        # allow time for any exception msg send over socket to receive at other end before shutting down\n        time.sleep(0.1)\n\n        # when done, close the connection properly and cleanup the socket file so it can be recreated\n        self.shutdown()", "loc": 50}
{"file": "ansible\\lib\\ansible\\cli\\scripts\\ansible_connection_cli_stub.py", "class_name": "ConnectionProcess", "function_name": "connect_timeout", "parameters": ["self", "signum", "frame"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "display.display", "self.connection.get_option"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def connect_timeout(self, signum, frame):\n    msg = 'persistent connection idle timeout triggered, timeout value is %s secs.\\nSee the timeout setting options in the Network Debug and ' \\\n          'Troubleshooting Guide.' % self.connection.get_option('persistent_connect_timeout')\n    display.display(msg, log_only=True)\n    raise Exception(msg)", "loc": 5}
{"file": "ansible\\lib\\ansible\\cli\\scripts\\ansible_connection_cli_stub.py", "class_name": "ConnectionProcess", "function_name": "command_timeout", "parameters": ["self", "signum", "frame"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "display.display", "self.connection.get_option"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def command_timeout(self, signum, frame):\n    msg = 'command timeout triggered, timeout value is %s secs.\\nSee the timeout setting options in the Network Debug and Troubleshooting Guide.'\\\n          % self.connection.get_option('persistent_command_timeout')\n    display.display(msg, log_only=True)\n    raise Exception(msg)", "loc": 5}
{"file": "ansible\\lib\\ansible\\collections\\list.py", "class_name": null, "function_name": "list_collections", "parameters": ["coll_filter", "search_paths", "dedupe", "artifacts_manager"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_collection_name_from_path", "display.debug", "list_collection_dirs"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_collections(coll_filter=None, search_paths=None, dedupe=True, artifacts_manager=None):\n\n    collections = {}\n    for candidate in list_collection_dirs(search_paths=search_paths, coll_filter=coll_filter, artifacts_manager=artifacts_manager, dedupe=dedupe):\n        if collection := _get_collection_name_from_path(candidate):\n            collections[collection] = candidate\n        else:\n            display.debug(f'Skipping invalid collection in path: {candidate!r}')\n    return collections", "loc": 9}
{"file": "ansible\\lib\\ansible\\collections\\list.py", "class_name": null, "function_name": "list_collection_dirs", "parameters": ["search_paths", "coll_filter", "artifacts_manager", "dedupe"], "param_types": {}, "return_type": null, "param_doc": {"search_paths": "list of text-string paths, if none load default config", "coll_filter": "limit collections to just the specific namespace or collection, if None all are returned"}, "return_doc": "list of collection directory paths", "raises_doc": [], "called_functions": ["AnsibleError", "coll_name.split", "collection_filter.append", "find_existing_collections", "isinstance", "namespace_filter.add", "set", "sorted", "to_bytes"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Return paths for the specific collections found in passed or configured search paths", "source_code": "def list_collection_dirs(search_paths=None, coll_filter=None, artifacts_manager=None, dedupe=True):\n    \"\"\"\n    Return paths for the specific collections found in passed or configured search paths\n    :param search_paths: list of text-string paths, if none load default config\n    :param coll_filter: limit collections to just the specific namespace or collection, if None all are returned\n    :return: list of collection directory paths\n    \"\"\"\n\n    namespace_filter = None\n    collection_filter = None\n    has_pure_namespace_filter = False  # whether at least one coll_filter is a namespace-only filter\n    if coll_filter is not None:\n        if isinstance(coll_filter, str):\n            coll_filter = [coll_filter]\n        namespace_filter = set()\n        for coll_name in coll_filter:\n            if '.' in coll_name:\n                try:\n                    namespace, collection = coll_name.split('.')\n                except ValueError:\n                    raise AnsibleError(\"Invalid collection pattern supplied: %s\" % coll_name)\n                namespace_filter.add(namespace)\n                if not has_pure_namespace_filter:\n                    if collection_filter is None:\n                        collection_filter = []\n                    collection_filter.append(collection)\n            else:\n                namespace_filter.add(coll_name)\n                has_pure_namespace_filter = True\n                collection_filter = None\n        namespace_filter = sorted(namespace_filter)\n\n    for req in find_existing_collections(search_paths, artifacts_manager, namespace_filter=namespace_filter,\n                                         collection_filter=collection_filter, dedupe=dedupe):\n\n        if not has_pure_namespace_filter and coll_filter is not None and req.fqcn not in coll_filter:\n            continue\n        yield to_bytes(req.src)", "loc": 38}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": null, "function_name": "ensure_type", "parameters": ["value", "value_type", "origin", "origin_ftype"], "param_types": {"value": "object", "value_type": "str | None", "origin": "str | None", "origin_ftype": "str | None"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag_copy", "_ensure_type", "isinstance", "unquote"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts `value` to the requested `value_type`; raises `ValueError` for failed conversions. Values for `value_type` are: * boolean/bool: Return a `bool` by applying non-strict `bool` filter rules:", "source_code": "def ensure_type(value: object, value_type: str | None, origin: str | None = None, origin_ftype: str | None = None) -> t.Any:\n    \"\"\"\n    Converts `value` to the requested `value_type`; raises `ValueError` for failed conversions.\n\n    Values for `value_type` are:\n\n    * boolean/bool: Return a `bool` by applying non-strict `bool` filter rules:\n      'y', 'yes', 'on', '1', 'true', 't', 1, 1.0, True return True, any other value is False.\n    * integer/int: Return an `int`. Accepts any `str` parseable by `int` or numeric value with a zero mantissa (including `bool`).\n    * float: Return a `float`. Accepts any `str` parseable by `float` or numeric value (including `bool`).\n    * list: Return a `list`. Accepts `list` or `Sequence`. Also accepts, `str`, splitting on ',' while stripping whitespace and unquoting items.\n    * none: Return `None`. Accepts only the string \"None\".\n    * path: Return a resolved path. Accepts `str`.\n    * temppath/tmppath/tmp: Return a unique temporary directory inside the resolved path specified by the value.\n    * pathspec: Return a `list` of resolved paths. Accepts a `list` or `Sequence`. Also accepts `str`, splitting on ':'.\n    * pathlist: Return a `list` of resolved paths. Accepts a `list` or `Sequence`. Also accepts `str`, splitting on `,` while stripping whitespace from paths.\n    * dictionary/dict: Return a `dict`. Accepts `dict` or `Mapping`.\n    * string/str: Return a `str`. Accepts `bool`, `int`, `float`, `complex` or `str`.\n\n    Path resolution ensures paths are `str` with expansion of '{{CWD}}', environment variables and '~'.\n    Non-absolute paths are expanded relative to the basedir from `origin`, if specified.\n\n    No conversion is performed if `value_type` is unknown or `value` is `None`.\n    When `origin_ftype` is \"ini\", a `str` result will be unquoted.\n    \"\"\"\n\n    if value is None:\n        return None\n\n    original_value = value\n    copy_tags = value_type not in ('temppath', 'tmppath', 'tmp')\n\n    value = _ensure_type(value, value_type, origin)\n\n    if copy_tags and value is not original_value:\n        if isinstance(value, list):\n            value = [AnsibleTagHelper.tag_copy(original_value, item) for item in value]\n\n        value = AnsibleTagHelper.tag_copy(original_value, value)\n\n    if isinstance(value, str) and origin_ftype and origin_ftype == 'ini':\n        value = unquote(value)\n\n    return value", "loc": 44}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": null, "function_name": "resolve_path", "parameters": ["path", "basedir"], "param_types": {"path": "str", "basedir": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.getcwd", "path.replace", "unfrackpath"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "resolve relative or 'variable' paths", "source_code": "def resolve_path(path: str, basedir: str | None = None) -> str:\n    \"\"\" resolve relative or 'variable' paths \"\"\"\n    if '{{CWD}}' in path:  # allow users to force CWD using 'magic' {{CWD}}\n        path = path.replace('{{CWD}}', os.getcwd())\n\n    return unfrackpath(path, follow=False, basedir=basedir)", "loc": 6}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "load_galaxy_server_defs", "parameters": ["self", "server_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["config_def.update", "dict", "key.upper", "section.upper", "self.get_config_value", "self.initialize_plugin_configuration_definitions", "server_config_def"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_galaxy_server_defs(self, server_list):\n\n    def server_config_def(section, key, required, option_type):\n        config_def = {\n            'description': 'The %s of the %s Galaxy server' % (key, section),\n            'ini': [\n                {\n                    'section': 'galaxy_server.%s' % section,\n                    'key': key,\n                }\n            ],\n            'env': [\n                {'name': 'ANSIBLE_GALAXY_SERVER_%s_%s' % (section.upper(), key.upper())},\n            ],\n            'required': required,\n            'type': option_type,\n        }\n        if key in GALAXY_SERVER_ADDITIONAL:\n            config_def.update(GALAXY_SERVER_ADDITIONAL[key])\n            # ensure we always have a default timeout\n            if key == 'timeout' and 'default' not in config_def:\n                config_def['default'] = self.get_config_value('GALAXY_SERVER_TIMEOUT')\n\n        return config_def\n\n    if server_list:\n        for server_key in server_list:\n            if not server_key:\n                # To filter out empty strings or non truthy values as an empty server list env var is equal to [''].\n                continue\n\n            # Config definitions are looked up dynamically based on the C.GALAXY_SERVER_LIST entry. We look up the\n            # section [galaxy_server.<server>] for the values url, username, password, and token.\n            defs = dict((k, server_config_def(server_key, k, req, value_type)) for k, req, value_type in GALAXY_SERVER_DEF)\n            self.initialize_plugin_configuration_definitions('galaxy_server', server_key, defs)", "loc": 35}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "template_default", "parameters": ["self", "value", "variables", "key_name"], "param_types": {"key_name": "str"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["NativeEnvironment", "NativeEnvironment().from_string", "isinstance", "self._errors.append", "template.render", "value.endswith", "value.startswith"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def template_default(self, value, variables, key_name: str = '<unknown>'):\n    if isinstance(value, str) and (value.startswith('{{') and value.endswith('}}')) and variables is not None:\n        # template default values if possible\n        # NOTE: cannot use is_template due to circular dep\n        try:\n            # FIXME: This really should be using an immutable sandboxed native environment, not just native environment\n            template = NativeEnvironment().from_string(value)\n            value = template.render(variables)\n        except Exception as ex:\n            self._errors.append((f'Failed to template default for config {key_name}.', ex))\n    return value", "loc": 11}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_plugin_options_and_origins", "parameters": ["self", "plugin_type", "name", "keys", "variables", "direct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_config_value_and_origin", "self.get_configuration_definitions"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_options_and_origins(self, plugin_type, name, keys=None, variables=None, direct=None):\n    options = {}\n    origins = {}\n    defs = self.get_configuration_definitions(plugin_type=plugin_type, name=name)\n    for option in defs:\n        options[option], origins[option] = self.get_config_value_and_origin(option, plugin_type=plugin_type, plugin_name=name, keys=keys,\n                                                                            variables=variables, direct=direct)\n    return options, origins", "loc": 8}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_plugin_vars", "parameters": ["self", "plugin_type", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["pvars.append", "self.get_configuration_definitions", "self.get_configuration_definitions(plugin_type=plugin_type, name=name).values"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_vars(self, plugin_type, name):\n\n    pvars = []\n    for pdef in self.get_configuration_definitions(plugin_type=plugin_type, name=name).values():\n        if 'vars' in pdef and pdef['vars']:\n            for var_entry in pdef['vars']:\n                pvars.append(var_entry['name'])\n    return pvars", "loc": 8}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_plugin_options_from_var", "parameters": ["self", "plugin_type", "name", "variable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["options.append", "self.get_configuration_definitions", "self.get_configuration_definitions(plugin_type=plugin_type, name=name).items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_options_from_var(self, plugin_type, name, variable):\n\n    options = []\n    for option_name, pdef in self.get_configuration_definitions(plugin_type=plugin_type, name=name).items():\n        if 'vars' in pdef and pdef['vars']:\n            for var_entry in pdef['vars']:\n                if variable == var_entry['name']:\n                    options.append(option_name)\n    return options", "loc": 9}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_configuration_definition", "parameters": ["self", "name", "plugin_type", "plugin_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._base_defs.get", "self._plugins.get", "self._plugins.get(plugin_type, {}).get", "self._plugins.get(plugin_type, {}).get(plugin_name, {}).get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_configuration_definition(self, name, plugin_type=None, plugin_name=None):\n\n    ret = {}\n    if plugin_type is None:\n        ret = self._base_defs.get(name, None)\n    elif plugin_name is None:\n        ret = self._plugins.get(plugin_type, {}).get(name, None)\n    else:\n        ret = self._plugins.get(plugin_type, {}).get(plugin_name, {}).get(name, None)\n\n    return ret", "loc": 11}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "has_configuration_definition", "parameters": ["self", "plugin_type", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def has_configuration_definition(self, plugin_type, name):\n\n    has = False\n    if plugin_type in self._plugins:\n        has = (name in self._plugins[plugin_type])\n\n    return has", "loc": 7}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_configuration_definitions", "parameters": ["self", "plugin_type", "name", "ignore_private"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cdef.startswith", "list", "ret.keys", "self._plugins.get", "self._plugins.get(plugin_type, {}).get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "just list the possible settings, either base or for specific plugins or plugin", "source_code": "def get_configuration_definitions(self, plugin_type=None, name=None, ignore_private=False):\n    \"\"\" just list the possible settings, either base or for specific plugins or plugin \"\"\"\n\n    ret = {}\n    if plugin_type is None:\n        ret = self._base_defs\n    elif name is None:\n        ret = self._plugins.get(plugin_type, {})\n    else:\n        ret = self._plugins.get(plugin_type, {}).get(name, {})\n\n    if ignore_private:  # ignore 'test' config entries, they should not change runtime behaviors\n        for cdef in list(ret.keys()):\n            if cdef.startswith('_Z_'):\n                del ret[cdef]\n    return ret", "loc": 16}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_config_value", "parameters": ["self", "config", "cfile", "plugin_type", "plugin_name", "keys", "variables", "direct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "self.get_config_value_and_origin"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "wrapper", "source_code": "def get_config_value(self, config, cfile=None, plugin_type=None, plugin_name=None, keys=None, variables=None, direct=None):\n    \"\"\" wrapper \"\"\"\n\n    try:\n        value, _drop = self.get_config_value_and_origin(config, cfile=cfile, plugin_type=plugin_type, plugin_name=plugin_name,\n                                                        keys=keys, variables=variables, direct=direct)\n    except AnsibleError:\n        raise\n    except Exception as ex:\n        raise AnsibleError(f\"Unhandled exception when retrieving {config!r}.\") from ex\n    return value", "loc": 11}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "initialize_plugin_configuration_definitions", "parameters": ["self", "plugin_type", "name", "defs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize_plugin_configuration_definitions(self, plugin_type, name, defs):\n\n    if plugin_type not in self._plugins:\n        self._plugins[plugin_type] = {}\n\n    self._plugins[plugin_type][name] = defs", "loc": 6}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": "ConfigManager", "function_name": "get_deprecated_msg_from_config", "parameters": ["dep_docs", "include_removal", "collection_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep_docs.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_deprecated_msg_from_config(dep_docs, include_removal=False, collection_name=None):\n\n    removal = ''\n    if include_removal:\n        if 'removed_at_date' in dep_docs:\n            removal = f\"Will be removed in a release after {dep_docs['removed_at_date']}\\n\\t\"\n        elif collection_name:\n            removal = f\"Will be removed in: {collection_name} {dep_docs['removed_in']}\\n\\t\"\n        else:\n            removal = f\"Will be removed in: Ansible {dep_docs['removed_in']}\\n\\t\"\n\n    # TODO: choose to deprecate either singular or plural\n    alt = dep_docs.get('alternatives', dep_docs.get('alternative', 'none'))\n    return f\"Reason: {dep_docs['why']}\\n\\t{removal}Alternatives: {alt}\"", "loc": 14}
{"file": "ansible\\lib\\ansible\\config\\manager.py", "class_name": null, "function_name": "server_config_def", "parameters": ["section", "key", "required", "option_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["config_def.update", "key.upper", "section.upper", "self.get_config_value"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def server_config_def(section, key, required, option_type):\n    config_def = {\n        'description': 'The %s of the %s Galaxy server' % (key, section),\n        'ini': [\n            {\n                'section': 'galaxy_server.%s' % section,\n                'key': key,\n            }\n        ],\n        'env': [\n            {'name': 'ANSIBLE_GALAXY_SERVER_%s_%s' % (section.upper(), key.upper())},\n        ],\n        'required': required,\n        'type': option_type,\n    }\n    if key in GALAXY_SERVER_ADDITIONAL:\n        config_def.update(GALAXY_SERVER_ADDITIONAL[key])\n        # ensure we always have a default timeout\n        if key == 'timeout' and 'default' not in config_def:\n            config_def['default'] = self.get_config_value('GALAXY_SERVER_TIMEOUT')\n\n    return config_def", "loc": 22}
{"file": "ansible\\lib\\ansible\\errors\\__init__.py", "class_name": "AnsibleJSONParserError", "function_name": "handle_exception", "parameters": ["cls", "exception", "origin"], "param_types": {"exception": "Exception", "origin": "_tags.Origin"}, "return_type": "t.NoReturn", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "isinstance", "origin.replace", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_exception(cls, exception: Exception, origin: _tags.Origin) -> t.NoReturn:\n    if isinstance(exception, JSONDecodeError):\n        origin = origin.replace(line_num=exception.lineno, col_num=exception.colno)\n\n    message = str(exception)\n\n    error = cls(message, obj=origin)\n\n    raise error from exception", "loc": 9}
{"file": "ansible\\lib\\ansible\\errors\\__init__.py", "class_name": "AnsibleVariableTypeError", "function_name": "from_value", "parameters": ["cls"], "param_types": {}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "isinstance", "native_type_name", "type"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_value(cls, *, obj: t.Any) -> t.Self:\n    # avoid an incorrect error message when `obj` is a type\n    type_name = type(obj).__name__ if isinstance(obj, type) else native_type_name(obj)\n\n    return cls(message=f'Type {type_name!r} is unsupported for variable storage.', obj=obj)", "loc": 5}
{"file": "ansible\\lib\\ansible\\executor\\interpreter_discovery.py", "class_name": null, "function_name": "discover_interpreter", "parameters": ["action", "interpreter_name", "discovery_mode", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'; '.join", "'echo FOUND; {0}; echo ENDFOUND'.format", "C.config.get_config_value", "ValueError", "action._low_level_execute_command", "discovery_mode.endswith", "discovery_mode.startswith", "display.debug", "display.deprecated", "display.error_as_warning", "display.vvv", "display.warning", "foundre.match", "get_versioned_doclink", "interp.startswith", "interp.strip", "match.groups", "match.groups()[0].splitlines", "res.get", "task_vars.get", "u'found interpreters: {0}'.format", "u'raw interpreter discovery output: {0}'.format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Probe the target host for a Python interpreter from the `INTERPRETER_PYTHON_FALLBACK` list, returning the first found or `/usr/bin/python3` if none.", "source_code": "def discover_interpreter(action, interpreter_name, discovery_mode, task_vars):\n    \"\"\"Probe the target host for a Python interpreter from the `INTERPRETER_PYTHON_FALLBACK` list, returning the first found or `/usr/bin/python3` if none.\"\"\"\n    host = task_vars.get('inventory_hostname', 'unknown')\n    res = None\n    found_interpreters = [_FALLBACK_INTERPRETER]  # fallback value\n    is_silent = discovery_mode.endswith('_silent')\n\n    if discovery_mode.startswith('auto_legacy'):\n        display.deprecated(\n            msg=f\"The '{discovery_mode}' option for 'INTERPRETER_PYTHON' now has the same effect as 'auto'.\",\n            version='2.21',\n        )\n\n    try:\n        bootstrap_python_list = C.config.get_config_value('INTERPRETER_PYTHON_FALLBACK', variables=task_vars)\n\n        display.vvv(msg=f\"Attempting {interpreter_name} interpreter discovery.\", host=host)\n\n        # not all command -v impls accept a list of commands, so we have to call it once per python\n        command_list = [\"command -v '%s'\" % py for py in bootstrap_python_list]\n        shell_bootstrap = \"echo FOUND; {0}; echo ENDFOUND\".format('; '.join(command_list))\n\n        # FUTURE: in most cases we probably don't want to use become, but maybe sometimes we do?\n        res = action._low_level_execute_command(shell_bootstrap, sudoable=False)\n\n        raw_stdout = res.get('stdout', u'')\n\n        match = foundre.match(raw_stdout)\n\n        if not match:\n            display.debug(u'raw interpreter discovery output: {0}'.format(raw_stdout), host=host)\n            raise ValueError('unexpected output from Python interpreter discovery')\n\n        found_interpreters = [interp.strip() for interp in match.groups()[0].splitlines() if interp.startswith('/')]\n\n        display.debug(u\"found interpreters: {0}\".format(found_interpreters), host=host)\n\n        if not found_interpreters:\n            if not is_silent:\n                display.warning(msg=f'No python interpreters found for host {host!r} (tried {bootstrap_python_list!r}).')\n\n            # this is lame, but returning None or throwing an exception is uglier\n            return _FALLBACK_INTERPRETER\n    except AnsibleError:\n        raise\n    except Exception as ex:\n        if not is_silent:\n            display.error_as_warning(msg=f'Unhandled error in Python interpreter discovery for host {host!r}.', exception=ex)\n\n            if res and res.get('stderr'):  # the current ssh plugin implementation always has stderr, making coverage of the false case difficult\n                display.vvv(msg=f\"Interpreter discovery remote stderr:\\n{res.get('stderr')}\", host=host)\n\n    if not is_silent:\n        display.warning(\n            msg=(\n                f\"Host {host!r} is using the discovered Python interpreter at {found_interpreters[0]!r}, \"\n                \"but future installation of another Python interpreter could cause a different interpreter to be discovered.\"\n            ),\n            help_text=f\"See {get_versioned_doclink('reference_appendices/interpreter_discovery.html')} for more information.\",\n        )\n\n    return found_interpreters[0]", "loc": 62}
{"file": "ansible\\lib\\ansible\\executor\\module_common.py", "class_name": "_ModuleUtilsProcessEntry", "function_name": "from_module", "parameters": ["cls", "module", "append"], "param_types": {"module": "types.ModuleType", "append": "str | None"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.from_module_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_module(cls, module: types.ModuleType, append: str | None = None) -> t.Self:\n    name = module.__name__\n\n    if append:\n        name += '.' + append\n\n    return cls.from_module_name(name)", "loc": 7}
{"file": "ansible\\lib\\ansible\\executor\\module_common.py", "class_name": "ModuleDepFinder", "function_name": "generic_visit", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ast.iter_fields", "generic_visit", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Overridden ``generic_visit`` that makes some assumptions about our use case, and improves performance by calling visitors directly instead of calling ``visit`` to offload calling visitors.", "source_code": "def generic_visit(self, node):\n    \"\"\"Overridden ``generic_visit`` that makes some assumptions about our\n    use case, and improves performance by calling visitors directly instead\n    of calling ``visit`` to offload calling visitors.\n    \"\"\"\n    generic_visit = self.generic_visit\n    visit_map = self._visit_map\n    for field, value in ast.iter_fields(node):\n        if isinstance(value, list):\n            for item in value:\n                if isinstance(item, (Import, ImportFrom)):\n                    item.parent = node\n                    visit_map[item.__class__](item)\n                elif isinstance(item, AST):\n                    generic_visit(item)", "loc": 15}
{"file": "ansible\\lib\\ansible\\executor\\module_common.py", "class_name": "ModuleDepFinder", "function_name": "visit_Import", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["alias.name.split", "alias.name.startswith", "self.generic_visit", "self.optional_imports.add", "self.submodules.add", "tuple"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Handle import ansible.module_utils.MODLIB[.MODLIBn] [as asname] We save these as interesting submodules when the imported library is in ansible.module_utils or ansible.collections", "source_code": "def visit_Import(self, node):\n    \"\"\"\n    Handle import ansible.module_utils.MODLIB[.MODLIBn] [as asname]\n\n    We save these as interesting submodules when the imported library is in ansible.module_utils\n    or ansible.collections\n    \"\"\"\n    for alias in node.names:\n        if (alias.name.startswith('ansible.module_utils.') or\n                alias.name.startswith('ansible_collections.')):\n            py_mod = tuple(alias.name.split('.'))\n            self.submodules.add(py_mod)\n            # if the import's parent is the root document, it's a required import, otherwise it's optional\n            if node.parent != self._tree:\n                self.optional_imports.add(py_mod)\n    self.generic_visit(node)", "loc": 16}
{"file": "ansible\\lib\\ansible\\executor\\module_common.py", "class_name": "ModuleDepFinder", "function_name": "visit_ImportFrom", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "node_module.endswith", "node_module.split", "node_module.startswith", "self.generic_visit", "self.module_fqn.split", "self.optional_imports.add", "self.submodules.add", "tuple"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Handle from ansible.module_utils.MODLIB import [.MODLIBn] [as asname] Also has to handle relative imports We save these as interesting submodules when the imported library is in ansible.module_utils", "source_code": "def visit_ImportFrom(self, node):\n    \"\"\"\n    Handle from ansible.module_utils.MODLIB import [.MODLIBn] [as asname]\n\n    Also has to handle relative imports\n\n    We save these as interesting submodules when the imported library is in ansible.module_utils\n    or ansible.collections\n    \"\"\"\n\n    # FIXME: These should all get skipped:\n    # from ansible.executor import module_common\n    # from ...executor import module_common\n    # from ... import executor (Currently it gives a non-helpful error)\n    if node.level > 0:\n        # if we're in a package init, we have to add one to the node level (and make it none if 0 to preserve the right slicing behavior)\n        level_slice_offset = -node.level + 1 or None if self.is_pkg_init else -node.level\n        if self.module_fqn:\n            parts = tuple(self.module_fqn.split('.'))\n            if node.module:\n                # relative import: from .module import x\n                node_module = '.'.join(parts[:level_slice_offset] + (node.module,))\n            else:\n                # relative import: from . import x\n                node_module = '.'.join(parts[:level_slice_offset])\n        else:\n            # fall back to an absolute import\n            node_module = node.module\n    else:\n        # absolute import: from module import x\n        node_module = node.module\n\n    # Specialcase: six is a special case because of its\n    # import logic\n    py_mod = None\n    if node.names[0].name == '_six':\n        self.submodules.add(('_six',))\n    elif node_module.startswith('ansible.module_utils'):\n        # from ansible.module_utils.MODULE1[.MODULEn] import IDENTIFIER [as asname]\n        # from ansible.module_utils.MODULE1[.MODULEn] import MODULEn+1 [as asname]\n        # from ansible.module_utils.MODULE1[.MODULEn] import MODULEn+1 [,IDENTIFIER] [as asname]\n        # from ansible.module_utils import MODULE1 [,MODULEn] [as asname]\n        py_mod = tuple(node_module.split('.'))\n\n    elif node_module.startswith('ansible_collections.'):\n        if node_module.endswith('plugins.module_utils') or '.plugins.module_utils.' in node_module:\n            # from ansible_collections.ns.coll.plugins.module_utils import MODULE [as aname] [,MODULE2] [as aname]\n            # from ansible_collections.ns.coll.plugins.module_utils.MODULE import IDENTIFIER [as aname]\n            # FIXME: Unhandled cornercase (needs to be ignored):\n            # from ansible_collections.ns.coll.plugins.[!module_utils].[FOO].plugins.module_utils import IDENTIFIER\n            py_mod = tuple(node_module.split('.'))\n        else:\n            # Not from module_utils so ignore.  for instance:\n            # from ansible_collections.ns.coll.plugins.lookup import IDENTIFIER\n            pass\n\n    if py_mod:\n        for alias in node.names:\n            self.submodules.add(py_mod + (alias.name,))\n            # if the import's parent is the root document, it's a required import, otherwise it's optional\n            if node.parent != self._tree:\n                self.optional_imports.add(py_mod + (alias.name,))\n\n    self.generic_visit(node)", "loc": 64}
{"file": "ansible\\lib\\ansible\\executor\\module_common.py", "class_name": "_CachedModule", "function_name": "dump", "parameters": ["self", "path"], "param_types": {"path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["pathlib.Path", "pickle.dump", "temp_path.open", "temp_path.rename"], "control_structures": [], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def dump(self, path: str) -> None:\n    temp_path = pathlib.Path(path + '-part')\n\n    with temp_path.open('wb') as cache_file:\n        pickle.dump(self, cache_file)\n\n    temp_path.rename(path)", "loc": 7}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "HostState", "function_name": "copy", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HostState", "self.always_child_state.copy", "self.rescue_child_state.copy", "self.tasks_child_state.copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(self):\n    new_state = HostState(self._blocks)\n    new_state.handlers = self.handlers[:]\n    new_state.handler_notifications = self.handler_notifications[:]\n    new_state.cur_block = self.cur_block\n    new_state.cur_regular_task = self.cur_regular_task\n    new_state.cur_rescue_task = self.cur_rescue_task\n    new_state.cur_always_task = self.cur_always_task\n    new_state.cur_handlers_task = self.cur_handlers_task\n    new_state.run_state = self.run_state\n    new_state.fail_state = self.fail_state\n    new_state.pre_flushing_run_state = self.pre_flushing_run_state\n    new_state.update_handlers = self.update_handlers\n    new_state.pending_setup = self.pending_setup\n    new_state.did_rescue = self.did_rescue\n    new_state.did_start_at_task = self.did_start_at_task\n    if self.tasks_child_state is not None:\n        new_state.tasks_child_state = self.tasks_child_state.copy()\n    if self.rescue_child_state is not None:\n        new_state.rescue_child_state = self.rescue_child_state.copy()\n    if self.always_child_state is not None:\n        new_state.always_child_state = self.always_child_state.copy()\n    return new_state", "loc": 23}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "get_host_state", "parameters": ["self", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HostState", "self._host_states[host.name].copy", "self.set_state_for_host"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_host_state(self, host):\n    # Since we're using the PlayIterator to carry forward failed hosts,\n    # in the event that a previous host was not in the current inventory\n    # we create a stub state for it now\n    if host.name not in self._host_states:\n        self.set_state_for_host(host.name, HostState(blocks=[]))\n\n    return self._host_states[host.name].copy()", "loc": 8}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "get_next_task_for_host", "parameters": ["self", "host", "peek"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.debug", "self._get_next_task_from_state", "self.get_host_state", "self.set_state_for_host"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_next_task_for_host(self, host, peek=False):\n\n    display.debug(\"getting the next task for host %s\" % host.name)\n    s = self.get_host_state(host)\n\n    task = None\n    if s.run_state == IteratingStates.COMPLETE:\n        display.debug(\"host %s is done iterating, returning\" % host.name)\n        return (s, None)\n\n    (s, task) = self._get_next_task_from_state(s, host=host)\n\n    if not peek:\n        self.set_state_for_host(host.name, s)\n\n    display.debug(\"done getting next task for host %s\" % host.name)\n    display.debug(\" ^ state is: %s\" % s)\n    return (s, task)", "loc": 18}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "mark_host_failed", "parameters": ["self", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.debug", "self._play._removed_hosts.append", "self._set_failed_state", "self.get_host_state", "self.set_state_for_host"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mark_host_failed(self, host):\n    s = self.get_host_state(host)\n    display.debug(\"marking host %s failed, current state: %s\" % (host, s))\n    if s.run_state == IteratingStates.HANDLERS:\n        # we are failing `meta: flush_handlers`, so just reset the state to whatever\n        # it was before and let `_set_failed_state` figure out the next state\n        s.run_state = s.pre_flushing_run_state\n        s.update_handlers = True\n    s = self._set_failed_state(s)\n    display.debug(\"^ failed state is now: %s\" % s)\n    self.set_state_for_host(host.name, s)\n    self._play._removed_hosts.append(host.name)", "loc": 12}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "get_active_state", "parameters": ["self", "state"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_active_state"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Finds the active state, recursively if necessary when there are child states.", "source_code": "def get_active_state(self, state):\n    \"\"\"\n    Finds the active state, recursively if necessary when there are child states.\n    \"\"\"\n    if state.run_state == IteratingStates.TASKS and state.tasks_child_state is not None:\n        return self.get_active_state(state.tasks_child_state)\n    elif state.run_state == IteratingStates.RESCUE and state.rescue_child_state is not None:\n        return self.get_active_state(state.rescue_child_state)\n    elif state.run_state == IteratingStates.ALWAYS and state.always_child_state is not None:\n        return self.get_active_state(state.always_child_state)\n    return state", "loc": 11}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "is_any_block_rescuing", "parameters": ["self", "state"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.is_any_block_rescuing", "state.get_current_block"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Given the current HostState state, determines if the current block, or any child blocks, are in rescue mode.", "source_code": "def is_any_block_rescuing(self, state):\n    \"\"\"\n    Given the current HostState state, determines if the current block, or any child blocks,\n    are in rescue mode.\n    \"\"\"\n    if state.run_state in (IteratingStates.TASKS, IteratingStates.HANDLERS) and state.get_current_block().rescue:\n        return True\n    if state.tasks_child_state is not None:\n        return self.is_any_block_rescuing(state.tasks_child_state)\n    if state.rescue_child_state is not None:\n        return self.is_any_block_rescuing(state.rescue_child_state)\n    if state.always_child_state is not None:\n        return self.is_any_block_rescuing(state.always_child_state)\n    return False", "loc": 14}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "add_notification", "parameters": ["self", "hostname", "notification"], "param_types": {"hostname": "str", "notification": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["host_state.handler_notifications.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_notification(self, hostname: str, notification: str) -> None:\n    # preserve order\n    host_state = self._host_states[hostname]\n    if notification not in host_state.handler_notifications:\n        host_state.handler_notifications.append(notification)", "loc": 5}
{"file": "ansible\\lib\\ansible\\executor\\play_iterator.py", "class_name": "PlayIterator", "function_name": "end_host", "parameters": ["self", "hostname"], "param_types": {"hostname": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._play._removed_hosts.append", "self.get_active_state", "self.get_state_for_host", "self.set_fail_state_for_host", "self.set_run_state_for_host"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Used by ``end_host``, ``end_batch`` and ``end_play`` meta tasks to end executing given host.", "source_code": "def end_host(self, hostname: str) -> None:\n    \"\"\"Used by ``end_host``, ``end_batch`` and ``end_play`` meta tasks to end executing given host.\"\"\"\n    state = self.get_active_state(self.get_state_for_host(hostname))\n    if state.run_state == IteratingStates.RESCUE:\n        # This is a special case for when ending a host occurs in rescue.\n        # By definition the meta task responsible for ending the host\n        # is the last task, so we need to clear the fail state to mark\n        # the host as rescued.\n        # The reason we need to do that is because this operation is\n        # normally done when PlayIterator transitions from rescue to\n        # always when only then we can say that rescue didn't fail\n        # but with ending a host via meta task, we don't get to that transition.\n        self.set_fail_state_for_host(hostname, FailedStates.NONE)\n    self.set_run_state_for_host(hostname, IteratingStates.COMPLETE)\n    self._play._removed_hosts.append(hostname)", "loc": 15}
{"file": "ansible\\lib\\ansible\\executor\\stats.py", "class_name": "AggregateStats", "function_name": "increment", "parameters": ["self", "what", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "getattr(self, what).get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "helper function to bump a statistic", "source_code": "def increment(self, what, host):\n    \"\"\" helper function to bump a statistic \"\"\"\n\n    self.processed[host] = 1\n    prev = (getattr(self, what)).get(host, 0)\n    getattr(self, what)[host] = prev + 1", "loc": 6}
{"file": "ansible\\lib\\ansible\\executor\\stats.py", "class_name": "AggregateStats", "function_name": "decrement", "parameters": ["self", "what", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "getattr"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decrement(self, what, host):\n    _what = getattr(self, what)\n    try:\n        if _what[host] - 1 < 0:\n            # This should never happen, but let's be safe\n            raise KeyError(\"Don't be so negative\")\n        _what[host] -= 1\n    except KeyError:\n        _what[host] = 0", "loc": 9}
{"file": "ansible\\lib\\ansible\\executor\\stats.py", "class_name": "AggregateStats", "function_name": "summarize", "parameters": ["self", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self.changed.get", "self.dark.get", "self.failures.get", "self.ignored.get", "self.ok.get", "self.rescued.get", "self.skipped.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "return information about a particular host", "source_code": "def summarize(self, host):\n    \"\"\" return information about a particular host \"\"\"\n\n    return dict(\n        ok=self.ok.get(host, 0),\n        failures=self.failures.get(host, 0),\n        unreachable=self.dark.get(host, 0),\n        changed=self.changed.get(host, 0),\n        skipped=self.skipped.get(host, 0),\n        rescued=self.rescued.get(host, 0),\n        ignored=self.ignored.get(host, 0),\n    )", "loc": 12}
{"file": "ansible\\lib\\ansible\\executor\\stats.py", "class_name": "AggregateStats", "function_name": "set_custom_stats", "parameters": ["self", "which", "what", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "allow setting of a custom stat", "source_code": "def set_custom_stats(self, which, what, host=None):\n    \"\"\" allow setting of a custom stat\"\"\"\n\n    if host is None:\n        host = '_run'\n    if host not in self.custom:\n        self.custom[host] = {which: what}\n    else:\n        self.custom[host][which] = what", "loc": 9}
{"file": "ansible\\lib\\ansible\\executor\\stats.py", "class_name": "AggregateStats", "function_name": "update_custom_stats", "parameters": ["self", "which", "what", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "merge_hash", "self.set_custom_stats", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "allow aggregation of a custom stat", "source_code": "def update_custom_stats(self, which, what, host=None):\n    \"\"\" allow aggregation of a custom stat\"\"\"\n\n    if host is None:\n        host = '_run'\n    if host not in self.custom or which not in self.custom[host]:\n        return self.set_custom_stats(which, what, host)\n\n    # mismatching types\n    if not isinstance(what, type(self.custom[host][which])):\n        return None\n\n    if isinstance(what, MutableMapping):\n        self.custom[host][which] = merge_hash(self.custom[host][which], what)\n    else:\n        # let overloaded + take care of other types\n        self.custom[host][which] += what", "loc": 17}
{"file": "ansible\\lib\\ansible\\executor\\task_executor.py", "class_name": null, "function_name": "start_connection", "parameters": ["play_context", "options", "task_uuid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "C.config.get_config_value", "become_loader.print_paths", "cliconf_loader.print_paths", "connection_loader.print_paths", "display.display", "display.vvvv", "env.update", "getattr", "hasattr", "httpapi_loader.print_paths", "json.loads", "netconf_loader.print_paths", "os.environ.copy", "os.getppid", "os.pathsep.join", "p.communicate", "pathlib.Path", "play_context.dump_attrs", "result.get", "str", "subprocess.Popen", "terminal_loader.print_paths", "to_native", "to_text", "verbosity.append", "write_to_stream"], "control_structures": ["For", "If", "Try"], "behavior_type": ["file_io", "network_io", "serialization"], "doc_summary": "Starts the persistent connection", "source_code": "def start_connection(play_context, options, task_uuid):\n    \"\"\"\n    Starts the persistent connection\n    \"\"\"\n\n    env = os.environ.copy()\n    env.update({\n        # HACK; most of these paths may change during the controller's lifetime\n        # (eg, due to late dynamic role includes, multi-playbook execution), without a way\n        # to invalidate/update, the persistent connection helper won't always see the same plugins the controller\n        # can.\n        'ANSIBLE_BECOME_PLUGINS': become_loader.print_paths(),\n        'ANSIBLE_CLICONF_PLUGINS': cliconf_loader.print_paths(),\n        'ANSIBLE_COLLECTIONS_PATH': to_native(os.pathsep.join(AnsibleCollectionConfig.collection_paths)),\n        'ANSIBLE_CONNECTION_PLUGINS': connection_loader.print_paths(),\n        'ANSIBLE_HTTPAPI_PLUGINS': httpapi_loader.print_paths(),\n        'ANSIBLE_NETCONF_PLUGINS': netconf_loader.print_paths(),\n        'ANSIBLE_TERMINAL_PLUGINS': terminal_loader.print_paths(),\n    })\n    verbosity = []\n    if display.verbosity:\n        verbosity.append('-%s' % ('v' * display.verbosity))\n\n    if not (cli_stub_path := C.config.get_config_value('_ANSIBLE_CONNECTION_PATH')):\n        cli_stub_path = str(pathlib.Path(scripts.__file__).parent / CLI_STUB_NAME)\n\n    p = subprocess.Popen(\n        [sys.executable, cli_stub_path, *verbosity, to_text(os.getppid()), to_text(task_uuid)],\n        stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env,\n    )\n\n    write_to_stream(p.stdin, options)\n    write_to_stream(p.stdin, play_context.dump_attrs())\n\n    (stdout, stderr) = p.communicate()\n\n    if p.returncode == 0:\n        result = json.loads(to_text(stdout, errors='surrogate_then_replace'))\n    else:\n        try:\n            result = json.loads(to_text(stderr, errors='surrogate_then_replace'))\n        except json.decoder.JSONDecodeError:\n            result = {'error': to_text(stderr, errors='surrogate_then_replace')}\n\n    if 'messages' in result:\n        for level, message in result['messages']:\n            if level == 'log':\n                display.display(message, log_only=True)\n            elif level in ('debug', 'v', 'vv', 'vvv', 'vvvv', 'vvvvv', 'vvvvvv'):\n                getattr(display, level)(message, host=play_context.remote_addr)\n            else:\n                if hasattr(display, level):\n                    getattr(display, level)(message)\n                else:\n                    display.vvvv(message, host=play_context.remote_addr)\n\n    if 'error' in result:\n        if display.verbosity > 2:\n            if result.get('exception'):\n                msg = \"The full traceback is:\\n\" + result['exception']\n                display.display(msg, color=C.COLOR_ERROR)\n        raise AnsibleError(result['error'])\n\n    return result['socket_path']", "loc": 64}
{"file": "ansible\\lib\\ansible\\executor\\task_executor.py", "class_name": "TaskExecutor", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_error_utils.result_dict_from_exception", "dict", "display.debug", "isinstance", "item.get", "item.pop", "len", "res.get", "res.update", "result.update", "self._connection.close", "self._execute", "self._get_loop_items", "self._run_loop", "self._task.update_result_no_log", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "The main executor entrypoint, where we determine if the specified task requires looping and either runs the task with self._run_loop() or self._execute(). After that, the returned results are parsed and", "source_code": "def run(self):\n    \"\"\"\n    The main executor entrypoint, where we determine if the specified\n    task requires looping and either runs the task with self._run_loop()\n    or self._execute(). After that, the returned results are parsed and\n    returned as a dict.\n    \"\"\"\n\n    display.debug(\"in run() - task %s\" % self._task._uuid)\n\n    try:\n        try:\n            items = self._get_loop_items()\n        except AnsibleUndefinedVariable as e:\n            # save the error raised here for use later\n            items = None\n            self._loop_eval_error = e\n\n        if items is not None:\n            if len(items) > 0:\n                item_results = self._run_loop(items)\n\n                # create the overall result item\n                res = dict(results=item_results)\n\n                # loop through the item results and set the global changed/failed/skipped result flags based on any item.\n                res['skipped'] = True\n                for item in item_results:\n                    if item.get('_ansible_no_log'):\n                        res.update(_ansible_no_log=True)  # ensure no_log processing recognizes at least one item needs to be censored\n\n                    if 'changed' in item and item['changed'] and not res.get('changed'):\n                        res['changed'] = True\n                    if res['skipped'] and ('skipped' not in item or ('skipped' in item and not item['skipped'])):\n                        res['skipped'] = False\n                    # FIXME: normalize `failed` to a bool, warn if the action/module used non-bool\n                    if 'failed' in item and item['failed']:\n                        item_ignore = item.pop('_ansible_ignore_errors')\n                        if not res.get('failed'):\n                            res['failed'] = True\n                            res['msg'] = 'One or more items failed'\n                            self._task.ignore_errors = item_ignore\n                        elif self._task.ignore_errors and not item_ignore:\n                            self._task.ignore_errors = item_ignore\n                    if 'unreachable' in item and item['unreachable']:\n                        item_ignore_unreachable = item.pop('_ansible_ignore_unreachable')\n                        if not res.get('unreachable'):\n                            res['unreachable'] = True\n                            self._task.ignore_unreachable = item_ignore_unreachable\n                        elif self._task.ignore_unreachable and not item_ignore_unreachable:\n                            self._task.ignore_unreachable = item_ignore_unreachable\n\n                    # ensure to accumulate these\n                    for array in ['warnings', 'deprecations']:\n                        if array in item and item[array]:\n                            if array not in res:\n                                res[array] = []\n                            if not isinstance(item[array], list):\n                                item[array] = [item[array]]\n                            res[array] = res[array] + item[array]\n                            del item[array]\n\n                # FIXME: normalize `failed` to a bool, warn if the action/module used non-bool\n                if not res.get('failed', False):\n                    res['msg'] = 'All items completed'\n                if res['skipped']:\n                    res['msg'] = 'All items skipped'\n            else:\n                res = dict(changed=False, skipped=True, skipped_reason='No items in the list', results=[])\n        else:\n            display.debug(\"calling self._execute()\")\n            res = self._execute(self._task_templar, self._job_vars)\n            display.debug(\"_execute() done\")\n\n        # make sure changed is set in the result, if it's not present\n        if 'changed' not in res:\n            res['changed'] = False\n\n        return res\n    except Exception as ex:\n        result = _error_utils.result_dict_from_exception(ex)\n\n        self._task.update_result_no_log(self._task_templar, result)\n\n        if not isinstance(ex, AnsibleError):\n            result.update(msg=f'Unexpected failure during task execution: {result[\"msg\"]}')\n\n        return result\n    finally:\n        try:\n            self._connection.close()\n        except AttributeError:\n            pass\n        except Exception as e:\n            display.debug(u\"error closing connection: %s\" % to_text(e))", "loc": 95}
{"file": "ansible\\lib\\ansible\\executor\\task_executor.py", "class_name": null, "function_name": "invoke_lookup", "parameters": [], "param_types": {}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_invoke_lookup", "dict"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Scope-capturing wrapper around _invoke_lookup to avoid functools.partial obscuring its usage from type-checking tools.", "source_code": "def invoke_lookup() -> t.Any:\n    \"\"\"Scope-capturing wrapper around _invoke_lookup to avoid functools.partial obscuring its usage from type-checking tools.\"\"\"\n    return _invoke_lookup(\n        plugin_name=self._task.loop_with,\n        lookup_terms=terms,\n        lookup_kwargs=dict(wantlist=True),\n        invoked_as_with=True,\n    )", "loc": 8}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": null, "function_name": "censor_result", "parameters": ["result"], "param_types": {"result": "_c.Mapping[str, t.Any]"}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["censored_result.update", "result.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def censor_result(result: _c.Mapping[str, t.Any]) -> dict[str, t.Any]:\n    censored_result = {key: value for key in _PRESERVE if (value := result.get(key, ...)) is not ...}\n    censored_result.update(censored=\"the output has been hidden due to the fact that 'no_log: true' was specified for this result\")\n\n    return censored_result", "loc": 5}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": "_BaseTaskResult", "function_name": "is_skipped", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all", "bool", "isinstance", "loop_res.get", "self._return_data.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_skipped(self) -> bool:\n    if self._loop_results:\n        # Loop tasks are only considered skipped if all items were skipped.\n        # some squashed results (eg, dnf) are not dicts and can't be skipped individually\n        if all(isinstance(loop_res, dict) and loop_res.get('skipped', False) for loop_res in self._loop_results):\n            return True\n\n    # regular tasks and squashed non-dict results\n    return bool(self._return_data.get('skipped', False))", "loc": 9}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": "_BaseTaskResult", "function_name": "is_failed", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "isinstance", "self._check_key"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_failed(self) -> bool:\n    if 'failed_when_result' in self._return_data or any(isinstance(loop_res, dict) and 'failed_when_result' in loop_res for loop_res in self._loop_results):\n        return self._check_key('failed_when_result')\n\n    return self._check_key('failed')", "loc": 5}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": "_BaseTaskResult", "function_name": "needs_debugger", "parameters": ["self", "globally_enabled"], "param_types": {"globally_enabled": "bool"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.is_failed", "self.is_skipped", "self.is_unreachable", "self.task_fields.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def needs_debugger(self, globally_enabled: bool = False) -> bool:\n    _debugger = self.task_fields.get('debugger')\n    _ignore_errors = constants.TASK_DEBUGGER_IGNORE_ERRORS and self.task_fields.get('ignore_errors')\n\n    ret = False\n\n    if globally_enabled and ((self.is_failed() and not _ignore_errors) or self.is_unreachable()):\n        ret = True\n\n    if _debugger in ('always',):\n        ret = True\n    elif _debugger in ('never',):\n        ret = False\n    elif _debugger in ('on_failed',) and self.is_failed() and not _ignore_errors:\n        ret = True\n    elif _debugger in ('on_unreachable',) and self.is_unreachable():\n        ret = True\n    elif _debugger in ('on_skipped',) and self.is_skipped():\n        ret = True\n\n    return ret", "loc": 21}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": "_RawTaskResult", "function_name": "as_callback_task_result", "parameters": ["self"], "param_types": {}, "return_type": "CallbackTaskResult", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CallbackTaskResult", "_SUB_PRESERVE.items", "censor_result", "censored_result.update", "isinstance", "loop_res.get", "module_response_deepcopy", "module_response_deepcopy(self._return_data).items", "return_data.update", "self._return_data.get", "strip_internal_keys", "sub_data.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a `CallbackTaskResult` from this instance.", "source_code": "def as_callback_task_result(self) -> CallbackTaskResult:\n    \"\"\"Return a `CallbackTaskResult` from this instance.\"\"\"\n    ignore: tuple[str, ...]\n\n    # statuses are already reflected on the event type\n    if self.task and self.task.action in constants._ACTION_DEBUG:\n        # debug is verbose by default to display vars, no need to add invocation\n        ignore = _IGNORE + ('invocation',)\n    else:\n        ignore = _IGNORE\n\n    subset: dict[str, dict[str, object]] = {}\n\n    # preserve subset for later\n    for sub, sub_keys in _SUB_PRESERVE.items():\n        sub_data = self._return_data.get(sub)\n\n        if isinstance(sub_data, dict):\n            subset[sub] = {key: value for key, value in sub_data.items() if key in sub_keys}\n\n    # DTFIX-FUTURE: is checking no_log here redundant now that we use _ansible_no_log everywhere?\n    if isinstance(self.task.no_log, bool) and self.task.no_log or self._return_data.get('_ansible_no_log'):\n        censored_result = censor_result(self._return_data)\n\n        if self._loop_results:\n            # maintain shape for loop results so callback behavior recognizes a loop was performed\n            censored_result.update(results=[\n                censor_result(loop_res) if isinstance(loop_res, dict) and loop_res.get('_ansible_no_log') else loop_res for loop_res in self._loop_results\n            ])\n\n        return_data = censored_result\n    elif self._return_data:\n        return_data = {k: v for k, v in module_response_deepcopy(self._return_data).items() if k not in ignore}\n\n        # remove almost ALL internal keys, keep ones relevant to callback\n        strip_internal_keys(return_data, exceptions=CLEAN_EXCEPTIONS)\n    else:\n        return_data = {}\n\n    # keep subset\n    return_data.update(subset)\n\n    return CallbackTaskResult(self.host, self.task, return_data, self.task_fields)", "loc": 43}
{"file": "ansible\\lib\\ansible\\executor\\task_result.py", "class_name": "CallbackTaskResult", "function_name": "result", "parameters": ["self"], "param_types": {}, "return_type": "_c.MutableMapping[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_vars.transform_to_native_types", "t.cast"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def result(self) -> _c.MutableMapping[str, t.Any]:\n    \"\"\"\n    Returns a cached copy of the task result dictionary for consumption by callbacks.\n    Internal custom types are transformed to native Python types to facilitate access and serialization.\n    \"\"\"\n    return t.cast(_c.MutableMapping[str, t.Any], _vars.transform_to_native_types(self._return_data))", "loc": 6}
{"file": "ansible\\lib\\ansible\\executor\\powershell\\module_manifest.py", "class_name": "PSModuleDepFinder", "function_name": "scan_exec_script", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_ScriptInfo", "_get_powershell_script", "self.scan_module"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def scan_exec_script(self, name: str) -> None:\n    # scans lib/ansible/executor/powershell for scripts used in the module\n    # exec side. It also scans these scripts for any dependencies\n    if name in self.scripts:\n        return\n\n    exec_code = _get_powershell_script(name)\n    self.scripts[name] = _ScriptInfo(\n        content=exec_code,\n        path=name,\n    )\n    self.scan_module(exec_code, powershell=True)", "loc": 12}
{"file": "ansible\\lib\\ansible\\executor\\powershell\\module_manifest.py", "class_name": "PSModuleDepFinder", "function_name": "scan_module", "parameters": ["self", "module_data", "fqn", "powershell"], "param_types": {"module_data": "bytes", "fqn": "str | None", "powershell": "bool"}, "return_type": "set[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "_get_powershell_signed_hashlist", "dependencies.add", "dependencies.update", "fqn.split", "fqn.startswith", "match.group", "match.group(1).rstrip", "match.groupdict", "match_dict.get", "module_data.split", "module_utils.add", "pattern.match", "self._parse_version_match", "self._re_become.match", "self._re_os_version.match", "self._re_ps_version.match", "self._scan_module_util", "self.signed_hashlist.add", "set", "to_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def scan_module(\n    self,\n    module_data: bytes,\n    fqn: str | None = None,\n    powershell: bool = True,\n) -> set[str]:\n    lines = module_data.split(b'\\n')\n    module_utils: set[tuple[str, str, bool]] = set()\n\n    if fqn and fqn.startswith(\"ansible_collections.\"):\n        submodules = fqn.split('.')\n        collection_name = '.'.join(submodules[:3])\n\n        collection_hashlist = _get_powershell_signed_hashlist(collection_name)\n        if collection_hashlist and collection_hashlist.path not in self.signed_hashlist:\n            self.signed_hashlist.add(collection_hashlist.path)\n            self.scripts[collection_hashlist.path] = collection_hashlist\n\n    if powershell:\n        checks = [\n            # PS module contains '#Requires -Module Ansible.ModuleUtils.*'\n            # PS module contains '#AnsibleRequires -Powershell Ansible.*' (or collections module_utils ref)\n            (self._re_ps_module, \".psm1\"),\n            # PS module contains '#AnsibleRequires -CSharpUtil Ansible.*' (or collections module_utils ref)\n            (self._re_cs_in_ps_module, \".cs\"),\n        ]\n    else:\n        checks = [\n            # CS module contains 'using Ansible.*;' or 'using ansible_collections.ns.coll.plugins.module_utils.*;'\n            (self._re_cs_module, \".cs\"),\n        ]\n\n    for line in lines:\n        for patterns, util_extension in checks:\n            for pattern in patterns:\n                match = pattern.match(line)\n                if match:\n                    # tolerate windows line endings by stripping any remaining\n                    # newline chars\n                    module_util_name = to_text(match.group(1).rstrip())\n                    match_dict = match.groupdict()\n                    optional = match_dict.get('optional', None) is not None\n                    module_utils.add((module_util_name, util_extension, optional))\n                    break\n\n        if not powershell:\n            continue\n\n        if ps_version_match := self._re_ps_version.match(line):\n            self._parse_version_match(ps_version_match, \"ps_version\")\n\n        if os_version_match := self._re_os_version.match(line):\n            self._parse_version_match(os_version_match, \"os_version\")\n\n        # once become is set, no need to keep on checking recursively\n        if not self.become and self._re_become.match(line):\n            self.become = True\n\n    dependencies: set[str] = set()\n    for name, ext, optional in set(module_utils):\n        util_name = self._scan_module_util(name, ext, fqn, optional)\n        if util_name:\n            dependencies.add(util_name)\n            util_deps = self._util_deps[util_name]\n            dependencies.update(util_deps)\n\n    return dependencies", "loc": 67}
{"file": "ansible\\lib\\ansible\\executor\\process\\worker.py", "class_name": "WorkerQueue", "function_name": "get", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "super", "super(WorkerQueue, self).get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self, *args, **kwargs):\n    result = super(WorkerQueue, self).get(*args, **kwargs)\n    if isinstance(result, AnsibleError):\n        raise result\n    return result", "loc": 5}
{"file": "ansible\\lib\\ansible\\executor\\process\\worker.py", "class_name": "WorkerProcess", "function_name": "start", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super(WorkerProcess, self).start"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "multiprocessing.Process replaces the worker's stdin with a new file but we wish to preserve it if it is connected to a terminal. Therefore dup a copy prior to calling the real start(),", "source_code": "def start(self) -> None:\n    \"\"\"\n    multiprocessing.Process replaces the worker's stdin with a new file\n    but we wish to preserve it if it is connected to a terminal.\n    Therefore dup a copy prior to calling the real start(),\n    ensuring the descriptor is preserved somewhere in the new child, and\n    make sure it is closed in the parent when start() completes.\n    \"\"\"\n\n    # FUTURE: this lock can be removed once a more generalized pre-fork thread pause is in place\n    with display._lock:\n        super(WorkerProcess, self).start()", "loc": 12}
{"file": "ansible\\lib\\ansible\\executor\\process\\worker.py", "class_name": "WorkerProcess", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_task.TaskContext", "display.set_queue", "self._detach", "self._hard_exit", "self._run", "signal.signal", "traceback.format_exc"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Wrap _run() to ensure no possibility an errant exception can cause control to return to the StrategyBase task loop, or any other code higher in the stack.", "source_code": "def run(self) -> None:\n    \"\"\"\n    Wrap _run() to ensure no possibility an errant exception can cause\n    control to return to the StrategyBase task loop, or any other code\n    higher in the stack.\n\n    As multiprocessing in Python 2.x provides no protection, it is possible\n    a try/except added in far-away code can cause a crashed child process\n    to suddenly assume the role and prior state of its parent.\n    \"\"\"\n    # Set the queue on Display so calls to Display.display are proxied over the queue\n    display.set_queue(self._final_q)\n    self._detach()\n    # propagate signals\n    signal.signal(signal.SIGINT, self._term)\n    signal.signal(signal.SIGTERM, self._term)\n    try:\n        with _task.TaskContext(self._task):\n            return self._run()\n    except BaseException:\n        self._hard_exit(traceback.format_exc())", "loc": 21}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": null, "function_name": "should_retry_error", "parameters": ["exception"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def should_retry_error(exception):\n    # Note: cloud.redhat.com masks rate limit errors with 403 (Forbidden) error codes.\n    # Since 403 could reflect the actual problem (such as an expired token), we should\n    # not retry by default.\n    if isinstance(exception, GalaxyError) and exception.http_code in RETRY_HTTP_ERROR_CODES:\n        return True\n\n    if isinstance(exception, AnsibleError) and (cause := exception.__cause__):\n        # URLError is often a proxy for an underlying error, handle wrapped exceptions\n        if isinstance(cause, URLError):\n            cause = cause.reason\n\n        # Handle common URL related errors\n        if isinstance(cause, (TimeoutError, BadStatusLine, IncompleteRead)):\n            return True\n\n    return False", "loc": 17}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": null, "function_name": "g_connect", "parameters": ["versions"], "param_types": {}, "return_type": null, "param_doc": {"versions": "A list of API versions that the function supports."}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleError", "_urljoin", "available_versions.keys", "display.vvvv", "method", "n_url.endswith", "self._available_api_versions.keys", "self._call_galaxy", "set", "set(versions).intersection"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Wrapper to lazily initialize connection info to Galaxy and verify the API versions required are available on the endpoint.", "source_code": "def g_connect(versions):\n    \"\"\"\n    Wrapper to lazily initialize connection info to Galaxy and verify the API versions required are available on the\n    endpoint.\n\n    :param versions: A list of API versions that the function supports.\n    \"\"\"\n    def decorator(method):\n        def wrapped(self, *args, **kwargs):\n            if not self._available_api_versions:\n                display.vvvv(\"Initial connection to galaxy_server: %s\" % self.api_server)\n\n                # Determine the type of Galaxy server we are talking to. First try it unauthenticated then with Bearer\n                # auth for Automation Hub.\n                n_url = self.api_server\n                error_context_msg = 'Error when finding available api versions from %s (%s)' % (self.name, n_url)\n\n                if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':\n                    n_url = 'https://galaxy.ansible.com/api/'\n\n                try:\n                    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n                except (AnsibleError, GalaxyError, ValueError, KeyError) as err:\n                    # Either the URL doesn't exist, or other error. Or the URL exists, but isn't a galaxy API\n                    # root (not JSON, no 'available_versions') so try appending '/api/'\n                    if n_url.endswith('/api') or n_url.endswith('/api/'):\n                        raise\n\n                    # Let exceptions here bubble up but raise the original if this returns a 404 (/api/ wasn't found).\n                    n_url = _urljoin(n_url, '/api/')\n                    try:\n                        data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n                    except GalaxyError as new_err:\n                        if new_err.http_code == 404:\n                            raise err\n                        raise\n\n                if 'available_versions' not in data:\n                    raise AnsibleError(\"Tried to find galaxy API root at %s but no 'available_versions' are available \"\n                                       \"on %s\" % (n_url, self.api_server))\n\n                # Update api_server to point to the \"real\" API root, which in this case could have been the configured\n                # url + '/api/' appended.\n                self.api_server = n_url\n\n                self._available_api_versions = available_versions = data['available_versions']\n                display.vvvv(\"Found API version '%s' with Galaxy server %s (%s)\"\n                             % (', '.join(available_versions.keys()), self.name, self.api_server))\n\n            # Verify that the API versions the function works with are available on the server specified.\n            available_versions = set(self._available_api_versions.keys())\n            common_versions = set(versions).intersection(available_versions)\n            if not common_versions:\n                raise AnsibleError(\"Galaxy action %s requires API versions '%s' but only '%s' are available on %s %s\"\n                                   % (method.__name__, \", \".join(versions), \", \".join(available_versions),\n                                      self.name, self.api_server))\n\n            return method(self, *args, **kwargs)\n        return wrapped\n    return decorator", "loc": 60}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": null, "function_name": "get_cache_id", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["urlparse"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Gets the cache ID for the URL specified.", "source_code": "def get_cache_id(url):\n    \"\"\" Gets the cache ID for the URL specified. \"\"\"\n    url_info = urlparse(url)\n\n    port = None\n    try:\n        port = url_info.port\n    except ValueError:\n        pass  # While the URL is probably invalid, let the caller figure that out when using it\n\n    # Cannot use netloc because it could contain credentials if the server specified had them in there.\n    return '%s:%s' % (url_info.hostname, port or '')", "loc": 12}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": null, "function_name": "decorator", "parameters": ["method"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleError", "_urljoin", "available_versions.keys", "display.vvvv", "method", "n_url.endswith", "self._available_api_versions.keys", "self._call_galaxy", "set", "set(versions).intersection"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decorator(method):\n    def wrapped(self, *args, **kwargs):\n        if not self._available_api_versions:\n            display.vvvv(\"Initial connection to galaxy_server: %s\" % self.api_server)\n\n            # Determine the type of Galaxy server we are talking to. First try it unauthenticated then with Bearer\n            # auth for Automation Hub.\n            n_url = self.api_server\n            error_context_msg = 'Error when finding available api versions from %s (%s)' % (self.name, n_url)\n\n            if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':\n                n_url = 'https://galaxy.ansible.com/api/'\n\n            try:\n                data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n            except (AnsibleError, GalaxyError, ValueError, KeyError) as err:\n                # Either the URL doesn't exist, or other error. Or the URL exists, but isn't a galaxy API\n                # root (not JSON, no 'available_versions') so try appending '/api/'\n                if n_url.endswith('/api') or n_url.endswith('/api/'):\n                    raise\n\n                # Let exceptions here bubble up but raise the original if this returns a 404 (/api/ wasn't found).\n                n_url = _urljoin(n_url, '/api/')\n                try:\n                    data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n                except GalaxyError as new_err:\n                    if new_err.http_code == 404:\n                        raise err\n                    raise\n\n            if 'available_versions' not in data:\n                raise AnsibleError(\"Tried to find galaxy API root at %s but no 'available_versions' are available \"\n                                   \"on %s\" % (n_url, self.api_server))\n\n            # Update api_server to point to the \"real\" API root, which in this case could have been the configured\n            # url + '/api/' appended.\n            self.api_server = n_url\n\n            self._available_api_versions = available_versions = data['available_versions']\n            display.vvvv(\"Found API version '%s' with Galaxy server %s (%s)\"\n                         % (', '.join(available_versions.keys()), self.name, self.api_server))\n\n        # Verify that the API versions the function works with are available on the server specified.\n        available_versions = set(self._available_api_versions.keys())\n        common_versions = set(versions).intersection(available_versions)\n        if not common_versions:\n            raise AnsibleError(\"Galaxy action %s requires API versions '%s' but only '%s' are available on %s %s\"\n                               % (method.__name__, \", \".join(versions), \", \".join(available_versions),\n                                  self.name, self.api_server))\n\n        return method(self, *args, **kwargs)\n    return wrapped", "loc": 52}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "authenticate", "parameters": ["self", "github_token"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "GalaxyError", "_urljoin", "g_connect", "json.loads", "open_url", "resp.read", "to_text", "urlencode", "user_agent"], "control_structures": ["Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Retrieve an authentication token", "source_code": "def authenticate(self, github_token):\n    \"\"\"\n    Retrieve an authentication token\n    \"\"\"\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"tokens\") + '/'\n    args = urlencode({\"github_token\": github_token})\n\n    try:\n        resp = open_url(url, data=args, validate_certs=self.validate_certs, method=\"POST\", http_agent=user_agent(), timeout=self._server_timeout)\n    except HTTPError as e:\n        raise GalaxyError(e, 'Attempting to authenticate to galaxy')\n    except Exception as ex:\n        raise AnsibleError('Unable to authenticate to galaxy.') from ex\n\n    data = json.loads(to_text(resp.read(), errors='surrogate_or_strict'))\n    return data", "loc": 16}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "create_import_task", "parameters": ["self", "github_user", "github_repo", "reference", "role_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_urljoin", "data.get", "g_connect", "self._call_galaxy", "urlencode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Post an import request", "source_code": "def create_import_task(self, github_user, github_repo, reference=None, role_name=None):\n    \"\"\"\n    Post an import request\n    \"\"\"\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"imports\") + '/'\n    args = {\n        \"github_user\": github_user,\n        \"github_repo\": github_repo,\n        \"github_reference\": reference if reference else \"\"\n    }\n    if role_name:\n        args['alternate_role_name'] = role_name\n    data = self._call_galaxy(url, args=urlencode(args), method=\"POST\")\n    if data.get('results', None):\n        return data['results']\n    return data", "loc": 16}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_import_task", "parameters": ["self", "task_id", "github_user", "github_repo"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_urljoin", "g_connect", "self._call_galaxy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Check the status of an import task.", "source_code": "def get_import_task(self, task_id=None, github_user=None, github_repo=None):\n    \"\"\"\n    Check the status of an import task.\n    \"\"\"\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"imports\")\n    if task_id is not None:\n        url = \"%s?id=%d\" % (url, task_id)\n    elif github_user is not None and github_repo is not None:\n        url = \"%s?github_user=%s&github_repo=%s\" % (url, github_user, github_repo)\n    else:\n        raise AnsibleError(\"Expected task_id or github_user and github_repo\")\n\n    data = self._call_galaxy(url)\n    return data['results']", "loc": 14}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "lookup_role_by_name", "parameters": ["self", "role_name", "notify"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "AnsibleError", "_urljoin", "display.display", "g_connect", "len", "role_name.split", "self._call_galaxy", "to_bytes", "to_text", "urlquote"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Find a role by name.", "source_code": "def lookup_role_by_name(self, role_name, notify=True):\n    \"\"\"\n    Find a role by name.\n    \"\"\"\n    role_name = to_text(urlquote(to_bytes(role_name)))\n\n    try:\n        parts = role_name.split(\".\")\n        user_name = \".\".join(parts[0:-1])\n        role_name = parts[-1]\n        if notify:\n            display.display(\"- downloading role '%s', owned by %s\" % (role_name, user_name))\n    except Exception:\n        raise AnsibleError(\"Invalid role name (%s). Specify role as format: username.rolename\" % role_name)\n\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"roles\",\n                   \"?owner__username=%s&name=%s\" % (user_name, role_name))\n    data = self._call_galaxy(url)\n    if len(data[\"results\"]) != 0:\n        return data[\"results\"][0]\n    return None", "loc": 21}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "fetch_role_related", "parameters": ["self", "related", "role_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_urljoin", "data.get", "display.warning", "g_connect", "self._call_galaxy", "to_text", "urlparse"], "control_structures": ["Try", "While"], "behavior_type": ["logic"], "doc_summary": "Fetch the list of related items for the given role. The url comes from the 'related' field of the role.", "source_code": "def fetch_role_related(self, related, role_id):\n    \"\"\"\n    Fetch the list of related items for the given role.\n    The url comes from the 'related' field of the role.\n    \"\"\"\n\n    results = []\n    try:\n        url = _urljoin(self.api_server, self.available_api_versions['v1'], \"roles\", role_id, related,\n                       \"?page_size=50\")\n        data = self._call_galaxy(url)\n        results = data['results']\n        done = (data.get('next_link', None) is None)\n\n        # https://github.com/ansible/ansible/issues/64355\n        # api_server contains part of the API path but next_link includes the /api part so strip it out.\n        url_info = urlparse(self.api_server)\n        base_url = \"%s://%s/\" % (url_info.scheme, url_info.netloc)\n\n        while not done:\n            url = _urljoin(base_url, data['next_link'])\n            data = self._call_galaxy(url)\n            results += data['results']\n            done = (data.get('next_link', None) is None)\n    except Exception as e:\n        display.warning(\"Unable to retrieve role (id=%s) data (%s), but this is not fatal so we continue: %s\"\n                        % (role_id, related, to_text(e)))\n    return results", "loc": 28}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_list", "parameters": ["self", "what"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_urljoin", "data.get", "g_connect", "self._call_galaxy", "to_native"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Fetch the list of items specified.", "source_code": "def get_list(self, what):\n    \"\"\"\n    Fetch the list of items specified.\n    \"\"\"\n    try:\n        url = _urljoin(self.api_server, self.available_api_versions['v1'], what, \"?page_size\")\n        data = self._call_galaxy(url)\n        if \"results\" in data:\n            results = data['results']\n        else:\n            results = data\n        done = True\n        if \"next\" in data:\n            done = (data.get('next_link', None) is None)\n        while not done:\n            url = _urljoin(self.api_server, data['next_link'])\n            data = self._call_galaxy(url)\n            results += data['results']\n            done = (data.get('next_link', None) is None)\n        return results\n    except Exception as error:\n        raise AnsibleError(\"Failed to download the %s list: %s\" % (what, to_native(error)))", "loc": 22}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "search_roles", "parameters": ["self", "search"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'+'.join", "_urljoin", "g_connect", "isinstance", "kwargs.get", "platforms.split", "self._call_galaxy", "tags.split", "to_bytes", "to_text", "urlquote"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def search_roles(self, search, **kwargs):\n\n    search_url = _urljoin(self.api_server, self.available_api_versions['v1'], \"search\", \"roles\", \"?\")\n\n    if search:\n        search_url += '&autocomplete=' + to_text(urlquote(to_bytes(search)))\n\n    tags = kwargs.get('tags', None)\n    platforms = kwargs.get('platforms', None)\n    page_size = kwargs.get('page_size', None)\n    author = kwargs.get('author', None)\n\n    if tags and isinstance(tags, str):\n        tags = tags.split(',')\n        search_url += '&tags_autocomplete=' + '+'.join(tags)\n\n    if platforms and isinstance(platforms, str):\n        platforms = platforms.split(',')\n        search_url += '&platforms_autocomplete=' + '+'.join(platforms)\n\n    if page_size:\n        search_url += '&page_size=%s' % page_size\n\n    if author:\n        search_url += '&username_autocomplete=%s' % author\n\n    data = self._call_galaxy(search_url)\n    return data", "loc": 28}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "add_secret", "parameters": ["self", "source", "github_user", "github_repo", "secret"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_urljoin", "g_connect", "self._call_galaxy", "urlencode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_secret(self, source, github_user, github_repo, secret):\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"notification_secrets\") + '/'\n    args = urlencode({\n        \"source\": source,\n        \"github_user\": github_user,\n        \"github_repo\": github_repo,\n        \"secret\": secret\n    })\n    data = self._call_galaxy(url, args=args, method=\"POST\")\n    return data", "loc": 10}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "delete_role", "parameters": ["self", "github_user", "github_repo"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_urljoin", "g_connect", "self._call_galaxy"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def delete_role(self, github_user, github_repo):\n    url = _urljoin(self.api_server, self.available_api_versions['v1'], \"removerole\",\n                   \"?github_user=%s&github_repo=%s\" % (github_user, github_repo))\n    data = self._call_galaxy(url, auth_required=True, method='DELETE')\n    return data", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "wait_import_task", "parameters": ["self", "task_url", "timeout"], "param_types": {}, "return_type": null, "param_doc": {"task_id": "The id of the import task to wait for. This can be parsed out of the return", "timeout": "The timeout in seconds, 0 is no timeout."}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "data.get", "data['error'].get", "display.display", "display.error", "display.vvv", "display.warning", "g_connect", "level.lower", "min", "self._call_galaxy", "time.sleep", "time.time", "to_native"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Waits until the import process on the Galaxy server has completed or the timeout is reached.", "source_code": "def wait_import_task(self, task_url, timeout=0):\n    \"\"\"\n    Waits until the import process on the Galaxy server has completed or the timeout is reached.\n\n    :param task_id: The id of the import task to wait for. This can be parsed out of the return\n        value for GalaxyAPI.publish_collection.\n    :param timeout: The timeout in seconds, 0 is no timeout.\n    \"\"\"\n    state = 'waiting'\n    data = None\n\n    display.display(\"Waiting until Galaxy import task %s has completed\" % task_url)\n    start = time.time()\n    wait = C.GALAXY_COLLECTION_IMPORT_POLL_INTERVAL\n\n    while timeout == 0 or (time.time() - start) < timeout:\n        try:\n            data = self._call_galaxy(task_url, method='GET', auth_required=True,\n                                     error_context_msg='Error when getting import task results at %s' % task_url)\n        except GalaxyError as e:\n            if e.http_code != 404:\n                raise\n            # The import job may not have started, and as such, the task url may not yet exist\n            display.vvv('Galaxy import process has not started, wait %s seconds before trying again' % wait)\n            time.sleep(wait)\n            continue\n\n        state = data.get('state', 'waiting')\n\n        if data.get('finished_at', None):\n            break\n\n        display.vvv('Galaxy import process has a status of %s, wait %d seconds before trying again'\n                    % (state, wait))\n        time.sleep(wait)\n\n        # poor man's exponential backoff algo so we don't flood the Galaxy API, cap at 30 seconds.\n        wait = min(30, wait * C.GALAXY_COLLECTION_IMPORT_POLL_FACTOR)\n    if state == 'waiting':\n        raise AnsibleError(\"Timeout while waiting for the Galaxy import process to finish, check progress at '%s'\"\n                           % to_native(task_url))\n\n    for message in data.get('messages', []):\n        level = message['level']\n        if level.lower() == 'error':\n            display.error(\"Galaxy import error message: %s\" % message['message'])\n        elif level.lower() == 'warning':\n            display.warning(\"Galaxy import warning message: %s\" % message['message'])\n        else:\n            display.vvv(\"Galaxy import message: %s - %s\" % (level, message['message']))\n\n    if state == 'failed':\n        code = to_native(data['error'].get('code', 'UNKNOWN'))\n        description = to_native(\n            data['error'].get('description', \"Unknown error, see %s for more details\" % task_url))\n        raise AnsibleError(\"Galaxy import process failed: %s (Code: %s)\" % (description, code))", "loc": 56}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_collection_metadata", "parameters": ["self", "namespace", "name"], "param_types": {}, "return_type": null, "param_doc": {"namespace": "The collection namespace.", "name": "The collection name."}, "return_doc": "", "raises_doc": [], "called_functions": ["CollectionMetadata", "_urljoin", "data.get", "g_connect", "self._call_galaxy"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Gets the collection information from the Galaxy server about a specific Collection.", "source_code": "def get_collection_metadata(self, namespace, name):\n    \"\"\"\n    Gets the collection information from the Galaxy server about a specific Collection.\n\n    :param namespace: The collection namespace.\n    :param name: The collection name.\n    return: CollectionMetadata about the collection.\n    \"\"\"\n    api_path = self.available_api_versions['v3']\n    field_map = [\n        ('created_str', 'created_at'),\n        ('modified_str', 'updated_at'),\n    ]\n\n    info_url = _urljoin(self.api_server, api_path, 'collections', namespace, name, '/')\n    error_context_msg = 'Error when getting the collection info for %s.%s from %s (%s)' \\\n                        % (namespace, name, self.name, self.api_server)\n    data = self._call_galaxy(info_url, error_context_msg=error_context_msg)\n\n    metadata = {}\n    for name, api_field in field_map:\n        metadata[name] = data.get(api_field, None)\n\n    return CollectionMetadata(namespace, name, **metadata)", "loc": 24}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_collection_version_metadata", "parameters": ["self", "namespace", "name", "version"], "param_types": {}, "return_type": null, "param_doc": {"namespace": "The collection namespace.", "name": "The collection name.", "version": "Version of the collection to get the information for."}, "return_doc": "CollectionVersionMetadata about the collection at the version requested.", "raises_doc": [], "called_functions": ["AnsibleError", "CollectionVersionMetadata", "_urljoin", "data.get", "download_url_info.path.startswith", "g_connect", "self._call_galaxy", "self._set_cache", "urljoin", "urlparse"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Gets the collection information from the Galaxy server about a specific Collection version.", "source_code": "def get_collection_version_metadata(self, namespace, name, version):\n    \"\"\"\n    Gets the collection information from the Galaxy server about a specific Collection version.\n\n    :param namespace: The collection namespace.\n    :param name: The collection name.\n    :param version: Version of the collection to get the information for.\n    :return: CollectionVersionMetadata about the collection at the version requested.\n    \"\"\"\n    api_path = self.available_api_versions['v3']\n    url_paths = [self.api_server, api_path, 'collections', namespace, name, 'versions', version, '/']\n\n    n_collection_url = _urljoin(*url_paths)\n    error_context_msg = 'Error when getting collection version metadata for %s.%s:%s from %s (%s)' \\\n                        % (namespace, name, version, self.name, self.api_server)\n    data = self._call_galaxy(n_collection_url, error_context_msg=error_context_msg, cache=True)\n    self._set_cache()\n\n    signatures = data.get('signatures') or []\n\n    download_url_info = urlparse(data['download_url'])\n    if not download_url_info.scheme and not download_url_info.path.startswith('/'):\n        # galaxy does a lot of redirects, with much more complex pathing than we use\n        # within this codebase, without updating _call_galaxy to be able to return\n        # the final URL, we can't reliably build a relative URL.\n        raise AnsibleError(f'Invalid non absolute download_url: {data[\"download_url\"]}')\n\n    download_url = urljoin(self.api_server, data['download_url'])\n\n    return CollectionVersionMetadata(data['namespace']['name'], data['collection']['name'], data['version'],\n                                     download_url, data['artifact']['sha256'],\n                                     data['metadata']['dependencies'], data['href'], signatures)", "loc": 32}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_collection_versions", "parameters": ["self", "namespace", "name"], "param_types": {}, "return_type": null, "param_doc": {"namespace": "The collection namespace.", "name": "The collection name."}, "return_doc": "A list of versions that are available.", "raises_doc": [], "called_functions": ["AnsibleError", "_urljoin", "g_connect", "get_cache_id", "modified_cache.get", "next_link.get", "next_link_info.path.startswith", "self._cache.setdefault", "self._call_galaxy", "self._set_cache", "self.get_collection_metadata", "server_cache.setdefault", "to_native", "urljoin", "urlparse"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Gets a list of available versions for a collection on a Galaxy server.", "source_code": "def get_collection_versions(self, namespace, name):\n    \"\"\"\n    Gets a list of available versions for a collection on a Galaxy server.\n\n    :param namespace: The collection namespace.\n    :param name: The collection name.\n    :return: A list of versions that are available.\n    \"\"\"\n    api_path = self.available_api_versions['v3']\n    pagination_path = ['links', 'next']\n\n    versions_url = _urljoin(self.api_server, api_path, 'collections', namespace, name, 'versions', '/?limit=%d' % COLLECTION_PAGE_SIZE)\n    versions_url_info = urlparse(versions_url)\n    cache_key = versions_url_info.path\n\n    # We should only rely on the cache if the collection has not changed. This may slow things down but it ensures\n    # we are not waiting a day before finding any new collections that have been published.\n    if self._cache:\n        server_cache = self._cache.setdefault(get_cache_id(versions_url), {})\n        modified_cache = server_cache.setdefault('modified', {})\n\n        try:\n            modified_date = self.get_collection_metadata(namespace, name).modified_str\n        except GalaxyError as err:\n            if err.http_code != 404:\n                raise\n            # No collection found, return an empty list to keep things consistent with the various APIs\n            return []\n\n        cached_modified_date = modified_cache.get('%s.%s' % (namespace, name), None)\n        if cached_modified_date != modified_date:\n            modified_cache['%s.%s' % (namespace, name)] = modified_date\n            if versions_url_info.path in server_cache:\n                del server_cache[cache_key]\n\n            self._set_cache()\n\n    error_context_msg = 'Error when getting available collection versions for %s.%s from %s (%s)' \\\n                        % (namespace, name, self.name, self.api_server)\n\n    try:\n        data = self._call_galaxy(versions_url, error_context_msg=error_context_msg, cache=True, cache_key=cache_key)\n    except GalaxyError as err:\n        if err.http_code != 404:\n            raise\n        # v3 doesn't raise a 404 so we need to mimic the empty response from APIs that do.\n        return []\n\n    if 'data' in data:\n        # v3 automation-hub is the only known API that uses `data`\n        # since v3 pulp_ansible does not, we cannot rely on version\n        # to indicate which key to use\n        results_key = 'data'\n    else:\n        results_key = 'results'\n\n    versions = []\n    while True:\n        versions += [v['version'] for v in data[results_key]]\n\n        next_link = data\n        for path in pagination_path:\n            next_link = next_link.get(path, {})\n\n        if not next_link:\n            break\n        next_link_info = urlparse(next_link)\n        if not next_link_info.scheme and not next_link_info.path.startswith('/'):\n            raise AnsibleError(f'Invalid non absolute pagination link: {next_link}')\n        next_link = urljoin(self.api_server, next_link)\n\n        data = self._call_galaxy(to_native(next_link, errors='surrogate_or_strict'),\n                                 error_context_msg=error_context_msg, cache=True, cache_key=cache_key)\n    self._set_cache()\n\n    return versions", "loc": 76}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": "GalaxyAPI", "function_name": "get_collection_signatures", "parameters": ["self", "namespace", "name", "version"], "param_types": {}, "return_type": null, "param_doc": {"namespace": "The collection namespace.", "name": "The collection name.", "version": "Version of the collection to get the information for."}, "return_doc": "A list of signature strings.", "raises_doc": [], "called_functions": ["_urljoin", "data.get", "display.vvvv", "g_connect", "self._call_galaxy", "self._set_cache"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Gets the collection signatures from the Galaxy server about a specific Collection version.", "source_code": "def get_collection_signatures(self, namespace, name, version):\n    \"\"\"\n    Gets the collection signatures from the Galaxy server about a specific Collection version.\n\n    :param namespace: The collection namespace.\n    :param name: The collection name.\n    :param version: Version of the collection to get the information for.\n    :return: A list of signature strings.\n    \"\"\"\n    api_path = self.available_api_versions['v3']\n    url_paths = [self.api_server, api_path, 'collections', namespace, name, 'versions', version, '/']\n\n    n_collection_url = _urljoin(*url_paths)\n    error_context_msg = 'Error when getting collection version metadata for %s.%s:%s from %s (%s)' \\\n                        % (namespace, name, version, self.name, self.api_server)\n    data = self._call_galaxy(n_collection_url, error_context_msg=error_context_msg, cache=True)\n    self._set_cache()\n\n    signatures = [signature_info[\"signature\"] for signature_info in data.get(\"signatures\") or []]\n    if not signatures:\n        display.vvvv(f\"Server {self.api_server} has not signed {namespace}.{name}:{version}\")\n    return signatures", "loc": 22}
{"file": "ansible\\lib\\ansible\\galaxy\\api.py", "class_name": null, "function_name": "wrapped", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleError", "_urljoin", "available_versions.keys", "display.vvvv", "method", "n_url.endswith", "self._available_api_versions.keys", "self._call_galaxy", "set", "set(versions).intersection"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(self, *args, **kwargs):\n    if not self._available_api_versions:\n        display.vvvv(\"Initial connection to galaxy_server: %s\" % self.api_server)\n\n        # Determine the type of Galaxy server we are talking to. First try it unauthenticated then with Bearer\n        # auth for Automation Hub.\n        n_url = self.api_server\n        error_context_msg = 'Error when finding available api versions from %s (%s)' % (self.name, n_url)\n\n        if self.api_server == 'https://galaxy.ansible.com' or self.api_server == 'https://galaxy.ansible.com/':\n            n_url = 'https://galaxy.ansible.com/api/'\n\n        try:\n            data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n        except (AnsibleError, GalaxyError, ValueError, KeyError) as err:\n            # Either the URL doesn't exist, or other error. Or the URL exists, but isn't a galaxy API\n            # root (not JSON, no 'available_versions') so try appending '/api/'\n            if n_url.endswith('/api') or n_url.endswith('/api/'):\n                raise\n\n            # Let exceptions here bubble up but raise the original if this returns a 404 (/api/ wasn't found).\n            n_url = _urljoin(n_url, '/api/')\n            try:\n                data = self._call_galaxy(n_url, method='GET', error_context_msg=error_context_msg, cache=True)\n            except GalaxyError as new_err:\n                if new_err.http_code == 404:\n                    raise err\n                raise\n\n        if 'available_versions' not in data:\n            raise AnsibleError(\"Tried to find galaxy API root at %s but no 'available_versions' are available \"\n                               \"on %s\" % (n_url, self.api_server))\n\n        # Update api_server to point to the \"real\" API root, which in this case could have been the configured\n        # url + '/api/' appended.\n        self.api_server = n_url\n\n        self._available_api_versions = available_versions = data['available_versions']\n        display.vvvv(\"Found API version '%s' with Galaxy server %s (%s)\"\n                     % (', '.join(available_versions.keys()), self.name, self.api_server))\n\n    # Verify that the API versions the function works with are available on the server specified.\n    available_versions = set(self._available_api_versions.keys())\n    common_versions = set(versions).intersection(available_versions)\n    if not common_versions:\n        raise AnsibleError(\"Galaxy action %s requires API versions '%s' but only '%s' are available on %s %s\"\n                           % (method.__name__, \", \".join(versions), \", \".join(available_versions),\n                              self.name, self.api_server))\n\n    return method(self, *args, **kwargs)", "loc": 50}
{"file": "ansible\\lib\\ansible\\galaxy\\role.py", "class_name": "GalaxyRole", "function_name": "metadata_dependencies", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "isinstance", "self.metadata.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def metadata_dependencies(self):\n    \"\"\"\n    Returns a list of dependencies from role metadata\n    \"\"\"\n    if self._metadata_dependencies is None:\n        self._metadata_dependencies = []\n\n        if self.metadata is not None:\n            self._metadata_dependencies = self.metadata.get('dependencies') or []\n\n    if not isinstance(self._metadata_dependencies, MutableSequence):\n        raise AnsibleParserError(\n            f\"Expected role dependencies to be a list. Role {self} has meta/main.yml with dependencies {self._metadata_dependencies}\"\n        )\n\n    return self._metadata_dependencies", "loc": 16}
{"file": "ansible\\lib\\ansible\\galaxy\\token.py", "class_name": "KeycloakToken", "function_name": "get", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GalaxyError", "data.get", "display.vvv", "json.load", "open_url", "self._form_payload", "time.time", "to_native", "user_agent"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def get(self):\n    if self._expiration and time.time() >= self._expiration:\n        self._token = None\n\n    if self._token:\n        return self._token\n\n    payload = self._form_payload()\n\n    display.vvv(f'Authenticating via {self.auth_url}')\n    try:\n        resp = open_url(to_native(self.auth_url),\n                        data=payload,\n                        validate_certs=self.validate_certs,\n                        method='POST',\n                        http_agent=user_agent())\n    except HTTPError as e:\n        raise GalaxyError(e, 'Unable to get access token')\n    display.vvv('Authentication successful')\n\n    data = json.load(resp)\n\n    # So that we have a buffer, expire the token in ~2/3 the given value\n    expires_in = data['expires_in'] // 3 * 2\n    self._expiration = time.time() + expires_in\n    display.vvv(f'Authentication token expires in {expires_in} seconds')\n\n    self._token = data.get('access_token')\n    if token_type := data.get('token_type'):\n        self.token_type = token_type\n\n    return self._token", "loc": 32}
{"file": "ansible\\lib\\ansible\\galaxy\\token.py", "class_name": "GalaxyToken", "function_name": "config", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._read"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def config(self):\n    if self._config is None:\n        self._config = self._read()\n\n    # Prioritise the token passed into the constructor\n    if self._token:\n        self._config['token'] = None if self._token is NoTokenSentinel else self._token\n\n    return self._config", "loc": 9}
{"file": "ansible\\lib\\ansible\\galaxy\\token.py", "class_name": "GalaxyToken", "function_name": "headers", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers(self):\n    headers = {}\n    token = self.get()\n    if token:\n        headers['Authorization'] = '%s %s' % (self.token_type, self.get())\n    return headers", "loc": 6}
{"file": "ansible\\lib\\ansible\\galaxy\\token.py", "class_name": "BasicAuthToken", "function_name": "get", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._encode_token"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self):\n    if self._token:\n        return self._token\n\n    self._token = self._encode_token(self.username, self.password)\n\n    return self._token", "loc": 7}
{"file": "ansible\\lib\\ansible\\galaxy\\user_agent.py", "class_name": null, "function_name": "user_agent", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["platform.system", "u'ansible-galaxy/{ansible_version} ({platform}; python:{py_major}.{py_minor}.{py_micro})'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def user_agent():\n    \"\"\"Returns a user agent used by ansible-galaxy to include the Ansible version, platform and python version.\"\"\"\n\n    python_version = sys.version_info\n    return u\"ansible-galaxy/{ansible_version} ({platform}; python:{py_major}.{py_minor}.{py_micro})\".format(\n        ansible_version=ansible_version,\n        platform=platform.system(),\n        py_major=python_version.major,\n        py_minor=python_version.minor,\n        py_micro=python_version.micro,\n    )", "loc": 11}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": null, "function_name": "parse_scm", "parameters": ["collection", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collection.split", "collection.startswith", "fragment.strip", "name.endswith", "path.endswith", "path.split", "urldefrag"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Extract name, version, path and subdir out of the SCM pointer.", "source_code": "def parse_scm(collection, version):\n    \"\"\"Extract name, version, path and subdir out of the SCM pointer.\"\"\"\n    if ',' in collection:\n        collection, version = collection.split(',', 1)\n    elif version == '*' or not version:\n        version = 'HEAD'\n\n    if collection.startswith('git+'):\n        path = collection[4:]\n    else:\n        path = collection\n\n    path, fragment = urldefrag(path)\n    fragment = fragment.strip(os.path.sep)\n\n    if path.endswith(os.path.sep + '.git'):\n        name = path.split(os.path.sep)[-2]\n    elif '://' not in path and '@' not in path:\n        name = path\n    else:\n        name = path.split('/')[-1]\n        if name.endswith('.git'):\n            name = name[:-4]\n\n    return name, version, path, fragment", "loc": 25}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_galaxy_artifact_source_info", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'The is no known source for {coll!s}'.format", "RuntimeError"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_galaxy_artifact_source_info(self, collection):\n    # type: (Candidate) -> dict[str, t.Union[str, list[dict[str, str]]]]\n    server = collection.src.api_server\n\n    try:\n        download_url = self._galaxy_collection_cache[collection][0]\n        signatures_url, signatures = self._galaxy_collection_origin_cache[collection]\n    except KeyError as key_err:\n        raise RuntimeError(\n            'The is no known source for {coll!s}'.\n            format(coll=collection),\n        ) from key_err\n\n    return {\n        \"format_version\": \"1.0.0\",\n        \"namespace\": collection.namespace,\n        \"name\": collection.name,\n        \"version\": collection.ver,\n        \"server\": server,\n        \"version_url\": signatures_url,\n        \"download_url\": download_url,\n        \"signatures\": signatures,\n    }", "loc": 23}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_galaxy_artifact_path", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Collection '{coll!s}' obtained from server {server!s} {url!s}\".format", "\"Failed to download collection tar from '{coll_src!s}' due to the following unforeseen error: {download_err!s}\".format", "\"Failed to download collection tar from '{coll_src!s}': {download_err!s}\".format", "\"Fetching a collection tarball for '{collection!s}' from Ansible Galaxy\".format", "'There is no known source for {coll!s}'.format", "AnsibleError", "RuntimeError", "_download_file", "display.vvv", "display.vvvv", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Given a Galaxy-stored collection, return a cached path. If it's not yet on disk, this method downloads the artifact first.", "source_code": "def get_galaxy_artifact_path(self, collection):\n    # type: (t.Union[Candidate, Requirement]) -> bytes\n    \"\"\"Given a Galaxy-stored collection, return a cached path.\n\n    If it's not yet on disk, this method downloads the artifact first.\n    \"\"\"\n    try:\n        return self._galaxy_artifact_cache[collection]\n    except KeyError:\n        pass\n\n    try:\n        url, sha256_hash, token = self._galaxy_collection_cache[collection]\n    except KeyError as key_err:\n        raise RuntimeError(\n            'There is no known source for {coll!s}'.\n            format(coll=collection),\n        ) from key_err\n\n    display.vvvv(\n        \"Fetching a collection tarball for '{collection!s}' from \"\n        'Ansible Galaxy'.format(collection=collection),\n    )\n\n    try:\n        b_artifact_path = _download_file(\n            url,\n            self._b_working_directory,\n            expected_hash=sha256_hash,\n            validate_certs=self._validate_certs,\n            token=token,\n        )  # type: bytes\n    except URLError as err:\n        raise AnsibleError(\n            'Failed to download collection tar '\n            \"from '{coll_src!s}': {download_err!s}\".\n            format(\n                coll_src=to_native(collection.src),\n                download_err=to_native(err),\n            ),\n        ) from err\n    except Exception as err:\n        raise AnsibleError(\n            'Failed to download collection tar '\n            \"from '{coll_src!s}' due to the following unforeseen error: \"\n            '{download_err!s}'.\n            format(\n                coll_src=to_native(collection.src),\n                download_err=to_native(err),\n            ),\n        ) from err\n    else:\n        display.vvv(\n            \"Collection '{coll!s}' obtained from \"\n            'server {server!s} {url!s}'.format(\n                coll=collection, server=collection.src or 'Galaxy',\n                url=collection.src.api_server if collection.src is not None\n                else '',\n            )\n        )\n\n    self._galaxy_artifact_cache[collection] = b_artifact_path\n    return b_artifact_path", "loc": 63}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_artifact_path", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Collection requirement '{collection!s}' is a URL to a tar artifact\".format", "\"Failed to download collection tar from '{coll_src!s}': {download_err!s}\".format", "'The artifact is of an unexpected type {art_type!s}'.format", "AnsibleError", "RuntimeError", "_download_file", "_extract_collection_from_git", "display.vvvv", "to_bytes", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Given a concrete collection pointer, return a cached path. If it's not yet on disk, this method downloads the artifact first.", "source_code": "def get_artifact_path(self, collection):\n    # type: (Collection) -> bytes\n    \"\"\"Given a concrete collection pointer, return a cached path.\n\n    If it's not yet on disk, this method downloads the artifact first.\n    \"\"\"\n    try:\n        return self._artifact_cache[collection.src]\n    except KeyError:\n        pass\n\n    # NOTE: SCM needs to be special-cased as it may contain either\n    # NOTE: one collection in its root, or a number of top-level\n    # NOTE: collection directories instead.\n    # NOTE: The idea is to store the SCM collection as unpacked\n    # NOTE: directory structure under the temporary location and use\n    # NOTE: a \"virtual\" collection that has pinned requirements on\n    # NOTE: the directories under that SCM checkout that correspond\n    # NOTE: to collections.\n    # NOTE: This brings us to the idea that we need two separate\n    # NOTE: virtual Requirement/Candidate types --\n    # NOTE: (single) dir + (multidir) subdirs\n    if collection.is_url:\n        display.vvvv(\n            \"Collection requirement '{collection!s}' is a URL \"\n            'to a tar artifact'.format(collection=collection.fqcn),\n        )\n        try:\n            b_artifact_path = _download_file(\n                collection.src,\n                self._b_working_directory,\n                expected_hash=None,  # NOTE: URLs don't support checksums\n                validate_certs=self._validate_certs,\n                timeout=self.timeout\n            )\n        except Exception as err:\n            raise AnsibleError(\n                'Failed to download collection tar '\n                \"from '{coll_src!s}': {download_err!s}\".\n                format(\n                    coll_src=to_native(collection.src),\n                    download_err=to_native(err),\n                ),\n            ) from err\n    elif collection.is_scm:\n        b_artifact_path = _extract_collection_from_git(\n            collection.src,\n            collection.ver,\n            self._b_working_directory,\n        )\n    elif collection.is_file or collection.is_dir or collection.is_subdirs:\n        b_artifact_path = to_bytes(collection.src)\n    else:\n        # NOTE: This may happen `if collection.is_online_index_pointer`\n        raise RuntimeError(\n            'The artifact is of an unexpected type {art_type!s}'.\n            format(art_type=collection.type)\n        )\n\n    self._artifact_cache[collection.src] = b_artifact_path\n    return b_artifact_path", "loc": 61}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_artifact_path_from_unknown", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_artifact_path", "self.get_galaxy_artifact_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_artifact_path_from_unknown(self, collection):\n    # type: (Candidate) -> bytes\n    if collection.is_concrete_artifact:\n        return self.get_artifact_path(collection)\n    return self.get_galaxy_artifact_path(collection)", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_direct_collection_fqcn", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "self._get_direct_collection_name", "self._get_direct_collection_namespace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Extract FQCN from the given on-disk collection artifact. If the collection is virtual, ``None`` is returned instead of a string.", "source_code": "def get_direct_collection_fqcn(self, collection):\n    # type: (Collection) -> t.Optional[str]\n    \"\"\"Extract FQCN from the given on-disk collection artifact.\n\n    If the collection is virtual, ``None`` is returned instead\n    of a string.\n    \"\"\"\n    if collection.is_virtual:\n        # NOTE: should it be something like \"<virtual>\"?\n        return None\n\n    return '.'.join((  # type: ignore[type-var]\n        self._get_direct_collection_namespace(collection),  # type: ignore[arg-type]\n        self._get_direct_collection_name(collection),\n    ))", "loc": 15}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_direct_collection_dependencies", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_direct_collection_meta"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Extract deps from the given on-disk collection artifact.", "source_code": "def get_direct_collection_dependencies(self, collection):\n    # type: (t.Union[Candidate, Requirement]) -> dict[str, str]\n    \"\"\"Extract deps from the given on-disk collection artifact.\"\"\"\n    collection_dependencies = self.get_direct_collection_meta(collection)['dependencies']\n    if collection_dependencies is None:\n        collection_dependencies = {}\n    return collection_dependencies  # type: ignore[return-value]", "loc": 7}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\concrete_artifact_manager.py", "class_name": "ConcreteArtifactsManager", "function_name": "get_direct_collection_meta", "parameters": ["self", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Failed to find the collection dir deps: {err!s}'.format", "AnsibleError", "_get_meta_from_dir", "_get_meta_from_tar", "dict.fromkeys", "map", "self.get_artifact_path", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Extract meta from the given on-disk collection artifact.", "source_code": "def get_direct_collection_meta(self, collection):\n    # type: (Collection) -> dict[str, t.Union[str, dict[str, str], list[str], None, t.Type[Sentinel]]]\n    \"\"\"Extract meta from the given on-disk collection artifact.\"\"\"\n    try:  # FIXME: use unique collection identifier as a cache key?\n        return self._artifact_meta_cache[collection.src]\n    except KeyError:\n        b_artifact_path = self.get_artifact_path(collection)\n\n    if collection.is_url or collection.is_file:\n        collection_meta = _get_meta_from_tar(b_artifact_path)\n    elif collection.is_dir:  # should we just build a coll instead?\n        # FIXME: what if there's subdirs?\n        try:\n            collection_meta = _get_meta_from_dir(b_artifact_path, self.require_build_metadata)\n        except LookupError as lookup_err:\n            raise AnsibleError(\n                'Failed to find the collection dir deps: {err!s}'.\n                format(err=to_native(lookup_err)),\n            ) from lookup_err\n    elif collection.is_scm:\n        collection_meta = {\n            'name': None,\n            'namespace': None,\n            'dependencies': {to_native(b_artifact_path): '*'},\n            'version': '*',\n        }\n    elif collection.is_subdirs:\n        collection_meta = {\n            'name': None,\n            'namespace': None,\n            # NOTE: Dropping b_artifact_path since it's based on src anyway\n            'dependencies': dict.fromkeys(\n                map(to_native, collection.namespace_collection_paths),\n                '*',\n            ),\n            'version': '*',\n        }\n    else:\n        raise RuntimeError\n\n    self._artifact_meta_cache[collection.src] = collection_meta\n    return collection_meta", "loc": 42}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\galaxy_api_proxy.py", "class_name": "MultiGalaxyAPIProxy", "function_name": "get_collection_versions", "parameters": ["self", "requirement"], "param_types": {"requirement": "Requirement"}, "return_type": "t.Iterable[tuple[str, GalaxyAPI]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._concrete_art_mgr.get_direct_collection_version", "self._get_collection_versions", "set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get a set of unique versions for FQCN on Galaxy servers.", "source_code": "def get_collection_versions(self, requirement: Requirement) -> t.Iterable[tuple[str, GalaxyAPI]]:\n    \"\"\"Get a set of unique versions for FQCN on Galaxy servers.\"\"\"\n    if requirement.is_concrete_artifact:\n        return {\n            (\n                self._concrete_art_mgr.\n                get_direct_collection_version(requirement),\n                requirement.src,\n            ),\n        }\n\n    api_lookup_order = (\n        (requirement.src, )\n        if isinstance(requirement.src, GalaxyAPI)\n        else self._apis\n    )\n    return set(\n        (version, api)\n        for api, version in self._get_collection_versions(\n            requirement,\n        )\n    )", "loc": 22}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\galaxy_api_proxy.py", "class_name": "MultiGalaxyAPIProxy", "function_name": "get_collection_version_metadata", "parameters": ["self", "collection_candidate"], "param_types": {"collection_candidate": "Candidate"}, "return_type": "CollectionVersionMetadata", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Skipping Galaxy server {server!s}. Got an unexpected error when getting available versions of collection {fqcn!s}: {err!s}'.format", "api.get_collection_version_metadata", "display.warning", "isinstance", "self._assert_that_offline_mode_is_not_requested", "self._concrete_art_mgr.save_collection_source", "to_text"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Retrieve collection metadata of a given candidate.", "source_code": "def get_collection_version_metadata(self, collection_candidate: Candidate) -> CollectionVersionMetadata:\n    \"\"\"Retrieve collection metadata of a given candidate.\"\"\"\n    self._assert_that_offline_mode_is_not_requested()\n\n    api_lookup_order = (\n        (collection_candidate.src, )\n        if isinstance(collection_candidate.src, GalaxyAPI)\n        else self._apis\n    )\n\n    last_err: t.Optional[Exception]\n\n    for api in api_lookup_order:\n        try:\n            version_metadata = api.get_collection_version_metadata(\n                collection_candidate.namespace,\n                collection_candidate.name,\n                collection_candidate.ver,\n            )\n        except GalaxyError as api_err:\n            last_err = api_err\n        except Exception as unknown_err:\n            # `verify` doesn't use `get_collection_versions` since the version is already known.\n            # Do the same as `install` and `download` by trying all APIs before failing.\n            # Warn for debugging purposes, since the Galaxy server may be unexpectedly down.\n            last_err = unknown_err\n            display.warning(\n                \"Skipping Galaxy server {server!s}. \"\n                \"Got an unexpected error when getting \"\n                \"available versions of collection {fqcn!s}: {err!s}\".\n                format(\n                    server=api.api_server,\n                    fqcn=collection_candidate.fqcn,\n                    err=to_text(unknown_err),\n                )\n            )\n        else:\n            self._concrete_art_mgr.save_collection_source(\n                collection_candidate,\n                version_metadata.download_url,\n                version_metadata.artifact_sha256,\n                api.token,\n                version_metadata.signatures_url,\n                version_metadata.signatures,\n            )\n            return version_metadata\n\n    raise last_err", "loc": 48}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\galaxy_api_proxy.py", "class_name": "MultiGalaxyAPIProxy", "function_name": "get_collection_dependencies", "parameters": ["self", "collection_candidate"], "param_types": {"collection_candidate": "Candidate"}, "return_type": "dict[str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._concrete_art_mgr.get_direct_collection_dependencies", "self.get_collection_version_metadata"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Retrieve collection dependencies of a given candidate.", "source_code": "def get_collection_dependencies(self, collection_candidate: Candidate) -> dict[str, str]:\n    # FIXME: return Requirement instances instead?\n    \"\"\"Retrieve collection dependencies of a given candidate.\"\"\"\n    if collection_candidate.is_concrete_artifact:\n        return (\n            self.\n            _concrete_art_mgr.\n            get_direct_collection_dependencies\n        )(collection_candidate)\n\n    return (\n        self.\n        get_collection_version_metadata(collection_candidate).\n        dependencies\n    )", "loc": 15}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\galaxy_api_proxy.py", "class_name": "MultiGalaxyAPIProxy", "function_name": "get_signatures", "parameters": ["self", "collection_candidate"], "param_types": {"collection_candidate": "Candidate"}, "return_type": "list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Skipping Galaxy server {server!s}. Got an unexpected error when getting available versions of collection {fqcn!s}: {err!s}'.format", "api.get_collection_signatures", "display.warning", "isinstance", "self._assert_that_offline_mode_is_not_requested", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_signatures(self, collection_candidate: Candidate) -> list[str]:\n    self._assert_that_offline_mode_is_not_requested()\n    namespace = collection_candidate.namespace\n    name = collection_candidate.name\n    version = collection_candidate.ver\n    last_err: Exception | None = None\n\n    api_lookup_order = (\n        (collection_candidate.src, )\n        if isinstance(collection_candidate.src, GalaxyAPI)\n        else self._apis\n    )\n\n    for api in api_lookup_order:\n        try:\n            return api.get_collection_signatures(namespace, name, version)\n        except GalaxyError as api_err:\n            last_err = api_err\n        except Exception as unknown_err:\n            # Warn for debugging purposes, since the Galaxy server may be unexpectedly down.\n            last_err = unknown_err\n            display.warning(\n                \"Skipping Galaxy server {server!s}. \"\n                \"Got an unexpected error when getting \"\n                \"available versions of collection {fqcn!s}: {err!s}\".\n                format(\n                    server=api.api_server,\n                    fqcn=collection_candidate.fqcn,\n                    err=to_text(unknown_err),\n                )\n            )\n    if last_err:\n        raise last_err\n\n    return []", "loc": 35}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\gpg.py", "class_name": null, "function_name": "parse_gpg_errors", "parameters": ["status_out"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "dc_fields", "fields.extend", "len", "line.split", "remainder.split", "status_out.splitlines"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_gpg_errors(status_out):  # type: (str) -> t.Iterator[GpgBaseError]\n    for line in status_out.splitlines():\n        if not line:\n            continue\n        try:\n            _dummy, status, remainder = line.split(maxsplit=2)\n        except ValueError:\n            _dummy, status = line.split(maxsplit=1)\n            remainder = None\n\n        try:\n            cls = GPG_ERROR_MAP[status]\n        except KeyError:\n            continue\n\n        fields = [status]\n        if remainder:\n            fields.extend(\n                remainder.split(\n                    None,\n                    len(dc_fields(cls)) - 2\n                )\n            )\n\n        yield cls(*fields)", "loc": 25}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "verify_file_signatures", "parameters": ["fqcn", "manifest_file", "detached_signatures", "keyring", "required_successful_count", "ignore_signature_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "display.vvvv", "error.report", "error_messages.append", "int", "re.match", "re.match(SIGNATURE_COUNT_RE, required_successful_count).groupdict", "to_text", "verify_file_signature"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def verify_file_signatures(fqcn, manifest_file, detached_signatures, keyring, required_successful_count, ignore_signature_errors):\n    # type: (str, str, list[str], str, str, list[str]) -> bool\n    successful = 0\n    error_messages = []\n\n    signature_count_requirements = re.match(SIGNATURE_COUNT_RE, required_successful_count).groupdict()\n\n    strict = signature_count_requirements['strict'] or False\n    require_all = signature_count_requirements['all']\n    require_count = signature_count_requirements['count']\n    if require_count is not None:\n        require_count = int(require_count)\n\n    for signature in detached_signatures:\n        signature = to_text(signature, errors='surrogate_or_strict')\n        try:\n            verify_file_signature(manifest_file, signature, keyring, ignore_signature_errors)\n        except CollectionSignatureError as error:\n            if error.ignore:\n                # Do not include ignored errors in either the failed or successful count\n                continue\n            error_messages.append(error.report(fqcn))\n        else:\n            successful += 1\n\n            if require_all:\n                continue\n\n            if successful == require_count:\n                break\n\n    if strict and not successful:\n        verified = False\n        display.display(f\"Signature verification failed for '{fqcn}': no successful signatures\")\n    elif require_all:\n        verified = not error_messages\n        if not verified:\n            display.display(f\"Signature verification failed for '{fqcn}': some signatures failed\")\n    else:\n        verified = not detached_signatures or require_count == successful\n        if not verified:\n            display.display(f\"Signature verification failed for '{fqcn}': fewer successful signatures than required\")\n\n    if not verified:\n        for msg in error_messages:\n            display.vvvv(msg)\n\n    return verified", "loc": 48}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "verify_file_signature", "parameters": ["manifest_file", "detached_signature", "keyring", "ignore_signature_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CollectionSignatureError", "GPG_ERROR_MAP.keys", "GPG_ERROR_MAP.values", "chain", "error.get_gpg_error_description", "len", "list", "list(GPG_ERROR_MAP.values()).index", "next", "parse_gpg_errors", "reasons.append", "run_gpg_verify", "set"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Run the gpg command and parse any errors. Raises CollectionSignatureError on failure.", "source_code": "def verify_file_signature(manifest_file, detached_signature, keyring, ignore_signature_errors):\n    # type: (str, str, str, list[str]) -> None\n    \"\"\"Run the gpg command and parse any errors. Raises CollectionSignatureError on failure.\"\"\"\n    gpg_result, gpg_verification_rc = run_gpg_verify(manifest_file, detached_signature, keyring, display)\n\n    if gpg_result:\n        errors = parse_gpg_errors(gpg_result)\n        try:\n            error = next(errors)\n        except StopIteration:\n            pass\n        else:\n            reasons = []\n            ignored_reasons = 0\n\n            for error in chain([error], errors):\n                # Get error status (dict key) from the class (dict value)\n                status_code = list(GPG_ERROR_MAP.keys())[list(GPG_ERROR_MAP.values()).index(error.__class__)]\n                if status_code in ignore_signature_errors:\n                    ignored_reasons += 1\n                reasons.append(error.get_gpg_error_description())\n\n            ignore = len(reasons) == ignored_reasons\n            raise CollectionSignatureError(reasons=set(reasons), stdout=gpg_result, rc=gpg_verification_rc, ignore=ignore)\n\n    if gpg_verification_rc:\n        raise CollectionSignatureError(stdout=gpg_result, rc=gpg_verification_rc)\n\n    # No errors and rc is 0, verify was successful\n    return None", "loc": 30}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "publish_collection", "parameters": ["collection_path", "api", "wait", "timeout"], "param_types": {}, "return_type": null, "param_doc": {"collection_path": "The path to the collection tarball to publish.", "api": "A GalaxyAPI to publish the collection to.", "wait": "Whether to wait until the import process is complete.", "timeout": "The time in seconds to wait for the import process to finish, 0 is indefinite."}, "return_doc": "", "raises_doc": [], "called_functions": ["'Collection has been published to the Galaxy server {api.name!s} {api.api_server!s}'.format", "_display_progress", "api.publish_collection", "api.wait_import_task", "display.display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Publish an Ansible collection tarball into an Ansible Galaxy server.", "source_code": "def publish_collection(collection_path, api, wait, timeout):\n    \"\"\"Publish an Ansible collection tarball into an Ansible Galaxy server.\n\n    :param collection_path: The path to the collection tarball to publish.\n    :param api: A GalaxyAPI to publish the collection to.\n    :param wait: Whether to wait until the import process is complete.\n    :param timeout: The time in seconds to wait for the import process to finish, 0 is indefinite.\n    \"\"\"\n    import_uri = api.publish_collection(collection_path)\n\n    if wait:\n        with _display_progress(\n                \"Collection has been published to the Galaxy server \"\n                \"{api.name!s} {api.api_server!s}\".format(api=api),\n        ):\n            api.wait_import_task(import_uri, timeout)\n        display.display(\"Collection has been successfully published and imported to the Galaxy server %s %s\"\n                        % (api.name, api.api_server))\n    else:\n        display.display(\"Collection has been pushed to the Galaxy server %s %s, not waiting until import has \"\n                        \"completed due to --no-wait being set. Import task results can be found at %s\"\n                        % (api.name, api.api_server, import_uri))", "loc": 22}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "validate_collection_name", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {"name": "The input name with optional range specifier split by ':'."}, "return_doc": "The input value, required for argparse validation.", "raises_doc": [], "called_functions": ["AnsibleCollectionRef.is_valid_collection_name", "AnsibleError", "name.partition"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validates the collection name as an input from the user or a requirements file fit the requirements.", "source_code": "def validate_collection_name(name):  # type: (str) -> str\n    \"\"\"Validates the collection name as an input from the user or a requirements file fit the requirements.\n\n    :param name: The input name with optional range specifier split by ':'.\n    :return: The input value, required for argparse validation.\n    \"\"\"\n    collection, dummy, dummy = name.partition(':')\n    if AnsibleCollectionRef.is_valid_collection_name(collection):\n        return name\n\n    raise AnsibleError(\"Invalid collection name '%s', \"\n                       \"name must be in the format <namespace>.<collection>. \\n\"\n                       \"Please make sure namespace and collection name contains \"\n                       \"characters from [a-zA-Z0-9_] only.\" % name)", "loc": 14}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "find_existing_collections", "parameters": ["path_filter", "artifacts_manager", "namespace_filter", "collection_filter", "dedupe"], "param_types": {}, "return_type": null, "param_doc": {"path": "Collection dirs layout search path.", "artifacts_manager": "Artifacts manager."}, "return_doc": "", "raises_doc": [], "called_functions": ["Candidate.from_dir_path_as_unknown", "_normalize_collection_path", "collection_path.as_posix", "display.vvv", "display.warning", "files", "files('ansible_collections').glob", "is_sequence", "path.is_dir", "path.relative_to", "paths.add", "seen.add", "set", "to_bytes", "to_text", "u\"Found installed collection {coll!s} at '{path!s}'\".format"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Locate all collections under a given path.", "source_code": "def find_existing_collections(path_filter, artifacts_manager, namespace_filter=None, collection_filter=None, dedupe=True):\n    \"\"\"Locate all collections under a given path.\n\n    :param path: Collection dirs layout search path.\n    :param artifacts_manager: Artifacts manager.\n    \"\"\"\n    if path_filter and not is_sequence(path_filter):\n        path_filter = [path_filter]\n    if namespace_filter and not is_sequence(namespace_filter):\n        namespace_filter = [namespace_filter]\n    if collection_filter and not is_sequence(collection_filter):\n        collection_filter = [collection_filter]\n\n    paths = set()\n    for path in files('ansible_collections').glob('*/*/'):\n        path = _normalize_collection_path(path)\n        if not path.is_dir():\n            continue\n        if path_filter:\n            for pf in path_filter:\n                try:\n                    path.relative_to(_normalize_collection_path(pf))\n                except ValueError:\n                    continue\n                break\n            else:\n                continue\n        paths.add(path)\n\n    seen = set()\n    for path in paths:\n        namespace = path.parent.name\n        name = path.name\n        if namespace_filter and namespace not in namespace_filter:\n            continue\n        if collection_filter and name not in collection_filter:\n            continue\n\n        if dedupe:\n            try:\n                collection_path = files(f'ansible_collections.{namespace}.{name}')\n            except ImportError:\n                continue\n            if collection_path in seen:\n                continue\n            seen.add(collection_path)\n        else:\n            collection_path = path\n\n        b_collection_path = to_bytes(collection_path.as_posix())\n\n        try:\n            req = Candidate.from_dir_path_as_unknown(b_collection_path, artifacts_manager)\n        except ValueError as val_err:\n            display.warning(f'{val_err}')\n            continue\n\n        display.vvv(\n            u\"Found installed collection {coll!s} at '{path!s}'\".\n            format(coll=to_text(req), path=to_text(req.src))\n        )\n        yield req", "loc": 62}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "verify_artifact_manifest", "parameters": ["manifest_file", "signatures", "keyring", "required_signature_count", "ignore_signature_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.vvvv", "to_text", "to_text(manifest_file, errors='surrogate_or_strict').split", "verify_file_signatures"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def verify_artifact_manifest(manifest_file, signatures, keyring, required_signature_count, ignore_signature_errors):\n    # type: (str, list[str], str, str, list[str]) -> None\n    failed_verify = False\n    coll_path_parts = to_text(manifest_file, errors='surrogate_or_strict').split(os.path.sep)\n    collection_name = '%s.%s' % (coll_path_parts[-3], coll_path_parts[-2])  # get 'ns' and 'coll' from /path/to/ns/coll/MANIFEST.json\n    if not verify_file_signatures(collection_name, manifest_file, signatures, keyring, required_signature_count, ignore_signature_errors):\n        raise AnsibleError(f\"Not installing {collection_name} because GnuPG signature verification failed.\")\n    display.vvvv(f\"GnuPG signature verification succeeded for {collection_name}\")", "loc": 8}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "install_artifact", "parameters": ["b_coll_targz_path", "b_collection_path", "b_temp_path", "signatures", "keyring", "required_signature_count", "ignore_signature_errors"], "param_types": {}, "return_type": null, "param_doc": {"b_coll_targz_path": "Collection tarball to be installed.", "b_collection_path": "Collection dirs layout path.", "b_temp_path": "Temporary dir path.", "signatures": "frozenset of signatures to verify the MANIFEST.json", "keyring": "The keyring used during GPG verification", "required_signature_count": "The number of signatures that must successfully verify the collection", "ignore_signature_errors": "GPG errors to ignore during signature verification"}, "return_doc": "", "raises_doc": [], "called_functions": ["_extract_tar_dir", "_extract_tar_file", "_tarfile_extract", "collection_tar.getmember", "files_obj.read", "json.loads", "os.listdir", "os.path.dirname", "os.path.join", "os.rmdir", "shutil.rmtree", "tarfile.open", "to_text", "verify_artifact_manifest"], "control_structures": ["For", "If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Install a collection from tarball under a given path.", "source_code": "def install_artifact(b_coll_targz_path, b_collection_path, b_temp_path, signatures, keyring, required_signature_count, ignore_signature_errors):\n    \"\"\"Install a collection from tarball under a given path.\n\n    :param b_coll_targz_path: Collection tarball to be installed.\n    :param b_collection_path: Collection dirs layout path.\n    :param b_temp_path: Temporary dir path.\n    :param signatures: frozenset of signatures to verify the MANIFEST.json\n    :param keyring: The keyring used during GPG verification\n    :param required_signature_count: The number of signatures that must successfully verify the collection\n    :param ignore_signature_errors: GPG errors to ignore during signature verification\n    \"\"\"\n    try:\n        with tarfile.open(b_coll_targz_path, mode='r') as collection_tar:\n            # Verify the signature on the MANIFEST.json before extracting anything else\n            _extract_tar_file(collection_tar, MANIFEST_FILENAME, b_collection_path, b_temp_path)\n\n            if keyring is not None:\n                manifest_file = os.path.join(to_text(b_collection_path, errors='surrogate_or_strict'), MANIFEST_FILENAME)\n                verify_artifact_manifest(manifest_file, signatures, keyring, required_signature_count, ignore_signature_errors)\n\n            files_member_obj = collection_tar.getmember('FILES.json')\n            with _tarfile_extract(collection_tar, files_member_obj) as (dummy, files_obj):\n                files = json.loads(to_text(files_obj.read(), errors='surrogate_or_strict'))\n\n            _extract_tar_file(collection_tar, 'FILES.json', b_collection_path, b_temp_path)\n\n            for file_info in files['files']:\n                file_name = file_info['name']\n                if file_name == '.':\n                    continue\n\n                if file_info['ftype'] == 'file':\n                    _extract_tar_file(collection_tar, file_name, b_collection_path, b_temp_path,\n                                      expected_hash=file_info['chksum_sha256'])\n\n                else:\n                    _extract_tar_dir(collection_tar, file_name, b_collection_path)\n\n    except Exception:\n        # Ensure we don't leave the dir behind in case of a failure.\n        shutil.rmtree(b_collection_path)\n\n        b_namespace_path = os.path.dirname(b_collection_path)\n        if not os.listdir(b_namespace_path):\n            os.rmdir(b_namespace_path)\n\n        raise", "loc": 47}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "install_src", "parameters": ["collection", "b_collection_path", "b_collection_output_path", "artifacts_manager"], "param_types": {}, "return_type": null, "param_doc": {"collection": "Collection to be installed.", "b_collection_path": "Collection dirs layout path.", "b_collection_output_path": "The installation directory for the \\", "artifacts_manager": "Artifacts manager."}, "return_doc": "", "raises_doc": [{"type": "AnsibleError", "desc": "If no collection metadata found."}], "called_functions": ["'Created collection for {coll!s} at {path!s}'.format", "_build_collection_dir", "_build_files_manifest", "_build_manifest", "artifacts_manager.get_direct_collection_meta", "display.display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Install the collection from source control into given dir. Generates the Ansible collection artifact data from a galaxy.yml and installs the artifact to a directory. This should follow the same pattern as build_collection, but instead of creating an artifact, install it.", "source_code": "def install_src(collection, b_collection_path, b_collection_output_path, artifacts_manager):\n    r\"\"\"Install the collection from source control into given dir.\n\n    Generates the Ansible collection artifact data from a galaxy.yml and\n    installs the artifact to a directory.\n    This should follow the same pattern as build_collection, but instead\n    of creating an artifact, install it.\n\n    :param collection: Collection to be installed.\n    :param b_collection_path: Collection dirs layout path.\n    :param b_collection_output_path: The installation directory for the \\\n                                     collection artifact.\n    :param artifacts_manager: Artifacts manager.\n\n    :raises AnsibleError: If no collection metadata found.\n    \"\"\"\n    collection_meta = artifacts_manager.get_direct_collection_meta(collection)\n\n    if 'build_ignore' not in collection_meta:  # installed collection, not src\n        # FIXME: optimize this? use a different process? copy instead of build?\n        collection_meta['build_ignore'] = []\n        collection_meta['manifest'] = Sentinel\n    collection_manifest = _build_manifest(**collection_meta)\n    file_manifest = _build_files_manifest(\n        b_collection_path,\n        collection_meta['namespace'], collection_meta['name'],\n        collection_meta['build_ignore'],\n        collection_meta['manifest'],\n        collection_meta['license_file'],\n    )\n\n    collection_output_path = _build_collection_dir(\n        b_collection_path, b_collection_output_path,\n        collection_manifest, file_manifest,\n    )\n\n    display.display(\n        'Created collection for {coll!s} at {path!s}'.\n        format(coll=collection, path=collection_output_path)\n    )", "loc": 40}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": "CollectionSignatureError", "function_name": "report", "parameters": ["self", "collection_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._report_expected", "self._report_unexpected"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def report(self, collection_name):\n    if self.reasons:\n        return self._report_expected(collection_name)\n\n    return self._report_unexpected(collection_name)", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "progress", "parameters": ["display_queue", "actual_display"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["actual_display.debug", "actual_display.display", "display_queue.get", "func", "getattr", "threading.current_thread", "time.sleep"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def progress(display_queue, actual_display):\n    actual_display.debug(\"Starting display_progress display thread\")\n    t = threading.current_thread()\n\n    while True:\n        for c in \"|/-\\\\\":\n            actual_display.display(c + \"\\b\", newline=False)\n            time.sleep(0.1)\n\n            # Display a message from the main thread\n            while True:\n                try:\n                    method, args, kwargs = display_queue.get(block=False, timeout=0.1)\n                except queue.Empty:\n                    break\n                else:\n                    func = getattr(actual_display, method)\n                    func(*args, **kwargs)\n\n            if getattr(t, \"finish\", False):\n                actual_display.debug(\"Received end signal for display_progress display thread\")\n                return", "loc": 22}
{"file": "ansible\\lib\\ansible\\galaxy\\collection\\__init__.py", "class_name": null, "function_name": "reset_stat", "parameters": ["tarinfo"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["tarinfo.isdir"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reset_stat(tarinfo):\n    if tarinfo.type != tarfile.SYMTYPE:\n        existing_is_exec = tarinfo.mode & stat.S_IXUSR\n        tarinfo.mode = S_IRWXU_RXG_RXO if existing_is_exec or tarinfo.isdir() else S_IRWU_RG_RO\n    tarinfo.uid = tarinfo.gid = 0\n    tarinfo.uname = tarinfo.gname = ''\n\n    return tarinfo", "loc": 8}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "from_dir_path", "parameters": ["cls", "dir_path", "art_mgr"], "param_types": {"dir_path": "bytes", "art_mgr": "ConcreteArtifactsManager"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Collection at '{path!s}' has invalid metadata\".format", "ValueError", "_is_collection_dir", "art_mgr.get_direct_collection_fqcn", "art_mgr.get_direct_collection_version", "cls", "dir_path.endswith", "dir_path.rstrip", "display.warning", "pathlib.Path", "to_bytes", "to_text", "u\"Collection at '{path!s}' does not have a {manifest_json!s} file, nor has it {galaxy_yml!s}: cannot detect version.\".format", "u\"Collection at '{path!s}' has a {manifest_json!s} or {galaxy_yml!s} file but it contains invalid metadata.\".format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Make collection from an directory with metadata.", "source_code": "def from_dir_path(\n        cls,\n        dir_path: bytes,\n        art_mgr: ConcreteArtifactsManager,\n) -> t.Self:\n    \"\"\"Make collection from an directory with metadata.\"\"\"\n    if dir_path.endswith(to_bytes(os.path.sep)):\n        dir_path = dir_path.rstrip(to_bytes(os.path.sep))\n    if not _is_collection_dir(dir_path):\n        dir_pathlib = pathlib.Path(to_text(dir_path))\n        display.warning(\n            u\"Collection at '{path!s}' does not have a {manifest_json!s} \"\n            u'file, nor has it {galaxy_yml!s}: cannot detect version.'.\n            format(\n                galaxy_yml=to_text(_GALAXY_YAML),\n                manifest_json=to_text(_MANIFEST_JSON),\n                path=to_text(dir_path, errors='surrogate_or_strict'),\n            ),\n        )\n        raise ValueError(\n            '`dir_path` argument must be an installed or a source'\n            ' collection directory.',\n        )\n\n    tmp_inst_req = cls(None, None, dir_path, 'dir', None)\n    req_version = art_mgr.get_direct_collection_version(tmp_inst_req)\n    try:\n        req_name = art_mgr.get_direct_collection_fqcn(tmp_inst_req)\n    except TypeError as err:\n        # Looks like installed/source dir but isn't: doesn't have valid metadata.\n        display.warning(\n            u\"Collection at '{path!s}' has a {manifest_json!s} \"\n            u\"or {galaxy_yml!s} file but it contains invalid metadata.\".\n            format(\n                galaxy_yml=to_text(_GALAXY_YAML),\n                manifest_json=to_text(_MANIFEST_JSON),\n                path=to_text(dir_path, errors='surrogate_or_strict'),\n            ),\n        )\n        raise ValueError(\n            \"Collection at '{path!s}' has invalid metadata\".\n            format(path=to_text(dir_path, errors='surrogate_or_strict'))\n        ) from err\n\n    return cls(req_name, req_version, dir_path, 'dir', None)", "loc": 45}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "from_dir_path_implicit", "parameters": ["cls", "dir_path"], "param_types": {"dir_path": "bytes"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "cls", "dir_path.endswith", "dir_path.rstrip", "to_bytes", "to_text", "u_dir_path.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Construct a collection instance based on an arbitrary dir. This alternative constructor infers the FQCN based on the parent and current directory names. It also sets the version to \"*\"", "source_code": "def from_dir_path_implicit(\n        cls,\n        dir_path: bytes,\n) -> t.Self:\n    \"\"\"Construct a collection instance based on an arbitrary dir.\n\n    This alternative constructor infers the FQCN based on the parent\n    and current directory names. It also sets the version to \"*\"\n    regardless of whether any of known metadata files are present.\n    \"\"\"\n    # There is no metadata, but it isn't required for a functional collection. Determine the namespace.name from the path.\n    if dir_path.endswith(to_bytes(os.path.sep)):\n        dir_path = dir_path.rstrip(to_bytes(os.path.sep))\n    u_dir_path = to_text(dir_path, errors='surrogate_or_strict')\n    path_list = u_dir_path.split(os.path.sep)\n    req_name = '.'.join(path_list[-2:])\n    return cls(req_name, '*', dir_path, 'dir', None)", "loc": 17}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "from_string", "parameters": ["cls", "collection_input", "artifacts_manager", "supplemental_signatures"], "param_types": {"collection_input": "str", "artifacts_manager": "ConcreteArtifactsManager", "supplemental_signatures": "list[str] | None"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleCollectionRef.is_valid_collection_name", "AnsibleError", "PkgReq", "_is_concrete_artifact_pointer", "cls.from_requirement_dict", "collection_input.partition", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_string(\n    cls,\n    collection_input: str,\n    artifacts_manager: ConcreteArtifactsManager,\n    supplemental_signatures: list[str] | None,\n) -> t.Self:\n    req: dict[str, str | list[str] | None] = {}\n    if _is_concrete_artifact_pointer(collection_input) or AnsibleCollectionRef.is_valid_collection_name(collection_input):\n        # Arg is a file path or URL to a collection, or just a collection\n        req['name'] = collection_input\n    elif ':' in collection_input:\n        req['name'], _sep, req['version'] = collection_input.partition(':')\n        if not req['version']:\n            del req['version']\n    else:\n        if not HAS_PACKAGING:\n            raise AnsibleError(\"Failed to import packaging, check that a supported version is installed\")\n        try:\n            pkg_req = PkgReq(collection_input)\n        except Exception as e:\n            # packaging doesn't know what this is, let it fly, better errors happen in from_requirement_dict\n            req['name'] = collection_input\n        else:\n            req['name'] = pkg_req.name\n            if pkg_req.specifier:\n                req['version'] = to_text(pkg_req.specifier)\n    req['signatures'] = supplemental_signatures\n\n    return cls.from_requirement_dict(req, artifacts_manager)", "loc": 29}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "may_have_offline_galaxy_info", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_is_collection_dir"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def may_have_offline_galaxy_info(self) -> bool:\n    if self.fqcn is None:\n        # Virtual collection\n        return False\n    elif not self.is_dir or self.src is None or not _is_collection_dir(self.src):\n        # Not a dir or isn't on-disk\n        return False\n    return True", "loc": 8}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "namespace", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "self._get_separate_ns_n_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def namespace(self) -> str:\n    if self.is_virtual:\n        raise TypeError(f'{self.type} collections do not have a namespace')\n\n    return self._get_separate_ns_n_name()[0]", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "name", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "self._get_separate_ns_n_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def name(self) -> str:\n    if self.is_virtual:\n        raise TypeError(f'{self.type} collections do not have a name')\n\n    return self._get_separate_ns_n_name()[-1]", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "canonical_package_id", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'<virtual namespace from {src!s} of type {src_type!s}>'.format", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def canonical_package_id(self) -> str:\n    if not self.is_virtual:\n        return to_native(self.fqcn)\n\n    return (\n        '<virtual namespace from {src!s} of type {src_type!s}>'.\n        format(src=to_native(self.src), src_type=to_native(self.type))\n    )", "loc": 8}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "namespace_collection_paths", "parameters": ["self"], "param_types": {}, "return_type": "list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_find_collections_in_subdirs", "to_native"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def namespace_collection_paths(self) -> list[str]:\n    return [\n        to_native(path)\n        for path in _find_collections_in_subdirs(self.src)\n    ]", "loc": 5}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "_ComputedReqKindsMixin", "function_name": "is_pinned", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["version_spec_start_char.isdigit", "version_spec_start_char.startswith"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Indicate if the version set is considered pinned. This essentially computes whether the version field of the current requirement explicitly requests a specific version and not an allowed", "source_code": "def is_pinned(self) -> bool:\n    \"\"\"Indicate if the version set is considered pinned.\n\n    This essentially computes whether the version field of the current\n    requirement explicitly requests a specific version and not an allowed\n    version range.\n\n    It is then used to help the resolvelib-based dependency resolver judge\n    whether it's acceptable to consider a pre-release candidate version\n    despite pre-release installs not being requested by the end-user\n    explicitly.\n\n    See https://github.com/ansible/ansible/pull/81606 for extra context.\n    \"\"\"\n    version_spec_start_char = self.ver[0]\n    return version_spec_start_char.isdigit() or not (\n        version_spec_start_char.startswith(('<', '>', '!', '*'))\n    )", "loc": 18}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\dataclasses.py", "class_name": "Candidate", "function_name": "with_signatures_repopulated", "parameters": ["self"], "param_types": {}, "return_type": "Candidate", "param_doc": {}, "return_doc": "", "raises_doc": [{"type": "AnsibleAssertionError", "desc": "If the supplied candidate is not sourced from a Galaxy-like index."}], "called_functions": ["AnsibleAssertionError", "frozenset", "self.__class__", "self.src.get_collection_signatures"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Populate a new Candidate instance with Galaxy signatures.", "source_code": "def with_signatures_repopulated(self) -> Candidate:\n    \"\"\"Populate a new Candidate instance with Galaxy signatures.\n    :raises AnsibleAssertionError: If the supplied candidate is not sourced from a Galaxy-like index.\n    \"\"\"\n    if self.type != 'galaxy':\n        raise AnsibleAssertionError(f\"Invalid collection type for {self!r}: unable to get signatures from a galaxy server.\")\n\n    signatures = self.src.get_collection_signatures(self.namespace, self.name, self.ver)\n    return self.__class__(self.fqcn, self.ver, self.src, self.type, frozenset([*self.signatures, *signatures]))", "loc": 9}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\providers.py", "class_name": "CollectionDependencyProvider", "function_name": "get_preference", "parameters": ["self", "identifier", "resolutions", "candidates", "information", "backtrack_causes"], "param_types": {"identifier": "str", "resolutions": "_c.Mapping[str, Candidate]", "candidates": "_c.Mapping[str, _c.Iterator[Candidate]]", "information": "_c.Mapping[str, _c.Iterator[RequirementInformation[Requirement, Candidate]]]", "backtrack_causes": "_c.Sequence[RequirementInformation[Requirement, Candidate],]"}, "return_type": "float | int", "param_doc": {"identifier": "The value returned by ``identify()``.", "resolutions": "Mapping of identifier, candidate pairs.", "candidates": "Possible candidates for the identifier.", "information": "Requirement information of each package.", "backtrack_causes": "Sequence of requirement information that were"}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "float", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return sort key function return value for given requirement. This result should be based on preference that is defined as \"I think this requirement should be resolved first\". The lower the return value is, the more preferred this group of arguments is.", "source_code": "def get_preference(\n    self,\n    identifier: str,\n    resolutions: _c.Mapping[str, Candidate],\n    candidates: _c.Mapping[str, _c.Iterator[Candidate]],\n    information: _c.Mapping[\n        str,\n        _c.Iterator[RequirementInformation[Requirement, Candidate]],\n    ],\n    backtrack_causes: _c.Sequence[\n        RequirementInformation[Requirement, Candidate],\n    ],\n) -> float | int:\n    \"\"\"Return sort key function return value for given requirement.\n\n    This result should be based on preference that is defined as\n    \"I think this requirement should be resolved first\".\n    The lower the return value is, the more preferred this\n    group of arguments is.\n\n    :param identifier: The value returned by ``identify()``.\n\n    :param resolutions: Mapping of identifier, candidate pairs.\n\n    :param candidates: Possible candidates for the identifier.\n        Mapping of identifier, list of candidate pairs.\n\n    :param information: Requirement information of each package.\n        Mapping of identifier, list of named tuple pairs.\n        The named tuples have the entries ``requirement`` and ``parent``.\n\n    :param backtrack_causes: Sequence of requirement information that were\n        the requirements that caused the resolver to most recently backtrack.\n\n    The preference could depend on various of issues, including\n    (not necessarily in this order):\n\n      * Is this package pinned in the current resolution result?\n\n      * How relaxed is the requirement? Stricter ones should\n        probably be worked on first? (I don't know, actually.)\n\n      * How many possibilities are there to satisfy this\n        requirement? Those with few left should likely be worked on\n        first, I guess?\n\n      * Are there any known conflicts for this requirement?\n        We should probably work on those with the most\n        known conflicts.\n\n    A sortable value should be returned (this will be used as the\n    `key` parameter of the built-in sorting function). The smaller\n    the value is, the more preferred this requirement is (i.e. the\n    sorting function is called with ``reverse=False``).\n    \"\"\"\n    if any(\n            candidate in self._preferred_candidates\n            for candidate in candidates\n    ):\n        # NOTE: Prefer pre-installed candidates over newer versions\n        # NOTE: available from Galaxy or other sources.\n        return float('-inf')\n    return len(candidates)", "loc": 63}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\providers.py", "class_name": "CollectionDependencyProvider", "function_name": "find_matches", "parameters": ["self", "identifier", "requirements", "incompatibilities"], "param_types": {"identifier": "str", "requirements": "_c.Mapping[str, _c.Iterator[Requirement]]", "incompatibilities": "_c.Mapping[str, _c.Iterator[Candidate]]"}, "return_type": "list[Candidate]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "list", "self._find_matches"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Find all possible candidates satisfying given requirements. This tries to get candidates based on the requirements' types. For concrete requirements (SCM, dir, namespace dir, local or", "source_code": "def find_matches(\n    self,\n    identifier: str,\n    requirements: _c.Mapping[str, _c.Iterator[Requirement]],\n    incompatibilities: _c.Mapping[str, _c.Iterator[Candidate]],\n) -> list[Candidate]:\n    r\"\"\"Find all possible candidates satisfying given requirements.\n\n    This tries to get candidates based on the requirements' types.\n\n    For concrete requirements (SCM, dir, namespace dir, local or\n    remote archives), the one-and-only match is returned\n\n    For a \"named\" requirement, Galaxy-compatible APIs are consulted\n    to find concrete candidates for this requirement. If there's a\n    pre-installed candidate, it's prepended in front of others.\n    \"\"\"\n    return [\n        match for match in self._find_matches(list(requirements[identifier]))\n        if not any(match.ver == incompat.ver for incompat in incompatibilities[identifier])\n    ]", "loc": 21}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\providers.py", "class_name": "CollectionDependencyProvider", "function_name": "is_satisfied_by", "parameters": ["self", "requirement", "candidate"], "param_types": {"requirement": "Requirement", "candidate": "Candidate"}, "return_type": "bool", "param_doc": {"requirement": "A requirement that produced the `candidate`.", "candidate": "A pinned candidate supposedly matching the \\"}, "return_doc": "Indication whether the `candidate` is a viable \\", "raises_doc": [], "called_functions": ["meets_requirements"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Whether the given requirement is satisfiable by a candidate.", "source_code": "def is_satisfied_by(\n    self,\n    requirement: Requirement,\n    candidate: Candidate,\n) -> bool:\n    r\"\"\"Whether the given requirement is satisfiable by a candidate.\n\n    :param requirement: A requirement that produced the `candidate`.\n\n    :param candidate: A pinned candidate supposedly matching the \\\n                      `requirement` specifier. It is guaranteed to \\\n                      have been generated from the `requirement`.\n\n    :returns: Indication whether the `candidate` is a viable \\\n              solution to the `requirement`.\n    \"\"\"\n    # NOTE: This is a set of Pipenv-inspired optimizations. Ref:\n    # https://github.com/sarugaku/passa/blob/2ac00f1/src/passa/models/providers.py#L58-L74\n    if (\n            requirement.is_virtual or\n            candidate.is_virtual or\n            requirement.ver == '*'\n    ):\n        return True\n\n    return meets_requirements(\n        version=candidate.ver,\n        requirements=requirement.ver,\n    )", "loc": 29}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\providers.py", "class_name": "CollectionDependencyProvider", "function_name": "get_dependencies", "parameters": ["self", "candidate"], "param_types": {"candidate": "Candidate"}, "return_type": "list[Requirement]", "param_doc": {}, "return_doc": "A collection of requirements that `candidate` \\", "raises_doc": [], "called_functions": ["req_map.items", "self._api_proxy.get_collection_dependencies", "self._make_req_from_dict"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get direct dependencies of a candidate.", "source_code": "def get_dependencies(self, candidate: Candidate) -> list[Requirement]:\n    r\"\"\"Get direct dependencies of a candidate.\n\n    :returns: A collection of requirements that `candidate` \\\n              specifies as its dependencies.\n    \"\"\"\n    # FIXME: If there's several galaxy servers set, there may be a\n    # FIXME: situation when the metadata of the same collection\n    # FIXME: differs. So how do we resolve this case? Priority?\n    # FIXME: Taking into account a pinned hash? Exploding on\n    # FIXME: any differences?\n    # NOTE: The underlying implementation currently uses first found\n    req_map = self._api_proxy.get_collection_dependencies(candidate)\n\n    # NOTE: This guard expression MUST perform an early exit only\n    # NOTE: after the `get_collection_dependencies()` call because\n    # NOTE: internally it populates the artifact URL of the candidate,\n    # NOTE: its SHA hash and the Galaxy API token. These are still\n    # NOTE: necessary with `--no-deps` because even with the disabled\n    # NOTE: dependency resolution the outer layer will still need to\n    # NOTE: know how to download and validate the artifact.\n    #\n    # NOTE: Virtual candidates should always return dependencies\n    # NOTE: because they are ephemeral and non-installable.\n    if not self._with_deps and not candidate.is_virtual:\n        return []\n\n    return [\n        self._make_req_from_dict({'name': dep_name, 'version': dep_req})\n        for dep_name, dep_req in req_map.items()\n    ]", "loc": 31}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\reporters.py", "class_name": "CollectionDependencyReporter", "function_name": "rejecting_candidate", "parameters": ["self", "criterion", "candidate"], "param_types": {"criterion": "Criterion[Candidate, Requirement]", "candidate": "Candidate"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.v", "self._maybe_log_rejection_message", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Augment rejection messages with conflict details.", "source_code": "def rejecting_candidate(  # resolvelib >= 0.9.0\n        self,\n        criterion: Criterion[Candidate, Requirement],\n        candidate: Candidate,\n) -> None:\n    \"\"\"Augment rejection messages with conflict details.\"\"\"\n    if not self._maybe_log_rejection_message(candidate):\n        return\n\n    msg = 'Will try a different candidate, due to conflict:'\n    for req_info in criterion.information:\n        req, parent = req_info.requirement, req_info.parent\n        msg += '\\n    '\n        if parent:\n            msg += f'{parent !s} depends on '\n        else:\n            msg += 'The user requested '\n        msg += str(req)\n    display.v(msg)", "loc": 19}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\versioning.py", "class_name": null, "function_name": "is_pre_release", "parameters": ["version"], "param_types": {"version": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SemanticVersion"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Figure out if a given version is a pre-release.", "source_code": "def is_pre_release(version: str) -> bool:\n    \"\"\"Figure out if a given version is a pre-release.\"\"\"\n    try:\n        return SemanticVersion(version).is_prerelease\n    except ValueError:\n        return False", "loc": 6}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\versioning.py", "class_name": null, "function_name": "meets_requirements", "parameters": ["version", "requirements"], "param_types": {"version": "str", "requirements": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LooseVersion", "SemanticVersion", "SemanticVersion.from_loose_version", "len", "op", "op_map.get", "requirements.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Verify if a given version satisfies all the requirements. Supported version identifiers are: * '=='", "source_code": "def meets_requirements(version: str, requirements: str) -> bool:\n    \"\"\"Verify if a given version satisfies all the requirements.\n\n    Supported version identifiers are:\n      * '=='\n      * '!='\n      * '>'\n      * '>='\n      * '<'\n      * '<='\n      * '*'\n\n    Each requirement is delimited by ','.\n    \"\"\"\n    op_map = {\n        '!=': operator.ne,\n        '==': operator.eq,\n        '=': operator.eq,\n        '>=': operator.ge,\n        '>': operator.gt,\n        '<=': operator.le,\n        '<': operator.lt,\n    }\n\n    for req in requirements.split(','):\n        op_pos = 2 if len(req) > 1 and req[1] == '=' else 1\n        op = op_map.get(req[:op_pos])\n\n        requirement = req[op_pos:]\n        if not op:\n            requirement = req\n            op = operator.eq\n\n        if requirement == '*' or version == '*':\n            continue\n\n        if not op(\n                SemanticVersion(version),\n                SemanticVersion.from_loose_version(LooseVersion(requirement)),\n        ):\n            break\n    else:\n        return True\n\n    # The loop was broken early, it does not meet all the requirements\n    return False", "loc": 46}
{"file": "ansible\\lib\\ansible\\galaxy\\dependency_resolution\\__init__.py", "class_name": null, "function_name": "build_collection_dependency_resolver", "parameters": ["galaxy_apis", "concrete_artifacts_manager", "preferred_candidates", "with_deps", "with_pre_releases", "upgrade", "include_signatures", "offline"], "param_types": {"galaxy_apis": "_c.Iterable[GalaxyAPI]", "concrete_artifacts_manager": "ConcreteArtifactsManager", "preferred_candidates": "_c.Iterable[Candidate] | None", "with_deps": "bool", "with_pre_releases": "bool", "upgrade": "bool", "include_signatures": "bool", "offline": "bool"}, "return_type": "CollectionDependencyResolver", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CollectionDependencyProvider", "CollectionDependencyReporter", "CollectionDependencyResolver", "MultiGalaxyAPIProxy"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a collection dependency resolver. The returned instance will have a ``resolve()`` method for further consumption.", "source_code": "def build_collection_dependency_resolver(\n        galaxy_apis: _c.Iterable[GalaxyAPI],\n        concrete_artifacts_manager: ConcreteArtifactsManager,\n        preferred_candidates: _c.Iterable[Candidate] | None = None,\n        with_deps: bool = True,\n        with_pre_releases: bool = False,\n        upgrade: bool = False,\n        include_signatures: bool = True,\n        offline: bool = False,\n) -> CollectionDependencyResolver:\n    \"\"\"Return a collection dependency resolver.\n\n    The returned instance will have a ``resolve()`` method for\n    further consumption.\n    \"\"\"\n    return CollectionDependencyResolver(\n        CollectionDependencyProvider(\n            apis=MultiGalaxyAPIProxy(galaxy_apis, concrete_artifacts_manager, offline=offline),\n            concrete_artifacts_manager=concrete_artifacts_manager,\n            preferred_candidates=preferred_candidates,\n            with_deps=with_deps,\n            with_pre_releases=with_pre_releases,\n            upgrade=upgrade,\n            include_signatures=include_signatures,\n        ),\n        CollectionDependencyReporter(),\n    )", "loc": 27}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "reconcile_inventory", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "display.debug", "display.warning", "group.get_ancestors", "group_names.add", "group_names.intersection", "host.get_groups", "host_names.add", "len", "self.add_child", "self.groups['all'].get_vars", "self.groups['ungrouped'].remove_host", "self.hosts.values", "set", "set(mygroups).difference"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Ensure inventory basic rules, run after updates.", "source_code": "def reconcile_inventory(self) -> None:\n    \"\"\"Ensure inventory basic rules, run after updates.\"\"\"\n\n    display.debug('Reconcile groups and hosts in inventory.')\n    self.current_source = None\n\n    group_names = set()\n    # set group vars from group_vars/ files and vars plugins\n    for g in self.groups:\n        group = self.groups[g]\n        group_names.add(group.name)\n\n        # ensure all groups inherit from 'all'\n        if group.name != 'all' and not group.get_ancestors():\n            self.add_child('all', group.name)\n\n    host_names = set()\n    # get host vars from host_vars/ files and vars plugins\n    for host in self.hosts.values():\n        host_names.add(host.name)\n\n        mygroups = host.get_groups()\n\n        if self.groups['ungrouped'] in mygroups:\n            # clear ungrouped of any incorrectly stored by parser\n            if set(mygroups).difference({self.groups['all'], self.groups['ungrouped']}):\n                self.groups['ungrouped'].remove_host(host)\n\n        elif not host.implicit:\n            # add ungrouped hosts to ungrouped, except implicit\n            length = len(mygroups)\n            if length == 0 or (length == 1 and self.groups['all'] in mygroups):\n                self.add_child('ungrouped', host.name)\n\n        # special case for implicit hosts\n        if host.implicit:\n            host.vars = combine_vars(self.groups['all'].get_vars(), host.vars)\n\n    # warn if overloading identifier as both group and host\n    for conflict in group_names.intersection(host_names):\n        display.warning(\"Found both group and host with same name: %s\" % conflict)\n\n    self._groups_dict_cache = {}", "loc": 43}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "get_host", "parameters": ["self", "hostname"], "param_types": {"hostname": "str"}, "return_type": "Host | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["helpers.remove_trust", "self._create_implicit_localhost", "self.hosts.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Fetch host object using name deal with implicit localhost.", "source_code": "def get_host(self, hostname: str) -> Host | None:\n    \"\"\"Fetch host object using name deal with implicit localhost.\"\"\"\n\n    hostname = helpers.remove_trust(hostname)\n\n    matching_host = self.hosts.get(hostname, None)\n\n    # if host is not in hosts dict\n    if matching_host is None and hostname in C.LOCALHOST:\n        # might need to create implicit localhost\n        matching_host = self._create_implicit_localhost(hostname)\n\n    return matching_host", "loc": 13}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "add_group", "parameters": ["self", "group"], "param_types": {"group": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Group", "display.debug", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Adds a group to inventory if not there already, returns named actually used.", "source_code": "def add_group(self, group: str) -> str:\n    \"\"\"Adds a group to inventory if not there already, returns named actually used.\"\"\"\n\n    if group:\n        if not isinstance(group, str):\n            raise AnsibleError(\"Invalid group name supplied, expected a string but got %s for %s\" % (type(group), group))\n        if group not in self.groups:\n            g = Group(group)\n            group = g.name  # the group object may have sanitized the group name; use whatever it has\n            if group not in self.groups:\n                self.groups[group] = g\n                self._groups_dict_cache = {}\n                display.debug(\"Added group %s to inventory\" % group)\n        else:\n            display.debug(\"group %s already in inventory\" % group)\n    else:\n        raise AnsibleError(\"Invalid empty/false group name provided: %s\" % group)\n\n    return group", "loc": 19}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "remove_group", "parameters": ["self", "group"], "param_types": {"group": "Group"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.debug", "h.remove_group"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_group(self, group: Group) -> None:\n\n    if group.name in self.groups:\n        del self.groups[group.name]\n        display.debug(\"Removed group %s from inventory\" % group.name)\n        self._groups_dict_cache = {}\n\n    for host in self.hosts:\n        h = self.hosts[host]\n        h.remove_group(group)", "loc": 10}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "add_host", "parameters": ["self", "host", "group", "port"], "param_types": {"host": "str", "group": "str | None", "port": "int | str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Host", "basedir", "display.debug", "display.vvvv", "display.warning", "g.add_host", "helpers.remove_trust", "isinstance", "self.set_variable", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Adds a host to inventory and possibly a group if not there already.", "source_code": "def add_host(self, host: str, group: str | None = None, port: int | str | None = None) -> str:\n    \"\"\"Adds a host to inventory and possibly a group if not there already.\"\"\"\n\n    host = helpers.remove_trust(host)\n\n    if host:\n        if not isinstance(host, str):\n            raise AnsibleError(\"Invalid host name supplied, expected a string but got %s for %s\" % (type(host), host))\n\n        # TODO: add to_safe_host_name\n        g = None\n        if group:\n            if group in self.groups:\n                g = self.groups[group]\n            else:\n                raise AnsibleError(\"Could not find group %s in inventory\" % group)\n\n        if host not in self.hosts:\n            h = Host(host, port)\n            self.hosts[host] = h\n            if self.current_source:  # set to 'first source' in which host was encountered\n                self.set_variable(host, 'inventory_file', self.current_source)\n                self.set_variable(host, 'inventory_dir', basedir(self.current_source))\n            else:\n                self.set_variable(host, 'inventory_file', None)\n                self.set_variable(host, 'inventory_dir', None)\n            display.debug(\"Added host %s to inventory\" % host)\n\n            # set default localhost from inventory to avoid creating an implicit one. Last localhost defined 'wins'.\n            if host in C.LOCALHOST:\n                if self.localhost is None:\n                    self.localhost = self.hosts[host]\n                    display.vvvv(\"Set default localhost to %s\" % h)\n                else:\n                    display.warning(\"A duplicate localhost-like entry was found (%s). First found localhost was %s\" % (h, self.localhost.name))\n        else:\n            h = self.hosts[host]\n\n        if g:\n            g.add_host(h)\n            self._groups_dict_cache = {}\n            display.debug(\"Added host %s to group %s\" % (host, group))\n    else:\n        raise AnsibleError(\"Invalid empty host name provided: %s\" % host)\n\n    return host", "loc": 46}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "remove_host", "parameters": ["self", "host"], "param_types": {"host": "Host"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["g.remove_host"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_host(self, host: Host) -> None:\n\n    if host.name in self.hosts:\n        del self.hosts[host.name]\n\n    for group in self.groups:\n        g = self.groups[group]\n        g.remove_host(host)", "loc": 8}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "set_variable", "parameters": ["self", "entity", "varname", "value"], "param_types": {"entity": "str", "varname": "str", "value": "t.Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.debug", "inv_object.set_variable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets a variable for an inventory object.", "source_code": "def set_variable(self, entity: str, varname: str, value: t.Any) -> None:\n    \"\"\"Sets a variable for an inventory object.\"\"\"\n\n    inv_object: Host | Group\n\n    if entity in self.groups:\n        inv_object = self.groups[entity]\n    elif entity in self.hosts:\n        inv_object = self.hosts[entity]\n    else:\n        raise AnsibleError(\"Could not identify group or host named %s\" % entity)\n\n    inv_object.set_variable(varname, value)\n    display.debug('set %s for %s' % (varname, entity))", "loc": 14}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "add_child", "parameters": ["self", "group", "child"], "param_types": {"group": "str", "child": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.debug", "g.add_child_group", "g.add_host"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add host or group to group.", "source_code": "def add_child(self, group: str, child: str) -> bool:\n    \"\"\"Add host or group to group.\"\"\"\n    if group in self.groups:\n        g = self.groups[group]\n        if child in self.groups:\n            added = g.add_child_group(self.groups[child])\n        elif child in self.hosts:\n            added = g.add_host(self.hosts[child])\n        else:\n            raise AnsibleError(\"%s is not a known host nor group\" % child)\n        self._groups_dict_cache = {}\n        display.debug('Group %s now contains %s' % (group, child))\n    else:\n        raise AnsibleError(\"%s is not a known group\" % group)\n    return added", "loc": 15}
{"file": "ansible\\lib\\ansible\\inventory\\data.py", "class_name": "InventoryData", "function_name": "get_groups_dict", "parameters": ["self"], "param_types": {}, "return_type": "dict[str, list[str]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["group.get_hosts", "self.groups.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "We merge a 'magic' var 'groups' with group name keys and hostname list values into every host variable set. Cache for speed.", "source_code": "def get_groups_dict(self) -> dict[str, list[str]]:\n    \"\"\"\n    We merge a 'magic' var 'groups' with group name keys and hostname list values into every host variable set. Cache for speed.\n    \"\"\"\n    if not self._groups_dict_cache:\n        for group_name, group in self.groups.items():\n            self._groups_dict_cache[group_name] = [h.name for h in group.get_hosts()]\n\n    return self._groups_dict_cache", "loc": 9}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": null, "function_name": "to_safe_group_name", "parameters": ["name", "replacer", "force", "silent"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.INVALID_VARIABLE_NAMES.findall", "C.INVALID_VARIABLE_NAMES.sub", "display.vvvv", "display.warning", "set", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_safe_group_name(name, replacer=\"_\", force=False, silent=False):\n    # Converts 'bad' characters in a string to underscores (or provided replacer) so they can be used as Ansible hosts or groups\n\n    warn = ''\n    if name:  # when deserializing we might not have name yet\n        invalid_chars = C.INVALID_VARIABLE_NAMES.findall(name)\n        if invalid_chars:\n            msg = 'invalid character(s) \"%s\" in group name (%s)' % (to_text(set(invalid_chars)), to_text(name))\n            if C.TRANSFORM_INVALID_GROUP_CHARS not in ('never', 'ignore') or force:\n                name = C.INVALID_VARIABLE_NAMES.sub(replacer, name)\n                if not (silent or C.TRANSFORM_INVALID_GROUP_CHARS == 'silently'):\n                    display.vvvv('Replacing ' + msg)\n                    warn = 'Invalid characters were found in group names and automatically replaced, use -vvvv to see details'\n            else:\n                if C.TRANSFORM_INVALID_GROUP_CHARS == 'never':\n                    display.vvvv('Not replacing %s' % msg)\n                    warn = 'Invalid characters were found in group names but not replaced, use -vvvv to see details'\n\n    if warn:\n        display.warning(warn)\n\n    return name", "loc": 22}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "add_child_group", "parameters": ["self", "group"], "param_types": {"group": "Group"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Exception", "group._check_children_depth", "group.get_ancestors", "group.get_hosts", "group.parent_groups.append", "h.populate_ancestors", "max", "new_ancestors.add", "new_ancestors.difference_update", "self.child_groups.append", "self.clear_hosts_cache", "self.get_ancestors", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_child_group(self, group: Group) -> bool:\n    added = False\n    if self == group:\n        raise Exception(\"can't add group to itself\")\n\n    # don't add if it's already there\n    if group not in self.child_groups:\n\n        # prepare list of group's new ancestors this edge creates\n        start_ancestors = group.get_ancestors()\n        new_ancestors = self.get_ancestors()\n        if group in new_ancestors:\n            raise AnsibleError(\"Adding group '%s' as child to '%s' creates a recursive dependency loop.\" % (to_native(group.name), to_native(self.name)))\n        new_ancestors.add(self)\n        new_ancestors.difference_update(start_ancestors)\n\n        added = True\n        self.child_groups.append(group)\n\n        # update the depth of the child\n        group.depth = max([self.depth + 1, group.depth])\n\n        # update the depth of the grandchildren\n        group._check_children_depth()\n\n        # now add self to child's parent_groups list, but only if there\n        # isn't already a group with the same name\n        if self.name not in [g.name for g in group.parent_groups]:\n            group.parent_groups.append(self)\n            for h in group.get_hosts():\n                h.populate_ancestors(additions=new_ancestors)\n\n        self.clear_hosts_cache()\n    return added", "loc": 34}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "add_host", "parameters": ["self", "host"], "param_types": {"host": "Host"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["host.add_group", "self._hosts.add", "self.clear_hosts_cache", "self.hosts.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_host(self, host: Host) -> bool:\n    added = False\n    if host.name not in self.host_names:\n        self.hosts.append(host)\n        self._hosts.add(host.name)\n        host.add_group(self)\n        self.clear_hosts_cache()\n        added = True\n    return added", "loc": 9}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "remove_host", "parameters": ["self", "host"], "param_types": {"host": "Host"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["host.remove_group", "self._hosts.remove", "self.clear_hosts_cache", "self.hosts.remove"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_host(self, host: Host) -> bool:\n    removed = False\n    if host.name in self.host_names:\n        self.hosts.remove(host)\n        self._hosts.remove(host.name)\n        host.remove_group(self)\n        self.clear_hosts_cache()\n        removed = True\n    return removed", "loc": 9}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "set_variable", "parameters": ["self", "key", "value"], "param_types": {"key": "str", "value": "t.Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Display", "Display().deprecated", "combine_vars", "helpers.remove_trust", "int", "isinstance", "self.set_priority", "validate_variable_name"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_variable(self, key: str, value: t.Any) -> None:\n    key = helpers.remove_trust(key)\n\n    try:\n        validate_variable_name(key)\n    except AnsibleError as ex:\n        Display().deprecated(msg=f'Accepting inventory variable with invalid name {key!r}.', version='2.23', help_text=ex._help_text, obj=ex.obj)\n\n    if key == 'ansible_group_priority':\n        self.set_priority(int(value))\n    else:\n        if key in self.vars and isinstance(self.vars[key], MutableMapping) and isinstance(value, Mapping):\n            self.vars = combine_vars(self.vars, {key: value})\n        else:\n            self.vars[key] = value", "loc": 15}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "clear_hosts_cache", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_ancestors"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def clear_hosts_cache(self) -> None:\n\n    self._hosts_cache = None\n    for g in self.get_ancestors():\n        g._hosts_cache = None", "loc": 5}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "get_hosts", "parameters": ["self"], "param_types": {}, "return_type": "list[Host]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_hosts"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_hosts(self) -> list[Host]:\n\n    if self._hosts_cache is None:\n        self._hosts_cache = self._get_hosts()\n    return self._hosts_cache", "loc": 5}
{"file": "ansible\\lib\\ansible\\inventory\\group.py", "class_name": "Group", "function_name": "set_priority", "parameters": ["self", "priority"], "param_types": {"priority": "int | str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_priority(self, priority: int | str) -> None:\n    try:\n        self.priority = int(priority)\n    except TypeError:\n        # FIXME: warn about invalid priority\n        pass", "loc": 6}
{"file": "ansible\\lib\\ansible\\inventory\\helpers.py", "class_name": null, "function_name": "get_group_vars", "parameters": ["groups"], "param_types": {}, "return_type": null, "param_doc": {"groups": "list of ansible.inventory.group.Group objects"}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "group.get_vars", "sort_groups"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Combine all the group vars from a list of inventory groups.", "source_code": "def get_group_vars(groups):\n    \"\"\"\n    Combine all the group vars from a list of inventory groups.\n\n    :param groups: list of ansible.inventory.group.Group objects\n    :rtype: dict\n    \"\"\"\n    results = {}\n    for group in sort_groups(groups):\n        results = combine_vars(results, group.get_vars())\n\n    return results", "loc": 12}
{"file": "ansible\\lib\\ansible\\inventory\\host.py", "class_name": "Host", "function_name": "populate_ancestors", "parameters": ["self", "additions"], "param_types": {"additions": "c.Iterable[Group] | None"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.add_group", "self.groups.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate_ancestors(self, additions: c.Iterable[Group] | None = None) -> None:\n    # populate ancestors\n    if additions is None:\n        for group in self.groups:\n            self.add_group(group)\n    else:\n        for group in additions:\n            if group not in self.groups:\n                self.groups.append(group)", "loc": 9}
{"file": "ansible\\lib\\ansible\\inventory\\host.py", "class_name": "Host", "function_name": "add_group", "parameters": ["self", "group"], "param_types": {"group": "Group"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["group.get_ancestors", "self.groups.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_group(self, group: Group) -> bool:\n    added = False\n    # populate ancestors first\n    for oldg in group.get_ancestors():\n        if oldg not in self.groups:\n            self.groups.append(oldg)\n\n    # actually add group\n    if group not in self.groups:\n        self.groups.append(group)\n        added = True\n    return added", "loc": 12}
{"file": "ansible\\lib\\ansible\\inventory\\host.py", "class_name": "Host", "function_name": "remove_group", "parameters": ["self", "group"], "param_types": {"group": "Group"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["childg.get_ancestors", "group.get_ancestors", "self.groups.remove", "self.remove_group"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_group(self, group: Group) -> bool:\n    removed = False\n    if group in self.groups:\n        self.groups.remove(group)\n        removed = True\n\n        # remove exclusive ancestors, xcept all!\n        for oldg in group.get_ancestors():\n            if oldg.name != 'all':\n                for childg in self.groups:\n                    if oldg in childg.get_ancestors():\n                        break\n                else:\n                    self.remove_group(oldg)\n    return removed", "loc": 15}
{"file": "ansible\\lib\\ansible\\inventory\\host.py", "class_name": "Host", "function_name": "set_variable", "parameters": ["self", "key", "value"], "param_types": {"key": "str", "value": "t.Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Display", "Display().deprecated", "combine_vars", "helpers.remove_trust", "isinstance", "validate_variable_name"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_variable(self, key: str, value: t.Any) -> None:\n    key = helpers.remove_trust(key)\n\n    try:\n        validate_variable_name(key)\n    except AnsibleError as ex:\n        Display().deprecated(msg=f'Accepting inventory variable with invalid name {key!r}.', version='2.23', help_text=ex._help_text, obj=ex.obj)\n\n    if key in self.vars and isinstance(self.vars[key], MutableMapping) and isinstance(value, Mapping):\n        self.vars = combine_vars(self.vars, {key: value})\n    else:\n        self.vars[key] = value", "loc": 12}
{"file": "ansible\\lib\\ansible\\inventory\\host.py", "class_name": "Host", "function_name": "get_magic_vars", "parameters": ["self"], "param_types": {}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "patterns['ipv4'].match", "patterns['ipv6'].match", "self.get_groups", "self.name.split", "sorted"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_magic_vars(self) -> dict[str, t.Any]:\n    results: dict[str, t.Any] = dict(\n        inventory_hostname=self.name,\n    )\n\n    # FUTURE: these values should be dynamically calculated on access ala the rest of magic vars\n    if patterns['ipv4'].match(self.name) or patterns['ipv6'].match(self.name):\n        results['inventory_hostname_short'] = self.name\n    else:\n        results['inventory_hostname_short'] = self.name.split('.')[0]\n\n    results['group_names'] = sorted([g.name for g in self.get_groups() if g.name != 'all'])\n\n    return results", "loc": 14}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": null, "function_name": "order_patterns", "parameters": ["patterns"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["pattern_exclude.append", "pattern_intersection.append", "pattern_regular.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "takes a list of patterns and reorders them by modifier to apply them consistently", "source_code": "def order_patterns(patterns):\n    \"\"\" takes a list of patterns and reorders them by modifier to apply them consistently \"\"\"\n\n    # FIXME: this goes away if we apply patterns incrementally or by groups\n    pattern_regular = []\n    pattern_intersection = []\n    pattern_exclude = []\n    for p in patterns:\n        if not p:\n            continue\n\n        if p[0] == \"!\":\n            pattern_exclude.append(p)\n        elif p[0] == \"&\":\n            pattern_intersection.append(p)\n        else:\n            pattern_regular.append(p)\n\n    # if no regular pattern was given, hence only exclude and/or intersection\n    # make that magically work\n    if pattern_regular == []:\n        pattern_regular = ['all']\n\n    # when applying the host selectors, run those without the \"&\" or \"!\"\n    # first, then the &s, then the !s.\n    return pattern_regular + pattern_intersection + pattern_exclude", "loc": 26}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": null, "function_name": "split_host_pattern", "parameters": ["pattern"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "itertools.chain.from_iterable", "list", "p.strip", "parse_address", "pattern.split", "re.findall", "split_host_pattern", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Takes a string containing host patterns separated by commas (or a list thereof) and returns a list of single patterns (which may not contain commas). Whitespace is ignored.", "source_code": "def split_host_pattern(pattern):\n    \"\"\"\n    Takes a string containing host patterns separated by commas (or a list\n    thereof) and returns a list of single patterns (which may not contain\n    commas). Whitespace is ignored.\n\n    Also accepts ':' as a separator for backwards compatibility, but it is\n    not recommended due to the conflict with IPv6 addresses and host ranges.\n\n    Example: 'a,b[1], c[2:3] , d' -> ['a', 'b[1]', 'c[2:3]', 'd']\n    \"\"\"\n\n    if isinstance(pattern, list):\n        results = (split_host_pattern(p) for p in pattern)\n        # flatten the results\n        return list(itertools.chain.from_iterable(results))\n    elif not isinstance(pattern, str):\n        pattern = to_text(pattern, errors='surrogate_or_strict')\n\n    # If it's got commas in it, we'll treat it as a straightforward\n    # comma-separated list of patterns.\n    if u',' in pattern:\n        patterns = pattern.split(u',')\n\n    # If it doesn't, it could still be a single pattern. This accounts for\n    # non-separator uses of colons: IPv6 addresses and [x:y] host ranges.\n    else:\n        try:\n            (base, port) = parse_address(pattern, allow_ranges=True)\n            patterns = [pattern]\n        except Exception:\n            # The only other case we accept is a ':'-separated list of patterns.\n            # This mishandles IPv6 addresses, and is retained only for backwards\n            # compatibility.\n            patterns = re.findall(\n                to_text(r\"\"\"(?:     # We want to match something comprising:\n                        [^\\s:\\[\\]]  # (anything other than whitespace or ':[]'\n                        |           # ...or...\n                        \\[[^\\]]*\\]  # a single complete bracketed expression)\n                    )+              # occurring once or more\n                \"\"\"), pattern, re.X\n            )\n\n    return [p.strip() for p in patterns if p.strip()]", "loc": 44}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "parse_sources", "parameters": ["self", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "combine_vars", "display.warning", "get_vars_from_inventory_sources", "self._inventory.reconcile_inventory", "self.groups.values", "self.hosts.values", "self.parse_source", "unfrackpath"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "iterate over inventory sources and parse each one to populate it", "source_code": "def parse_sources(self, cache=False):\n    \"\"\" iterate over inventory sources and parse each one to populate it\"\"\"\n\n    parsed = False\n    # allow for multiple inventory parsing\n    for source in self._sources:\n\n        if source:\n            if ',' not in source:\n                source = unfrackpath(source, follow=False)\n            parse = self.parse_source(source, cache=cache)\n            if parse and not parsed:\n                parsed = True\n\n    if parsed:\n        # do post processing\n        self._inventory.reconcile_inventory()\n    else:\n        if C.INVENTORY_UNPARSED_IS_FAILED:\n            raise AnsibleError(\"No inventory was parsed, please check your configuration and options.\")\n        elif C.INVENTORY_UNPARSED_WARNING:\n            display.warning(\"No inventory was parsed, only implicit localhost is available\")\n\n    for group in self.groups.values():\n        group.vars = combine_vars(group.vars, get_vars_from_inventory_sources(self._loader, self._sources, [group], 'inventory'))\n    for host in self.hosts.values():\n        host.vars = combine_vars(host.vars, get_vars_from_inventory_sources(self._loader, self._sources, [host], 'inventory'))", "loc": 27}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "refresh_inventory", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["InventoryData", "self.add_dynamic_group", "self.add_dynamic_host", "self.clear_caches", "self.parse_sources"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "recalculate inventory", "source_code": "def refresh_inventory(self):\n    \"\"\" recalculate inventory \"\"\"\n\n    self.clear_caches()\n    self._inventory = InventoryData()\n    self.parse_sources(cache=False)\n    for host in self._cached_dynamic_hosts:\n        self.add_dynamic_host(host, {'refresh': True})\n    for host, result in self._cached_dynamic_grouping:\n        result['refresh'] = True\n        self.add_dynamic_group(host, result)", "loc": 11}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "get_hosts", "parameters": ["self", "pattern", "ignore_limits", "ignore_restrictions", "order"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "attrgetter", "deduplicate_list", "isinstance", "pattern_list.extend", "self._evaluate_patterns", "set", "shuffle", "sorted", "split_host_pattern", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Takes a pattern or list of patterns and returns a list of matching inventory host names, taking into account any active restrictions or applied subsets", "source_code": "def get_hosts(self, pattern=\"all\", ignore_limits=False, ignore_restrictions=False, order=None):\n    \"\"\"\n    Takes a pattern or list of patterns and returns a list of matching\n    inventory host names, taking into account any active restrictions\n    or applied subsets\n    \"\"\"\n\n    hosts = []\n\n    # Check if pattern already computed\n    if isinstance(pattern, list):\n        pattern_list = pattern[:]\n    else:\n        pattern_list = [pattern]\n\n    if pattern_list:\n        if not ignore_limits and self._subset:\n            pattern_list.extend(self._subset)\n\n        if not ignore_restrictions and self._restriction:\n            pattern_list.extend(self._restriction)\n\n        # This is only used as a hash key in the self._hosts_patterns_cache dict\n        # a tuple is faster than stringifying\n        pattern_hash = tuple(pattern_list)\n\n        if pattern_hash not in self._hosts_patterns_cache:\n\n            patterns = split_host_pattern(pattern)\n            hosts = self._evaluate_patterns(patterns)\n\n            # mainly useful for hostvars[host] access\n            if not ignore_limits and self._subset:\n                # exclude hosts not in a subset, if defined\n                subset_uuids = set(s._uuid for s in self._evaluate_patterns(self._subset))\n                hosts = [h for h in hosts if h._uuid in subset_uuids]\n\n            if not ignore_restrictions and self._restriction:\n                # exclude hosts mentioned in any restriction (ex: failed hosts)\n                hosts = [h for h in hosts if h.name in self._restriction]\n\n            self._hosts_patterns_cache[pattern_hash] = deduplicate_list(hosts)\n\n        # sort hosts list if needed (should only happen when called from strategy)\n        if order in ['sorted', 'reverse_sorted']:\n            hosts = sorted(self._hosts_patterns_cache[pattern_hash][:], key=attrgetter('name'), reverse=(order == 'reverse_sorted'))\n        elif order == 'reverse_inventory':\n            hosts = self._hosts_patterns_cache[pattern_hash][::-1]\n        else:\n            hosts = self._hosts_patterns_cache[pattern_hash][:]\n            if order == 'shuffle':\n                shuffle(hosts)\n            elif order not in [None, 'inventory']:\n                raise AnsibleOptionsError(\"Invalid 'order' specified for inventory hosts: %s\" % order)\n\n    return hosts", "loc": 56}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "list_hosts", "parameters": ["self", "pattern"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.get_hosts"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return a list of hostnames for a pattern", "source_code": "def list_hosts(self, pattern=\"all\"):\n    \"\"\" return a list of hostnames for a pattern \"\"\"\n    # FIXME: cache?\n    result = self.get_hosts(pattern)\n\n    # allow implicit localhost if pattern matches and no other results\n    if len(result) == 0 and pattern in C.LOCALHOST:\n        result = [pattern]\n\n    return result", "loc": 10}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "restrict_to_hosts", "parameters": ["self", "restriction"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "set", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Restrict list operations to the hosts given in restriction.  This is used to batch serial operations in main playbook code, don't use this for other reasons.", "source_code": "def restrict_to_hosts(self, restriction):\n    \"\"\"\n    Restrict list operations to the hosts given in restriction.  This is used\n    to batch serial operations in main playbook code, don't use this for other\n    reasons.\n    \"\"\"\n    if restriction is None:\n        return\n    elif not isinstance(restriction, list):\n        restriction = [restriction]\n    self._restriction = set(to_text(h.name) for h in restriction)", "loc": 11}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "add_dynamic_host", "parameters": ["self", "host_info", "result_item"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "dict", "host_info.get", "new_group.add_host", "new_host.get_vars", "result_item.get", "self._cached_dynamic_hosts.append", "self._inventory.add_group", "self.add_host", "self.hosts.get", "self.reconcile_inventory"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Helper function to add a new host to inventory based on a task result.", "source_code": "def add_dynamic_host(self, host_info, result_item):\n    \"\"\"\n    Helper function to add a new host to inventory based on a task result.\n    \"\"\"\n\n    changed = False\n    if not result_item.get('refresh'):\n        self._cached_dynamic_hosts.append(host_info)\n\n    if host_info:\n        host_name = host_info.get('host_name')\n\n        # Check if host in inventory, add if not\n        if host_name not in self.hosts:\n            self.add_host(host_name, 'all')\n            changed = True\n        new_host = self.hosts.get(host_name)\n\n        # Set/update the vars for this host\n        new_host_vars = new_host.get_vars()\n        new_host_combined_vars = combine_vars(new_host_vars, host_info.get('host_vars', dict()))\n        if new_host_vars != new_host_combined_vars:\n            new_host.vars = new_host_combined_vars\n            changed = True\n\n        new_groups = host_info.get('groups', [])\n        for group_name in new_groups:\n            if group_name not in self.groups:\n                group_name = self._inventory.add_group(group_name)\n                changed = True\n            new_group = self.groups[group_name]\n            if new_group.add_host(self.hosts[host_name]):\n                changed = True\n\n        # reconcile inventory, ensures inventory rules are followed\n        if changed:\n            self.reconcile_inventory()\n\n        result_item['changed'] = changed", "loc": 39}
{"file": "ansible\\lib\\ansible\\inventory\\manager.py", "class_name": "InventoryManager", "function_name": "add_dynamic_group", "parameters": ["self", "host", "result_item"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "group.add_host", "group.get_hosts", "parent_group.add_child_group", "real_host.add_group", "real_host.get_groups", "result_item.get", "self._cached_dynamic_grouping.append", "self.add_group", "self.hosts.get", "self.reconcile_inventory"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Helper function to add a group (if it does not exist), and to assign the specified host to that group.", "source_code": "def add_dynamic_group(self, host, result_item):\n    \"\"\"\n    Helper function to add a group (if it does not exist), and to assign the\n    specified host to that group.\n    \"\"\"\n\n    changed = False\n\n    if not result_item.get('refresh'):\n        self._cached_dynamic_grouping.append((host, result_item))\n\n    # the host here is from the executor side, which means it was a\n    # serialized/cloned copy and we'll need to look up the proper\n    # host object from the master inventory\n    real_host = self.hosts.get(host.name)\n    if real_host is None:\n        if host.name == self.localhost.name:\n            real_host = self.localhost\n        elif not result_item.get('refresh'):\n            raise AnsibleError('%s cannot be matched in inventory' % host.name)\n        else:\n            # host was removed from inventory during refresh, we should not process\n            return\n\n    group_name = result_item.get('add_group')\n    parent_group_names = result_item.get('parent_groups', [])\n\n    if group_name not in self.groups:\n        group_name = self.add_group(group_name)\n\n    for name in parent_group_names:\n        if name not in self.groups:\n            # create the new group and add it to inventory\n            self.add_group(name)\n            changed = True\n\n    group = self._inventory.groups[group_name]\n    for parent_group_name in parent_group_names:\n        parent_group = self.groups[parent_group_name]\n        new = parent_group.add_child_group(group)\n        if new and not changed:\n            changed = True\n\n    if real_host not in group.get_hosts():\n        changed = group.add_host(real_host)\n\n    if group not in real_host.get_groups():\n        changed = real_host.add_group(group)\n\n    if changed:\n        self.reconcile_inventory()\n\n    result_item['changed'] = changed", "loc": 53}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "package_split", "parameters": ["pkgspec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "re.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def package_split(pkgspec):\n    parts = re.split(r'(>?=)', pkgspec, maxsplit=1)\n    if len(parts) > 1:\n        return parts\n    return parts[0], None, None", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "package_version_compare", "parameters": ["version", "other_version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["apt_pkg.VersionCompare", "apt_pkg.version_compare"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def package_version_compare(version, other_version):\n    try:\n        return apt_pkg.version_compare(version, other_version)\n    except AttributeError:\n        return apt_pkg.VersionCompare(version, other_version)", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "package_best_match", "parameters": ["pkgname", "version_cmp", "version", "release", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["apt_pkg.Policy", "apt_pkg.config.find_file", "fnmatch.fnmatch", "policy.create_pin", "policy.get_candidate_ver", "policy.read_pindir", "policy.read_pinfile"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def package_best_match(pkgname, version_cmp, version, release, cache):\n    policy = apt_pkg.Policy(cache)\n\n    policy.read_pinfile(apt_pkg.config.find_file(\"Dir::Etc::preferences\"))\n    policy.read_pindir(apt_pkg.config.find_file(\"Dir::Etc::preferencesparts\"))\n\n    if release:\n        # 990 is the priority used in `apt-get -t`\n        policy.create_pin('Release', pkgname, release, 990)\n    if version_cmp == \"=\":\n        # Installing a specific version from command line overrides all pinning\n        # We don't mimic this exactly, but instead set a priority which is higher than all APT built-in pin priorities.\n        policy.create_pin('Version', pkgname, version, 1001)\n    pkg = cache[pkgname]\n    pkgver = policy.get_candidate_ver(pkg)\n    if not pkgver:\n        return None\n    if version_cmp == \"=\" and not fnmatch.fnmatch(pkgver.ver_str, version):\n        # Even though we put in a pin policy, it can be ignored if there is no\n        # possible candidate.\n        return None\n    return pkgver.ver_str", "loc": 22}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "package_status", "parameters": ["m", "pkgname", "version_cmp", "version", "default_release", "cache", "state"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A tuple of (installed, installed_version, version_installable, has_files). *installed* indicates whether", "raises_doc": [], "called_functions": ["apt_pkg.version_compare", "cache.get_providing_packages", "cache.is_virtual_package", "fnmatch.fnmatch", "len", "m.fail_json", "package_best_match", "package_status"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def package_status(m, pkgname, version_cmp, version, default_release, cache, state):\n    \"\"\"\n    :return: A tuple of (installed, installed_version, version_installable, has_files). *installed* indicates whether\n    the package (regardless of version) is installed. *installed_version* indicates whether the installed package\n    matches the provided version criteria. *version_installable* provides the latest matching version that can be\n    installed. In the case of virtual packages where we can't determine an applicable match, True is returned.\n    *has_files* indicates whether the package has files on the filesystem (even if not installed, meaning a purge is\n    required).\n    \"\"\"\n    try:\n        # get the package from the cache, as well as the\n        # low-level apt_pkg.Package object which contains\n        # state fields not directly accessible from the\n        # higher-level apt.package.Package object.\n        pkg = cache[pkgname]\n        ll_pkg = cache._cache[pkgname]  # the low-level package object\n    except KeyError:\n        if state == 'install':\n            try:\n                provided_packages = cache.get_providing_packages(pkgname)\n                if provided_packages:\n                    # When this is a virtual package satisfied by only\n                    # one installed package, return the status of the target\n                    # package to avoid requesting re-install\n                    if cache.is_virtual_package(pkgname) and len(provided_packages) == 1:\n                        package = provided_packages[0]\n                        installed, installed_version, version_installable, has_files = \\\n                            package_status(m, package.name, version_cmp, version, default_release, cache, state='install')\n                        if installed:\n                            return installed, installed_version, version_installable, has_files\n\n                    # Otherwise return nothing so apt will sort out\n                    # what package to satisfy this with\n                    return False, False, True, False\n\n                m.fail_json(msg=\"No package matching '%s' is available\" % pkgname)\n            except AttributeError:\n                # python-apt version too old to detect virtual packages\n                # mark as not installed and let apt-get install deal with it\n                return False, False, True, False\n        else:\n            return False, False, None, False\n    try:\n        has_files = len(pkg.installed_files) > 0\n    except UnicodeDecodeError:\n        has_files = True\n    except AttributeError:\n        has_files = False  # older python-apt cannot be used to determine non-purged\n\n    try:\n        package_is_installed = ll_pkg.current_state == apt_pkg.CURSTATE_INSTALLED\n    except AttributeError:  # python-apt 0.7.X has very weak low-level object\n        try:\n            # might not be necessary as python-apt post-0.7.X should have current_state property\n            package_is_installed = pkg.is_installed\n        except AttributeError:\n            # assume older version of python-apt is installed\n            package_is_installed = pkg.isInstalled\n\n    version_best = package_best_match(pkgname, version_cmp, version, default_release, cache._cache)\n    version_is_installed = False\n    version_installable = None\n    if package_is_installed:\n        try:\n            installed_version = pkg.installed.version\n        except AttributeError:\n            installed_version = pkg.installedVersion\n\n        if version_cmp == \"=\":\n            # check if the version is matched as well\n            version_is_installed = fnmatch.fnmatch(installed_version, version)\n            if version_best and installed_version != version_best and fnmatch.fnmatch(version_best, version):\n                version_installable = version_best\n        elif version_cmp == \">=\":\n            version_is_installed = apt_pkg.version_compare(installed_version, version) >= 0\n            if version_best and installed_version != version_best and apt_pkg.version_compare(version_best, version) >= 0:\n                version_installable = version_best\n        else:\n            version_is_installed = True\n            if version_best and installed_version != version_best:\n                version_installable = version_best\n    else:\n        version_installable = version_best\n\n    return package_is_installed, version_is_installed, version_installable, has_files", "loc": 85}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "expand_dpkg_options", "parameters": ["dpkg_options_compressed"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dpkg_options.strip", "dpkg_options_compressed.split"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def expand_dpkg_options(dpkg_options_compressed):\n    options_list = dpkg_options_compressed.split(',')\n    dpkg_options = \"\"\n    for dpkg_option in options_list:\n        dpkg_options = '%s -o \"Dpkg::Options::=--%s\"' \\\n                       % (dpkg_options, dpkg_option)\n    return dpkg_options.strip()", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "expand_pkgspec_from_fnmatches", "parameters": ["m", "pkgspec", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fnmatch.filter", "frozenset", "frozenset('*?[]!').intersection", "isinstance", "m.fail_json", "new_pkgspec.append", "new_pkgspec.extend", "package_split", "to_text", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def expand_pkgspec_from_fnmatches(m, pkgspec, cache):\n    # Note: apt-get does implicit regex matching when an exact package name\n    # match is not found.  Something like this:\n    # matches = [pkg.name for pkg in cache if re.match(pkgspec, pkg.name)]\n    # (Should also deal with the ':' for multiarch like the fnmatch code below)\n    #\n    # We have decided not to do similar implicit regex matching but might take\n    # a PR to add some sort of explicit regex matching:\n    # https://github.com/ansible/ansible-modules-core/issues/1258\n    new_pkgspec = []\n    if pkgspec:\n        for pkgspec_pattern in pkgspec:\n\n            if not isinstance(pkgspec_pattern, str):\n                m.fail_json(msg=\"Invalid type for package name, expected string but got %s\" % type(pkgspec_pattern))\n\n            pkgname_pattern, version_cmp, version = package_split(pkgspec_pattern)\n\n            # note that none of these chars is allowed in a (debian) pkgname\n            if frozenset('*?[]!').intersection(pkgname_pattern):\n                # handle multiarch pkgnames, the idea is that \"apt*\" should\n                # only select native packages. But \"apt*:i386\" should still work\n                if \":\" not in pkgname_pattern:\n                    # Filter the multiarch packages from the cache only once\n                    try:\n                        pkg_name_cache = _non_multiarch  # pylint: disable=used-before-assignment\n                    except NameError:\n                        pkg_name_cache = _non_multiarch = [pkg.name for pkg in cache if ':' not in pkg.name]  # noqa: F841\n                else:\n                    # Create a cache of pkg_names including multiarch only once\n                    try:\n                        pkg_name_cache = _all_pkg_names  # pylint: disable=used-before-assignment\n                    except NameError:\n                        pkg_name_cache = _all_pkg_names = [pkg.name for pkg in cache]  # noqa: F841\n\n                matches = fnmatch.filter(pkg_name_cache, pkgname_pattern)\n\n                if not matches:\n                    m.fail_json(msg=\"No package(s) matching '%s' available\" % to_text(pkgname_pattern))\n                else:\n                    new_pkgspec.extend(matches)\n            else:\n                # No wildcards in name\n                new_pkgspec.append(pkgspec_pattern)\n    return new_pkgspec", "loc": 45}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "parse_diff", "parameters": ["output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "diff.index", "enumerate", "len", "next", "re.match", "to_native", "to_native(output).splitlines"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_diff(output):\n    diff = to_native(output).splitlines()\n    try:\n        # check for start marker from aptitude\n        diff_start = diff.index('Resolving dependencies...')\n    except ValueError:\n        try:\n            # check for start marker from apt-get\n            diff_start = diff.index('Reading state information...')\n        except ValueError:\n            # show everything\n            diff_start = -1\n    try:\n        # check for end marker line from both apt-get and aptitude\n        diff_end = next(i for i, item in enumerate(diff) if re.match('[0-9]+ (packages )?upgraded', item))\n    except StopIteration:\n        diff_end = len(diff)\n    diff_start += 1\n    diff_end += 1\n    return {'prepared': '\\n'.join(diff[diff_start:diff_end])}", "loc": 20}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "mark_installed", "parameters": ["m", "packages", "manual"], "param_types": {"m": "AnsibleModule", "packages": "list[str]", "manual": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.fail_json", "m.get_bin_path", "m.run_command", "m.warn", "shlex.join"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Mark packages as manually or automatically installed.", "source_code": "def mark_installed(m: AnsibleModule, packages: list[str], manual: bool) -> None:\n    \"\"\"Mark packages as manually or automatically installed.\"\"\"\n    if not packages:\n        return\n\n    if manual:\n        mark_msg = \"manually\"\n        mark_op = \"manual\"\n    else:\n        mark_msg = \"auto\"\n        mark_op = \"auto\"\n\n    apt_mark_cmd_path = m.get_bin_path(\"apt-mark\")\n\n    # https://github.com/ansible/ansible/issues/40531\n    if apt_mark_cmd_path is None:\n        m.warn(f\"Could not find apt-mark binary, not marking package(s) as {mark_msg} installed.\")\n        return\n\n    cmd = [apt_mark_cmd_path, mark_op] + packages\n    rc, out, err = m.run_command(cmd)\n\n    if rc != 0:\n        m.fail_json(msg=f\"Command {shlex.join(cmd)!r} failed.\", stdout=out, stderr=err, rc=rc)", "loc": 24}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "get_field_of_deb", "parameters": ["m", "deb_file", "field"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.fail_json", "m.get_bin_path", "m.run_command", "to_native", "to_native(stdout).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_field_of_deb(m, deb_file, field=\"Version\"):\n    cmd_dpkg = m.get_bin_path(\"dpkg\", True)\n    cmd = cmd_dpkg + \" --field %s %s\" % (deb_file, field)\n    rc, stdout, stderr = m.run_command(cmd)\n    if rc != 0:\n        m.fail_json(msg=\"%s failed\" % cmd, stdout=stdout, stderr=stderr)\n    return to_native(stdout).strip('\\n')", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "remove", "parameters": ["m", "pkgspec", "cache", "purge", "force", "dpkg_options", "autoremove", "allow_change_held_packages"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "PolicyRcD", "expand_dpkg_options", "expand_pkgspec_from_fnmatches", "m.exit_json", "m.fail_json", "m.run_command", "package_split", "package_status", "parse_diff", "pkg_list.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove(m, pkgspec, cache, purge=False, force=False,\n           dpkg_options=expand_dpkg_options(DPKG_OPTIONS), autoremove=False,\n           allow_change_held_packages=False):\n    pkg_list = []\n    pkgspec = expand_pkgspec_from_fnmatches(m, pkgspec, cache)\n    for package in pkgspec:\n        name, version_cmp, version = package_split(package)\n        installed, installed_version, upgradable, has_files = package_status(m, name, version_cmp, version, None, cache, state='remove')\n        if installed_version or (has_files and purge):\n            pkg_list.append(\"'%s'\" % package)\n    packages = ' '.join(pkg_list)\n\n    if not packages:\n        m.exit_json(changed=False)\n    else:\n        if force:\n            force_yes = '--force-yes'\n        else:\n            force_yes = ''\n\n        if purge:\n            purge = '--purge'\n        else:\n            purge = ''\n\n        if autoremove:\n            autoremove = '--auto-remove'\n        else:\n            autoremove = ''\n\n        if m.check_mode:\n            check_arg = '--simulate'\n        else:\n            check_arg = ''\n\n        if allow_change_held_packages:\n            allow_change_held_packages = '--allow-change-held-packages'\n        else:\n            allow_change_held_packages = ''\n\n        cmd = \"%s -q -y %s %s %s %s %s %s remove %s\" % (\n            APT_GET_CMD,\n            dpkg_options,\n            purge,\n            force_yes,\n            autoremove,\n            check_arg,\n            allow_change_held_packages,\n            packages\n        )\n\n        with PolicyRcD(m):\n            rc, out, err = m.run_command(cmd)\n\n        if m._diff:\n            diff = parse_diff(out)\n        else:\n            diff = {}\n        if rc:\n            m.fail_json(msg=\"'apt-get remove %s' failed: %s\" % (packages, err), stdout=out, stderr=err, rc=rc)\n        m.exit_json(changed=True, stdout=out, stderr=err, diff=diff)", "loc": 61}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "cleanup", "parameters": ["m", "purge", "force", "operation", "dpkg_options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AssertionError", "PolicyRcD", "expand_dpkg_options", "frozenset", "m.exit_json", "m.fail_json", "m.run_command", "parse_diff"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cleanup(m, purge=False, force=False, operation=None,\n            dpkg_options=expand_dpkg_options(DPKG_OPTIONS)):\n\n    if operation not in frozenset(['autoremove', 'autoclean']):\n        raise AssertionError('Expected \"autoremove\" or \"autoclean\" cleanup operation, got %s' % operation)\n\n    if force:\n        force_yes = '--force-yes'\n    else:\n        force_yes = ''\n\n    if purge:\n        purge = '--purge'\n    else:\n        purge = ''\n\n    if m.check_mode:\n        check_arg = '--simulate'\n    else:\n        check_arg = ''\n\n    cmd = \"%s -y %s %s %s %s %s\" % (APT_GET_CMD, dpkg_options, purge, force_yes, operation, check_arg)\n\n    with PolicyRcD(m):\n        rc, out, err = m.run_command(cmd)\n\n    if m._diff:\n        diff = parse_diff(out)\n    else:\n        diff = {}\n    if rc:\n        m.fail_json(msg=\"'apt-get %s' failed: %s\" % (operation, err), stdout=out, stderr=err, rc=rc)\n\n    changed = CLEAN_OP_CHANGED_STR[operation] in out\n\n    m.exit_json(changed=changed, stdout=out, stderr=err, diff=diff)", "loc": 36}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "aptclean", "parameters": ["m"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.fail_json", "m.run_command", "parse_diff"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def aptclean(m):\n    clean_rc, clean_out, clean_err = m.run_command(['apt-get', 'clean'])\n    clean_diff = parse_diff(clean_out) if m._diff else {}\n\n    if clean_rc:\n        m.fail_json(msg=\"apt-get clean failed\", stdout=clean_out, rc=clean_rc)\n    if clean_err:\n        m.fail_json(msg=\"apt-get clean failed: %s\" % clean_err, stdout=clean_out, rc=clean_rc)\n    return (clean_out, clean_err, clean_diff)", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "upgrade", "parameters": ["m", "mode", "force", "default_release", "use_apt_get", "dpkg_options", "autoremove", "fail_on_autoremove", "allow_unauthenticated", "allow_downgrade"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PolicyRcD", "expand_dpkg_options", "m.exit_json", "m.fail_json", "m.get_bin_path", "m.run_command", "m.warn", "parse_diff"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def upgrade(m, mode=\"yes\", force=False, default_release=None,\n            use_apt_get=False,\n            dpkg_options=expand_dpkg_options(DPKG_OPTIONS), autoremove=False, fail_on_autoremove=False,\n            allow_unauthenticated=False,\n            allow_downgrade=False,\n            ):\n\n    if autoremove:\n        autoremove = '--auto-remove'\n    else:\n        autoremove = ''\n\n    if m.check_mode:\n        check_arg = '--simulate'\n    else:\n        check_arg = ''\n\n    apt_cmd = None\n    prompt_regex = None\n    if mode == \"dist\" or (mode == \"full\" and use_apt_get):\n        # apt-get dist-upgrade\n        apt_cmd = APT_GET_CMD\n        upgrade_command = \"dist-upgrade %s\" % (autoremove)\n    elif mode == \"full\" and not use_apt_get:\n        # aptitude full-upgrade\n        apt_cmd = APTITUDE_CMD\n        upgrade_command = \"full-upgrade\"\n    else:\n        if use_apt_get:\n            apt_cmd = APT_GET_CMD\n            upgrade_command = \"upgrade --with-new-pkgs %s\" % (autoremove)\n        else:\n            # aptitude safe-upgrade # mode=yes # default\n            apt_cmd = APTITUDE_CMD\n            upgrade_command = \"safe-upgrade\"\n            prompt_regex = r\"(^Do you want to ignore this warning and proceed anyway\\?|^\\*\\*\\*.*\\[default=.*\\])\"\n\n    if force:\n        if apt_cmd == APT_GET_CMD:\n            force_yes = '--force-yes'\n        else:\n            force_yes = '--assume-yes --allow-untrusted'\n    else:\n        force_yes = ''\n\n    if fail_on_autoremove:\n        if apt_cmd == APT_GET_CMD:\n            fail_on_autoremove = '--no-remove'\n        else:\n            m.warn(\"APTITUDE does not support '--no-remove', ignoring the 'fail_on_autoremove' parameter.\")\n            fail_on_autoremove = ''\n    else:\n        fail_on_autoremove = ''\n\n    allow_unauthenticated = '--allow-unauthenticated' if allow_unauthenticated else ''\n\n    if allow_downgrade:\n        if apt_cmd == APT_GET_CMD:\n            allow_downgrade = '--allow-downgrades'\n        else:\n            m.warn(\"APTITUDE does not support '--allow-downgrades', ignoring the 'allow_downgrade' parameter.\")\n            allow_downgrade = ''\n    else:\n        allow_downgrade = ''\n\n    if apt_cmd is None:\n        if use_apt_get:\n            apt_cmd = APT_GET_CMD\n        else:\n            m.fail_json(msg=\"Unable to find APTITUDE in path. Please make sure \"\n                            \"to have APTITUDE in path or use 'force_apt_get=True'\")\n    apt_cmd_path = m.get_bin_path(apt_cmd, required=True)\n\n    cmd = '%s -y %s %s %s %s %s %s %s' % (\n        apt_cmd_path,\n        dpkg_options,\n        force_yes,\n        fail_on_autoremove,\n        allow_unauthenticated,\n        allow_downgrade,\n        check_arg,\n        upgrade_command,\n    )\n\n    if default_release:\n        cmd += \" -t '%s'\" % (default_release,)\n\n    with PolicyRcD(m):\n        rc, out, err = m.run_command(cmd, prompt_regex=prompt_regex)\n\n    if m._diff:\n        diff = parse_diff(out)\n    else:\n        diff = {}\n    if rc:\n        m.fail_json(msg=\"'%s %s' failed: %s\" % (apt_cmd, upgrade_command, err), stdout=out, rc=rc)\n    if (apt_cmd == APT_GET_CMD and APT_GET_ZERO in out) or (apt_cmd == APTITUDE_CMD and APTITUDE_ZERO in out):\n        m.exit_json(changed=False, msg=out, stdout=out, stderr=err)\n    m.exit_json(changed=True, msg=out, stdout=out, stderr=err, diff=diff)", "loc": 99}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "get_updated_cache_time", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "``tuple``", "raises_doc": [], "called_functions": ["datetime.datetime.fromtimestamp", "get_cache_mtime", "int", "mtimestamp.timetuple", "time.mktime"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return the mtime time stamp and the updated cache time. Always retrieve the mtime of the apt cache or set the `cache_mtime` variable to 0", "source_code": "def get_updated_cache_time():\n    \"\"\"Return the mtime time stamp and the updated cache time.\n    Always retrieve the mtime of the apt cache or set the `cache_mtime`\n    variable to 0\n    :returns: ``tuple``\n    \"\"\"\n    cache_mtime = get_cache_mtime()\n    mtimestamp = datetime.datetime.fromtimestamp(cache_mtime)\n    updated_cache_time = int(time.mktime(mtimestamp.timetuple()))\n    return mtimestamp, updated_cache_time", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\apt.py", "class_name": null, "function_name": "get_cache", "parameters": ["module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["apt.Cache", "module.fail_json", "module.run_command", "to_native", "to_native(e).lower"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Attempt to get the cache object and update till it works", "source_code": "def get_cache(module):\n    \"\"\"Attempt to get the cache object and update till it works\"\"\"\n    cache = None\n    try:\n        cache = apt.Cache()\n    except SystemError as e:\n        if '/var/lib/apt/lists/' in to_native(e).lower():\n            # update cache until files are fixed or retries exceeded\n            retries = 0\n            while retries < 2:\n                (rc, so, se) = module.run_command(['apt-get', 'update', '-q'])\n                retries += 1\n                if rc == 0:\n                    break\n            if rc != 0:\n                module.fail_json(msg='Updating the cache to correct corrupt package lists failed:\\n%s\\n%s' % (to_native(e), so + se), rc=rc)\n            # try again\n            cache = apt.Cache()\n        else:\n            module.fail_json(msg=to_native(e))\n    return cache", "loc": 21}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "lang_env", "parameters": ["module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "get_best_parsable_locale", "hasattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def lang_env(module):\n\n    if not hasattr(lang_env, 'result'):\n        locale = get_best_parsable_locale(module)\n        lang_env.result = dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale, LANGUAGE=locale)\n\n    return lang_env.result", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "find_needed_binaries", "parameters": ["module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.exit_json", "module.get_bin_path", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_needed_binaries(module):\n    global apt_key_bin\n    global gpg_bin\n\n    try:\n        apt_key_bin = module.get_bin_path('apt-key', required=True)\n    except ValueError as e:\n        module.exit_json(f'{to_native(e)}. Apt-key has been deprecated. See the deb822_repository as an alternative.')\n\n    try:\n        gpg_bin = module.get_bin_path('gpg', required=True)\n    except ValueError as e:\n        module.exit_json(msg=to_native(e))", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "add_http_proxy", "parameters": ["cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.environ.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_http_proxy(cmd):\n\n    for envvar in ('HTTPS_PROXY', 'https_proxy', 'HTTP_PROXY', 'http_proxy'):\n        proxy = os.environ.get(envvar)\n        if proxy:\n            break\n\n    if proxy:\n        cmd += ' --keyserver-options http-proxy=%s' % proxy\n\n    return cmd", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "parse_key_id", "parameters": ["key_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "The portion of key_id suitable for apt-key del, the portion", "raises_doc": [], "called_functions": ["ValueError", "int", "key_id.startswith", "key_id.upper", "len", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "validate the key_id and break it into segments :arg key_id: The key_id as supplied by the user.  A valid key_id will be 8, 16, or more hexadecimal chars with an optional leading ``0x``.", "source_code": "def parse_key_id(key_id):\n    \"\"\"validate the key_id and break it into segments\n\n    :arg key_id: The key_id as supplied by the user.  A valid key_id will be\n        8, 16, or more hexadecimal chars with an optional leading ``0x``.\n    :returns: The portion of key_id suitable for apt-key del, the portion\n        suitable for comparisons with --list-public-keys, and the portion that\n        can be used with --recv-key.  If key_id is long enough, these will be\n        the last 8 characters of key_id, the last 16 characters, and all of\n        key_id.  If key_id is not long enough, some of the values will be the\n        same.\n\n    * apt-key del <= 1.10 has a bug with key_id != 8 chars\n    * apt-key adv --list-public-keys prints 16 chars\n    * apt-key adv --recv-key can take more chars\n\n    \"\"\"\n    # Make sure the key_id is valid hexadecimal\n    int(to_native(key_id), 16)\n\n    key_id = key_id.upper()\n    if key_id.startswith('0X'):\n        key_id = key_id[2:]\n\n    key_id_len = len(key_id)\n    if (key_id_len != 8 and key_id_len != 16) and key_id_len <= 16:\n        raise ValueError('key_id must be 8, 16, or 16+ hexadecimal characters in length')\n\n    short_key_id = key_id[-8:]\n\n    fingerprint = key_id\n    if key_id_len > 16:\n        fingerprint = key_id[-16:]\n\n    return short_key_id, fingerprint, key_id", "loc": 35}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "parse_output_for_keys", "parameters": ["output", "short_format"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["code.split", "found.append", "line.split", "line.startswith", "shorten_key_ids", "to_native", "to_native(output).split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_output_for_keys(output, short_format=False):\n\n    found = []\n    lines = to_native(output).split('\\n')\n    for line in lines:\n        if (line.startswith(\"pub\") or line.startswith(\"sub\")) and \"expired\" not in line:\n            try:\n                # apt key format\n                tokens = line.split()\n                code = tokens[1]\n                (len_type, real_code) = code.split(\"/\")\n            except (IndexError, ValueError):\n                # gpg format\n                try:\n                    tokens = line.split(':')\n                    real_code = tokens[4]\n                except (IndexError, ValueError):\n                    # invalid line, skip\n                    continue\n            found.append(real_code)\n\n    if found and short_format:\n        found = shorten_key_ids(found)\n\n    return found", "loc": 25}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "all_keys", "parameters": ["module", "keyring", "short_format"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.run_command", "parse_output_for_keys"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def all_keys(module, keyring, short_format):\n    if keyring is not None:\n        cmd = \"%s --keyring %s adv --list-public-keys --keyid-format=long\" % (apt_key_bin, keyring)\n    else:\n        cmd = \"%s adv --list-public-keys --keyid-format=long\" % apt_key_bin\n    (rc, out, err) = module.run_command(cmd)\n    if rc != 0:\n        module.fail_json(msg=\"Unable to list public keys\", cmd=cmd, rc=rc, stdout=out, stderr=err)\n\n    return parse_output_for_keys(out, short_format)", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "shorten_key_ids", "parameters": ["key_id_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["short.append"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Takes a list of key ids, and converts them to the 'short' format, by reducing them to their last 8 characters.", "source_code": "def shorten_key_ids(key_id_list):\n    \"\"\"\n    Takes a list of key ids, and converts them to the 'short' format,\n    by reducing them to their last 8 characters.\n    \"\"\"\n    short = []\n    for key in key_id_list:\n        short.append(key[-8:])\n    return short", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "download_key", "parameters": ["module", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fetch_url", "module.fail_json", "rsp.read"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def download_key(module, url):\n\n    try:\n        # note: validate_certs and other args are pulled from module directly\n        rsp, info = fetch_url(module, url, use_proxy=True)\n        if info['status'] != 200:\n            module.fail_json(msg=\"Failed to download key at %s: %s\" % (url, info['msg']))\n\n        return rsp.read()\n    except Exception:\n        module.fail_json(msg=f\"Error getting key id from url: {url}\")", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "get_key_id_from_file", "parameters": ["module", "filename", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lang_env", "module.fail_json", "module.run_command", "native_data.find", "parse_output_for_keys", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_key_id_from_file(module, filename, data=None):\n\n    native_data = to_native(data)\n    is_armored = native_data.find(\"-----BEGIN PGP PUBLIC KEY BLOCK-----\") >= 0\n\n    key = None\n\n    cmd = [gpg_bin, '--with-colons', filename]\n\n    (rc, out, err) = module.run_command(cmd, environ_update=lang_env(module), data=(native_data if is_armored else data), binary_data=not is_armored)\n    if rc != 0:\n        module.fail_json(msg=\"Unable to extract key from '%s'\" % ('inline data' if data is not None else filename), stdout=out, stderr=err)\n\n    keys = parse_output_for_keys(out)\n    # assume we only want first key?\n    if keys:\n        key = keys[0]\n\n    return key", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "add_key", "parameters": ["module", "keyfile", "keyring", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_key(module, keyfile, keyring, data=None):\n    if data is not None:\n        if keyring:\n            cmd = \"%s --keyring %s add -\" % (apt_key_bin, keyring)\n        else:\n            cmd = \"%s add -\" % apt_key_bin\n        (rc, out, err) = module.run_command(cmd, data=data, binary_data=True)\n        if rc != 0:\n            module.fail_json(\n                msg=\"Unable to add a key from binary data\",\n                cmd=cmd,\n                rc=rc,\n                stdout=out,\n                stderr=err,\n            )\n    else:\n        if keyring:\n            cmd = \"%s --keyring %s add %s\" % (apt_key_bin, keyring, keyfile)\n        else:\n            cmd = \"%s add %s\" % (apt_key_bin, keyfile)\n        (rc, out, err) = module.run_command(cmd)\n        if rc != 0:\n            module.fail_json(\n                msg=\"Unable to add a key from file %s\" % (keyfile),\n                cmd=cmd,\n                rc=rc,\n                keyfile=keyfile,\n                stdout=out,\n                stderr=err,\n            )\n    return True", "loc": 31}
{"file": "ansible\\lib\\ansible\\modules\\apt_key.py", "class_name": null, "function_name": "remove_key", "parameters": ["module", "key_id", "keyring"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_key(module, key_id, keyring):\n    if keyring:\n        cmd = '%s --keyring %s del %s' % (apt_key_bin, keyring, key_id)\n    else:\n        cmd = '%s del %s' % (apt_key_bin, key_id)\n    (rc, out, err) = module.run_command(cmd)\n    if rc != 0:\n        module.fail_json(\n            msg=\"Unable to remove a key with id %s\" % (key_id),\n            cmd=cmd,\n            rc=rc,\n            key_id=key_id,\n            stdout=out,\n            stderr=err,\n        )\n    return True", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\apt_repository.py", "class_name": null, "function_name": "install_python_apt", "parameters": ["module", "apt_pkg_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.get_bin_path", "module.run_command", "se.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def install_python_apt(module, apt_pkg_name):\n\n    if not module.check_mode:\n        apt_get_path = module.get_bin_path('apt-get')\n        if apt_get_path:\n            rc, so, se = module.run_command([apt_get_path, 'update'])\n            if rc != 0:\n                module.fail_json(msg=\"Failed to auto-install %s. Error was: '%s'\" % (apt_pkg_name, se.strip()))\n            rc, so, se = module.run_command([apt_get_path, 'install', apt_pkg_name, '-y', '-q'])\n            if rc != 0:\n                module.fail_json(msg=\"Failed to auto-install %s. Error was: '%s'\" % (apt_pkg_name, se.strip()))\n    else:\n        module.fail_json(msg=\"%s must be installed to use check mode\" % apt_pkg_name)", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\apt_repository.py", "class_name": "SourcesList", "function_name": "dump", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "chunks.append", "lines.append", "self.files.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def dump(self):\n    dumpstruct = {}\n    for filename, sources in self.files.items():\n        if sources:\n            lines = []\n            for n, valid, enabled, source, comment in sources:\n                chunks = []\n                if not enabled:\n                    chunks.append('# ')\n                chunks.append(source)\n                if comment:\n                    chunks.append(' # ')\n                    chunks.append(comment)\n                chunks.append('\\n')\n                lines.append(''.join(chunks))\n            dumpstruct[filename] = ''.join(lines)\n    return dumpstruct", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\apt_repository.py", "class_name": "SourcesList", "function_name": "add_source", "parameters": ["self", "line", "comment", "file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._add_valid_source", "self._parse", "self._suggest_filename"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_source(self, line, comment='', file=None):\n    source = self._parse(line, raise_if_invalid_or_disabled=True)[2]\n\n    # Prefer separate files for new sources.\n    self._add_valid_source(source, comment, file=file or self._suggest_filename(source))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\apt_repository.py", "class_name": "UbuntuSourcesList", "function_name": "remove_source", "parameters": ["self", "line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.startswith", "self._expand_ppa", "self._parse", "self._remove_valid_source"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_source(self, line):\n    if line.startswith('ppa:'):\n        source = self._expand_ppa(line)[0]\n    else:\n        source = self._parse(line, raise_if_invalid_or_disabled=True)[2]\n    self._remove_valid_source(source)", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\apt_repository.py", "class_name": "UbuntuSourcesList", "function_name": "repos_urls", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_repositories.append", "self._expand_ppa", "self.files.values", "source_line.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def repos_urls(self):\n    _repositories = []\n    for parsed_repos in self.files.values():\n        for parsed_repo in parsed_repos:\n            valid = parsed_repo[1]\n            enabled = parsed_repo[2]\n            source_line = parsed_repo[3]\n\n            if not valid or not enabled:\n                continue\n\n            if source_line.startswith('ppa:'):\n                source, ppa_owner, ppa_name = self._expand_ppa(source_line)\n                _repositories.append(source)\n            else:\n                _repositories.append(source_line)\n\n    return _repositories", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\async_status.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "dict", "f.read", "json.dumps", "json.loads", "module.exit_json", "module.fail_json", "open", "os.path.exists", "os.path.join", "os.unlink", "print", "sys.exit"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def main():\n\n    module = AnsibleModule(\n        argument_spec=dict(\n            jid=dict(type=\"str\", required=True),\n            mode=dict(type=\"str\", default=\"status\", choices=[\"cleanup\", \"status\"]),\n            # passed in from the async_status action plugin\n            _async_dir=dict(type=\"path\", required=True),\n        ),\n        supports_check_mode=True,\n    )\n\n    mode = module.params['mode']\n    jid = module.params['jid']\n    async_dir = module.params['_async_dir']\n\n    # setup logging directory\n    log_path = os.path.join(async_dir, jid)\n\n    if not os.path.exists(log_path):\n        module.fail_json(msg=\"could not find job\", ansible_job_id=jid, started=True, finished=True)\n\n    if mode == 'cleanup':\n        os.unlink(log_path)\n        module.exit_json(ansible_job_id=jid, erased=log_path)\n\n    # NOT in cleanup mode, assume regular status mode\n    # no remote kill mode currently exists, but probably should\n    # consider log_path + \".pid\" file and also unlink that above\n\n    data = None\n    try:\n        with open(log_path) as f:\n            data = json.loads(f.read())\n    except Exception:\n        if not data:\n            # file not written yet?  That means it is running\n            module.exit_json(results_file=log_path, ansible_job_id=jid, started=True, finished=False)\n        else:\n            module.fail_json(ansible_job_id=jid, results_file=log_path,\n                             msg=\"Could not parse job output: %s\" % data, started=True, finished=True)\n\n    if 'started' not in data:\n        data['finished'] = True\n        data['ansible_job_id'] = jid\n    elif 'finished' not in data:\n        data['finished'] = False\n\n    # just write the module output directly to stdout and exit; bypass other processing done by exit_json since it's already been done\n    print(f\"\\n{json.dumps(data)}\")  # pylint: disable=ansible-bad-function\n    sys.exit(0)  # pylint: disable=ansible-bad-function", "loc": 51}
{"file": "ansible\\lib\\ansible\\modules\\async_wrapper.py", "class_name": null, "function_name": "end", "parameters": ["res", "exit_msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "print", "sys.exit", "sys.stdout.flush"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def end(res=None, exit_msg=0):\n    if res is not None:\n        print(json.dumps(res))\n    sys.stdout.flush()\n    sys.exit(exit_msg)", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\async_wrapper.py", "class_name": null, "function_name": "jwrite", "parameters": ["info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "notice", "open", "os.rename", "tjob.close", "tjob.write"], "control_structures": ["Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def jwrite(info):\n    jobfile = job_path + \".tmp\"\n    tjob = open(jobfile, \"w\")\n    try:\n        tjob.write(json.dumps(info))\n    except OSError as ex:\n        notice(f'failed to write to {jobfile!r}: {ex}')\n        raise\n    finally:\n        tjob.close()\n        os.rename(jobfile, job_path)", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\blockinfile.py", "class_name": null, "function_name": "check_file_attrs", "parameters": ["module", "changed", "message", "diff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.load_file_common_arguments", "module.set_file_attributes_if_different"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_file_attrs(module, changed, message, diff):\n\n    file_args = module.load_file_common_arguments(module.params)\n    if module.set_file_attributes_if_different(file_args, False, diff=diff):\n\n        if changed:\n            message += \" and \"\n        changed = True\n        message += \"ownership, perms or SE linux context changed\"\n\n    return message, changed", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "is_empty", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_empty(self):\n    if len(self.lines) == 0:\n        return True\n    for line in self.lines:\n        if line.strip():\n            return False\n    return True", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "add_job", "parameters": ["self", "name", "job"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.do_comment", "self.lines.append"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_job(self, name, job):\n    # Add the comment\n    self.lines.append(self.do_comment(name))\n\n    # Add the job\n    self.lines.append(\"%s\" % (job))", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "add_env", "parameters": ["self", "decl", "insertafter", "insertbefore"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.find_env", "self.lines.insert", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_env(self, decl, insertafter=None, insertbefore=None):\n    if not (insertafter or insertbefore):\n        self.lines.insert(0, decl)\n        return\n\n    if insertafter:\n        other_name = insertafter\n    elif insertbefore:\n        other_name = insertbefore\n    other_decl = self.find_env(other_name)\n    if len(other_decl) > 0:\n        if insertafter:\n            index = other_decl[0] + 1\n        elif insertbefore:\n            index = other_decl[0]\n        self.lines.insert(index, decl)\n        return\n\n    self.module.fail_json(msg=\"Variable named '%s' not found.\" % other_name)", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "remove_job_file", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CronTabError", "os.unlink", "sys.exc_info"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_job_file(self):\n    try:\n        os.unlink(self.cron_file)\n        return True\n    except OSError:\n        # cron file does not exist\n        return False\n    except Exception:\n        raise CronTabError(\"Unexpected error:\", sys.exc_info()[0])", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "find_job", "parameters": ["self", "name", "job"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "re.match", "re.sub", "self.do_comment", "self.lines.insert"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_job(self, name, job=None):\n    # attempt to find job by 'Ansible:' header comment\n    comment = None\n    for l in self.lines:\n        if comment is not None:\n            if comment == name:\n                return [comment, l]\n            else:\n                comment = None\n        elif re.match(r'%s' % self.ansible, l):\n            comment = re.sub(r'%s' % self.ansible, '', l)\n\n    # failing that, attempt to find job by exact match\n    if job:\n        for i, l in enumerate(self.lines):\n            if l == job:\n                # if no leading ansible header, insert one\n                if not re.match(r'%s' % self.ansible, self.lines[i - 1]):\n                    self.lines.insert(i, self.do_comment(name))\n                    return [self.lines[i], l, True]\n                # if a leading blank ansible header AND job has a name, update header\n                elif name and self.lines[i - 1] == self.do_comment(None):\n                    self.lines[i - 1] = self.do_comment(name)\n                    return [self.lines[i - 1], l, True]\n\n    return []", "loc": 26}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "find_env", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_env(self, name):\n    for index, l in enumerate(self.lines):\n        if re.match(r'^%s=' % name, l):\n            return [index, l]\n\n    return []", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "get_cron_job", "parameters": ["self", "minute", "hour", "day", "month", "weekday", "job", "special", "disabled"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["job.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cron_job(self, minute, hour, day, month, weekday, job, special, disabled):\n    # normalize any leading/trailing newlines (ansible/ansible-modules-core#3791)\n    job = job.strip('\\r\\n')\n\n    if disabled:\n        disable_prefix = '#'\n    else:\n        disable_prefix = ''\n\n    if special:\n        if self.cron_file:\n            return \"%s@%s %s %s\" % (disable_prefix, special, self.user, job)\n        return \"%s@%s %s\" % (disable_prefix, special, job)\n    if self.cron_file:\n        return \"%s%s %s %s %s %s %s %s\" % (disable_prefix, minute, hour, day, month, weekday, self.user, job)\n    return \"%s%s %s %s %s %s %s\" % (disable_prefix, minute, hour, day, month, weekday, job)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "get_jobnames", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["jobnames.append", "re.match", "re.sub"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_jobnames(self):\n    jobnames = []\n\n    for l in self.lines:\n        if re.match(r'%s' % self.ansible, l):\n            jobnames.append(re.sub(r'%s' % self.ansible, '', l))\n\n    return jobnames", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "get_envnames", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["envnames.append", "l.split", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_envnames(self):\n    envnames = []\n\n    for l in self.lines:\n        if re.match(r'^\\S+=', l):\n            envnames.append(l.split('=')[0])\n\n    return envnames", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\cron.py", "class_name": "CronTab", "function_name": "render", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "crons.append", "result.rstrip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Render this crontab as it would be in the crontab.", "source_code": "def render(self):\n    \"\"\"\n    Render this crontab as it would be in the crontab.\n    \"\"\"\n    crons = []\n    for cron in self.lines:\n        crons.append(cron)\n\n    result = '\\n'.join(crons)\n    if result:\n        result = result.rstrip('\\r\\n') + '\\n'\n    return result", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\deb822_repository.py", "class_name": null, "function_name": "format_multiline", "parameters": ["v"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "line.strip", "textwrap.indent", "v.strip", "v.strip().splitlines"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_multiline(v):\n    return '\\n' + textwrap.indent(\n        '\\n'.join(line.strip() or '.' for line in v.strip().splitlines()),\n        '    '\n    )", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\deb822_repository.py", "class_name": null, "function_name": "format_field_name", "parameters": ["v"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["v.replace", "v.replace('_', '-').title"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_field_name(v):\n    if v == 'name':\n        return 'X-Repolib-Name'\n    elif v == 'uris':\n        return 'URIs'\n    return v.replace('_', '-').title()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\deb822_repository.py", "class_name": null, "function_name": "install_python_debian", "parameters": ["module", "deb_pkg_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.get_bin_path", "module.run_command", "se.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def install_python_debian(module, deb_pkg_name):\n\n    if not module.check_mode:\n        apt_path = module.get_bin_path('apt', required=True)\n        if apt_path:\n            rc, so, se = module.run_command([apt_path, 'update'])\n            if rc != 0:\n                module.fail_json(msg=f\"Failed update while auto installing {deb_pkg_name} due to '{se.strip()}'\")\n            rc, so, se = module.run_command([apt_path, 'install', deb_pkg_name, '-y', '-q'])\n            if rc != 0:\n                module.fail_json(msg=f\"Failed to auto-install {deb_pkg_name} due to : '{se.strip()}'\")\n    else:\n        module.fail_json(msg=f\"{deb_pkg_name} must be installed to use check mode\")", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\debconf.py", "class_name": null, "function_name": "get_password_value", "parameters": ["module", "pkg", "question", "vtype"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "line.startswith", "module.fail_json", "module.get_bin_path", "module.run_command", "out.split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_password_value(module, pkg, question, vtype):\n    getsel = module.get_bin_path('debconf-get-selections', True)\n    cmd = [getsel]\n    rc, out, err = module.run_command(cmd)\n    if rc != 0:\n        module.fail_json(msg=f\"Failed to get the value '{question}' from '{pkg}': {err}\")\n\n    for line in out.split(\"\\n\"):\n        if not line.startswith(pkg):\n            continue\n\n        # line is a collection of tab separated values\n        fields = line.split('\\t')\n        if len(fields) <= 3:\n            # No password found, return a blank password\n            return ''\n        try:\n            if fields[1] == question and fields[2] == vtype:\n                # If correct question and question type found, return password value\n                return fields[3]\n        except IndexError:\n            # Fail safe\n            return ''", "loc": 23}
{"file": "ansible\\lib\\ansible\\modules\\debconf.py", "class_name": null, "function_name": "get_selections", "parameters": ["module", "pkg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "key.strip", "key.strip('*').strip", "line.split", "module.fail_json", "module.get_bin_path", "module.run_command", "out.splitlines", "value.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_selections(module, pkg):\n    cmd = [module.get_bin_path('debconf-show', True), pkg]\n    rc, out, err = module.run_command(' '.join(cmd))\n\n    if rc != 0:\n        module.fail_json(msg=err)\n\n    selections = {}\n\n    for line in out.splitlines():\n        (key, value) = line.split(':', 1)\n        selections[key.strip('*').strip()] = value.strip()\n\n    return selections", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\debconf.py", "class_name": null, "function_name": "set_selection", "parameters": ["module", "pkg", "question", "vtype", "value", "unseen"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "cmd.append", "module.get_bin_path", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_selection(module, pkg, question, vtype, value, unseen):\n    setsel = module.get_bin_path('debconf-set-selections', True)\n    cmd = [setsel]\n    if unseen:\n        cmd.append('-u')\n\n    data = ' '.join([pkg, question, vtype, value])\n\n    return module.run_command(cmd, data=data)", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\debconf.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleModule", "after.update", "dict", "existing.split", "get_password_value", "get_selections", "i.strip", "isinstance", "module.exit_json", "module.fail_json", "prev.copy", "set_selection", "sorted", "to_native", "to_text", "to_text(prev[question]).lower", "to_text(value).lower"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(type='str', required=True, aliases=['pkg']),\n            question=dict(type='str', aliases=['selection', 'setting']),\n            vtype=dict(type='str', choices=['boolean', 'error', 'multiselect', 'note', 'password', 'seen', 'select', 'string', 'text', 'title']),\n            value=dict(type='raw', aliases=['answer']),\n            unseen=dict(type='bool', default=False),\n        ),\n        required_together=(['question', 'vtype', 'value'],),\n        supports_check_mode=True,\n    )\n\n    # TODO: enable passing array of options and/or debconf file from get-selections dump\n    pkg = module.params[\"name\"]\n    question = module.params[\"question\"]\n    vtype = module.params[\"vtype\"]\n    value = module.params[\"value\"]\n    unseen = module.params[\"unseen\"]\n\n    prev = get_selections(module, pkg)\n\n    changed = False\n    msg = \"\"\n\n    if question is not None:\n        if vtype is None or value is None:\n            module.fail_json(msg=\"when supplying a question you must supply a valid vtype and value\")\n\n        # ensure we compare booleans supplied to the way debconf sees them (true/false strings)\n        if vtype == 'boolean':\n            value = to_text(value).lower()\n\n        # if question doesn't exist, value cannot match\n        if question not in prev:\n            changed = True\n        else:\n            existing = prev[question]\n\n            if vtype == 'boolean':\n                existing = to_text(prev[question]).lower()\n            elif vtype == 'password':\n                existing = get_password_value(module, pkg, question, vtype)\n            elif vtype == 'multiselect' and isinstance(value, list):\n                try:\n                    value = sorted(value)\n                except TypeError as exc:\n                    module.fail_json(msg=\"Invalid value provided for 'multiselect': %s\" % to_native(exc))\n                existing = sorted([i.strip() for i in existing.split(\",\")])\n\n            if value != existing:\n                changed = True\n\n    if changed:\n        if not module.check_mode:\n            if vtype == 'multiselect' and isinstance(value, list):\n                try:\n                    value = \", \".join(value)\n                except TypeError as exc:\n                    module.fail_json(msg=\"Invalid value provided for 'multiselect': %s\" % to_native(exc))\n            rc, msg, e = set_selection(module, pkg, question, vtype, value, unseen)\n            if rc:\n                module.fail_json(msg=e)\n\n        curr = {question: value}\n        if question in prev:\n            prev = {question: prev[question]}\n        else:\n            prev[question] = ''\n\n        diff_dict = {}\n        if module._diff:\n            after = prev.copy()\n            after.update(curr)\n            diff_dict = {'before': prev, 'after': after}\n\n        module.exit_json(changed=changed, msg=msg, current=curr, previous=prev, diff=diff_dict)\n\n    module.exit_json(changed=changed, msg=msg, current=prev)", "loc": 79}
{"file": "ansible\\lib\\ansible\\modules\\dnf.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Failed to synchronize repodata: {0}'.format", "AnsibleModule", "DnfModule", "dict", "module.fail_json", "module_implementation.run", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    # state=installed name=pkgspec\n    # state=removed name=pkgspec\n    # state=latest name=pkgspec\n    #\n    # informational commands:\n    #   list=installed\n    #   list=updates\n    #   list=available\n    #   list=repos\n    #   list=pkgspec\n\n    yumdnf_argument_spec['argument_spec']['use_backend'] = dict(default='auto', choices=['auto', 'dnf', 'yum', 'yum4', 'dnf4', 'dnf5'])\n\n    module = AnsibleModule(\n        **yumdnf_argument_spec\n    )\n\n    module_implementation = DnfModule(module)\n    try:\n        module_implementation.run()\n    except dnf.exceptions.RepoError as de:\n        module.fail_json(\n            msg=\"Failed to synchronize repodata: {0}\".format(to_native(de)),\n            rc=1,\n            results=[],\n            changed=False\n        )", "loc": 28}
{"file": "ansible\\lib\\ansible\\modules\\dnf.py", "class_name": "DnfModule", "function_name": "list_items", "parameters": ["self", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dnf.subject.Subject", "dnf.subject.Subject(command).get_best_query", "getattr", "self._package_dict", "self.base.repos.iter_enabled", "self.base.sack.query", "self.module.exit_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "List package info based on the command.", "source_code": "def list_items(self, command):\n    \"\"\"List package info based on the command.\"\"\"\n    # Rename updates to upgrades\n    if command == 'updates':\n        command = 'upgrades'\n\n    # Return the corresponding packages\n    if command in ['installed', 'upgrades', 'available']:\n        results = [\n            self._package_dict(package)\n            for package in getattr(self.base.sack.query(), command)()]\n    # Return the enabled repository ids\n    elif command in ['repos', 'repositories']:\n        results = [\n            {'repoid': repo.id, 'state': 'enabled'}\n            for repo in self.base.repos.iter_enabled()]\n    # Return any matching packages\n    else:\n        packages = dnf.subject.Subject(command).get_best_query(self.base.sack)\n        results = [self._package_dict(package) for package in packages]\n\n    self.module.exit_json(msg=\"\", results=results)", "loc": 22}
{"file": "ansible\\lib\\ansible\\modules\\dnf.py", "class_name": "DnfModule", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dnf.module.module_base.ModuleBase", "dnf.util.am_i_root", "self._base", "self.base.close", "self.ensure", "self.list_items", "self.module.exit_json", "self.module.fail_json"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self):\n    if self.update_cache and not self.names and not self.list:\n        self.base = self._base(\n            self.conf_file, self.disable_gpg_check, self.disablerepo,\n            self.enablerepo, self.installroot, self.sslverify\n        )\n        self.module.exit_json(\n            msg=\"Cache updated\",\n            changed=False,\n            results=[],\n            rc=0\n        )\n\n    # Set state as installed by default\n    # This is not set in AnsibleModule() because the following shouldn't happen\n    # - dnf: autoremove=yes state=installed\n    if self.state is None:\n        self.state = 'installed'\n\n    if self.list:\n        self.base = self._base(\n            self.conf_file, self.disable_gpg_check, self.disablerepo,\n            self.enablerepo, self.installroot, self.sslverify\n        )\n        self.list_items(self.list)\n    else:\n        # Note: base takes a long time to run so we want to check for failure\n        # before running it.\n        if not self.download_only and not dnf.util.am_i_root():\n            self.module.fail_json(\n                msg=\"This command has to be run under the root user.\",\n                results=[],\n            )\n        self.base = self._base(\n            self.conf_file, self.disable_gpg_check, self.disablerepo,\n            self.enablerepo, self.installroot, self.sslverify\n        )\n\n        if self.with_modules:\n            self.module_base = dnf.module.module_base.ModuleBase(self.base)\n        try:\n            self.ensure()\n        finally:\n            self.base.close()", "loc": 44}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "get_resolve_spec_settings", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["libdnf5.base.ResolveSpecSettings", "settings.set_group_with_name", "settings.set_with_binaries", "settings.set_with_provides"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_resolve_spec_settings():\n    settings = libdnf5.base.ResolveSpecSettings()\n    try:\n        settings.set_group_with_name(True)\n        # Disable checking whether SPEC is a binary -> `/usr/(s)bin/<SPEC>`,\n        # this prevents scenarios like the following:\n        #   * the `sssd-common` package is installed and provides `/usr/sbin/sssd` binary\n        #   * the `sssd` package is NOT installed\n        #   * due to `set_with_binaries(True)` being default `is_installed(base, \"sssd\")` would \"unexpectedly\" return True\n        # If users wish to target the `sssd` binary they can by specifying the full path `name=/usr/sbin/sssd` explicitly\n        # due to settings.set_with_filenames(True) being default.\n        settings.set_with_binaries(False)\n        # Disable checking whether SPEC is provided by an installed package.\n        # Consider following real scenario from the rpmfusion repo:\n        #   * the `ffmpeg-libs` package is installed and provides `libavcodec-freeworld`\n        #   * but `libavcodec-freeworld` is NOT installed (???)\n        #   * due to `set_with_provides(True)` being default `is_installed(base, \"libavcodec-freeworld\")`\n        #     would  \"unexpectedly\" return True\n        # We disable provides only for this `is_installed` check, for actual installation we leave the default\n        # setting to mirror the dnf cmdline behavior.\n        settings.set_with_provides(False)\n    except AttributeError:\n        # dnf5 < 5.2.0.0\n        settings.group_with_name = True\n        settings.with_binaries = False\n        settings.with_provides = False\n    return settings", "loc": 27}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "is_installed", "parameters": ["base", "spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_resolve_spec_settings", "installed_query.filter_installed", "installed_query.resolve_pkg_spec", "libdnf5.rpm.PackageQuery"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_installed(base, spec):\n    settings = get_resolve_spec_settings()\n\n    installed_query = libdnf5.rpm.PackageQuery(base)\n    installed_query.filter_installed()\n    match, nevra = installed_query.resolve_pkg_spec(spec, settings, True)\n    return match", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "is_newer_version_installed", "parameters": ["base", "spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_resolve_spec_settings", "installed.filter_installed", "installed.filter_latest_evr", "installed.filter_name", "installed_package.get_evr", "libdnf5.rpm.PackageQuery", "libdnf5.rpm.PackageQuery(base).resolve_pkg_spec", "libdnf5.rpm.rpmvercmp", "list", "spec.endswith", "spec.split", "spec_nevra.get_epoch", "spec_nevra.get_name", "spec_nevra.get_release", "spec_nevra.get_version", "spec_nevra.has_just_name", "target.filter_epoch", "target.filter_latest_evr", "target.filter_name", "target.filter_release", "target.filter_version", "target_package.get_evr"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_newer_version_installed(base, spec):\n    # expects a versioned package spec\n    if \"/\" in spec:\n        spec = spec.split(\"/\")[-1]\n        if spec.endswith(\".rpm\"):\n            spec = spec[:-4]\n\n    settings = get_resolve_spec_settings()\n    match, spec_nevra = libdnf5.rpm.PackageQuery(base).resolve_pkg_spec(spec, settings, True)\n    if not match or spec_nevra.has_just_name():\n        return False\n    spec_name = spec_nevra.get_name()\n\n    installed = libdnf5.rpm.PackageQuery(base)\n    installed.filter_installed()\n    installed.filter_name([spec_name])\n    installed.filter_latest_evr()\n    try:\n        installed_package = list(installed)[-1]\n    except IndexError:\n        return False\n\n    target = libdnf5.rpm.PackageQuery(base)\n    target.filter_name([spec_name])\n    target.filter_version([spec_nevra.get_version()])\n    spec_release = spec_nevra.get_release()\n    if spec_release:\n        target.filter_release([spec_release])\n    spec_epoch = spec_nevra.get_epoch()\n    if spec_epoch:\n        target.filter_epoch([spec_epoch])\n    target.filter_latest_evr()\n    try:\n        target_package = list(target)[-1]\n    except IndexError:\n        return False\n\n    # FIXME https://github.com/rpm-software-management/dnf5/issues/1104\n    return libdnf5.rpm.rpmvercmp(installed_package.get_evr(), target_package.get_evr()) == 1", "loc": 39}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "package_to_dict", "parameters": ["package"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["package.get_arch", "package.get_epoch", "package.get_name", "package.get_nevra", "package.get_release", "package.get_repo_id", "package.get_version", "package.is_installed", "str"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def package_to_dict(package):\n    return {\n        \"nevra\": package.get_nevra(),\n        \"envra\": package.get_nevra(),  # dnf module compat\n        \"name\": package.get_name(),\n        \"arch\": package.get_arch(),\n        \"epoch\": str(package.get_epoch()),\n        \"release\": package.get_release(),\n        \"version\": package.get_version(),\n        \"repo\": package.get_repo_id(),\n        \"yumstate\": \"installed\" if package.is_installed() else \"available\",\n    }", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "get_unneeded_pkgs", "parameters": ["base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["libdnf5.rpm.PackageQuery", "query.filter_installed", "query.filter_unneeded"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_unneeded_pkgs(base):\n    query = libdnf5.rpm.PackageQuery(base)\n    query.filter_installed()\n    query.filter_unneeded()\n    yield from query", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "Dnf5Module", "Dnf5Module(module).run", "dict", "module.fail_json", "str", "yumdnf_argument_spec['argument_spec'].update"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    yumdnf_argument_spec[\"argument_spec\"].update(\n        dict(\n            auto_install_module_deps=dict(type=\"bool\", default=True),\n        )\n    )\n    module = AnsibleModule(**yumdnf_argument_spec)\n    try:\n        Dnf5Module(module).run()\n    except LIBDNF5_ERRORS as e:\n        module.fail_json(msg=str(e), failures=[], rc=1)", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\dnf5.py", "class_name": "Dnf5Module", "function_name": "fail_on_non_existing_plugins", "parameters": ["self", "base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "', '.join", "base.get_plugins_info", "msg.append", "p.get_name", "self.module.fail_json", "set", "set(self.disable_plugin).difference", "set(self.enable_plugin).difference"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fail_on_non_existing_plugins(self, base):\n    # https://github.com/rpm-software-management/dnf5/issues/1460\n    try:\n        plugin_names = [p.get_name() for p in base.get_plugins_info()]\n    except AttributeError:\n        # plugins functionality requires python3-libdnf5 5.2.0.0+\n        # silently ignore here, the module will fail later when\n        # base.enable_disable_plugins is attempted to be used if\n        # user specifies enable_plugin/disable_plugin\n        return\n\n    msg = []\n    if enable_unmatched := set(self.enable_plugin).difference(plugin_names):\n        msg.append(\n            f\"No matches were found for the following plugin name patterns while enabling libdnf5 plugins: {', '.join(enable_unmatched)}.\"\n        )\n    if disable_unmatched := set(self.disable_plugin).difference(plugin_names):\n        msg.append(\n            f\"No matches were found for the following plugin name patterns while disabling libdnf5 plugins: {', '.join(disable_unmatched)}.\"\n        )\n    if msg:\n        self.module.fail_json(msg=\" \".join(msg))", "loc": 22}
{"file": "ansible\\lib\\ansible\\modules\\dpkg_selections.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "dict", "get_best_parsable_locale", "module.exit_json", "module.fail_json", "module.get_bin_path", "module.run_command", "out.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(required=True),\n            selection=dict(choices=['install', 'hold', 'deinstall', 'purge'], required=True)\n        ),\n        supports_check_mode=True,\n    )\n\n    dpkg = module.get_bin_path('dpkg', True)\n\n    locale = get_best_parsable_locale(module)\n    DPKG_ENV = dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale, LC_CTYPE=locale, LANGUAGE=locale)\n    module.run_command_environ_update = DPKG_ENV\n\n    name = module.params['name']\n    selection = module.params['selection']\n\n    # Get current settings.\n    rc, out, err = module.run_command([dpkg, '--get-selections', name], check_rc=True)\n    if 'no packages found matching' in err:\n        module.fail_json(msg=\"Failed to find package '%s' to perform selection '%s'.\" % (name, selection))\n    elif not out:\n        current = 'not present'\n    else:\n        current = out.split()[1]\n\n    changed = current != selection\n\n    if module.check_mode or not changed:\n        module.exit_json(changed=changed, before=current, after=selection)\n\n    module.run_command([dpkg, '--set-selections'], data=\"%s %s\" % (name, selection), check_rc=True)\n    module.exit_json(changed=changed, before=current, after=selection)", "loc": 34}
{"file": "ansible\\lib\\ansible\\modules\\expect.py", "class_name": null, "function_name": "response_closure", "parameters": ["module", "question", "responses"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "next", "to_bytes", "to_bytes(r).rstrip"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def response_closure(module, question, responses):\n    resp_gen = (b'%s\\n' % to_bytes(r).rstrip(b'\\n') for r in responses)\n\n    def wrapped(info):\n        try:\n            return next(resp_gen)\n        except StopIteration:\n            module.fail_json(msg=\"No remaining responses for '%s', \"\n                                 \"output was '%s'\" %\n                                 (question,\n                                  info['child_result_list'][-1]))\n\n    return wrapped", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\expect.py", "class_name": null, "function_name": "wrapped", "parameters": ["info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "next"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(info):\n    try:\n        return next(resp_gen)\n    except StopIteration:\n        module.fail_json(msg=\"No remaining responses for '%s', \"\n                             \"output was '%s'\" %\n                             (question,\n                              info['child_result_list'][-1]))", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "get_timestamp_for_time", "parameters": ["formatted_time", "time_format"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "time.mktime", "time.strptime", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_timestamp_for_time(formatted_time, time_format):\n    if formatted_time == 'preserve':\n        return None\n    if formatted_time == 'now':\n        return Sentinel\n    try:\n        struct = time.strptime(formatted_time, time_format)\n        struct_time = time.mktime(struct)\n    except (ValueError, OverflowError) as e:\n        module.fail_json(\n            msg=f\"Error while obtaining timestamp for time {formatted_time} using format {time_format}: {to_native(e, nonstring='simplerepr')}\",\n        )\n\n    return struct_time", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "update_timestamp_for_file", "parameters": ["path", "mtime", "atime", "diff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "os.stat", "os.utime", "time.time", "to_bytes", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_timestamp_for_file(path, mtime, atime, diff=None):\n    b_path = to_bytes(path, errors='surrogate_or_strict')\n\n    try:\n        # When mtime and atime are set to 'now', rely on utime(path, None) which does not require ownership of the file\n        # https://github.com/ansible/ansible/issues/50943\n        if mtime is Sentinel and atime is Sentinel:\n            # It's not exact but we can't rely on os.stat(path).st_mtime after setting os.utime(path, None) as it may\n            # not be updated. Just use the current time for the diff values\n            mtime = atime = time.time()\n\n            previous_mtime = os.stat(b_path).st_mtime\n            previous_atime = os.stat(b_path).st_atime\n\n            set_time = None\n        else:\n            # If both parameters are None 'preserve', nothing to do\n            if mtime is None and atime is None:\n                return False\n\n            previous_mtime = os.stat(b_path).st_mtime\n            previous_atime = os.stat(b_path).st_atime\n\n            if mtime is None:\n                mtime = previous_mtime\n            elif mtime is Sentinel:\n                mtime = time.time()\n\n            if atime is None:\n                atime = previous_atime\n            elif atime is Sentinel:\n                atime = time.time()\n\n            # If both timestamps are already ok, nothing to do\n            if mtime == previous_mtime and atime == previous_atime:\n                return False\n\n            set_time = (atime, mtime)\n\n        if not module.check_mode:\n            os.utime(b_path, set_time)\n\n        if diff is not None:\n            if 'before' not in diff:\n                diff['before'] = {}\n            if 'after' not in diff:\n                diff['after'] = {}\n            if mtime != previous_mtime:\n                diff['before']['mtime'] = previous_mtime\n                diff['after']['mtime'] = mtime\n            if atime != previous_atime:\n                diff['before']['atime'] = previous_atime\n                diff['after']['atime'] = atime\n    except OSError as e:\n        module.fail_json(\n            msg=f\"Error while updating modification or access time: {to_native(e, nonstring='simplerepr')}\",\n            path=path\n        )\n    return True", "loc": 59}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "keep_backward_compatibility_on_timestamps", "parameters": ["parameter", "state"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def keep_backward_compatibility_on_timestamps(parameter, state):\n    if state in ['file', 'hard', 'directory', 'link'] and parameter is None:\n        return 'preserve'\n    if state == 'touch' and parameter is None:\n        return 'now'\n    return parameter", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "check_owner_exists", "parameters": ["module", "owner"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getpwnam", "getpwuid", "int", "module.warn"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_owner_exists(module, owner):\n    try:\n        uid = int(owner)\n        try:\n            getpwuid(uid).pw_name\n        except KeyError:\n            module.warn('failed to look up user with uid %s. Create user up to this point in real play' % uid)\n    except ValueError:\n        try:\n            getpwnam(owner).pw_uid\n        except KeyError:\n            module.warn('failed to look up user %s. Create user up to this point in real play' % owner)", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "check_group_exists", "parameters": ["module", "group"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getgrgid", "getgrnam", "int", "module.warn"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_group_exists(module, group):\n    try:\n        gid = int(group)\n        try:\n            getgrgid(gid).gr_name\n        except KeyError:\n            module.warn('failed to look up group with gid %s. Create group up to this point in real play' % gid)\n    except ValueError:\n        try:\n            getgrnam(group).gr_gid\n        except KeyError:\n            module.warn('failed to look up group %s. Create group up to this point in real play' % group)", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\file.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "additional_parameter_handling", "check_group_exists", "check_owner_exists", "dict", "ensure_absent", "ensure_directory", "ensure_file_attributes", "ensure_hardlink", "ensure_symlink", "execute_diff_peek", "execute_touch", "keep_backward_compatibility_on_timestamps", "module.exit_json", "module.fail_json", "module.load_file_common_arguments", "result.pop", "to_bytes"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n\n    global module\n\n    module = AnsibleModule(\n        argument_spec=dict(\n            state=dict(type='str', choices=['absent', 'directory', 'file', 'hard', 'link', 'touch']),\n            path=dict(type='path', required=True, aliases=['dest', 'name']),\n            _original_basename=dict(type='str'),  # Internal use only, for recursive ops\n            recurse=dict(type='bool', default=False),\n            force=dict(type='bool', default=False),  # Note: Should not be in file_common_args in future\n            follow=dict(type='bool', default=True),  # Note: Different default than file_common_args\n            _diff_peek=dict(type='bool'),  # Internal use only, for internal checks in the action plugins\n            src=dict(type='path'),  # Note: Should not be in file_common_args in future\n            modification_time=dict(type='str'),\n            modification_time_format=dict(type='str', default='%Y%m%d%H%M.%S'),\n            access_time=dict(type='str'),\n            access_time_format=dict(type='str', default='%Y%m%d%H%M.%S'),\n        ),\n        add_file_common_args=True,\n        supports_check_mode=True,\n    )\n\n    try:\n        additional_parameter_handling(module.params)\n        params = module.params\n\n        state = params['state']\n        recurse = params['recurse']\n        force = params['force']\n        follow = params['follow']\n        path = params['path']\n        src = params['src']\n\n        if module.check_mode and state != 'absent':\n            file_args = module.load_file_common_arguments(module.params)\n            if file_args['owner']:\n                check_owner_exists(module, file_args['owner'])\n            if file_args['group']:\n                check_group_exists(module, file_args['group'])\n\n        timestamps = {}\n        timestamps['modification_time'] = keep_backward_compatibility_on_timestamps(params['modification_time'], state)\n        timestamps['modification_time_format'] = params['modification_time_format']\n        timestamps['access_time'] = keep_backward_compatibility_on_timestamps(params['access_time'], state)\n        timestamps['access_time_format'] = params['access_time_format']\n\n        # short-circuit for diff_peek\n        if params['_diff_peek'] is not None:\n            appears_binary = execute_diff_peek(to_bytes(path, errors='surrogate_or_strict'))\n            module.exit_json(path=path, changed=False, appears_binary=appears_binary)\n\n        if state == 'file':\n            result = ensure_file_attributes(path, follow, timestamps)\n        elif state == 'directory':\n            result = ensure_directory(path, follow, recurse, timestamps)\n        elif state == 'link':\n            result = ensure_symlink(path, src, follow, force, timestamps)\n        elif state == 'hard':\n            result = ensure_hardlink(path, src, follow, force, timestamps)\n        elif state == 'touch':\n            result = execute_touch(path, follow, timestamps)\n        elif state == 'absent':\n            result = ensure_absent(path)\n    except AnsibleModuleError as ex:\n        module.fail_json(**ex.results)\n\n    if not module._diff:\n        result.pop('diff', None)\n\n    module.exit_json(**result)", "loc": 71}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "pfilter", "parameters": ["f", "patterns", "excludes", "use_regex"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fnmatch.fnmatch", "r.match", "re.compile"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "filter using glob patterns", "source_code": "def pfilter(f, patterns=None, excludes=None, use_regex=False):\n    \"\"\"filter using glob patterns\"\"\"\n    if not patterns and not excludes:\n        return True\n\n    if use_regex:\n        if patterns and not excludes:\n            for p in patterns:\n                r = re.compile(p)\n                if r.match(f):\n                    return True\n\n        elif patterns and excludes:\n            for p in patterns:\n                r = re.compile(p)\n                if r.match(f):\n                    for e in excludes:\n                        r = re.compile(e)\n                        if r.match(f):\n                            return False\n                    return True\n\n    else:\n        if patterns and not excludes:\n            for p in patterns:\n                if fnmatch.fnmatch(f, p):\n                    return True\n\n        elif patterns and excludes:\n            for p in patterns:\n                if fnmatch.fnmatch(f, p):\n                    for e in excludes:\n                        if fnmatch.fnmatch(f, e):\n                            return False\n                    return True\n\n    return False", "loc": 37}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "agefilter", "parameters": ["st", "now", "age", "timestamp"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["abs", "getattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "filter files older than age", "source_code": "def agefilter(st, now, age, timestamp):\n    \"\"\"filter files older than age\"\"\"\n    if age is None:\n        return True\n    elif age >= 0 and now - getattr(st, \"st_%s\" % timestamp) >= abs(age):\n        return True\n    elif age < 0 and now - getattr(st, \"st_%s\" % timestamp) <= abs(age):\n        return True\n    return False", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "sizefilter", "parameters": ["st", "size"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["abs"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "filter files greater than size", "source_code": "def sizefilter(st, size):\n    \"\"\"filter files greater than size\"\"\"\n    if size is None:\n        return True\n    elif size >= 0 and st.st_size >= abs(size):\n        return True\n    elif size < 0 and st.st_size <= abs(size):\n        return True\n    return False", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "mode_filter", "parameters": ["st", "mode", "exact", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_Object", "bool", "int", "module._symbolic_mode_to_octal", "stat.S_IMODE"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mode_filter(st, mode, exact, module):\n    if not mode:\n        return True\n\n    st_mode = stat.S_IMODE(st.st_mode)\n\n    try:\n        mode = int(mode, 8)\n    except ValueError:\n        mode = module._symbolic_mode_to_octal(_Object(st_mode=0), mode)\n\n    mode = stat.S_IMODE(mode)\n\n    if exact:\n        return st_mode == mode\n\n    return bool(st_mode & mode)", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "statinfo", "parameters": ["st"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "grp.getgrgid", "pwd.getpwuid", "stat.S_IMODE", "stat.S_ISBLK", "stat.S_ISCHR", "stat.S_ISDIR", "stat.S_ISFIFO", "stat.S_ISLNK", "stat.S_ISREG", "stat.S_ISSOCK"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def statinfo(st):\n    pw_name = \"\"\n    gr_name = \"\"\n\n    try:  # user data\n        pw_name = pwd.getpwuid(st.st_uid).pw_name\n    except Exception:\n        pass\n\n    try:  # group data\n        gr_name = grp.getgrgid(st.st_gid).gr_name\n    except Exception:\n        pass\n\n    return {\n        'mode': \"%04o\" % stat.S_IMODE(st.st_mode),\n        'isdir': stat.S_ISDIR(st.st_mode),\n        'ischr': stat.S_ISCHR(st.st_mode),\n        'isblk': stat.S_ISBLK(st.st_mode),\n        'isreg': stat.S_ISREG(st.st_mode),\n        'isfifo': stat.S_ISFIFO(st.st_mode),\n        'islnk': stat.S_ISLNK(st.st_mode),\n        'issock': stat.S_ISSOCK(st.st_mode),\n        'uid': st.st_uid,\n        'gid': st.st_gid,\n        'size': st.st_size,\n        'inode': st.st_ino,\n        'dev': st.st_dev,\n        'nlink': st.st_nlink,\n        'atime': st.st_atime,\n        'mtime': st.st_mtime,\n        'ctime': st.st_ctime,\n        'gr_name': gr_name,\n        'pw_name': pw_name,\n        'wusr': bool(st.st_mode & stat.S_IWUSR),\n        'rusr': bool(st.st_mode & stat.S_IRUSR),\n        'xusr': bool(st.st_mode & stat.S_IXUSR),\n        'wgrp': bool(st.st_mode & stat.S_IWGRP),\n        'rgrp': bool(st.st_mode & stat.S_IRGRP),\n        'xgrp': bool(st.st_mode & stat.S_IXGRP),\n        'woth': bool(st.st_mode & stat.S_IWOTH),\n        'roth': bool(st.st_mode & stat.S_IROTH),\n        'xoth': bool(st.st_mode & stat.S_IXOTH),\n        'isuid': bool(st.st_mode & stat.S_ISUID),\n        'isgid': bool(st.st_mode & stat.S_ISGID),\n    }", "loc": 46}
{"file": "ansible\\lib\\ansible\\modules\\find.py", "class_name": null, "function_name": "handle_walk_errors", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_walk_errors(e):\n    if e.errno in (errno.EPERM, errno.EACCES, errno.ENOENT):\n        skipped[e.filename] = to_text(e)\n        return\n    raise e", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\getent.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "cmd.extend", "dict", "line.split", "module.exit_json", "module.fail_json", "module.get_bin_path", "module.params.get", "module.run_command", "out.splitlines", "results[dbtree][record[0]].append", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            database=dict(type='str', required=True),\n            key=dict(type='str', no_log=False),\n            service=dict(type='str'),\n            split=dict(type='str'),\n            fail_key=dict(type='bool', default=True),\n        ),\n        supports_check_mode=True,\n    )\n\n    colon = ['passwd', 'shadow', 'group', 'gshadow']\n\n    database = module.params['database']\n    key = module.params.get('key')\n    split = module.params.get('split')\n    service = module.params.get('service')\n    fail_key = module.params.get('fail_key')\n\n    getent_bin = module.get_bin_path('getent', True)\n\n    if key is not None:\n        cmd = [getent_bin, database, key]\n    else:\n        cmd = [getent_bin, database]\n\n    if service is not None:\n        cmd.extend(['-s', service])\n\n    if split is None and database in colon:\n        split = ':'\n\n    try:\n        rc, out, err = module.run_command(cmd)\n    except Exception as e:\n        module.fail_json(msg=to_native(e))\n\n    msg = \"Unexpected failure!\"\n    dbtree = 'getent_%s' % database\n    results = {dbtree: {}}\n\n    if rc == 0:\n        seen = {}\n        for line in out.splitlines():\n            record = line.split(split)\n\n            if record[0] in seen:\n                # more than one result for same key, ensure we store in a list\n                if seen[record[0]] == 1:\n                    results[dbtree][record[0]] = [results[dbtree][record[0]]]\n\n                results[dbtree][record[0]].append(record[1:])\n                seen[record[0]] += 1\n            else:\n                # new key/value, just assign\n                results[dbtree][record[0]] = record[1:]\n                seen[record[0]] = 1\n\n        module.exit_json(ansible_facts=results)\n\n    elif rc == 1:\n        msg = \"Missing arguments, or database unknown.\"\n    elif rc == 2:\n        msg = \"One or more supplied key could not be found in the database.\"\n        if not fail_key:\n            results[dbtree][key] = None\n            module.exit_json(ansible_facts=results, msg=msg)\n    elif rc == 3:\n        msg = \"Enumeration not supported on this database.\"\n\n    module.fail_json(msg=msg)", "loc": 72}
{"file": "ansible\\lib\\ansible\\modules\\get_url.py", "class_name": null, "function_name": "parse_digest_lines", "parameters": ["filename", "lines"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BSD_DIGEST_LINE.match", "GNU_DIGEST_LINE.match", "checksum_map.append", "len", "lines[0].split", "match.group", "match.group('path').lstrip", "re.compile"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_digest_lines(filename, lines):\n    \"\"\"Returns a list of tuple containing the filename and digest depending upon\n      the lines provided\n\n    Args:\n        filename (str): Name of the filename, used only when the digest is one-liner\n        lines (list): A list of lines containing filenames and checksums\n    \"\"\"\n    checksum_map = []\n    BSD_DIGEST_LINE = re.compile(r'^(\\w+) ?\\((?P<path>.+)\\) ?= (?P<digest>[\\w.]+)$')\n    GNU_DIGEST_LINE = re.compile(r'^(?P<digest>[\\w.]+) ([ *])(?P<path>.+)$')\n\n    if len(lines) == 1 and len(lines[0].split()) == 1:\n        # Only a single line with a single string\n        # treat it as a checksum only file\n        checksum_map.append((lines[0], filename))\n        return checksum_map\n    # The assumption here is the file is in the format of\n    # checksum filename\n    for line in lines:\n        match = BSD_DIGEST_LINE.match(line)\n        if match:\n            checksum_map.append((match.group('digest'), match.group('path')))\n        else:\n            match = GNU_DIGEST_LINE.match(line)\n            if match:\n                checksum_map.append((match.group('digest'), match.group('path').lstrip(\"./\")))\n\n    return checksum_map", "loc": 29}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_submodule_update_params", "parameters": ["module", "git_path", "cwd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command", "params.append", "part.replace", "part.startswith", "shlex.split", "stderr.split", "update_line.replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_submodule_update_params(module, git_path, cwd):\n    # or: git submodule [--quiet] update [--init] [-N|--no-fetch]\n    # [-f|--force] [--rebase] [--reference <repository>] [--merge]\n    # [--recursive] [--] [<path>...]\n\n    params = []\n\n    # run a bad submodule command to get valid params\n    cmd = \"%s submodule update --help\" % (git_path)\n    rc, stdout, stderr = module.run_command(cmd, cwd=cwd)\n    lines = stderr.split('\\n')\n    update_line = None\n    for line in lines:\n        if 'git submodule [--quiet] update ' in line:\n            update_line = line\n    if update_line:\n        update_line = update_line.replace('[', '')\n        update_line = update_line.replace(']', '')\n        update_line = update_line.replace('|', ' ')\n        parts = shlex.split(update_line)\n        for part in parts:\n            if part.startswith('--'):\n                part = part.replace('--', '')\n                params.append(part)\n\n    return params", "loc": 26}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "set_git_ssh_env", "parameters": ["key_file", "ssh_opts", "git_version", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LooseVersion", "os.environ.get", "write_ssh_wrapper"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "use environment variables to configure git's ssh execution, which varies by version but this function should handle all.", "source_code": "def set_git_ssh_env(key_file, ssh_opts, git_version, module):\n    \"\"\"\n        use environment variables to configure git's ssh execution,\n        which varies by version but this function should handle all.\n    \"\"\"\n\n    # initialise to existing ssh opts and/or append user provided\n    if ssh_opts is None:\n        ssh_opts = os.environ.get('GIT_SSH_OPTS', '')\n    else:\n        ssh_opts = os.environ.get('GIT_SSH_OPTS', '') + ' ' + ssh_opts\n\n    # hostkey acceptance\n    accept_key = \"StrictHostKeyChecking=no\"\n    if module.params['accept_hostkey'] and accept_key not in ssh_opts:\n        ssh_opts += \" -o %s\" % accept_key\n\n    # avoid prompts\n    force_batch = 'BatchMode=yes'\n    if force_batch not in ssh_opts:\n        ssh_opts += ' -o %s' % (force_batch)\n\n    # deal with key file\n    if key_file:\n        key_opt = '-i %s' % key_file\n        if key_opt not in ssh_opts:\n            ssh_opts += '  %s' % key_opt\n\n        ikey = 'IdentitiesOnly=yes'\n        if ikey not in ssh_opts:\n            ssh_opts += ' -o %s' % ikey\n\n    # older than 2.3 does not know how to use git_ssh_command,\n    # so we force it into get_ssh var\n    # https://github.com/gitster/git/commit/09d60d785c68c8fa65094ecbe46fbc2a38d0fc1f\n    if git_version is not None and git_version < LooseVersion('2.3.0'):\n        # for use in wrapper\n        os.environ[\"GIT_SSH_OPTS\"] = ssh_opts\n\n        # these versions don't support GIT_SSH_OPTS so have to write wrapper\n        wrapper = write_ssh_wrapper(module)\n\n        # force use of git_ssh_opts via wrapper, git_ssh cannot not handle arguments\n        os.environ['GIT_SSH'] = wrapper\n    else:\n        # we construct full finalized command string here\n        full_cmd = os.environ.get('GIT_SSH', os.environ.get('GIT_SSH_COMMAND', 'ssh'))\n        if ssh_opts:\n            full_cmd += ' ' + ssh_opts\n        # git_ssh_command can handle arguments to ssh\n        os.environ[\"GIT_SSH_COMMAND\"] = full_cmd", "loc": 51}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_version", "parameters": ["module", "git_path", "dest", "ref"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command", "to_native", "to_native(stdout).rstrip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "samples the version of the git repo", "source_code": "def get_version(module, git_path, dest, ref=\"HEAD\"):\n    \"\"\" samples the version of the git repo \"\"\"\n\n    cmd = \"%s rev-parse %s\" % (git_path, ref)\n    rc, stdout, stderr = module.run_command(cmd, cwd=dest)\n    sha = to_native(stdout).rstrip('\\n')\n    return sha", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "ssh_supports_acceptnewhostkey", "parameters": ["module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_bin_path", "module.fail_json", "module.run_command", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ssh_supports_acceptnewhostkey(module):\n    try:\n        ssh_path = get_bin_path('ssh')\n    except ValueError as err:\n        module.fail_json(\n            msg='Remote host is missing ssh command, so you cannot '\n            'use acceptnewhostkey option.', details=to_text(err))\n    supports_acceptnewhostkey = True\n    cmd = [ssh_path, '-o', 'StrictHostKeyChecking=accept-new', '-V']\n    rc, stdout, stderr = module.run_command(cmd)\n    if rc != 0:\n        supports_acceptnewhostkey = False\n    return supports_acceptnewhostkey", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_submodule_versions", "parameters": ["git_path", "module", "dest", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.startswith", "line.strip", "module.fail_json", "module.run_command", "out.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_submodule_versions(git_path, module, dest, version='HEAD'):\n    cmd = [git_path, 'submodule', 'foreach', git_path, 'rev-parse', version]\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(\n            msg='Unable to determine hashes of submodules',\n            stdout=out,\n            stderr=err,\n            rc=rc)\n    submodules = {}\n    subm_name = None\n    for line in out.splitlines():\n        if line.startswith(\"Entering '\"):\n            subm_name = line[10:-1]\n        elif len(line.strip()) == 40:\n            if subm_name is None:\n                module.fail_json()\n            submodules[subm_name] = line.strip()\n            subm_name = None\n        else:\n            module.fail_json(msg='Unable to parse submodule hash line: %s' % line.strip())\n    if subm_name is not None:\n        module.fail_json(msg='Unable to find hash for submodule: %s' % subm_name)\n\n    return submodules", "loc": 25}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "has_local_mods", "parameters": ["module", "git_path", "dest", "bare"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter", "len", "list", "module.run_command", "re.search", "stdout.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def has_local_mods(module, git_path, dest, bare):\n    if bare:\n        return False\n\n    cmd = \"%s status --porcelain\" % (git_path)\n    rc, stdout, stderr = module.run_command(cmd, cwd=dest)\n    lines = stdout.splitlines()\n    lines = list(filter(lambda c: not re.search('^\\\\?\\\\?.*$', c), lines))\n\n    return len(lines) > 0", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_diff", "parameters": ["module", "git_path", "dest", "repo", "remote", "depth", "bare", "before", "after"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fetch", "git_version", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the difference between 2 versions", "source_code": "def get_diff(module, git_path, dest, repo, remote, depth, bare, before, after):\n    \"\"\" Return the difference between 2 versions \"\"\"\n    if before is None:\n        return {'prepared': '>> Newly checked out %s' % after}\n    elif before != after:\n        # Ensure we have the object we are referring to during git diff !\n        git_version_used = git_version(git_path, module)\n        fetch(git_path, module, repo, dest, after, remote, depth, bare, '', git_version_used)\n        cmd = '%s diff %s %s' % (git_path, before, after)\n        (rc, out, err) = module.run_command(cmd, cwd=dest)\n        if rc == 0 and out:\n            return {'prepared': out}\n        elif rc == 0:\n            return {'prepared': '>> No visual differences between %s and %s' % (before, after)}\n        elif err:\n            return {'prepared': '>> Failed to get proper diff between %s and %s:\\n>> %s' % (before, after, err)}\n        else:\n            return {'prepared': '>> Failed to get proper diff between %s and %s' % (before, after)}\n    return {}", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "is_remote_tag", "parameters": ["git_path", "module", "dest", "remote", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_remote_tag(git_path, module, dest, remote, version):\n    cmd = '%s ls-remote %s -t refs/tags/%s' % (git_path, remote, version)\n    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)\n    if to_native(version, errors='surrogate_or_strict') in out:\n        return True\n    else:\n        return False", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_branches", "parameters": ["git_path", "module", "dest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["branches.append", "line.strip", "module.fail_json", "module.run_command", "out.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_branches(git_path, module, dest):\n    branches = []\n    cmd = '%s branch --no-color -a' % (git_path,)\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(msg=\"Could not determine branch data - received %s\" % out, stdout=out, stderr=err)\n    for line in out.split('\\n'):\n        if line.strip():\n            branches.append(line.strip())\n    return branches", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_annotated_tags", "parameters": ["git_path", "module", "dest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.strip", "line.strip().split", "module.fail_json", "module.run_command", "tags.append", "to_native", "to_native(out).split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_annotated_tags(git_path, module, dest):\n    tags = []\n    cmd = [git_path, 'for-each-ref', 'refs/tags/', '--format', '%(objecttype):%(refname:short)']\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(msg=\"Could not determine tag data - received %s\" % out, stdout=out, stderr=err)\n    for line in to_native(out).split('\\n'):\n        if line.strip():\n            tagtype, tagname = line.strip().split(':')\n            if tagtype == 'tag':\n                tags.append(tagname)\n    return tags", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "is_remote_branch", "parameters": ["git_path", "module", "dest", "remote", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_remote_branch(git_path, module, dest, remote, version):\n    cmd = '%s ls-remote %s -h refs/heads/%s' % (git_path, remote, version)\n    (rc, out, err) = module.run_command(cmd, check_rc=True, cwd=dest)\n    if to_native(version, errors='surrogate_or_strict') in out:\n        return True\n    else:\n        return False", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "is_local_branch", "parameters": ["git_path", "module", "dest", "branch"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_branches"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_local_branch(git_path, module, dest, branch):\n    branches = get_branches(git_path, module, dest)\n    lbranch = '%s' % branch\n    if lbranch in branches:\n        return True\n    elif '* %s' % branch in branches:\n        return True\n    else:\n        return False", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "is_not_a_branch", "parameters": ["git_path", "module", "dest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["branch.startswith", "get_branches"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_not_a_branch(git_path, module, dest):\n    branches = get_branches(git_path, module, dest)\n    for branch in branches:\n        if branch.startswith('* ') and ('no branch' in branch or 'detached from' in branch or 'detached at' in branch):\n            return True\n    return False", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_remote_url", "parameters": ["git_path", "module", "dest", "remote"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command", "to_native", "to_native(out).rstrip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return URL of remote source for repo.", "source_code": "def get_remote_url(git_path, module, dest, remote):\n    \"\"\"Return URL of remote source for repo.\"\"\"\n    command = [git_path, 'ls-remote', '--get-url', remote]\n    (rc, out, err) = module.run_command(command, cwd=dest)\n    if rc != 0:\n        # There was an issue getting remote URL, most likely\n        # command is not available in this version of Git.\n        return None\n    return to_native(out).rstrip('\\n')", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "set_remote_url", "parameters": ["git_path", "module", "repo", "dest", "remote"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_remote_url", "module.fail_json", "module.run_command", "unfrackgitpath"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "updates repo from remote sources", "source_code": "def set_remote_url(git_path, module, repo, dest, remote):\n    \"\"\" updates repo from remote sources \"\"\"\n    # Return if remote URL isn't changing.\n    remote_url = get_remote_url(git_path, module, dest, remote)\n    if remote_url == repo or unfrackgitpath(remote_url) == unfrackgitpath(repo):\n        return False\n\n    command = [git_path, 'remote', 'set-url', remote, repo]\n    (rc, out, err) = module.run_command(command, cwd=dest)\n    if rc != 0:\n        label = \"set a new url %s for %s\" % (repo, remote)\n        module.fail_json(msg=\"Failed to %s: %s %s\" % (label, out, err))\n\n    # Return False if remote_url is None to maintain previous behavior\n    # for Git versions prior to 1.7.5 that lack required functionality.\n    return remote_url is not None", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "fetch", "parameters": ["git_path", "module", "repo", "dest", "version", "remote", "depth", "bare", "refspec", "git_version_used", "force"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LooseVersion", "commands.append", "fetch_cmd.append", "fetch_cmd.extend", "get_head_branch", "is_remote_branch", "is_remote_tag", "module.fail_json", "module.run_command", "refspecs.append", "set_remote_url", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "updates repo from remote sources", "source_code": "def fetch(git_path, module, repo, dest, version, remote, depth, bare, refspec, git_version_used, force=False):\n    \"\"\" updates repo from remote sources \"\"\"\n    set_remote_url(git_path, module, repo, dest, remote)\n    commands = []\n\n    fetch_str = 'download remote objects and refs'\n    fetch_cmd = [git_path, 'fetch']\n\n    refspecs = []\n    if depth:\n        # try to find the minimal set of refs we need to fetch to get a\n        # successful checkout\n        currenthead = get_head_branch(git_path, module, dest, remote)\n        if refspec:\n            refspecs.append(refspec)\n        elif version == 'HEAD':\n            refspecs.append(currenthead)\n        elif is_remote_branch(git_path, module, dest, repo, version):\n            if currenthead != version:\n                # this workaround is only needed for older git versions\n                # 1.8.3 is broken, 1.9.x works\n                # ensure that remote branch is available as both local and remote ref\n                refspecs.append('+refs/heads/%s:refs/heads/%s' % (version, version))\n            refspecs.append('+refs/heads/%s:refs/remotes/%s/%s' % (version, remote, version))\n        elif is_remote_tag(git_path, module, dest, repo, version):\n            refspecs.append('+refs/tags/' + version + ':refs/tags/' + version)\n        if refspecs:\n            # if refspecs is empty, i.e. version is neither heads nor tags\n            # assume it is a version hash\n            # fall back to a full clone, otherwise we might not be able to checkout\n            # version\n            fetch_cmd.extend(['--depth', str(depth)])\n\n    if not depth or not refspecs:\n        # don't try to be minimalistic but do a full clone\n        # also do this if depth is given, but version is something that can't be fetched directly\n        if bare:\n            refspecs = ['+refs/heads/*:refs/heads/*', '+refs/tags/*:refs/tags/*']\n        else:\n            # ensure all tags are fetched\n            if git_version_used is not None and git_version_used >= LooseVersion('1.9'):\n                fetch_cmd.append('--tags')\n            else:\n                # old git versions have a bug in --tags that prevents updating existing tags\n                commands.append((fetch_str, fetch_cmd + [remote]))\n                refspecs = ['+refs/tags/*:refs/tags/*']\n        if refspec:\n            refspecs.append(refspec)\n\n    if force:\n        fetch_cmd.append('--force')\n\n    fetch_cmd.extend([remote])\n\n    commands.append((fetch_str, fetch_cmd + refspecs))\n\n    for (label, command) in commands:\n        (rc, out, err) = module.run_command(command, cwd=dest)\n        if rc != 0:\n            module.fail_json(msg=\"Failed to %s: %s %s\" % (label, out, err), cmd=command)", "loc": 60}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "set_remote_branch", "parameters": ["git_path", "module", "dest", "remote", "version", "depth"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "set refs for the remote branch version This assumes the branch does not yet exist locally and is therefore also not checked out. Can't use git remote set-branches, as it is not available in git 1.7.1 (centos6)", "source_code": "def set_remote_branch(git_path, module, dest, remote, version, depth):\n    \"\"\"set refs for the remote branch version\n\n    This assumes the branch does not yet exist locally and is therefore also not checked out.\n    Can't use git remote set-branches, as it is not available in git 1.7.1 (centos6)\n    \"\"\"\n\n    branchref = \"+refs/heads/%s:refs/heads/%s\" % (version, version)\n    branchref += ' +refs/heads/%s:refs/remotes/%s/%s' % (version, remote, version)\n    cmd = \"%s fetch --depth=%s %s %s\" % (git_path, depth, remote, branchref)\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(msg=\"Failed to fetch branch from remote: %s\" % version, stdout=out, stderr=err, rc=rc)", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "switch_version", "parameters": ["git_path", "module", "dest", "remote", "version", "verify_commit", "depth", "gpg_allowlist"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_head_branch", "is_local_branch", "is_remote_branch", "module.fail_json", "module.run_command", "set_remote_branch", "verify_commit_sign"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def switch_version(git_path, module, dest, remote, version, verify_commit, depth, gpg_allowlist):\n    cmd = ''\n    if version == 'HEAD':\n        branch = get_head_branch(git_path, module, dest, remote)\n        (rc, out, err) = module.run_command(\"%s checkout --force %s\" % (git_path, branch), cwd=dest)\n        if rc != 0:\n            module.fail_json(msg=\"Failed to checkout branch %s\" % branch,\n                             stdout=out, stderr=err, rc=rc)\n        cmd = \"%s reset --hard %s/%s --\" % (git_path, remote, branch)\n    else:\n        # FIXME check for local_branch first, should have been fetched already\n        if is_remote_branch(git_path, module, dest, remote, version):\n            if depth and not is_local_branch(git_path, module, dest, version):\n                # git clone --depth implies --single-branch, which makes\n                # the checkout fail if the version changes\n                # fetch the remote branch, to be able to check it out next\n                set_remote_branch(git_path, module, dest, remote, version, depth)\n            if not is_local_branch(git_path, module, dest, version):\n                cmd = \"%s checkout --track -b %s %s/%s\" % (git_path, version, remote, version)\n            else:\n                (rc, out, err) = module.run_command(\"%s checkout --force %s\" % (git_path, version), cwd=dest)\n                if rc != 0:\n                    module.fail_json(msg=\"Failed to checkout branch %s\" % version, stdout=out, stderr=err, rc=rc)\n                cmd = \"%s reset --hard %s/%s\" % (git_path, remote, version)\n        else:\n            cmd = \"%s checkout --force %s\" % (git_path, version)\n    (rc, out1, err1) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        if version != 'HEAD':\n            module.fail_json(msg=\"Failed to checkout %s\" % (version),\n                             stdout=out1, stderr=err1, rc=rc, cmd=cmd)\n        else:\n            module.fail_json(msg=\"Failed to checkout branch %s\" % (branch),\n                             stdout=out1, stderr=err1, rc=rc, cmd=cmd)\n\n    if verify_commit:\n        verify_commit_sign(git_path, module, dest, version, gpg_allowlist)\n\n    return (rc, out1, err1)", "loc": 39}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "verify_commit_sign", "parameters": ["git_path", "module", "dest", "version", "gpg_allowlist"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_annotated_tags", "get_gpg_fingerprint", "module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def verify_commit_sign(git_path, module, dest, version, gpg_allowlist):\n    if version in get_annotated_tags(git_path, module, dest):\n        git_sub = \"verify-tag\"\n    else:\n        git_sub = \"verify-commit\"\n    cmd = \"%s %s %s\" % (git_path, git_sub, version)\n    if gpg_allowlist:\n        cmd += \" --raw\"\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(msg='Failed to verify GPG signature of commit/tag \"%s\"' % version, stdout=out, stderr=err, rc=rc)\n    if gpg_allowlist:\n        fingerprint = get_gpg_fingerprint(err)\n        if fingerprint not in gpg_allowlist:\n            module.fail_json(msg='The gpg_allowlist does not include the public key \"%s\" for this commit' % fingerprint, stdout=out, stderr=err, rc=rc)\n    return (rc, out, err)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "get_gpg_fingerprint", "parameters": ["output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "output.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a fingerprint of the primary key. Ref: https://git.gnupg.org/cgi-bin/gitweb.cgi?p=gnupg.git;a=blob;f=doc/DETAILS;hb=HEAD#l482", "source_code": "def get_gpg_fingerprint(output):\n    \"\"\"Return a fingerprint of the primary key.\n\n    Ref:\n    https://git.gnupg.org/cgi-bin/gitweb.cgi?p=gnupg.git;a=blob;f=doc/DETAILS;hb=HEAD#l482\n    \"\"\"\n    for line in output.splitlines():\n        data = line.split()\n        if data[1] != 'VALIDSIG':\n            continue\n\n        # if signed with a subkey, this contains the primary key fingerprint\n        data_id = 11 if len(data) == 11 else 2\n        return data[data_id]", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "git_version", "parameters": ["git_path", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LooseVersion", "module.run_command", "re.search", "rematch.groups", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return the installed version of git", "source_code": "def git_version(git_path, module):\n    \"\"\"return the installed version of git\"\"\"\n    cmd = \"%s --version\" % git_path\n    (rc, out, err) = module.run_command(cmd)\n    if rc != 0:\n        # one could fail_json here, but the version info is not that important,\n        # so let's try to fail only on actual git commands\n        return None\n    rematch = re.search('git version (.*)$', to_native(out))\n    if not rematch:\n        return None\n    return LooseVersion(rematch.groups()[0])", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\git.py", "class_name": null, "function_name": "git_archive", "parameters": ["git_path", "module", "dest", "archive", "archive_fmt", "archive_prefix", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.insert", "module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Create git archive in given source directory", "source_code": "def git_archive(git_path, module, dest, archive, archive_fmt, archive_prefix, version):\n    \"\"\" Create git archive in given source directory \"\"\"\n    cmd = [git_path, 'archive', '--format', archive_fmt, '--output', archive, version]\n    if archive_prefix is not None:\n        cmd.insert(-1, '--prefix')\n        cmd.insert(-1, archive_prefix)\n    (rc, out, err) = module.run_command(cmd, cwd=dest)\n    if rc != 0:\n        module.fail_json(msg=\"Failed to perform archive operation\",\n                         details=\"Git archive command failed to create \"\n                                 \"archive %s using %s directory.\"\n                                 \"Error: %s\" % (archive, dest, err))\n    return rc, out, err", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "Group", "dict", "group.group_add", "group.group_del", "group.group_exists", "group.group_info", "group.group_mod", "module.debug", "module.exit_json", "module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            state=dict(type='str', default='present', choices=['absent', 'present']),\n            name=dict(type='str', required=True),\n            force=dict(type='bool', default=False),\n            gid=dict(type='int'),\n            system=dict(type='bool', default=False),\n            local=dict(type='bool', default=False),\n            non_unique=dict(type='bool', default=False),\n            gid_min=dict(type='int'),\n            gid_max=dict(type='int'),\n        ),\n        supports_check_mode=True,\n        required_if=[\n            ['non_unique', True, ['gid']],\n        ],\n    )\n\n    if module.params['force'] and module.params['local']:\n        module.fail_json(msg='force is not a valid option for local, force=True and local=True are mutually exclusive')\n\n    group = Group(module)\n\n    module.debug('Group instantiated - platform %s' % group.platform)\n    if group.distribution:\n        module.debug('Group instantiated - distribution %s' % group.distribution)\n\n    rc = None\n    out = ''\n    err = ''\n    result = {}\n    result['name'] = group.name\n    result['state'] = group.state\n\n    if group.state == 'absent':\n\n        if group.group_exists():\n            if module.check_mode:\n                module.exit_json(changed=True)\n            (rc, out, err) = group.group_del()\n            if rc != 0:\n                module.fail_json(name=group.name, msg=err)\n\n    elif group.state == 'present':\n\n        if not group.group_exists():\n            if module.check_mode:\n                module.exit_json(changed=True)\n            (rc, out, err) = group.group_add(gid=group.gid, system=group.system)\n        else:\n            (rc, out, err) = group.group_mod(gid=group.gid)\n\n        if rc is not None and rc != 0:\n            module.fail_json(name=group.name, msg=err)\n\n    if rc is None:\n        result['changed'] = False\n    else:\n        result['changed'] = True\n    if out:\n        result['stdout'] = out\n    if err:\n        result['stderr'] = err\n\n    if group.group_exists():\n        info = group.group_info()\n        result['system'] = group.system\n        result['gid'] = info[2]\n\n    module.exit_json(**result)", "loc": 71}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "Group", "function_name": "group_del", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_del(self):\n    if self.local:\n        command_name = 'lgroupdel'\n    else:\n        command_name = 'groupdel'\n    cmd = [self.module.get_bin_path(command_name, True), self.name]\n    return self.execute_command(cmd)", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "Group", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self._local_check_gid_exists", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    if self.local:\n        command_name = 'lgroupadd'\n        self._local_check_gid_exists()\n    else:\n        command_name = 'groupadd'\n    cmd = [self.module.get_bin_path(command_name, True)]\n    for key in kwargs:\n        if key == 'gid' and kwargs[key] is not None:\n            cmd.append('-g')\n            cmd.append(str(kwargs[key]))\n            if self.non_unique:\n                cmd.append('-o')\n        elif key == 'system' and kwargs[key] is True:\n            cmd.append('-r')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 23}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "Group", "function_name": "group_mod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "int", "len", "self._local_check_gid_exists", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, **kwargs):\n    if self.local:\n        command_name = 'lgroupmod'\n        self._local_check_gid_exists()\n    else:\n        command_name = 'groupmod'\n    cmd = [self.module.get_bin_path(command_name, True)]\n    info = self.group_info()\n    for key in kwargs:\n        if key == 'gid':\n            if kwargs[key] is not None and info[2] != int(kwargs[key]):\n                cmd.append('-g')\n                cmd.append(str(kwargs[key]))\n                if self.non_unique:\n                    cmd.append('-o')\n    if len(cmd) == 1:\n        return (None, '', '')\n    if self.module.check_mode:\n        return (0, '', '')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 21}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "Group", "function_name": "group_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["grp.getgrnam", "list", "self.group_exists"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_info(self):\n    if not self.group_exists():\n        return False\n    try:\n        info = list(grp.getgrnam(self.name))\n    except KeyError:\n        return False\n    return info", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "Linux", "function_name": "group_del", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_del(self):\n    if self.local:\n        command_name = 'lgroupdel'\n    else:\n        command_name = 'groupdel'\n    cmd = [self.module.get_bin_path(command_name, True)]\n    if self.force:\n        cmd.append('-f')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "SunOS", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('groupadd', True)]\n    for key in kwargs:\n        if key == 'gid' and kwargs[key] is not None:\n            cmd.append('-g')\n            cmd.append(str(kwargs[key]))\n            if self.non_unique:\n                cmd.append('-o')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "AIX", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('mkgroup', True)]\n    for key in kwargs:\n        if key == 'gid' and kwargs[key] is not None:\n            cmd.append('id=' + str(kwargs[key]))\n        elif key == 'system' and kwargs[key] is True:\n            cmd.append('-a')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "AIX", "function_name": "group_mod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "int", "len", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, **kwargs):\n    cmd = [self.module.get_bin_path('chgroup', True)]\n    info = self.group_info()\n    for key in kwargs:\n        if key == 'gid':\n            if kwargs[key] is not None and info[2] != int(kwargs[key]):\n                cmd.append('id=' + str(kwargs[key]))\n    if len(cmd) == 1:\n        return (None, '', '')\n    if self.module.check_mode:\n        return (0, '', '')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "FreeBsdGroup", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('pw', True), 'groupadd', self.name]\n    if self.gid is not None:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    return self.execute_command(cmd)", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "FreeBsdGroup", "function_name": "group_mod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "int", "len", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, **kwargs):\n    cmd = [self.module.get_bin_path('pw', True), 'groupmod', self.name]\n    info = self.group_info()\n    cmd_len = len(cmd)\n    if self.gid is not None and int(self.gid) != info[2]:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    # modify the group if cmd will do anything\n    if cmd_len != len(cmd):\n        if self.module.check_mode:\n            return (0, '', '')\n        return self.execute_command(cmd)\n    return (None, '', '')", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "DarwinGroup", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.get_lowest_available_system_gid", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('dseditgroup', True)]\n    cmd += ['-o', 'create']\n    if self.gid is not None:\n        cmd += ['-i', str(self.gid)]\n    elif 'system' in kwargs and kwargs['system'] is True:\n        gid = self.get_lowest_available_system_gid()\n        if gid is not False:\n            self.gid = str(gid)\n            cmd += ['-i', str(self.gid)]\n    cmd += ['-L', self.name]\n    (rc, out, err) = self.execute_command(cmd)\n    return (rc, out, err)", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "DarwinGroup", "function_name": "group_del", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.module.get_bin_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_del(self):\n    cmd = [self.module.get_bin_path('dseditgroup', True)]\n    cmd += ['-o', 'delete']\n    cmd += ['-L', self.name]\n    (rc, out, err) = self.execute_command(cmd)\n    return (rc, out, err)", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "DarwinGroup", "function_name": "group_mod", "parameters": ["self", "gid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, gid=None):\n    info = self.group_info()\n    if self.gid is not None and int(self.gid) != info[2]:\n        cmd = [self.module.get_bin_path('dseditgroup', True)]\n        cmd += ['-o', 'edit']\n        if gid is not None:\n            cmd += ['-i', str(gid)]\n        cmd += ['-L', self.name]\n        (rc, out, err) = self.execute_command(cmd)\n        return (rc, out, err)\n    return (None, '', '')", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "DarwinGroup", "function_name": "get_lowest_available_system_gid", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["group_info.split", "int", "len", "out.splitlines", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_lowest_available_system_gid(self):\n    # check for lowest available system gid (< 500)\n    try:\n        cmd = [self.module.get_bin_path('dscl', True)]\n        cmd += ['/Local/Default', '-list', '/Groups', 'PrimaryGroupID']\n        (rc, out, err) = self.execute_command(cmd)\n        lines = out.splitlines()\n        highest = 0\n        for group_info in lines:\n            parts = group_info.split(' ')\n            if len(parts) > 1:\n                gid = int(parts[-1])\n                if gid > highest and gid < 500:\n                    highest = gid\n        if highest == 0 or highest == 499:\n            return False\n        return (highest + 1)\n    except Exception:\n        return False", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "OpenBsdGroup", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('groupadd', True)]\n    if self.gid is not None:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "OpenBsdGroup", "function_name": "group_mod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "int", "len", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, **kwargs):\n    cmd = [self.module.get_bin_path('groupmod', True)]\n    info = self.group_info()\n    if self.gid is not None and int(self.gid) != info[2]:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    if len(cmd) == 1:\n        return (None, '', '')\n    if self.module.check_mode:\n        return (0, '', '')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "NetBsdGroup", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('groupadd', True)]\n    if self.gid is not None:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "NetBsdGroup", "function_name": "group_mod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "int", "len", "self.execute_command", "self.group_info", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_mod(self, **kwargs):\n    cmd = [self.module.get_bin_path('groupmod', True)]\n    info = self.group_info()\n    if self.gid is not None and int(self.gid) != info[2]:\n        cmd.append('-g')\n        cmd.append(str(self.gid))\n        if self.non_unique:\n            cmd.append('-o')\n    if len(cmd) == 1:\n        return (None, '', '')\n    if self.module.check_mode:\n        return (0, '', '')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\group.py", "class_name": "BusyBoxGroup", "function_name": "group_add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "self.execute_command", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_add(self, **kwargs):\n    cmd = [self.module.get_bin_path('addgroup', True)]\n    if self.gid is not None:\n        cmd.extend(['-g', str(self.gid)])\n\n    if self.system:\n        cmd.append('-S')\n\n    if self.gid_min is not None:\n        cmd.append('-K')\n        cmd.append('GID_MIN=' + str(self.gid_min))\n\n    if self.gid_max is not None:\n        cmd.append('-K')\n        cmd.append('GID_MAX=' + str(self.gid_max))\n\n    cmd.append(self.name)\n\n    return self.execute_command(cmd)", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "AnsibleModule", "Hostname", "STRATS.keys", "dict", "hostname.get_current_hostname", "hostname.get_permanent_hostname", "hostname.update_current_and_permanent_hostname", "list", "module.exit_json", "name.split", "socket.getfqdn", "socket.getfqdn().split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(type='str', required=True),\n            use=dict(type='str', choices=list(STRATS.keys()))\n        ),\n        supports_check_mode=True,\n    )\n\n    hostname = Hostname(module)\n    name = module.params['name']\n\n    current_hostname = hostname.get_current_hostname()\n    permanent_hostname = hostname.get_permanent_hostname()\n\n    changed = hostname.update_current_and_permanent_hostname()\n\n    if name != current_hostname:\n        name_before = current_hostname\n    else:\n        name_before = permanent_hostname\n\n    # NOTE: socket.getfqdn() calls gethostbyaddr(socket.gethostname()), which can be\n    # slow to return if the name does not resolve correctly.\n    kw = dict(changed=changed, name=name,\n              ansible_facts=dict(ansible_hostname=name.split('.')[0],\n                                 ansible_nodename=name,\n                                 ansible_fqdn=socket.getfqdn(),\n                                 ansible_domain='.'.join(socket.getfqdn().split('.')[1:])))\n\n    if changed:\n        kw['diff'] = {'after': 'hostname = ' + name + '\\n',\n                      'before': 'hostname = ' + name_before + '\\n'}\n\n    module.exit_json(**kw)", "loc": 35}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "BaseStrategy", "function_name": "update_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_current_hostname", "self.set_current_hostname"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_current_hostname(self):\n    name = self.module.params['name']\n    current_name = self.get_current_hostname()\n    if current_name != name:\n        if not self.module.check_mode:\n            self.set_current_hostname(name)\n        self.changed = True", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "BaseStrategy", "function_name": "update_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_permanent_hostname", "self.set_permanent_hostname"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_permanent_hostname(self):\n    name = self.module.params['name']\n    permanent_name = self.get_permanent_hostname()\n    if permanent_name != name:\n        if not self.module.check_mode:\n            self.set_permanent_hostname(name)\n        self.changed = True", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "UnimplementedStrategy", "function_name": "unimplemented_error", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_distribution", "platform.system", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unimplemented_error(self):\n    system = platform.system()\n    distribution = get_distribution()\n    if distribution is not None:\n        msg_platform = '%s (%s)' % (system, distribution)\n    else:\n        msg_platform = system\n    self.module.fail_json(\n        msg='hostname module cannot be used on platform %s' % msg_platform)", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "CommandStrategy", "function_name": "get_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_hostname(self):\n    cmd = [self.hostname_cmd]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "CommandStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    cmd = [self.hostname_cmd, name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "RedHatStrategy", "function_name": "get_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_file_lines", "line.split", "line.startswith", "self.module.fail_json", "to_native", "to_native(line).strip", "v.strip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_permanent_hostname(self):\n    try:\n        for line in get_file_lines(self.NETWORK_FILE):\n            line = to_native(line).strip()\n            if line.startswith('HOSTNAME'):\n                k, v = line.split('=')\n                return v.strip()\n        self.module.fail_json(\n            \"Unable to locate HOSTNAME entry in %s\" % self.NETWORK_FILE\n        )\n    except Exception as e:\n        self.module.fail_json(\n            msg=\"failed to read hostname: %s\" % to_native(e))", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "AlpineStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.get_bin_path", "self.module.run_command", "super", "super(AlpineStrategy, self).set_current_hostname"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    super(AlpineStrategy, self).set_current_hostname(name)\n    hostname_cmd = self.module.get_bin_path(self.COMMAND, True)\n\n    cmd = [hostname_cmd, '-F', self.FILE]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SystemdStrategy", "function_name": "get_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_hostname(self):\n    cmd = [self.hostnamectl_cmd, '--transient', 'status']\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SystemdStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    if len(name) > 64:\n        self.module.fail_json(msg=\"name cannot be longer than 64 characters on systemd servers, try a shorter name\")\n    cmd = [self.hostnamectl_cmd, '--transient', 'set-hostname', name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SystemdStrategy", "function_name": "get_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_permanent_hostname(self):\n    cmd = [self.hostnamectl_cmd, '--static', 'status']\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SystemdStrategy", "function_name": "set_permanent_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_permanent_hostname(self, name):\n    if len(name) > 64:\n        self.module.fail_json(msg=\"name cannot be longer than 64 characters on systemd servers, try a shorter name\")\n    cmd = [self.hostnamectl_cmd, '--pretty', '--static', 'set-hostname', name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SystemdStrategy", "function_name": "update_current_and_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.update_current_hostname", "self.update_permanent_hostname"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_current_and_permanent_hostname(self):\n    # Must set the permanent hostname prior to current to avoid NetworkManager complaints\n    # about setting the hostname outside of NetworkManager\n    self.update_permanent_hostname()\n    self.update_current_hostname()\n    return self.changed", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "OpenBSDStrategy", "function_name": "get_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_hostname(self):\n    cmd = [self.hostname_cmd]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "OpenBSDStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    cmd = [self.hostname_cmd, name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SolarisStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    cmd_option = '-t'\n    cmd = [self.hostname_cmd, cmd_option, name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SolarisStrategy", "function_name": "get_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_permanent_hostname(self):\n    fmri = 'svc:/system/identity:node'\n    pattern = 'config/nodename'\n    cmd = '/usr/sbin/svccfg -s %s listprop -o value %s' % (fmri, pattern)\n    rc, out, err = self.module.run_command(cmd, use_unsafe_shell=True)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "SolarisStrategy", "function_name": "set_permanent_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_permanent_hostname(self, name):\n    cmd = [self.hostname_cmd, name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "FreeBSDStrategy", "function_name": "get_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_hostname(self):\n    cmd = [self.hostname_cmd]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))\n    return to_native(out).strip()", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "FreeBSDStrategy", "function_name": "set_current_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_current_hostname(self, name):\n    cmd = [self.hostname_cmd, name]\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Command failed rc=%d, out=%s, err=%s\" % (rc, out, err))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "DarwinStrategy", "function_name": "get_current_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_hostname(self):\n    cmd = [self.scutil, '--get', 'HostName']\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0 and 'HostName: not set' not in err:\n        self.module.fail_json(msg=\"Failed to get current hostname rc=%d, out=%s, err=%s\" % (rc, out, err))\n\n    return to_native(out).strip()", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "DarwinStrategy", "function_name": "get_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command", "to_native", "to_native(out).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_permanent_hostname(self):\n    cmd = [self.scutil, '--get', 'ComputerName']\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        self.module.fail_json(msg=\"Failed to get permanent hostname rc=%d, out=%s, err=%s\" % (rc, out, err))\n\n    return to_native(out).strip()", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "DarwinStrategy", "function_name": "set_permanent_hostname", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Failed to set {3} to '{2}': {0} {1}\".format", "cmd.append", "self.module.fail_json", "self.module.run_command", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_permanent_hostname(self, name):\n    for hostname_type in self.name_types:\n        cmd = [self.scutil, '--set', hostname_type]\n        if hostname_type == 'LocalHostName':\n            cmd.append(to_native(self.scrubbed_name))\n        else:\n            cmd.append(to_native(name))\n        rc, out, err = self.module.run_command(cmd)\n        if rc != 0:\n            self.module.fail_json(msg=\"Failed to set {3} to '{2}': {0} {1}\".format(to_native(out), to_native(err), to_native(name), hostname_type))", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\hostname.py", "class_name": "DarwinStrategy", "function_name": "update_permanent_hostname", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.run_command", "self.module.run_command([self.scutil, '--get', name_type])[1].strip", "self.set_permanent_hostname", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_permanent_hostname(self):\n    name = self.module.params['name']\n\n    # Get all the current host name values in the order of self.name_types\n    all_names = tuple(self.module.run_command([self.scutil, '--get', name_type])[1].strip() for name_type in self.name_types)\n\n    # Get the expected host name values based on the order in self.name_types\n    expected_names = tuple(self.scrubbed_name if n == 'LocalHostName' else name for n in self.name_types)\n\n    # Ensure all three names are updated\n    if all_names != expected_names:\n        if not self.module.check_mode:\n            self.set_permanent_hostname(name)\n        self.changed = True", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "append_param", "parameters": ["rule", "param", "flag", "is_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["append_param", "rule.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def append_param(rule, param, flag, is_list):\n    if is_list:\n        for item in param:\n            append_param(rule, item, flag, False)\n    else:\n        if param is not None:\n            if param[0] == '!':\n                rule.extend(['!', flag, param[1:]])\n            else:\n                rule.extend([flag, param])", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "append_match_flag", "parameters": ["rule", "param", "flag", "negatable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["rule.extend"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def append_match_flag(rule, param, flag, negatable):\n    if param == 'match':\n        rule.extend([flag])\n    elif negatable and param == 'negate':\n        rule.extend(['!', flag])", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "construct_rule", "parameters": ["params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["append_csv", "append_jump", "append_match", "append_match_flag", "append_param", "append_tcp_flags", "params.get", "params.get('jump').lower", "params['jump'].lower"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_rule(params):\n    rule = []\n    append_param(rule, params['protocol'], '-p', False)\n    append_param(rule, params['source'], '-s', False)\n    append_param(rule, params['destination'], '-d', False)\n    append_param(rule, params['match'], '-m', True)\n    append_tcp_flags(rule, params['tcp_flags'], '--tcp-flags')\n    append_param(rule, params['jump'], '-j', False)\n    if params.get('jump') and params['jump'].lower() == 'tee':\n        append_param(rule, params['gateway'], '--gateway', False)\n    append_param(rule, params['log_prefix'], '--log-prefix', False)\n    append_param(rule, params['log_level'], '--log-level', False)\n    append_param(rule, params['to_destination'], '--to-destination', False)\n    append_match(rule, params['destination_ports'], 'multiport')\n    append_csv(rule, params['destination_ports'], '--dports')\n    append_param(rule, params['to_source'], '--to-source', False)\n    append_param(rule, params['goto'], '-g', False)\n    append_param(rule, params['in_interface'], '-i', False)\n    append_param(rule, params['out_interface'], '-o', False)\n    append_param(rule, params['fragment'], '-f', False)\n    append_param(rule, params['set_counters'], '-c', False)\n    append_param(rule, params['source_port'], '--source-port', False)\n    append_param(rule, params['destination_port'], '--destination-port', False)\n    append_param(rule, params['to_ports'], '--to-ports', False)\n    append_param(rule, params['set_dscp_mark'], '--set-dscp', False)\n    if params.get('set_dscp_mark') and params.get('jump').lower() != 'dscp':\n        append_jump(rule, params['set_dscp_mark'], 'DSCP')\n\n    append_param(\n        rule,\n        params['set_dscp_mark_class'],\n        '--set-dscp-class',\n        False)\n    if params.get('set_dscp_mark_class') and params.get('jump').lower() != 'dscp':\n        append_jump(rule, params['set_dscp_mark_class'], 'DSCP')\n    append_match_flag(rule, params['syn'], '--syn', True)\n    if 'conntrack' in params['match']:\n        append_csv(rule, params['ctstate'], '--ctstate')\n    elif 'state' in params['match']:\n        append_csv(rule, params['ctstate'], '--state')\n    elif params['ctstate']:\n        append_match(rule, params['ctstate'], 'conntrack')\n        append_csv(rule, params['ctstate'], '--ctstate')\n    if 'iprange' in params['match']:\n        append_param(rule, params['src_range'], '--src-range', False)\n        append_param(rule, params['dst_range'], '--dst-range', False)\n    elif params['src_range'] or params['dst_range']:\n        append_match(rule, params['src_range'] or params['dst_range'], 'iprange')\n        append_param(rule, params['src_range'], '--src-range', False)\n        append_param(rule, params['dst_range'], '--dst-range', False)\n    if 'set' in params['match']:\n        append_param(rule, params['match_set'], '--match-set', False)\n        append_match_flag(rule, 'match', params['match_set_flags'], False)\n    elif params['match_set']:\n        append_match(rule, params['match_set'], 'set')\n        append_param(rule, params['match_set'], '--match-set', False)\n        append_match_flag(rule, 'match', params['match_set_flags'], False)\n    append_match(rule, params['limit'] or params['limit_burst'], 'limit')\n    append_param(rule, params['limit'], '--limit', False)\n    append_param(rule, params['limit_burst'], '--limit-burst', False)\n    append_match(rule, params['uid_owner'], 'owner')\n    append_match_flag(rule, params['uid_owner'], '--uid-owner', True)\n    append_param(rule, params['uid_owner'], '--uid-owner', False)\n    append_match(rule, params['gid_owner'], 'owner')\n    append_match_flag(rule, params['gid_owner'], '--gid-owner', True)\n    append_param(rule, params['gid_owner'], '--gid-owner', False)\n    if params['jump'] is None:\n        append_jump(rule, params['reject_with'], 'REJECT')\n        append_jump(rule, params['set_dscp_mark_class'], 'DSCP')\n        append_jump(rule, params['set_dscp_mark'], 'DSCP')\n\n    append_param(rule, params['reject_with'], '--reject-with', False)\n    append_param(\n        rule,\n        params['icmp_type'],\n        ICMP_TYPE_OPTIONS[params['ip_version']],\n        False)\n    append_match(rule, params['comment'], 'comment')\n    append_param(rule, params['comment'], '--comment', False)\n    return rule", "loc": 80}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "push_arguments", "parameters": ["iptables_path", "action", "params", "make_rule"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.extend", "construct_rule"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def push_arguments(iptables_path, action, params, make_rule=True):\n    cmd = [iptables_path]\n    cmd.extend(['-t', params['table']])\n    cmd.extend([action, params['chain']])\n    if action == '-I' and params['rule_num']:\n        cmd.extend([params['rule_num']])\n    if params['wait']:\n        cmd.extend(['-w', params['wait']])\n    if make_rule:\n        cmd.extend(construct_rule(params))\n    return cmd", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "get_chain_policy", "parameters": ["iptables_path", "module", "params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "module.run_command", "out.split", "push_arguments", "re.search", "result.group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_chain_policy(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-L', params, make_rule=False)\n    if module.params['numeric']:\n        cmd.append('--numeric')\n    rc, out, err = module.run_command(cmd, check_rc=True)\n    chain_header = out.split(\"\\n\")[0]\n    result = re.search(r'\\(policy ([A-Z]+)\\)', chain_header)\n    if result:\n        return result.group(1)\n    return None", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\iptables.py", "class_name": null, "function_name": "check_chain_present", "parameters": ["iptables_path", "module", "params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "module.run_command", "push_arguments"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_chain_present(iptables_path, module, params):\n    cmd = push_arguments(iptables_path, '-L', params, make_rule=False)\n    if module.params['numeric']:\n        cmd.append('--numeric')\n    rc, out, err = module.run_command(cmd, check_rc=False)\n    return (rc == 0)", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\known_hosts.py", "class_name": null, "function_name": "sanity_check", "parameters": ["module", "host", "key", "sshkeygen"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "module.fail_json", "module.run_command", "outf.flush", "outf.write", "re.search", "tempfile.NamedTemporaryFile"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Check supplied key is sensible host and key are parameters provided by the user; If the host provided is inconsistent with the key supplied, then this function", "source_code": "def sanity_check(module, host, key, sshkeygen):\n    \"\"\"Check supplied key is sensible\n\n    host and key are parameters provided by the user; If the host\n    provided is inconsistent with the key supplied, then this function\n    quits, providing an error to the user.\n    sshkeygen is the path to ssh-keygen, found earlier with get_bin_path\n    \"\"\"\n    # If no key supplied, we're doing a removal, and have nothing to check here.\n    if not key:\n        return\n    # Rather than parsing the key ourselves, get ssh-keygen to do it\n    # (this is essential for hashed keys, but otherwise useful, as the\n    # key question is whether ssh-keygen thinks the key matches the host).\n\n    # The approach is to write the key to a temporary file,\n    # and then attempt to look up the specified host in that file.\n\n    if re.search(r'\\S+(\\s+)?,(\\s+)?', host):\n        module.fail_json(msg=\"Comma separated list of names is not supported. \"\n                             \"Please pass a single name to lookup in the known_hosts file.\")\n\n    with tempfile.NamedTemporaryFile(mode='w+') as outf:\n        try:\n            outf.write(key)\n            outf.flush()\n        except OSError as ex:\n            raise Exception(f\"Failed to write to temporary file {outf.name!r}.\") from ex\n\n        sshkeygen_command = [sshkeygen, '-F', host, '-f', outf.name]\n        rc, stdout, stderr = module.run_command(sshkeygen_command)\n\n    if stdout == '':  # host not found\n        results = {\n            \"msg\": \"Host parameter does not match hashed host field in supplied key\",\n            \"rc\": rc,\n        }\n        if stderr:\n            results[\"stderr\"] = stderr\n        module.fail_json(**results)", "loc": 40}
{"file": "ansible\\lib\\ansible\\modules\\known_hosts.py", "class_name": null, "function_name": "hash_host_key", "parameters": ["host", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "base64.b64encode", "hmac.new", "hmac.new(hmac_key, to_bytes(host), hashlib.sha1).digest", "key.strip", "key.strip().split", "os.urandom", "to_bytes", "to_native"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def hash_host_key(host, key):\n    hmac_key = os.urandom(20)\n    hashed_host = hmac.new(hmac_key, to_bytes(host), hashlib.sha1).digest()\n    parts = key.strip().split()\n    # @ indicates the optional marker field used for @cert-authority or @revoked\n    i = 1 if parts[0][0] == '@' else 0\n    parts[i] = '|1|%s|%s' % (to_native(base64.b64encode(hmac_key)), to_native(base64.b64encode(hashed_host)))\n    return ' '.join(parts)", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\known_hosts.py", "class_name": null, "function_name": "normalize_known_hosts_key", "parameters": ["key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "key.split", "key.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Transform a key, either taken from a known_host file or provided by the user, into a normalized form. The host part (which might include multiple hostnames or be hashed) gets", "source_code": "def normalize_known_hosts_key(key):\n    \"\"\"\n    Transform a key, either taken from a known_host file or provided by the\n    user, into a normalized form.\n    The host part (which might include multiple hostnames or be hashed) gets\n    replaced by the provided host. Also, any spurious information gets removed\n    from the end (like the username@host tag usually present in hostkeys, but\n    absent in known_hosts files)\n    \"\"\"\n    key = key.strip()  # trim trailing newline\n    k = key.split()\n    d = dict()\n    # The optional \"marker\" field, used for @cert-authority or @revoked\n    if k[0][0] == '@':\n        d['options'] = k[0]\n        d['host'] = k[1]\n        d['type'] = k[2]\n        d['key'] = k[3]\n    else:\n        d['host'] = k[0]\n        d['type'] = k[1]\n        d['key'] = k[2]\n    return d", "loc": 23}
{"file": "ansible\\lib\\ansible\\modules\\known_hosts.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "copy.copy", "dict", "enforce_state", "module.exit_json", "results.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(required=True, type='str', aliases=['host']),\n            key=dict(required=False, type='str', no_log=False),\n            path=dict(default=\"~/.ssh/known_hosts\", type='path'),\n            hash_host=dict(required=False, type='bool', default=False),\n            state=dict(default='present', choices=['absent', 'present']),\n        ),\n        supports_check_mode=True\n    )\n\n    # TODO: deprecate returning everything that was passed in\n    results = copy.copy(module.params)\n    results.update(enforce_state(module, module.params))\n    module.exit_json(**results)", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\lineinfile.py", "class_name": null, "function_name": "check_file_attrs", "parameters": ["module", "changed", "message", "diff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.load_file_common_arguments", "module.set_fs_attributes_if_different"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_file_attrs(module, changed, message, diff):\n\n    file_args = module.load_file_common_arguments(module.params)\n    if module.set_fs_attributes_if_different(file_args, False, diff=diff):\n\n        if changed:\n            message += \" and \"\n        changed = True\n        message += \"ownership, perms or SE linux context changed\"\n\n    return message, changed", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\lineinfile.py", "class_name": null, "function_name": "matcher", "parameters": ["cur_line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cur_line.rstrip", "found.append", "re_c.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def matcher(cur_line):\n    if regexp is not None:\n        match_found = re_c.search(cur_line)\n    elif search_string is not None:\n        match_found = search_string in cur_line\n    else:\n        match_found = line == cur_line.rstrip('\\r\\n')\n    if match_found:\n        found.append(cur_line)\n    return not match_found", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "get_device_by_uuid", "parameters": ["module", "uuid"], "param_types": {"module": "AnsibleModule", "uuid": "str"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.lru_cache", "handle_timeout", "module.get_bin_path", "suppress"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get device information by UUID.", "source_code": "def get_device_by_uuid(module: AnsibleModule, uuid : str) -> str | None:\n    \"\"\"Get device information by UUID.\"\"\"\n    blkid_output = None\n    if (blkid_binary := module.get_bin_path(\"blkid\")):\n        cmd = [blkid_binary, \"--uuid\", uuid]\n        with suppress(subprocess.CalledProcessError):\n            blkid_output = handle_timeout(module)(subprocess.check_output)(cmd, text=True, timeout=module.params[\"timeout\"])\n    return blkid_output", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "list_uuids_linux", "parameters": [], "param_types": {}, "return_type": "list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.lru_cache", "os.listdir", "suppress"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "List UUIDs from the system.", "source_code": "def list_uuids_linux() -> list[str]:\n    \"\"\"List UUIDs from the system.\"\"\"\n    with suppress(OSError):\n        return os.listdir(\"/dev/disk/by-uuid\")\n    return []", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "run_lsblk", "parameters": ["module"], "param_types": {"module": "AnsibleModule"}, "return_type": "list[list[str]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.lru_cache", "len", "line.split", "lsblk_output.splitlines", "module.get_bin_path", "subprocess.check_output"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return device, UUID pairs from lsblk.", "source_code": "def run_lsblk(module : AnsibleModule) -> list[list[str]]:\n    \"\"\"Return device, UUID pairs from lsblk.\"\"\"\n    lsblk_output = \"\"\n    if (lsblk_binary := module.get_bin_path(\"lsblk\")):\n        cmd = [lsblk_binary, \"--list\", \"--noheadings\", \"--paths\", \"--output\", \"NAME,UUID\", \"--exclude\", \"2\"]\n        lsblk_output = subprocess.check_output(cmd, text=True, timeout=module.params[\"timeout\"])\n    return [line.split() for line in lsblk_output.splitlines() if len(line.split()) == 2]", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "get_udevadm_device_uuid", "parameters": ["module", "device"], "param_types": {"module": "AnsibleModule", "device": "str"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.lru_cache", "line.split", "line.startswith", "module.get_bin_path", "subprocess.check_output", "udevadm_output.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Fallback to get the device's UUID for lsblk <= 2.23 which doesn't have the --paths option.", "source_code": "def get_udevadm_device_uuid(module : AnsibleModule, device : str) -> str | None:\n    \"\"\"Fallback to get the device's UUID for lsblk <= 2.23 which doesn't have the --paths option.\"\"\"\n    udevadm_output = \"\"\n    if (udevadm_binary := module.get_bin_path(\"udevadm\")):\n        cmd = [udevadm_binary, \"info\", \"--query\", \"property\", \"--name\", device]\n        udevadm_output = subprocess.check_output(cmd, text=True, timeout=module.params[\"timeout\"])\n    uuid = None\n    for line in udevadm_output.splitlines():\n        # a snippet of the output of the udevadm command below will be:\n        # ...\n        # ID_FS_TYPE=ext4\n        # ID_FS_USAGE=filesystem\n        # ID_FS_UUID=57b1a3e7-9019-4747-9809-7ec52bba9179\n        # ...\n        if line.startswith(\"ID_FS_UUID=\"):\n            uuid = line.split(\"=\", 1)[1]\n            break\n    return uuid", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "handle_timeout", "parameters": ["module", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "functools.wraps", "module.fail_json", "module.warn", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Decorator to catch timeout exceptions and handle failing, warning, and ignoring the timeout.", "source_code": "def handle_timeout(module, default=None):\n    \"\"\"Decorator to catch timeout exceptions and handle failing, warning, and ignoring the timeout.\"\"\"\n    def decorator(func):\n        @functools.wraps(func)\n        def wrapper(*args, **kwargs):\n            try:\n                return func(*args, **kwargs)\n            except (subprocess.TimeoutExpired, _timeout.TimeoutError) as e:\n                if module.params[\"on_timeout\"] == \"error\":\n                    module.fail_json(msg=str(e))\n                elif module.params[\"on_timeout\"] == \"warn\":\n                    module.warn(str(e))\n                return default\n        return wrapper\n    return decorator", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "run_mount_bin", "parameters": ["module", "mount_bin"], "param_types": {"module": "AnsibleModule", "mount_bin": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["handle_timeout", "module.fail_json", "module.get_bin_path", "str"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Execute the specified mount binary with optional timeout.", "source_code": "def run_mount_bin(module: AnsibleModule, mount_bin: str) -> str:  # type: ignore # Missing return statement\n    \"\"\"Execute the specified mount binary with optional timeout.\"\"\"\n    mount_bin = module.get_bin_path(mount_bin, required=True)\n    try:\n        return handle_timeout(module, default=\"\")(subprocess.check_output)(\n            mount_bin, text=True, timeout=module.params[\"timeout\"]\n        )\n    except subprocess.CalledProcessError as e:\n        module.fail_json(msg=f\"Failed to execute {mount_bin}: {str(e)}\")", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "get_mount_pattern", "parameters": ["stdout"], "param_types": {"stdout": "str"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AIX_MOUNT_RE.match", "BSD_MOUNT_RE.match", "LINUX_MOUNT_RE.match", "all", "len", "line.startswith", "stdout.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_pattern(stdout: str):\n    lines = stdout.splitlines()\n    pattern = None\n    if all(LINUX_MOUNT_RE.match(line) for line in lines):\n        pattern = LINUX_MOUNT_RE\n    elif all(BSD_MOUNT_RE.match(line) for line in lines if not line.startswith(\"map \")):\n        pattern = BSD_MOUNT_RE\n    elif len(lines) > 2 and all(AIX_MOUNT_RE.match(line) for line in lines[2:]):\n        pattern = AIX_MOUNT_RE\n    return pattern", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_mounts_from_stdout", "parameters": ["stdout"], "param_types": {"stdout": "str"}, "return_type": "t.Iterable[MountInfo]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["MountInfo", "get_mount_pattern", "len", "match.group", "match.groupdict", "mount_info.pop", "mount_info.update", "pattern.match", "re.split", "stdout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "List mount dictionaries from mount stdout.", "source_code": "def gen_mounts_from_stdout(stdout: str) -> t.Iterable[MountInfo]:\n    \"\"\"List mount dictionaries from mount stdout.\"\"\"\n    if not (pattern := get_mount_pattern(stdout)):\n        stdout = \"\"\n\n    for line in stdout.splitlines():\n        if not (match := pattern.match(line)):\n            # AIX has a couple header lines for some reason\n            # MacOS \"map\" lines are skipped (e.g. \"map auto_home on /System/Volumes/Data/home (autofs, automounted, nobrowse)\")\n            # TODO: include MacOS lines\n            continue\n\n        mount = match.groupdict()[\"mount\"]\n        if pattern is LINUX_MOUNT_RE:\n            mount_info = match.groupdict()\n        elif pattern is BSD_MOUNT_RE:\n            # the group containing fstype is comma separated, and may include whitespace\n            mount_info = match.groupdict()\n            parts = re.split(r\"\\s*,\\s*\", match.group(\"fstype\"), maxsplit=1)\n            if len(parts) == 1:\n                mount_info[\"fstype\"] = parts[0]\n            else:\n                mount_info.update({\"fstype\": parts[0], \"options\": parts[1]})\n        elif pattern is AIX_MOUNT_RE:\n            mount_info = match.groupdict()\n            device = mount_info.pop(\"mounted\")\n            node = mount_info.pop(\"node\")\n            if device and node:\n                device = f\"{node}:{device}\"\n            mount_info[\"device\"] = device\n\n        yield MountInfo(mount, line, mount_info)", "loc": 32}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_fstab_entries", "parameters": ["lines"], "param_types": {"lines": "list[str]"}, "return_type": "t.Iterable[MountInfo]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["MountInfo", "int", "line.split", "line.startswith", "line.strip", "replace_octal_escapes", "suppress"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield tuples from /etc/fstab https://man7.org/linux/man-pages/man5/fstab.5.html. Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.", "source_code": "def gen_fstab_entries(lines: list[str]) -> t.Iterable[MountInfo]:\n    \"\"\"Yield tuples from /etc/fstab https://man7.org/linux/man-pages/man5/fstab.5.html.\n\n    Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.\n    \"\"\"\n    for line in lines:\n        if not (line := line.strip()) or line.startswith(\"#\"):\n            continue\n        fields = [replace_octal_escapes(field) for field in line.split()]\n        mount_info: dict[str, str | int] = {\n            \"device\": fields[0],\n            \"mount\": fields[1],\n            \"fstype\": fields[2],\n            \"options\": fields[3],\n        }\n        with suppress(IndexError):\n            # the last two fields are optional\n            mount_info[\"dump\"] = int(fields[4])\n            mount_info[\"passno\"] = int(fields[5])\n        yield MountInfo(fields[1], line, mount_info)", "loc": 20}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_vfstab_entries", "parameters": ["lines"], "param_types": {"lines": "list[str]"}, "return_type": "t.Iterable[MountInfo]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["MountInfo", "int", "line.split", "line.strip", "line.strip().startswith", "suppress"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield tuples from /etc/vfstab https://docs.oracle.com/cd/E36784_01/html/E36882/vfstab-4.html. Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.", "source_code": "def gen_vfstab_entries(lines: list[str]) -> t.Iterable[MountInfo]:\n    \"\"\"Yield tuples from /etc/vfstab https://docs.oracle.com/cd/E36784_01/html/E36882/vfstab-4.html.\n\n    Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.\n    \"\"\"\n    for line in lines:\n        if not line.strip() or line.strip().startswith(\"#\"):\n            continue\n        fields = line.split()\n        passno: str | int = fields[4]\n        with suppress(ValueError):\n            passno = int(passno)\n        mount_info: dict[str, str | int] = {\n            \"device\": fields[0],\n            \"device_to_fsck\": fields[1],\n            \"mount\": fields[2],\n            \"fstype\": fields[3],\n            \"passno\": passno,\n            \"mount_at_boot\": fields[5],\n            \"options\": fields[6],\n        }\n        yield MountInfo(fields[2], line, mount_info)", "loc": 22}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "list_aix_filesystems_stanzas", "parameters": ["lines"], "param_types": {"lines": "list[str]"}, "return_type": "list[list[str]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.rstrip", "line.rstrip().endswith", "line.startswith", "line.strip", "stanzas.append", "stanzas[-1].append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Parse stanzas from /etc/filesystems according to https://www.ibm.com/docs/hu/aix/7.2?topic=files-filesystems-file.", "source_code": "def list_aix_filesystems_stanzas(lines: list[str]) -> list[list[str]]:\n    \"\"\"Parse stanzas from /etc/filesystems according to https://www.ibm.com/docs/hu/aix/7.2?topic=files-filesystems-file.\"\"\"\n    stanzas = []\n    for line in lines:\n        if line.startswith(\"*\") or not line.strip():\n            continue\n        if line.rstrip().endswith(\":\"):\n            stanzas.append([line])\n        else:\n            if \"=\" not in line:\n                # Expected for Linux, return an empty list since this doesn't appear to be AIX /etc/filesystems\n                stanzas = []\n                break\n            stanzas[-1].append(line)\n    return stanzas", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_aix_filesystems_entries", "parameters": ["lines"], "param_types": {"lines": "list[str]"}, "return_type": "t.Iterable[MountInfoOptions]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "MountInfoOptions", "attr.strip", "line.split", "list_aix_filesystems_stanzas", "mount_info.get", "stanza.pop", "value.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield tuples from /etc/filesystems https://www.ibm.com/docs/hu/aix/7.2?topic=files-filesystems-file. Each tuple contains the mount point, lines of origin, and the dictionary of the parsed lines.", "source_code": "def gen_aix_filesystems_entries(lines: list[str]) -> t.Iterable[MountInfoOptions]:\n    \"\"\"Yield tuples from /etc/filesystems https://www.ibm.com/docs/hu/aix/7.2?topic=files-filesystems-file.\n\n    Each tuple contains the mount point, lines of origin, and the dictionary of the parsed lines.\n    \"\"\"\n    for stanza in list_aix_filesystems_stanzas(lines):\n        original = \"\\n\".join(stanza)\n        mount = stanza.pop(0)[:-1]  # snip trailing :\n        mount_info: dict[str, str] = {}\n        for line in stanza:\n            attr, value = line.split(\"=\", 1)\n            mount_info[attr.strip()] = value.strip()\n\n        device = \"\"\n        if (nodename := mount_info.get(\"nodename\")):\n            device = nodename\n        if (dev := mount_info.get(\"dev\")):\n            if device:\n                device += \":\"\n            device += dev\n\n        normalized_fields: dict[str, str | dict[str, str]] = {\n            \"mount\": mount,\n            \"device\": device or \"unknown\",\n            \"fstype\": mount_info.get(\"vfs\") or \"unknown\",\n            # avoid clobbering the mount point with the AIX mount option \"mount\"\n            \"attributes\": mount_info,\n        }\n        yield MountInfoOptions(mount, original, normalized_fields)", "loc": 29}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_mnttab_entries", "parameters": ["lines"], "param_types": {"lines": "list[str]"}, "return_type": "t.Iterable[MountInfo]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["MountInfo", "any", "datetime.date.fromtimestamp", "int", "len", "line.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield tuples from /etc/mnttab columns https://docs.oracle.com/cd/E36784_01/html/E36882/mnttab-4.html. Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.", "source_code": "def gen_mnttab_entries(lines: list[str]) -> t.Iterable[MountInfo]:\n    \"\"\"Yield tuples from /etc/mnttab columns https://docs.oracle.com/cd/E36784_01/html/E36882/mnttab-4.html.\n\n    Each tuple contains the mount point, line of origin, and the dictionary of the parsed line.\n    \"\"\"\n    if not any(len(fields[4]) == 10 for line in lines for fields in [line.split()]):\n        raise ValueError\n    for line in lines:\n        fields = line.split()\n        datetime.date.fromtimestamp(int(fields[4]))\n        mount_info: dict[str, str | int] = {\n            \"device\": fields[0],\n            \"mount\": fields[1],\n            \"fstype\": fields[2],\n            \"options\": fields[3],\n            \"time\": int(fields[4]),\n        }\n        yield MountInfo(fields[1], line, mount_info)", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "gen_mounts_by_file", "parameters": ["file"], "param_types": {"file": "str"}, "return_type": "t.Iterable[MountInfo | MountInfoOptions]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_mounts", "get_file_content", "get_file_content(file, '').splitlines", "list", "suppress"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield parsed mount entries from the first successful generator. Generators are tried in the following order to minimize false positives: - /etc/vfstab: 7 columns", "source_code": "def gen_mounts_by_file(file: str) -> t.Iterable[MountInfo | MountInfoOptions]:\n    \"\"\"Yield parsed mount entries from the first successful generator.\n\n    Generators are tried in the following order to minimize false positives:\n    - /etc/vfstab: 7 columns\n    - /etc/mnttab: 5 columns (mnttab[4] must contain UNIX timestamp)\n    - /etc/fstab: 4-6 columns (fstab[4] is optional and historically 0-9, but can be any int)\n    - /etc/filesystems: multi-line, not column-based, and specific to AIX\n    \"\"\"\n    if (lines := get_file_content(file, \"\").splitlines()):\n        for gen_mounts in [gen_vfstab_entries, gen_mnttab_entries, gen_fstab_entries, gen_aix_filesystems_entries]:\n            with suppress(IndexError, ValueError):\n                # mpypy error: misc: Incompatible types in \"yield from\" (actual type \"object\", expected type \"Union[MountInfo, MountInfoOptions]\n                # only works if either\n                # * the list of functions excludes gen_aix_filesystems_entries\n                # * the list of functions only contains gen_aix_filesystems_entries\n                yield from list(gen_mounts(lines))  # type: ignore[misc]\n                break", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "get_sources", "parameters": ["module"], "param_types": {"module": "AnsibleModule"}, "return_type": "list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json", "sources.append", "sources.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a list of filenames from the requested sources.", "source_code": "def get_sources(module: AnsibleModule) -> list[str]:\n    \"\"\"Return a list of filenames from the requested sources.\"\"\"\n    sources: list[str] = []\n    for source in module.params[\"sources\"] or [\"all\"]:\n        if not source:\n            module.fail_json(msg=\"sources contains an empty string\")\n\n        if source in {\"dynamic\", \"all\"}:\n            sources.extend(DYNAMIC_SOURCES)\n        if source in {\"static\", \"all\"}:\n            sources.extend(STATIC_SOURCES)\n\n        elif source not in {\"static\", \"dynamic\", \"all\"}:\n            sources.append(source)\n    return sources", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "get_mount_facts", "parameters": ["module"], "param_types": {"module": "AnsibleModule"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_timeout.timeout", "any", "device.split", "device.startswith", "fields.update", "fnmatch", "gen_mounts_by_source", "get_device_by_uuid", "get_partition_uuid", "handle_timeout", "mounts.append", "suppress"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "List and filter mounts, returning all mounts for each unique source.", "source_code": "def get_mount_facts(module: AnsibleModule):\n    \"\"\"List and filter mounts, returning all mounts for each unique source.\"\"\"\n    seconds = module.params[\"timeout\"]\n    mounts = []\n    for source, mount, origin, fields in gen_mounts_by_source(module):\n        device = fields[\"device\"]\n        fstype = fields[\"fstype\"]\n\n        # Convert UUIDs in Linux /etc/fstab to device paths\n        # TODO need similar for OpenBSD which lists UUIDS (without the UUID= prefix) in /etc/fstab, needs another approach though.\n        uuid = None\n        if device.startswith(\"UUID=\"):\n            uuid = device.split(\"=\", 1)[1]\n            device = get_device_by_uuid(module, uuid) or device\n\n        if not any(fnmatch(device, pattern) for pattern in module.params[\"devices\"] or [\"*\"]):\n            continue\n        if not any(fnmatch(fstype, pattern) for pattern in module.params[\"fstypes\"] or [\"*\"]):\n            continue\n\n        timed_func = _timeout.timeout(seconds, f\"Timed out getting mount size for mount {mount} (type {fstype})\")(get_mount_size)\n        if mount_size := handle_timeout(module)(timed_func)(mount):\n            fields.update(mount_size)\n\n        if uuid is None:\n            with suppress(subprocess.CalledProcessError):\n                uuid = get_partition_uuid(module, device)\n\n        fields.update({\"ansible_context\": {\"source\": source, \"source_data\": origin}, \"uuid\": uuid})\n        mounts.append(fields)\n\n    return mounts", "loc": 32}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "handle_deduplication", "parameters": ["module", "mounts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "duplicates_by_src.items", "len", "module.warn", "mounts_by_source.items", "mounts_by_source.setdefault", "mounts_by_source.setdefault(source, []).append", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return the unique mount points from the complete list of mounts, and handle the optional aggregate results.", "source_code": "def handle_deduplication(module, mounts):\n    \"\"\"Return the unique mount points from the complete list of mounts, and handle the optional aggregate results.\"\"\"\n    mount_points = {}\n    mounts_by_source = {}\n    for mount in mounts:\n        mount_point = mount[\"mount\"]\n        source = mount[\"ansible_context\"][\"source\"]\n        if mount_point not in mount_points:\n            mount_points[mount_point] = mount\n        mounts_by_source.setdefault(source, []).append(mount_point)\n\n    duplicates_by_src = {src: mnts for src, mnts in mounts_by_source.items() if len(set(mnts)) != len(mnts)}\n    if duplicates_by_src and module.params[\"include_aggregate_mounts\"] is None:\n        duplicates_by_src = {src: mnts for src, mnts in mounts_by_source.items() if len(set(mnts)) != len(mnts)}\n        duplicates_str = \", \".join([f\"{src} ({duplicates})\" for src, duplicates in duplicates_by_src.items()])\n        module.warn(f\"mount_facts: ignoring repeat mounts in the following sources: {duplicates_str}. \"\n                    \"You can disable this warning by configuring the 'include_aggregate_mounts' option as True or False.\")\n\n    if module.params[\"include_aggregate_mounts\"]:\n        aggregate_mounts = mounts\n    else:\n        aggregate_mounts = []\n\n    return mount_points, aggregate_mounts", "loc": 24}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "get_argument_spec", "get_mount_facts", "handle_deduplication", "isinstance", "module.exit_json", "module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=get_argument_spec(),\n        supports_check_mode=True,\n    )\n    if (seconds := module.params[\"timeout\"]) is not None and seconds <= 0:\n        module.fail_json(msg=f\"argument 'timeout' must be a positive number or null, not {seconds}\")\n    if (mount_binary := module.params[\"mount_binary\"]) is not None and not isinstance(mount_binary, str):\n        module.fail_json(msg=f\"argument 'mount_binary' must be a string or null, not {mount_binary}\")\n\n    mounts = get_mount_facts(module)\n    mount_points, aggregate_mounts = handle_deduplication(module, mounts)\n\n    module.exit_json(ansible_facts={\"mount_points\": mount_points, \"aggregate_mounts\": aggregate_mounts})", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "decorator", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "functools.wraps", "module.fail_json", "module.warn", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decorator(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except (subprocess.TimeoutExpired, _timeout.TimeoutError) as e:\n            if module.params[\"on_timeout\"] == \"error\":\n                module.fail_json(msg=str(e))\n            elif module.params[\"on_timeout\"] == \"warn\":\n                module.warn(str(e))\n            return default\n    return wrapper", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\mount_facts.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "functools.wraps", "module.fail_json", "module.warn", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    try:\n        return func(*args, **kwargs)\n    except (subprocess.TimeoutExpired, _timeout.TimeoutError) as e:\n        if module.params[\"on_timeout\"] == \"error\":\n            module.fail_json(msg=str(e))\n        elif module.params[\"on_timeout\"] == \"warn\":\n            module.warn(str(e))\n        return default", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "ALIASES.keys", "ALIASES.values", "AnsibleModule", "PKG_MANAGERS.keys", "PKG_MANAGER_NAMES.extend", "dict", "get_all_pkg_managers", "manager.get_packages", "manager.is_available", "managers.extend", "managers.remove", "module.exit_json", "module.fail_json", "module.warn", "packages[k].extend", "packages_found.keys", "seen.add", "set", "set(managers).difference", "sorted", "to_text", "x.lower"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n\n    # get supported pkg managers\n    PKG_MANAGERS = get_all_pkg_managers()\n    PKG_MANAGER_NAMES = sorted([x.lower() for x in PKG_MANAGERS.keys()])\n    # add aliases\n    PKG_MANAGER_NAMES.extend([alias for alist in ALIASES.values() for alias in alist])\n\n    # start work\n    global module\n\n    # choices are not set for 'manager' as they are computed dynamically and validated below instead of in argspec\n    module = AnsibleModule(argument_spec=dict(manager={'type': 'list', 'elements': 'str', 'default': ['auto']},\n                                              strategy={'choices': ['first', 'all'], 'default': 'first'}),\n                           supports_check_mode=True)\n    packages = {}\n    results = {'ansible_facts': {}}\n    managers = [x.lower() for x in module.params['manager']]\n    strategy = module.params['strategy']\n\n    if 'auto' in managers:\n        # keep order from user, we do dedupe below\n        managers.extend(PKG_MANAGER_NAMES)\n        managers.remove('auto')\n\n    unsupported = set(managers).difference(PKG_MANAGER_NAMES)\n    if unsupported:\n        if 'auto' in module.params['manager']:\n            msg = 'Could not auto detect a usable package manager, check warnings for details.'\n        else:\n            msg = 'Unsupported package managers requested: %s' % (', '.join(unsupported))\n        module.fail_json(msg=msg)\n\n    found = 0\n    seen = set()\n    for pkgmgr in managers:\n\n        if strategy == 'first' and found:\n            break\n\n        # substitute aliases for aliased\n        for aliased in ALIASES.keys():\n            if pkgmgr in ALIASES[aliased]:\n                pkgmgr = aliased\n                break\n\n        # dedupe as per above\n        if pkgmgr in seen:\n            continue\n\n        seen.add(pkgmgr)\n\n        manager = PKG_MANAGERS[pkgmgr]()\n        try:\n            packages_found = {}\n            if manager.is_available(handle_exceptions=False):\n                try:\n                    packages_found = manager.get_packages()\n                except Exception as e:\n                    module.warn('Failed to retrieve packages with %s: %s' % (pkgmgr, to_text(e)))\n\n            # only consider 'found' if it results in something\n            if packages_found:\n                found += 1\n                for k in packages_found.keys():\n                    if k in packages:\n                        packages[k].extend(packages_found[k])\n                    else:\n                        packages[k] = packages_found[k]\n            else:\n                module.warn('Found \"%s\" but no associated packages' % (pkgmgr))\n\n        except Exception as e:\n            if pkgmgr in module.params['manager']:\n                module.warn('Requested package manager %s was not usable by this module: %s' % (pkgmgr, to_text(e)))\n\n    if found == 0:\n        msg = ('Could not detect a supported package manager from the following list: %s, '\n               'or the required Python library is not installed. Check warnings for details.' % managers)\n        module.fail_json(msg=msg)\n\n    # Set the facts, this will override the facts in ansible_facts that might exist from previous runs\n    # when using operating system level or distribution package managers\n    results['ansible_facts']['packages'] = packages\n\n    module.exit_json(**results)", "loc": 86}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "APT", "function_name": "pkg_cache", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._lib.Cache"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pkg_cache(self):\n    if self._cache is not None:\n        return self._cache\n\n    self._cache = self._lib.Cache()\n    return self._cache", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PACMAN", "function_name": "list_installed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "dict", "get_best_parsable_locale", "module.run_command", "out.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_installed(self):\n    locale = get_best_parsable_locale(module)\n    rc, out, err = module.run_command([self._cli, '-Qi'], environ_update=dict(LC_ALL=locale))\n    if rc != 0 or err:\n        raise Exception(\"Unable to list packages rc=%s : %s\" % (rc, err))\n    return out.split(\"\\n\\n\")[:-1]", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PACMAN", "function_name": "get_package_details", "parameters": ["self", "package"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.lstrip", "m.group", "p.split", "package.splitlines", "raw_pkg_details['Provides'].split", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_package_details(self, package):\n    # parse values of details that might extend over several lines\n    raw_pkg_details = {}\n    last_detail = None\n    for line in package.splitlines():\n        m = re.match(r\"([\\w ]*[\\w]) +: (.*)\", line)\n        if m:\n            last_detail = m.group(1)\n            raw_pkg_details[last_detail] = m.group(2)\n        else:\n            # append value to previous detail\n            raw_pkg_details[last_detail] = raw_pkg_details[last_detail] + \"  \" + line.lstrip()\n\n    provides = None\n    if raw_pkg_details['Provides'] != 'None':\n        provides = [\n            p.split('=')[0]\n            for p in raw_pkg_details['Provides'].split('  ')\n        ]\n\n    return {\n        'name': raw_pkg_details['Name'],\n        'version': raw_pkg_details['Version'],\n        'arch': raw_pkg_details['Architecture'],\n        'provides': provides,\n    }", "loc": 26}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PKG", "function_name": "list_installed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\t%'.join", "Exception", "module.run_command", "out.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_installed(self):\n    rc, out, err = module.run_command([self._cli, 'query', \"%%%s\" % '\\t%'.join(['n', 'v', 'R', 't', 'a', 'q', 'o', 'p', 'V'])])\n    if rc != 0 or err:\n        raise Exception(\"Unable to list packages rc=%s : %s\" % (rc, err))\n    return out.splitlines()", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PKG", "function_name": "get_package_details", "parameters": ["self", "package"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "dict", "int", "package.split", "pkg['arch'].split", "pkg['category'].split", "pkg['version'].split", "zip"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_package_details(self, package):\n\n    pkg = dict(zip(self.atoms, package.split('\\t')))\n\n    if 'arch' in pkg:\n        try:\n            pkg['arch'] = pkg['arch'].split(':')[2]\n        except IndexError:\n            pass\n\n    if 'automatic' in pkg:\n        pkg['automatic'] = bool(int(pkg['automatic']))\n\n    if 'category' in pkg:\n        pkg['category'] = pkg['category'].split('/', 1)[0]\n\n    if 'version' in pkg:\n        if ',' in pkg['version']:\n            pkg['version'], pkg['port_epoch'] = pkg['version'].split(',', 1)\n        else:\n            pkg['port_epoch'] = 0\n\n        if '_' in pkg['version']:\n            pkg['version'], pkg['revision'] = pkg['version'].split('_', 1)\n        else:\n            pkg['revision'] = '0'\n\n    if 'vital' in pkg:\n        pkg['vital'] = bool(int(pkg['vital']))\n\n    return pkg", "loc": 31}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PORTAGE", "function_name": "list_installed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "RuntimeError", "module.run_command", "out.splitlines", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_installed(self):\n    rc, out, err = module.run_command(' '.join([self._cli, '-Iv', '|', 'xargs', '-n', '1024', 'qatom']), use_unsafe_shell=True)\n    if rc != 0:\n        raise RuntimeError(\"Unable to list packages rc=%s : %s\" % (rc, to_native(err)))\n    return out.splitlines()", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "APK", "function_name": "list_installed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "module.run_command", "out.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_installed(self):\n    rc, out, err = module.run_command([self._cli, 'info', '-v'])\n    if rc != 0:\n        raise Exception(\"Unable to list packages rc=%s : %s\" % (rc, err))\n    return out.splitlines()", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "APK", "function_name": "get_package_details", "parameters": ["self", "package"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["package.rsplit"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_package_details(self, package):\n    raw_pkg_details = {'name': package, 'version': '', 'release': ''}\n    nvr = package.rsplit('-', 2)\n    try:\n        return {\n            'name': nvr[0],\n            'version': nvr[1],\n            'release': nvr[2],\n        }\n    except IndexError:\n        return raw_pkg_details", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PKG_INFO", "function_name": "list_installed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "module.run_command", "out.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_installed(self):\n    rc, out, err = module.run_command([self._cli, '-a'])\n    if rc != 0 or err:\n        raise Exception(\"Unable to list packages rc=%s : %s\" % (rc, err))\n    return out.splitlines()", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\package_facts.py", "class_name": "PKG_INFO", "function_name": "get_package_details", "parameters": ["self", "package"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["package.split", "package.split(maxsplit=1)[0].rsplit"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_package_details(self, package):\n    raw_pkg_details = {'name': package, 'version': ''}\n    details = package.split(maxsplit=1)[0].rsplit('-', maxsplit=1)\n\n    try:\n        return {\n            'name': details[0],\n            'version': details[1],\n        }\n    except IndexError:\n        return raw_pkg_details", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\ping.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "Exception", "dict", "module.exit_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            data=dict(type='str', default='pong'),\n        ),\n        supports_check_mode=True\n    )\n\n    if module.params['data'] == 'crash':\n        raise Exception(\"boom\")\n\n    result = dict(\n        ping=module.params['data'],\n    )\n\n    module.exit_json(**result)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\pip.py", "class_name": "Package", "function_name": "is_satisfied_by", "parameters": ["self", "version_to_test"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LooseVersion", "all", "self._requirement.specifier.contains"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_satisfied_by(self, version_to_test):\n    if not self._plain_package:\n        return False\n    try:\n        return self._requirement.specifier.contains(version_to_test, prereleases=True)\n    except AttributeError:\n        # old setuptools has no specifier, do fallback\n        version_to_test = LooseVersion(version_to_test)\n        return all(\n            op_dict[op](version_to_test, LooseVersion(ver))\n            for op, ver in self._requirement.specs\n        )", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\replace.py", "class_name": null, "function_name": "check_file_attrs", "parameters": ["module", "changed", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.load_file_common_arguments", "module.set_file_attributes_if_different"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_file_attrs(module, changed, message):\n\n    file_args = module.load_file_common_arguments(module.params)\n    if module.set_file_attributes_if_different(file_args, False):\n\n        if changed:\n            message += \" and \"\n        changed = True\n        message += \"ownership, perms or SE linux context changed\"\n\n    return message, changed", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "RpmKey", "dict"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            state=dict(type='str', default='present', choices=['absent', 'present']),\n            key=dict(type='str', required=True, no_log=False),\n            fingerprint=dict(type='list', elements='str'),\n            validate_certs=dict(type='bool', default=True),\n        ),\n        supports_check_mode=True,\n    )\n\n    RpmKey(module)", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": "RpmKey", "function_name": "normalize_keyid", "parameters": ["self", "keyid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["keyid.strip", "keyid.strip().upper", "ret.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Ensure a keyid doesn't have a leading 0x, has leading or trailing whitespace, and make sure is uppercase", "source_code": "def normalize_keyid(self, keyid):\n    \"\"\"Ensure a keyid doesn't have a leading 0x, has leading or trailing whitespace, and make sure is uppercase\"\"\"\n    ret = keyid.strip().upper()\n    if ret.startswith('0x'):\n        return ret[2:]\n    elif ret.startswith('0X'):\n        return ret[2:]\n    else:\n        return ret", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": "RpmKey", "function_name": "getkeyid", "parameters": ["self", "keyfile"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.split", "line.startswith", "line.strip", "self.execute_command", "self.module.fail_json", "stdout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getkeyid(self, keyfile):\n    stdout, stderr = self.execute_command([self.gpg, '--no-tty', '--batch', '--with-colons', '--fixed-list-mode', keyfile])\n    for line in stdout.splitlines():\n        line = line.strip()\n        if line.startswith('pub:'):\n            return line.split(':')[4]\n\n    self.module.fail_json(msg=\"Unexpected gpg output\")", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": "RpmKey", "function_name": "getfingerprints", "parameters": ["self", "keyfile"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fingerprints.add", "line.split", "line.startswith", "line.strip", "self.execute_command", "self.module.fail_json", "set", "stdout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getfingerprints(self, keyfile):\n    stdout, stderr = self.execute_command([\n        self.gpg, '--no-tty', '--batch', '--with-colons',\n        '--fixed-list-mode', '--import', '--import-options', 'show-only',\n        '--dry-run', keyfile\n    ])\n\n    fingerprints = set()\n\n    for line in stdout.splitlines():\n        line = line.strip()\n        if line.startswith('fpr:'):\n            # As mentioned here,\n            #\n            # https://git.gnupg.org/cgi-bin/gitweb.cgi?p=gnupg.git;a=blob_plain;f=doc/DETAILS\n            #\n            # The description of the `fpr` field says\n            #\n            # \"fpr :: Fingerprint (fingerprint is in field 10)\"\n            #\n            fingerprints.add(line.split(':')[9])\n\n    if fingerprints:\n        return fingerprints\n\n    self.module.fail_json(msg=\"Unexpected gpg output\")", "loc": 26}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": "RpmKey", "function_name": "execute_command", "parameters": ["self", "cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def execute_command(self, cmd):\n    rc, stdout, stderr = self.module.run_command(cmd, use_unsafe_shell=True)\n    if rc != 0:\n        self.module.fail_json(msg=stderr)\n    return stdout, stderr", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\rpm_key.py", "class_name": "RpmKey", "function_name": "is_key_imported", "parameters": ["self", "keyid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.split", "self.execute_command", "self.module.run_command", "stdout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_key_imported(self, keyid):\n    cmd = self.rpm + ' -q  gpg-pubkey'\n    rc, stdout, stderr = self.module.run_command(cmd)\n    if rc != 0:  # No key is installed on system\n        return False\n    cmd += ' --qf \"%{description}\" | ' + self.gpg + ' --no-tty --batch --with-colons --fixed-list-mode -'\n    stdout, stderr = self.execute_command(cmd)\n    for line in stdout.splitlines():\n        if keyid in line.split(':')[4]:\n            return True\n    return False", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "Service", "dict", "module.debug", "module.exit_json", "module.fail_json", "service.check_ps", "service.check_service_changed", "service.get_service_status", "service.get_service_tools", "service.modify_service_state", "service.service_enable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            name=dict(type='str', required=True),\n            state=dict(type='str', choices=['started', 'stopped', 'reloaded', 'restarted']),\n            sleep=dict(type='int'),\n            pattern=dict(type='str'),\n            enabled=dict(type='bool'),\n            runlevel=dict(type='str', default='default'),\n            arguments=dict(type='str', default='', aliases=['args']),\n        ),\n        supports_check_mode=True,\n        required_one_of=[['state', 'enabled']],\n    )\n\n    service = Service(module)\n\n    module.debug('Service instantiated - platform %s' % service.platform)\n    if service.distribution:\n        module.debug('Service instantiated - distribution %s' % service.distribution)\n\n    rc = 0\n    out = ''\n    err = ''\n    result = {}\n    result['name'] = service.name\n\n    # Find service management tools\n    service.get_service_tools()\n\n    # Enable/disable service startup at boot if requested\n    if service.module.params['enabled'] is not None:\n        # FIXME: ideally this should detect if we need to toggle the enablement state, though\n        # it's unlikely the changed handler would need to fire in this case so it's a minor thing.\n        service.service_enable()\n        result['enabled'] = service.enable\n\n    if module.params['state'] is None:\n        # Not changing the running state, so bail out now.\n        result['changed'] = service.changed\n        module.exit_json(**result)\n\n    result['state'] = service.state\n\n    # Collect service status\n    if service.pattern:\n        service.check_ps()\n    else:\n        service.get_service_status()\n\n    # Calculate if request will change service state\n    service.check_service_changed()\n\n    # Modify service state if necessary\n    (rc, out, err) = service.modify_service_state()\n\n    if rc != 0:\n        if err and \"Job is already running\" in err:\n            # upstart got confused, one such possibility is MySQL on Ubuntu 12.04\n            # where status may report it has no start/stop links and we could\n            # not get accurate status\n            pass\n        else:\n            if err:\n                module.fail_json(msg=err)\n            else:\n                module.fail_json(msg=out)\n\n    result['changed'] = service.changed | service.svc_change\n    if service.module.params['enabled'] is not None:\n        result['enabled'] = service.module.params['enabled']\n\n    if not service.module.params['state']:\n        status = service.get_service_status()\n        if status is None:\n            result['state'] = 'absent'\n        elif status is False:\n            result['state'] = 'started'\n        else:\n            result['state'] = 'stopped'\n    else:\n        # as we may have just bounced the service the service command may not\n        # report accurate state at this moment so just show what we ran\n        if service.module.params['state'] in ['reloaded', 'restarted', 'started']:\n            result['state'] = 'started'\n        else:\n            result['state'] = 'stopped'\n\n    module.exit_json(**result)", "loc": 89}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "Service", "function_name": "execute_command", "parameters": ["self", "cmd", "daemonize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "fds.remove", "get_best_parsable_locale", "json.dumps", "json.loads", "os._exit", "os.chdir", "os.close", "os.dup2", "os.fork", "os.open", "os.pipe", "os.read", "os.setsid", "os.waitpid", "os.write", "p.poll", "p.stderr.fileno", "p.stdout.fileno", "p.wait", "select.select", "self.module.fail_json", "self.module.run_command", "shlex.split", "subprocess.Popen", "to_bytes", "to_text"], "control_structures": ["If", "While"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def execute_command(self, cmd, daemonize=False):\n\n    locale = get_best_parsable_locale(self.module)\n    lang_env = dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale)\n\n    # Most things don't need to be daemonized\n    if not daemonize:\n        # chkconfig localizes messages and we're screen scraping so make\n        # sure we use the C locale\n        return self.module.run_command(cmd, environ_update=lang_env)\n\n    # This is complex because daemonization is hard for people.\n    # What we do is daemonize a part of this module, the daemon runs the\n    # command, picks up the return code and output, and returns it to the\n    # main process.\n    pipe = os.pipe()\n    pid = os.fork()\n    if pid == 0:\n        os.close(pipe[0])\n        # Set stdin/stdout/stderr to /dev/null\n        fd = os.open(os.devnull, os.O_RDWR)\n        if fd != 0:\n            os.dup2(fd, 0)\n        if fd != 1:\n            os.dup2(fd, 1)\n        if fd != 2:\n            os.dup2(fd, 2)\n        if fd not in (0, 1, 2):\n            os.close(fd)\n\n        # Make us a daemon. Yes, that's all it takes.\n        pid = os.fork()\n        if pid > 0:\n            os._exit(0)\n        os.setsid()\n        os.chdir(\"/\")\n        pid = os.fork()\n        if pid > 0:\n            os._exit(0)\n\n        # Start the command\n        cmd = to_text(cmd, errors='surrogate_or_strict')\n        cmd = [to_bytes(c, errors='surrogate_or_strict') for c in shlex.split(cmd)]\n        # In either of the above cases, pass a list of byte strings to Popen\n\n        # chkconfig localizes messages and we're screen scraping so make\n        # sure we use the C locale\n        p = subprocess.Popen(cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=lang_env, preexec_fn=lambda: os.close(pipe[1]))\n        stdout = b\"\"\n        stderr = b\"\"\n        fds = [p.stdout, p.stderr]\n        # Wait for all output, or until the main process is dead and its output is done.\n        while fds:\n            rfd, wfd, efd = select.select(fds, [], fds, 1)\n            if not (rfd + wfd + efd) and p.poll() is not None:\n                break\n            if p.stdout in rfd:\n                dat = os.read(p.stdout.fileno(), 4096)\n                if not dat:\n                    fds.remove(p.stdout)\n                stdout += dat\n            if p.stderr in rfd:\n                dat = os.read(p.stderr.fileno(), 4096)\n                if not dat:\n                    fds.remove(p.stderr)\n                stderr += dat\n        p.wait()\n        # Return a JSON blob to parent\n        blob = json.dumps([p.returncode, to_text(stdout), to_text(stderr)])\n        os.write(pipe[1], to_bytes(blob, errors='surrogate_or_strict'))\n        os.close(pipe[1])\n        os._exit(0)\n    elif pid == -1:\n        self.module.fail_json(msg=\"unable to fork\")\n    else:\n        os.close(pipe[1])\n        os.waitpid(pid, 0)\n        # Wait for data from daemon process and process it.\n        data = b\"\"\n        while True:\n            rfd, wfd, efd = select.select([pipe[0]], [], [pipe[0]])\n            if pipe[0] in rfd:\n                dat = os.read(pipe[0], 4096)\n                if not dat:\n                    break\n                data += dat\n        return json.loads(to_text(data, errors='surrogate_or_strict'))", "loc": 87}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "Service", "function_name": "check_ps", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["platform.system", "psout.split", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_ps(self):\n    # Set ps flags\n    if platform.system() == 'SunOS':\n        psflags = '-ef'\n    else:\n        psflags = 'auxww'\n\n    # Find ps binary\n    psbin = self.module.get_bin_path('ps', True)\n\n    (rc, psout, pserr) = self.execute_command('%s %s' % (psbin, psflags))\n    # If rc is 0, set running as appropriate\n    if rc == 0:\n        self.running = False\n        lines = psout.split(\"\\n\")\n        for line in lines:\n            if self.pattern in line and \"pattern=\" not in line:\n                # so as to not confuse ./hacking/test-module.py\n                self.running = True\n                break", "loc": 20}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "Service", "function_name": "check_service_changed", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.exit_json", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_service_changed(self):\n    if self.state and self.running is None:\n        self.module.fail_json(msg=\"failed determining service state, possible typo of service name?\")\n    # Find out if state has changed\n    if not self.running and self.state in [\"reloaded\", \"started\"]:\n        self.svc_change = True\n    elif self.running and self.state in [\"reloaded\", \"stopped\"]:\n        self.svc_change = True\n    elif self.state == \"restarted\":\n        self.svc_change = True\n    if self.module.check_mode and self.svc_change:\n        self.module.exit_json(changed=True, msg='service state changed')", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "Service", "function_name": "modify_service_state", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.exit_json", "self.service_control"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_service_state(self):\n\n    # Only do something if state will change\n    if self.svc_change:\n        # Control service\n        if self.state in ['started']:\n            self.action = \"start\"\n        elif not self.running and self.state == 'reloaded':\n            self.action = \"start\"\n        elif self.state == 'stopped':\n            self.action = \"stop\"\n        elif self.state == 'reloaded':\n            self.action = \"reload\"\n        elif self.state == 'restarted':\n            self.action = \"restart\"\n\n        if self.module.check_mode:\n            self.module.exit_json(changed=True, msg='changing service state')\n\n        return self.service_control()\n\n    else:\n        # If nothing needs to change just say all is well\n        rc = 0\n        err = ''\n        out = ''\n        return rc, out, err", "loc": 27}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "LinuxService", "function_name": "get_systemd_service_enabled", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "glob.glob", "os.access", "out.startswith", "self.execute_command", "sysv_exists", "sysv_is_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_systemd_service_enabled(self):\n    def sysv_exists(name):\n        script = '/etc/init.d/' + name\n        return os.access(script, os.X_OK)\n\n    def sysv_is_enabled(name):\n        return bool(glob.glob('/etc/rc?.d/S??' + name))\n\n    service_name = self.__systemd_unit\n    (rc, out, err) = self.execute_command(\"%s is-enabled %s\" % (self.enable_cmd, service_name,))\n    if rc == 0:\n        return True\n    elif out.startswith('disabled'):\n        return False\n    elif sysv_exists(service_name):\n        return sysv_is_enabled(service_name)\n    else:\n        return False", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "LinuxService", "function_name": "get_systemd_status_dict", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "line.rstrip", "line.rstrip().endswith", "line.split", "out.splitlines", "self.execute_command", "self.module.fail_json", "value.lstrip", "value.lstrip().startswith", "value.rstrip", "value.rstrip().endswith", "value_buffer.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_systemd_status_dict(self):\n\n    # Check status first as show will not fail if service does not exist\n    (rc, out, err) = self.execute_command(\"%s show '%s'\" % (self.enable_cmd, self.__systemd_unit,))\n    if rc != 0:\n        self.module.fail_json(msg='failure %d running systemctl show for %r: %s' % (rc, self.__systemd_unit, err))\n    elif 'LoadState=not-found' in out:\n        self.module.fail_json(msg='systemd could not find the requested service \"%r\": %s' % (self.__systemd_unit, err))\n\n    key = None\n    value_buffer = []\n    status_dict = {}\n    for line in out.splitlines():\n        if '=' in line:\n            if not key:\n                key, value = line.split('=', 1)\n                # systemd fields that are shell commands can be multi-line\n                # We take a value that begins with a \"{\" as the start of\n                # a shell command and a line that ends with \"}\" as the end of\n                # the command\n                if value.lstrip().startswith('{'):\n                    if value.rstrip().endswith('}'):\n                        status_dict[key] = value\n                        key = None\n                    else:\n                        value_buffer.append(value)\n                else:\n                    status_dict[key] = value\n                    key = None\n            else:\n                if line.rstrip().endswith('}'):\n                    status_dict[key] = '\\n'.join(value_buffer)\n                    key = None\n                else:\n                    value_buffer.append(value)\n        else:\n            value_buffer.append(value)\n\n    return status_dict", "loc": 39}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "LinuxService", "function_name": "get_systemd_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["d.get", "self.get_systemd_status_dict", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_systemd_service_status(self):\n    d = self.get_systemd_status_dict()\n    if d.get('ActiveState') == 'active':\n        # run-once services (for which a single successful exit indicates\n        # that they are running as designed) should not be restarted here.\n        # Thus, we are not checking d['SubState'].\n        self.running = True\n        self.crashed = False\n    elif d.get('ActiveState') == 'failed':\n        self.running = False\n        self.crashed = True\n    elif d.get('ActiveState') is None:\n        self.module.fail_json(msg='No ActiveState value in systemctl show output for %r' % (self.__systemd_unit,))\n    else:\n        self.running = False\n        self.crashed = False\n    return self.running", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "LinuxService", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.get_systemd_service_status", "self.name.lower", "self.service_control", "self.svc_cmd.endswith", "status_stdout.count", "status_stdout.lower", "status_stdout.lower().replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    if self.svc_cmd and self.svc_cmd.endswith('systemctl'):\n        return self.get_systemd_service_status()\n\n    self.action = \"status\"\n    rc, status_stdout, status_stderr = self.service_control()\n\n    # if we have decided the service is managed by upstart, we check for some additional output...\n    if self.svc_initctl and self.running is None:\n        # check the job status by upstart response\n        initctl_rc, initctl_status_stdout, initctl_status_stderr = self.execute_command(\"%s status %s %s\" % (self.svc_initctl, self.name, self.arguments))\n        if \"stop/waiting\" in initctl_status_stdout:\n            self.running = False\n        elif \"start/running\" in initctl_status_stdout:\n            self.running = True\n\n    if self.svc_cmd and self.svc_cmd.endswith(\"rc-service\") and self.running is None:\n        openrc_rc, openrc_status_stdout, openrc_status_stderr = self.execute_command(\"%s %s status\" % (self.svc_cmd, self.name))\n        self.running = \"started\" in openrc_status_stdout\n        self.crashed = \"crashed\" in openrc_status_stderr\n\n    # Prefer a non-zero return code. For reference, see:\n    # http://refspecs.linuxbase.org/LSB_4.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html\n    if self.running is None and rc in [1, 2, 3, 4, 69]:\n        self.running = False\n\n    # if the job status is still not known check it by status output keywords\n    # Only check keywords if there's only one line of output (some init\n    # scripts will output verbosely in case of error and those can emit\n    # keywords that are picked up as false positives\n    if self.running is None and status_stdout.count('\\n') <= 1:\n        # first transform the status output that could irritate keyword matching\n        cleanout = status_stdout.lower().replace(self.name.lower(), '')\n        if \"stop\" in cleanout:\n            self.running = False\n        elif \"run\" in cleanout:\n            self.running = not (\"not \" in cleanout)\n        elif \"start\" in cleanout and \"not \" not in cleanout:\n            self.running = True\n        elif 'could not access pid file' in cleanout:\n            self.running = False\n        elif 'is dead and pid file exists' in cleanout:\n            self.running = False\n        elif 'dead but subsys locked' in cleanout:\n            self.running = False\n        elif 'dead but pid file exists' in cleanout:\n            self.running = False\n\n    # if the job status is still not known and we got a zero for the\n    # return code, assume here that the service is running\n    if self.running is None and rc == 0:\n        self.running = True\n\n    # if the job status is still not known check it by special conditions\n    if self.running is None:\n        if self.name == 'iptables' and \"ACCEPT\" in status_stdout:\n            # iptables status command output is lame\n            # TODO: lookup if we can use a return code for this instead?\n            self.running = True\n\n    return self.running", "loc": 61}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "LinuxService", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.svc_cmd.endswith", "time.sleep"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n\n    # Decide what command to run\n    svc_cmd = ''\n    arguments = self.arguments\n    if self.svc_cmd:\n        if not self.svc_cmd.endswith(\"systemctl\"):\n            if self.svc_cmd.endswith(\"initctl\"):\n                # initctl commands take the form <cmd> <action> <name>\n                svc_cmd = self.svc_cmd\n                arguments = \"%s %s\" % (self.name, arguments)\n            else:\n                # SysV and OpenRC take the form <cmd> <name> <action>\n                svc_cmd = \"%s %s\" % (self.svc_cmd, self.name)\n        else:\n            # systemd commands take the form <cmd> <action> <name>\n            svc_cmd = self.svc_cmd\n            arguments = \"%s %s\" % (self.__systemd_unit, arguments)\n    elif self.svc_cmd is None and self.svc_initscript:\n        # upstart\n        svc_cmd = \"%s\" % self.svc_initscript\n\n    # In OpenRC, if a service crashed, we need to reset its status to\n    # stopped with the zap command, before we can start it back.\n    if self.svc_cmd and self.svc_cmd.endswith('rc-service') and self.action == 'start' and self.crashed:\n        self.execute_command(\"%s zap\" % svc_cmd, daemonize=True)\n\n    if self.action != \"restart\":\n        if svc_cmd != '':\n            # upstart or systemd or OpenRC\n            rc_state, stdout, stderr = self.execute_command(\"%s %s %s\" % (svc_cmd, self.action, arguments), daemonize=True)\n        else:\n            # SysV\n            rc_state, stdout, stderr = self.execute_command(\"%s %s %s\" % (self.action, self.name, arguments), daemonize=True)\n    elif self.svc_cmd and self.svc_cmd.endswith('rc-service'):\n        # All services in OpenRC support restart.\n        rc_state, stdout, stderr = self.execute_command(\"%s %s %s\" % (svc_cmd, self.action, arguments), daemonize=True)\n    else:\n        # In other systems, not all services support restart. Do it the hard way.\n        if svc_cmd != '':\n            # upstart or systemd\n            rc1, stdout1, stderr1 = self.execute_command(\"%s %s %s\" % (svc_cmd, 'stop', arguments), daemonize=True)\n        else:\n            # SysV\n            rc1, stdout1, stderr1 = self.execute_command(\"%s %s %s\" % ('stop', self.name, arguments), daemonize=True)\n\n        if self.sleep:\n            time.sleep(self.sleep)\n\n        if svc_cmd != '':\n            # upstart or systemd\n            rc2, stdout2, stderr2 = self.execute_command(\"%s %s %s\" % (svc_cmd, 'start', arguments), daemonize=True)\n        else:\n            # SysV\n            rc2, stdout2, stderr2 = self.execute_command(\"%s %s %s\" % ('start', self.name, arguments), daemonize=True)\n\n        # merge return information\n        if rc1 != 0 and rc2 == 0:\n            rc_state = rc2\n            stdout = stdout2\n            stderr = stderr2\n        else:\n            rc_state = rc1 + rc2\n            stdout = stdout1 + stdout2\n            stderr = stderr1 + stderr2\n\n    return (rc_state, stdout, stderr)", "loc": 67}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "FreeBsdService", "function_name": "get_service_tools", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_tools(self):\n    self.svc_cmd = self.module.get_bin_path('service', True)\n    if not self.svc_cmd:\n        self.module.fail_json(msg='unable to find service binary')\n\n    self.sysrc_cmd = self.module.get_bin_path('sysrc')", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "FreeBsdService", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    rc, stdout, stderr = self.execute_command(\"%s %s %s %s\" % (self.svc_cmd, self.arguments, self.name, 'onestatus'))\n    if self.name == \"pf\":\n        self.running = \"Enabled\" in stdout\n    else:\n        if rc == 1:\n            self.running = False\n        elif rc == 0:\n            self.running = True", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "FreeBsdService", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "time.sleep"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n\n    if self.action == \"start\":\n        self.action = \"onestart\"\n    if self.action == \"stop\":\n        self.action = \"onestop\"\n    if self.action == \"reload\":\n        self.action = \"onereload\"\n\n    ret = self.execute_command(\"%s %s %s %s\" % (self.svc_cmd, self.arguments, self.name, self.action))\n\n    if self.sleep:\n        time.sleep(self.sleep)\n\n    return ret", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "OpenBsdService", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    if self.enable_cmd:\n        rc, stdout, stderr = self.execute_command(\"%s %s %s\" % (self.svc_cmd, 'check', self.name))\n    else:\n        rc, stdout, stderr = self.execute_command(\"%s %s\" % (self.svc_cmd, 'check'))\n\n    if stderr:\n        self.module.fail_json(msg=stderr)\n\n    if rc == 1:\n        self.running = False\n    elif rc == 0:\n        self.running = True", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "OpenBsdService", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n    if self.enable_cmd:\n        return self.execute_command(\"%s -f %s %s\" % (self.svc_cmd, self.action, self.name), daemonize=True)\n    else:\n        return self.execute_command(\"%s -f %s\" % (self.svc_cmd, self.action))", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "OpenBsdService", "function_name": "service_enable", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.module.fail_json", "super", "super(OpenBsdService, self).service_enable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_enable(self):\n\n    if not self.enable_cmd:\n        return super(OpenBsdService, self).service_enable()\n\n    rc, stdout, stderr = self.execute_command(\"%s %s %s %s\" % (self.enable_cmd, 'get', self.name, 'status'))\n\n    status_action = None\n    if self.enable:\n        if rc != 0:\n            status_action = \"on\"\n    elif self.enable is not None:\n        # should be explicit False at this point\n        if rc != 1:\n            status_action = \"off\"\n\n    if status_action is not None:\n        self.changed = True\n        if not self.module.check_mode:\n            rc, stdout, stderr = self.execute_command(\"%s set %s status %s\" % (self.enable_cmd, self.name, status_action))\n\n            if rc != 0:\n                if stderr:\n                    self.module.fail_json(msg=stderr)\n                else:\n                    self.module.fail_json(msg=\"rcctl failed to modify service status\")", "loc": 26}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "NetBsdService", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    self.svc_cmd = \"%s\" % self.svc_initscript\n    rc, stdout, stderr = self.execute_command(\"%s %s\" % (self.svc_cmd, 'onestatus'))\n    if rc == 1:\n        self.running = False\n    elif rc == 0:\n        self.running = True", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "NetBsdService", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n    if self.action == \"start\":\n        self.action = \"onestart\"\n    if self.action == \"stop\":\n        self.action = \"onestop\"\n\n    self.svc_cmd = \"%s\" % self.svc_initscript\n    return self.execute_command(\"%s %s\" % (self.svc_cmd, self.action), daemonize=True)", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "SunOSService", "function_name": "get_service_tools", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.get_bin_path", "self.svcadm_supports_sync"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_tools(self):\n    self.svcs_cmd = self.module.get_bin_path('svcs', True)\n\n    if not self.svcs_cmd:\n        self.module.fail_json(msg='unable to find svcs binary')\n\n    self.svcadm_cmd = self.module.get_bin_path('svcadm', True)\n\n    if not self.svcadm_cmd:\n        self.module.fail_json(msg='unable to find svcadm binary')\n\n    if self.svcadm_supports_sync():\n        self.svcadm_sync = '-s'\n    else:\n        self.svcadm_sync = ''", "loc": 15}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "SunOSService", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_sunos_svcs_status"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    status = self.get_sunos_svcs_status()\n    # Only 'online' is considered properly running. Everything else is off\n    # or has some sort of problem.\n    if status == 'online':\n        self.running = True\n    else:\n        self.running = False", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "SunOSService", "function_name": "get_sunos_svcs_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lines[-1].split", "self.execute_command", "self.module.fail_json", "stdout.rstrip", "stdout.rstrip('\\n').split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_sunos_svcs_status(self):\n    rc, stdout, stderr = self.execute_command(\"%s %s\" % (self.svcs_cmd, self.name))\n    if rc == 1:\n        if stderr:\n            self.module.fail_json(msg=stderr)\n        else:\n            self.module.fail_json(msg=stdout)\n\n    lines = stdout.rstrip(\"\\n\").split(\"\\n\")\n    status = lines[-1].split(\" \")[0]\n    # status is one of: online, offline, degraded, disabled, maintenance, uninitialized\n    # see man svcs(1)\n    return status", "loc": 13}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "SunOSService", "function_name": "service_enable", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.startswith", "self.execute_command", "self.module.fail_json", "stdout.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_enable(self):\n    # Get current service enablement status\n    rc, stdout, stderr = self.execute_command(\"%s -l %s\" % (self.svcs_cmd, self.name))\n\n    if rc != 0:\n        if stderr:\n            self.module.fail_json(msg=stderr)\n        else:\n            self.module.fail_json(msg=stdout)\n\n    enabled = False\n    temporary = False\n\n    # look for enabled line, which could be one of:\n    #    enabled   true (temporary)\n    #    enabled   false (temporary)\n    #    enabled   true\n    #    enabled   false\n    for line in stdout.split(\"\\n\"):\n        if line.startswith(\"enabled\"):\n            if \"true\" in line:\n                enabled = True\n            if \"temporary\" in line:\n                temporary = True\n\n    startup_enabled = (enabled and not temporary) or (not enabled and temporary)\n\n    if self.enable and startup_enabled:\n        return\n    elif (not self.enable) and (not startup_enabled):\n        return\n\n    if not self.module.check_mode:\n        # Mark service as started or stopped (this will have the side effect of\n        # actually stopping or starting the service)\n        if self.enable:\n            subcmd = \"enable -rs\"\n        else:\n            subcmd = \"disable -s\"\n\n        rc, stdout, stderr = self.execute_command(\"%s %s %s\" % (self.svcadm_cmd, subcmd, self.name))\n\n        if rc != 0:\n            if stderr:\n                self.module.fail_json(msg=stderr)\n            else:\n                self.module.fail_json(msg=stdout)\n\n    self.changed = True", "loc": 49}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "SunOSService", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute_command", "self.get_sunos_svcs_status", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n    status = self.get_sunos_svcs_status()\n\n    # if starting or reloading, clear maintenance states\n    if self.action in ['start', 'reload', 'restart'] and status in ['maintenance', 'degraded']:\n        rc, stdout, stderr = self.execute_command(\"%s clear %s\" % (self.svcadm_cmd, self.name))\n        if rc != 0:\n            return rc, stdout, stderr\n        status = self.get_sunos_svcs_status()\n\n    if status in ['maintenance', 'degraded']:\n        self.module.fail_json(msg=\"Failed to bring service out of %s status.\" % status)\n\n    if self.action == 'start':\n        subcmd = \"enable -rst\"\n    elif self.action == 'stop':\n        subcmd = \"disable -st\"\n    elif self.action == 'reload':\n        subcmd = \"refresh %s\" % (self.svcadm_sync)\n    elif self.action == 'restart' and status == 'online':\n        subcmd = \"restart %s\" % (self.svcadm_sync)\n    elif self.action == 'restart' and status != 'online':\n        subcmd = \"enable -rst\"\n\n    return self.execute_command(\"%s %s %s\" % (self.svcadm_cmd, subcmd, self.name))", "loc": 25}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "AIX", "function_name": "get_service_tools", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.fail_json", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_tools(self):\n    self.lssrc_cmd = self.module.get_bin_path('lssrc', True)\n\n    if not self.lssrc_cmd:\n        self.module.fail_json(msg='unable to find lssrc binary')\n\n    self.startsrc_cmd = self.module.get_bin_path('startsrc', True)\n\n    if not self.startsrc_cmd:\n        self.module.fail_json(msg='unable to find startsrc binary')\n\n    self.stopsrc_cmd = self.module.get_bin_path('stopsrc', True)\n\n    if not self.stopsrc_cmd:\n        self.module.fail_json(msg='unable to find stopsrc binary')\n\n    self.refresh_cmd = self.module.get_bin_path('refresh', True)\n\n    if not self.refresh_cmd:\n        self.module.fail_json(msg='unable to find refresh binary')", "loc": 20}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "AIX", "function_name": "get_service_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_aix_src_status"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_service_status(self):\n    status = self.get_aix_src_status()\n    # Only 'active' is considered properly running. Everything else is off\n    # or has some sort of problem.\n    if status == 'active':\n        self.running = True\n    else:\n        self.running = False", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "AIX", "function_name": "get_aix_src_status", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lines[-1].split", "self.execute_command", "self.module.fail_json", "state.split", "state.split()[-1].strip", "stdout.rstrip", "stdout.rstrip('\\n').split", "stdout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_aix_src_status(self):\n    # Check subsystem status\n    rc, stdout, stderr = self.execute_command(\"%s -s %s\" % (self.lssrc_cmd, self.name))\n    if rc == 1:\n        # If check for subsystem is not ok, check if service name is a\n        # group subsystem\n        rc, stdout, stderr = self.execute_command(\"%s -g %s\" % (self.lssrc_cmd, self.name))\n        if rc == 1:\n            if stderr:\n                self.module.fail_json(msg=stderr)\n            else:\n                self.module.fail_json(msg=stdout)\n        else:\n            # Check all subsystem status, if one subsystem is not active\n            # the group is considered not active.\n            lines = stdout.splitlines()\n            for state in lines[1:]:\n                if state.split()[-1].strip() != \"active\":\n                    status = state.split()[-1].strip()\n                    break\n            else:\n                status = \"active\"\n\n            # status is one of: active, inoperative\n            return status\n    else:\n        lines = stdout.rstrip(\"\\n\").split(\"\\n\")\n        status = lines[-1].split(\" \")[-1]\n\n        # status is one of: active, inoperative\n        return status", "loc": 31}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": "AIX", "function_name": "service_control", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["groups.append", "line.split", "line.split()[0].strip", "line.split()[1].strip", "self.execute_command", "self.module.fail_json", "stdout.splitlines", "subsystems.append", "time.sleep"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def service_control(self):\n\n    # Check if service name is a subsystem of a group subsystem\n    rc, stdout, stderr = self.execute_command(\"%s -a\" % (self.lssrc_cmd))\n    if rc == 1:\n        if stderr:\n            self.module.fail_json(msg=stderr)\n        else:\n            self.module.fail_json(msg=stdout)\n    else:\n        lines = stdout.splitlines()\n        subsystems = []\n        groups = []\n        for line in lines[1:]:\n            subsystem = line.split()[0].strip()\n            group = line.split()[1].strip()\n            subsystems.append(subsystem)\n            if group:\n                groups.append(group)\n\n        # Define if service name parameter:\n        # -s subsystem or -g group subsystem\n        if self.name in subsystems:\n            srccmd_parameter = \"-s\"\n        elif self.name in groups:\n            srccmd_parameter = \"-g\"\n\n    if self.action == 'start':\n        srccmd = self.startsrc_cmd\n    elif self.action == 'stop':\n        srccmd = self.stopsrc_cmd\n    elif self.action == 'reload':\n        srccmd = self.refresh_cmd\n    elif self.action == 'restart':\n        self.execute_command(\"%s %s %s\" % (self.stopsrc_cmd, srccmd_parameter, self.name))\n        if self.sleep:\n            time.sleep(self.sleep)\n        srccmd = self.startsrc_cmd\n\n    if self.arguments and self.action in ('start', 'restart'):\n        return self.execute_command(\"%s -a \\\"%s\\\" %s %s\" % (srccmd, self.arguments, srccmd_parameter, self.name))\n    else:\n        return self.execute_command(\"%s %s %s\" % (srccmd, srccmd_parameter, self.name))", "loc": 43}
{"file": "ansible\\lib\\ansible\\modules\\service.py", "class_name": null, "function_name": "check_systemd", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_systemd_managed", "location.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_systemd():\n\n    # tools must be installed\n    if location.get('systemctl', False):\n        return is_systemd_managed(self.module)\n    return False", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "all_services.update", "dict", "get_best_parsable_locale", "len", "module.exit_json", "svc_module", "svcmod.gather_services", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(argument_spec=dict(), supports_check_mode=True)\n    locale = get_best_parsable_locale(module)\n    module.run_command_environ_update = dict(LANG=locale, LC_ALL=locale)\n\n    if sys.platform.startswith('freebsd'):\n        # frebsd is not compatible but will match other classes\n        service_modules = (FreeBSDScanService,)\n    else:\n        service_modules = (ServiceScanService, SystemctlScanService, AIXScanService, OpenBSDScanService)\n\n    all_services = {}\n    for svc_module in service_modules:\n        svcmod = svc_module(module)\n        svc = svcmod.gather_services()\n        if svc:\n            all_services.update(svc)\n    if len(all_services) == 0:\n        results = dict(skipped=True, msg=\"Failed to find any services. This can be due to privileges or some other configuration issue.\")\n    else:\n        results = dict(ansible_facts=dict(services=all_services))\n    module.exit_json(**results)", "loc": 22}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "SystemctlScanService", "function_name": "gather_services", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._list_from_unit_files", "self._list_from_units", "self.module.get_bin_path", "self.systemd_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gather_services(self):\n\n    services = {}\n    if self.systemd_enabled():\n        systemctl_path = self.module.get_bin_path(\"systemctl\", opt_dirs=[\"/usr/bin\", \"/usr/local/bin\"])\n        if systemctl_path:\n            self._list_from_units(systemctl_path, services)\n            self._list_from_unit_files(systemctl_path, services)\n\n    return services", "loc": 10}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "AIXScanService", "function_name": "gather_services", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "platform.system", "self.module.get_bin_path", "self.module.run_command", "self.module.warn", "stdout.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gather_services(self):\n\n    services = {}\n    if platform.system() == 'AIX':\n        lssrc_path = self.module.get_bin_path(\"lssrc\")\n        if lssrc_path:\n            rc, stdout, stderr = self.module.run_command(\"%s -a\" % lssrc_path)\n            if rc != 0:\n                self.module.warn(\"lssrc could not retrieve service data (%s): %s\" % (rc, stderr))\n            else:\n                for line in stdout.split('\\n'):\n                    line_data = line.split()\n                    if len(line_data) < 2:\n                        continue  # Skipping because we expected more data\n                    if line_data[0] == \"Subsystem\":\n                        continue  # Skip header\n                    service_name = line_data[0]\n                    if line_data[-1] == \"active\":\n                        service_state = \"running\"\n                    elif line_data[-1] == \"inoperative\":\n                        service_state = \"stopped\"\n                    else:\n                        service_state = \"unknown\"\n                    services[service_name] = {\"name\": service_name, \"state\": service_state, \"source\": \"src\"}\n    return services", "loc": 25}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "OpenBSDScanService", "function_name": "query_rcctl", "parameters": ["self", "cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.run_command", "self.module.warn", "stderr.lower", "stdout.split", "svcs.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def query_rcctl(self, cmd):\n    svcs = []\n    rc, stdout, stderr = self.module.run_command(\"%s ls %s\" % (self.rcctl_path, cmd))\n    if 'needs root privileges' in stderr.lower():\n        self.module.warn('rcctl requires root privileges')\n    else:\n        for svc in stdout.split('\\n'):\n            if svc == '':\n                continue\n            else:\n                svcs.append(svc)\n    return svcs", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "OpenBSDScanService", "function_name": "get_info", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.run_command", "self.module.warn", "stderr.lower", "stdout.split", "variable.replace", "variable.replace(undy, '', 1).split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_info(self, name):\n    info = {}\n    rc, stdout, stderr = self.module.run_command(\"%s get %s\" % (self.rcctl_path, name))\n    if 'needs root privileges' in stderr.lower():\n        self.module.warn('rcctl requires root privileges')\n    else:\n        undy = '%s_' % name\n        for variable in stdout.split('\\n'):\n            if variable == '' or '=' not in variable:\n                continue\n            else:\n                k, v = variable.replace(undy, '', 1).split('=', 1)\n                info[k] = v\n    return info", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "OpenBSDScanService", "function_name": "gather_services", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_info", "self.module.get_bin_path", "self.query_rcctl", "services.keys", "services[svc].get", "services[svc].update"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gather_services(self):\n\n    services = {}\n    self.rcctl_path = self.module.get_bin_path(\"rcctl\")\n    if self.rcctl_path:\n\n        # populate services will all possible\n        for svc in self.query_rcctl('all'):\n            services[svc] = {'name': svc, 'source': 'rcctl', 'rogue': False}\n            services[svc].update(self.get_info(svc))\n\n        for svc in self.query_rcctl('on'):\n            services[svc].update({'status': 'enabled'})\n\n        for svc in self.query_rcctl('started'):\n            services[svc].update({'state': 'running'})\n\n        # Override the state for services which are marked as 'failed'\n        for svc in self.query_rcctl('failed'):\n            services[svc].update({'state': 'failed'})\n\n        for svc in services.keys():\n            # Based on the list of services that are enabled/failed, determine which are disabled\n            if services[svc].get('status') is None:\n                services[svc].update({'status': 'disabled'})\n\n            # and do the same for those are aren't running\n            if services[svc].get('state') is None:\n                services[svc].update({'state': 'stopped'})\n\n        for svc in self.query_rcctl('rogue'):\n            services[svc]['rogue'] = True\n\n    return services", "loc": 34}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "FreeBSDScanService", "function_name": "get_info", "parameters": ["self", "service"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["p.match", "re.compile", "self.module.run_command", "self.module.warn", "stderr.splitlines", "stdout.splitlines"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_info(self, service):\n\n    service_info = {'status': 'unknown'}\n    rc, stdout, stderr = self.module.run_command(\"%s %s describe\" % (self.service, service))\n    if rc == 0:\n        service_info['description'] = stdout\n        rc, stdout, stderr = self.module.run_command(\"%s %s status\" % (self.service, service))\n        if rc == 0:\n            service_info['status'] = 'running'\n            p = re.compile(r'^\\s?%s is running as pid (\\d+).' % service)\n            matches = p.match(stdout[0])\n            if matches:\n                # does not always get pid output\n                service_info['pid'] = matches[0]\n            else:\n                service_info['pid'] = 'N/A'\n        elif rc == 1:\n            if stdout and 'is not running' in stdout.splitlines()[0]:\n                service_info['status'] = 'stopped'\n            elif stderr and 'unknown directive' in stderr.splitlines()[0]:\n                service_info['status'] = 'unknown'\n                self.module.warn('Status query not supported for %s' % service)\n            else:\n                service_info['status'] = 'unknown'\n                out = stderr if stderr else stdout\n                self.module.warn('Could not retrieve status for %s: %s' % (service, out))\n    else:\n        out = stderr if stderr else stdout\n        self.module.warn(\"Failed to get info for %s, no system message (rc=%s): %s\" % (service, rc, out))\n\n    return service_info", "loc": 31}
{"file": "ansible\\lib\\ansible\\modules\\service_facts.py", "class_name": "FreeBSDScanService", "function_name": "gather_services", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_enabled", "self.get_info", "self.module.get_bin_path", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gather_services(self):\n\n    services = {}\n    if sys.platform.startswith('freebsd'):\n        self.service = self.module.get_bin_path(\"service\")\n        if self.service:\n            for svc in self.get_enabled():\n                services[svc] = self.get_info(svc)\n    return services", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\setup.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "PrefixFactNamespace", "ansible_collector.get_ansible_collector", "dict", "fact_collector.collect", "frozenset", "module.exit_json", "module.fail_json", "to_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            gather_subset=dict(default=[\"all\"], required=False, type='list', elements='str'),\n            gather_timeout=dict(default=10, required=False, type='int'),\n            filter=dict(default=[], required=False, type='list', elements='str'),\n            fact_path=dict(default='/etc/ansible/facts.d', required=False, type='path'),\n        ),\n        supports_check_mode=True,\n    )\n\n    gather_subset = module.params['gather_subset']\n    gather_timeout = module.params['gather_timeout']\n    filter_spec = module.params['filter']\n\n    # TODO: this mimics existing behavior where gather_subset=[\"!all\"] actually means\n    #       to collect nothing except for the below list\n    # TODO: decide what '!all' means, I lean towards making it mean none, but likely needs\n    #       some tweaking on how gather_subset operations are performed\n    minimal_gather_subset = frozenset(['apparmor', 'caps', 'cmdline', 'date_time',\n                                       'distribution', 'dns', 'env', 'fips', 'local',\n                                       'lsb', 'pkg_mgr', 'platform', 'python', 'selinux',\n                                       'service_mgr', 'ssh_pub_keys', 'user'])\n\n    all_collector_classes = default_collectors.collectors\n\n    # rename namespace_name to root_key?\n    namespace = PrefixFactNamespace(namespace_name='ansible',\n                                    prefix='ansible_')\n\n    try:\n        fact_collector = ansible_collector.get_ansible_collector(all_collector_classes=all_collector_classes,\n                                                                 namespace=namespace,\n                                                                 filter_spec=filter_spec,\n                                                                 gather_subset=gather_subset,\n                                                                 gather_timeout=gather_timeout,\n                                                                 minimal_gather_subset=minimal_gather_subset)\n    except (TypeError, CollectorNotFoundError, CycleFoundInFactDeps, UnresolvedFactDep) as e:\n        # bad subset given, collector, idk, deps declared but not found\n        module.fail_json(msg=to_text(e))\n\n    facts_dict = fact_collector.collect(module=module)\n\n    module.exit_json(ansible_facts=facts_dict)", "loc": 44}
{"file": "ansible\\lib\\ansible\\modules\\stat.py", "class_name": null, "function_name": "format_output", "parameters": ["module", "path", "st"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "dict", "getattr", "hasattr", "stat.S_IMODE", "stat.S_ISBLK", "stat.S_ISCHR", "stat.S_ISDIR", "stat.S_ISFIFO", "stat.S_ISLNK", "stat.S_ISREG", "stat.S_ISSOCK"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_output(module, path, st):\n    mode = st.st_mode\n\n    # back to ansible\n    output = dict(\n        exists=True,\n        path=path,\n        mode=\"%04o\" % stat.S_IMODE(mode),\n        isdir=stat.S_ISDIR(mode),\n        ischr=stat.S_ISCHR(mode),\n        isblk=stat.S_ISBLK(mode),\n        isreg=stat.S_ISREG(mode),\n        isfifo=stat.S_ISFIFO(mode),\n        islnk=stat.S_ISLNK(mode),\n        issock=stat.S_ISSOCK(mode),\n        uid=st.st_uid,\n        gid=st.st_gid,\n        size=st.st_size,\n        inode=st.st_ino,\n        dev=st.st_dev,\n        nlink=st.st_nlink,\n        atime=st.st_atime,\n        mtime=st.st_mtime,\n        ctime=st.st_ctime,\n        wusr=bool(mode & stat.S_IWUSR),\n        rusr=bool(mode & stat.S_IRUSR),\n        xusr=bool(mode & stat.S_IXUSR),\n        wgrp=bool(mode & stat.S_IWGRP),\n        rgrp=bool(mode & stat.S_IRGRP),\n        xgrp=bool(mode & stat.S_IXGRP),\n        woth=bool(mode & stat.S_IWOTH),\n        roth=bool(mode & stat.S_IROTH),\n        xoth=bool(mode & stat.S_IXOTH),\n        isuid=bool(mode & stat.S_ISUID),\n        isgid=bool(mode & stat.S_ISGID),\n    )\n\n    # Platform dependent flags:\n    for other in [\n            # Some Linux\n            ('st_blocks', 'blocks'),\n            ('st_blksize', 'block_size'),\n            ('st_rdev', 'device_type'),\n            ('st_flags', 'flags'),\n            # Some Berkeley based\n            ('st_gen', 'generation'),\n            ('st_birthtime', 'birthtime'),\n            # RISCOS\n            ('st_ftype', 'file_type'),\n            ('st_attrs', 'attrs'),\n            ('st_obtype', 'object_type'),\n            # macOS\n            ('st_rsize', 'real_size'),\n            ('st_creator', 'creator'),\n            ('st_type', 'file_type'),\n    ]:\n        if hasattr(st, other[0]):\n            output[other[1]] = getattr(st, other[0])\n\n    return output", "loc": 60}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "checkout", "parameters": ["self", "force"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "self._exec"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Creates new svn working directory if it does not already exist.", "source_code": "def checkout(self, force=False):\n    \"\"\"Creates new svn working directory if it does not already exist.\"\"\"\n    cmd = [\"checkout\"]\n    if force:\n        cmd.append(\"--force\")\n    cmd.extend([\"-r\", self.revision, self.repo, self.dest])\n    self._exec(cmd)", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "export", "parameters": ["self", "force"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "self._exec"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Export svn repo to directory", "source_code": "def export(self, force=False):\n    \"\"\"Export svn repo to directory\"\"\"\n    cmd = [\"export\"]\n    if force:\n        cmd.append(\"--force\")\n    cmd.extend([\"-r\", self.revision, self.repo, self.dest])\n\n    self._exec(cmd)", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "switch", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "self._exec"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Change working directory's repo.", "source_code": "def switch(self):\n    \"\"\"Change working directory's repo.\"\"\"\n    # switch to ensure we are pointing at correct repo.\n    # it also updates!\n    output = self._exec([\"switch\", \"--revision\", self.revision, self.repo, self.dest])\n    for line in output:\n        if re.search(r'^[ABDUCGE]\\s', line):\n            return True\n    return False", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "update", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "self._exec"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Update existing svn working directory.", "source_code": "def update(self):\n    \"\"\"Update existing svn working directory.\"\"\"\n    output = self._exec([\"update\", \"-r\", self.revision, self.dest])\n\n    for line in output:\n        if re.search(r'^[ABDUCGE]\\s', line):\n            return True\n    return False", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "revert", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "self._exec"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Revert svn working directory.", "source_code": "def revert(self):\n    \"\"\"Revert svn working directory.\"\"\"\n    output = self._exec([\"revert\", \"-R\", self.dest])\n    for line in output:\n        if re.search(r'^Reverted ', line) is None:\n            return True\n    return False", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "get_revision", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "re.search", "rev.group", "self._exec", "url.group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Revision and URL of subversion working directory.", "source_code": "def get_revision(self):\n    \"\"\"Revision and URL of subversion working directory.\"\"\"\n    text = '\\n'.join(self._exec([\"info\", self.dest]))\n    rev = re.search(self.REVISION_RE, text, re.MULTILINE)\n    if rev:\n        rev = rev.group(0)\n    else:\n        rev = 'Unable to get revision'\n\n    url = re.search(r'^URL\\s?:.*$', text, re.MULTILINE)\n    if url:\n        url = url.group(0)\n    else:\n        url = 'Unable to get URL'\n\n    return rev, url", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "get_remote_revision", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "re.search", "rev.group", "self._exec"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Revision and URL of subversion working directory.", "source_code": "def get_remote_revision(self):\n    \"\"\"Revision and URL of subversion working directory.\"\"\"\n    text = '\\n'.join(self._exec([\"info\", self.repo]))\n    rev = re.search(self.REVISION_RE, text, re.MULTILINE)\n    if rev:\n        rev = rev.group(0)\n    else:\n        rev = 'Unable to get remote revision'\n    return rev", "loc": 9}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "has_local_mods", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter", "len", "list", "re.compile", "self._exec"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "True if revisioned files have been added or modified. Unrevisioned files are ignored.", "source_code": "def has_local_mods(self):\n    \"\"\"True if revisioned files have been added or modified. Unrevisioned files are ignored.\"\"\"\n    lines = self._exec([\"status\", \"--quiet\", \"--ignore-externals\", self.dest])\n    # The --quiet option will return only modified files.\n    # Match only revisioned files, i.e. ignore status '?'.\n    regex = re.compile(r'^[^?X]')\n    # Has local mods if more than 0 modified revisioned files.\n    return len(list(filter(regex.match, lines))) > 0", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\subversion.py", "class_name": "Subversion", "function_name": "needs_update", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "curr.split", "curr.split(':')[1].strip", "head.group", "head.split", "head.split(':')[1].strip", "int", "re.search", "self._exec", "self.get_revision"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def needs_update(self):\n    curr, url = self.get_revision()\n    out2 = '\\n'.join(self._exec([\"info\", \"-r\", self.revision, self.dest]))\n    head = re.search(self.REVISION_RE, out2, re.MULTILINE)\n    if head:\n        head = head.group(0)\n    else:\n        head = 'Unable to get revision'\n    rev1 = int(curr.split(':')[1].strip())\n    rev2 = int(head.split(':')[1].strip())\n    change = False\n    if rev1 < rev2:\n        change = True\n    return change, curr, head", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\systemd_service.py", "class_name": null, "function_name": "parse_systemctl_show", "parameters": ["lines"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "'\\n'.join(multival).strip", "k.startswith", "line.rstrip", "line.rstrip().endswith", "line.split", "multival.append", "v.lstrip", "v.lstrip().startswith", "v.rstrip", "v.rstrip().endswith", "v.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_systemctl_show(lines):\n    # The output of 'systemctl show' can contain values that span multiple lines. At first glance it\n    # appears that such values are always surrounded by {}, so the previous version of this code\n    # assumed that any value starting with { was a multi-line value; it would then consume lines\n    # until it saw a line that ended with }. However, it is possible to have a single-line value\n    # that starts with { but does not end with } (this could happen in the value for Description=,\n    # for example), and the previous version of this code would then consume all remaining lines as\n    # part of that value. Cryptically, this would lead to Ansible reporting that the service file\n    # couldn't be found.\n    #\n    # To avoid this issue, the following code only accepts multi-line values for keys whose names\n    # start with Exec (e.g., ExecStart=), since these are the only keys whose values are known to\n    # span multiple lines.\n    parsed = {}\n    multival = []\n    k = None\n    for line in lines:\n        if k is None:\n            if '=' in line:\n                k, v = line.split('=', 1)\n                if k.startswith('Exec') and v.lstrip().startswith('{'):\n                    if not v.rstrip().endswith('}'):\n                        multival.append(v)\n                        continue\n                parsed[k] = v.strip()\n                k = None\n        else:\n            multival.append(line)\n            if line.rstrip().endswith('}'):\n                parsed[k] = '\\n'.join(multival).strip()\n                multival = []\n                k = None\n    return parsed", "loc": 33}
{"file": "ansible\\lib\\ansible\\modules\\sysvinit.py", "class_name": null, "function_name": "runme", "parameters": ["doit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["daemonize", "module.fail_json", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def runme(doit):\n\n    args = module.params['arguments']\n    cmd = \"%s %s %s\" % (script, doit, \"\" if args is None else args)\n\n    # how to run\n    if module.params['daemonize']:\n        (rc, out, err) = daemonize(module, cmd)\n    else:\n        (rc, out, err) = module.run_command(cmd)\n    # FIXME: ERRORS\n\n    if rc != 0:\n        module.fail_json(msg=\"Failed to %s service: %s\" % (action, name), rc=rc, stdout=out, stderr=err)\n\n    return (rc, out, err)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\tempfile.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleModule", "close", "dict", "mkdtemp", "mkstemp", "module.exit_json", "module.fail_json", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    module = AnsibleModule(\n        argument_spec=dict(\n            state=dict(type='str', default='file', choices=['file', 'directory']),\n            path=dict(type='path'),\n            prefix=dict(type='str', default='ansible.'),\n            suffix=dict(type='str', default=''),\n        ),\n    )\n\n    try:\n        if module.params['state'] == 'file':\n            handle, path = mkstemp(\n                prefix=module.params['prefix'],\n                suffix=module.params['suffix'],\n                dir=module.params['path'],\n            )\n            close(handle)\n        else:\n            path = mkdtemp(\n                prefix=module.params['prefix'],\n                suffix=module.params['suffix'],\n                dir=module.params['path'],\n            )\n\n        module.exit_json(changed=True, path=path)\n    except Exception as e:\n        module.fail_json(msg=to_native(e))", "loc": 28}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": null, "function_name": "pick_handler", "parameters": ["src", "dest", "file_args", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "handler", "module.fail_json", "obj.can_handle_archive", "reasons.add", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pick_handler(src, dest, file_args, module):\n    handlers = [ZipArchive, ZipZArchive, TgzArchive, TarArchive, TarBzipArchive, TarXzArchive, TarZstdArchive]\n    reasons = set()\n    for handler in handlers:\n        obj = handler(src, dest, file_args, module)\n        (can_handle, reason) = obj.can_handle_archive()\n        if can_handle:\n            return obj\n        reasons.add(reason)\n    reason_msg = '\\n'.join(reasons)\n    module.fail_json(msg='Failed to find handler for \"%s\". Make sure the required command to extract the file is installed.\\n%s' % (src, reason_msg))", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "ZipArchive", "function_name": "files_in_archive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["UnarchiveError", "ZipFile", "archive.close", "archive.namelist", "e.args[0].lower", "e.args[0].lower().startswith", "fnmatch.fnmatch", "self._files_in_archive.append", "self._legacy_file_list", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def files_in_archive(self):\n    if self._files_in_archive:\n        return self._files_in_archive\n\n    self._files_in_archive = []\n    try:\n        archive = ZipFile(self.src)\n    except BadZipFile as e:\n        if e.args[0].lower().startswith('bad magic number'):\n            # Python2.4 can't handle zipfiles with > 64K files.  Try using\n            # /usr/bin/unzip instead\n            self._legacy_file_list()\n        else:\n            raise\n    else:\n        try:\n            for member in archive.namelist():\n                if self.include_files:\n                    for include in self.include_files:\n                        if fnmatch.fnmatch(member, include):\n                            self._files_in_archive.append(to_native(member))\n                else:\n                    exclude_flag = False\n                    if self.excludes:\n                        for exclude in self.excludes:\n                            if fnmatch.fnmatch(member, exclude):\n                                exclude_flag = True\n                                break\n                    if not exclude_flag:\n                        self._files_in_archive.append(to_native(member))\n        except Exception as e:\n            archive.close()\n            raise UnarchiveError('Unable to list files in the archive: %s' % to_native(e))\n\n        archive.close()\n    return self._files_in_archive", "loc": 36}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "ZipArchive", "function_name": "unarchive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "dict", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unarchive(self):\n    cmd = [self.cmd_path, '-o']\n    if self.opts:\n        cmd.extend(self.opts)\n    cmd.append(self.src)\n    # NOTE: Including (changed) files as arguments is problematic (limits on command line/arguments)\n    # if self.includes:\n    # NOTE: Command unzip has this strange behaviour where it expects quoted filenames to also be escaped\n    # cmd.extend(map(shell_escape, self.includes))\n    if self.excludes:\n        cmd.extend(['-x'] + self.excludes)\n    if self.include_files:\n        cmd.extend(self.include_files)\n    cmd.extend(['-d', self.b_dest])\n    rc, out, err = self.module.run_command(cmd)\n    return dict(cmd=cmd, rc=rc, out=out, err=err)", "loc": 16}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "ZipArchive", "function_name": "can_handle_archive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"' or '\".join", "\"Unable to find required '{missing}' binary in the path.\".format", "get_bin_path", "missing.append", "self.module.debug", "self.module.run_command", "setattr"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def can_handle_archive(self):\n    missing = []\n    for b in self.binaries:\n        try:\n            setattr(self, b[1], get_bin_path(b[0]))\n        except ValueError:\n            missing.append(b[0])\n\n    if missing:\n        return False, \"Unable to find required '{missing}' binary in the path.\".format(missing=\"' or '\".join(missing))\n\n    cmd = [self.cmd_path, '-l', self.src]\n    rc, out, err = self.module.run_command(cmd)\n    if rc == 0:\n        return True, None\n\n    self.module.debug(err)\n\n    return False, 'Command \"%s\" could not handle archive: %s' % (self.cmd_path, err)", "loc": 19}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "TgzArchive", "function_name": "files_in_archive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["UnarchiveError", "cmd.append", "cmd.extend", "codecs.escape_decode", "dict", "filename.startswith", "fnmatch.fnmatch", "get_best_parsable_locale", "out.splitlines", "self._files_in_archive.append", "self.module.debug", "self.module.run_command", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def files_in_archive(self):\n    if self._files_in_archive:\n        return self._files_in_archive\n\n    cmd = [self.cmd_path, '--list', '-C', self.b_dest]\n    if self.zipflag:\n        cmd.append(self.zipflag)\n    if self.opts:\n        cmd.extend(['--show-transformed-names'] + self.opts)\n    if self.excludes:\n        cmd.extend(['--exclude=' + f for f in self.excludes])\n    cmd.extend(['-f', self.src])\n    if self.include_files:\n        cmd.extend(self.include_files)\n\n    locale = get_best_parsable_locale(self.module)\n    rc, out, err = self.module.run_command(cmd, cwd=self.b_dest, environ_update=dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale, LANGUAGE=locale))\n    if rc != 0:\n        self.module.debug(err)\n        raise UnarchiveError('Unable to list files in the archive: %s' % err)\n\n    for filename in out.splitlines():\n        # Compensate for locale-related problems in gtar output (octal unicode representation) #11348\n        # filename = filename.decode('string_escape')\n        filename = to_native(codecs.escape_decode(filename)[0])\n\n        # We don't allow absolute filenames.  If the user wants to unarchive rooted in \"/\"\n        # they need to use \"dest: '/'\".  This follows the defaults for gtar, pax, etc.\n        # Allowing absolute filenames here also causes bugs: https://github.com/ansible/ansible/issues/21397\n        if filename.startswith('/'):\n            filename = filename[1:]\n\n        exclude_flag = False\n        if self.excludes:\n            for exclude in self.excludes:\n                if fnmatch.fnmatch(filename, exclude):\n                    exclude_flag = True\n                    break\n\n        if not exclude_flag:\n            self._files_in_archive.append(to_native(filename))\n\n    return self._files_in_archive", "loc": 43}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "TgzArchive", "function_name": "is_unarchived", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["EMPTY_FILE_RE.search", "GROUP_DIFF_RE.search", "MODE_DIFF_RE.search", "OWNER_DIFF_RE.search", "cmd.append", "cmd.extend", "dict", "err.splitlines", "get_best_parsable_locale", "old_out.splitlines", "os.getuid", "quote", "regex.search", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_unarchived(self):\n    cmd = [self.cmd_path, '--diff', '-C', self.b_dest]\n    if self.zipflag:\n        cmd.append(self.zipflag)\n    if self.opts:\n        cmd.extend(['--show-transformed-names'] + self.opts)\n    if self.file_args['owner']:\n        cmd.append('--owner=' + quote(self.file_args['owner']))\n    if self.file_args['group']:\n        cmd.append('--group=' + quote(self.file_args['group']))\n    if self.module.params['keep_newer']:\n        cmd.append('--keep-newer-files')\n    if self.excludes:\n        cmd.extend(['--exclude=' + f for f in self.excludes])\n    cmd.extend(['-f', self.src])\n    if self.include_files:\n        cmd.extend(self.include_files)\n    locale = get_best_parsable_locale(self.module)\n    rc, out, err = self.module.run_command(cmd, cwd=self.b_dest, environ_update=dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale, LANGUAGE=locale))\n\n    # Check whether the differences are in something that we're\n    # setting anyway\n\n    # What is different\n    unarchived = True\n    old_out = out\n    out = ''\n    run_uid = os.getuid()\n    # When unarchiving as a user, or when owner/group/mode is supplied --diff is insufficient\n    # Only way to be sure is to check request with what is on disk (as we do for zip)\n    # Leave this up to set_fs_attributes_if_different() instead of inducing a (false) change\n    for line in old_out.splitlines() + err.splitlines():\n        # FIXME: Remove the bogus lines from error-output as well !\n        # Ignore bogus errors on empty filenames (when using --split-component)\n        if EMPTY_FILE_RE.search(line):\n            continue\n        if run_uid == 0 and not self.file_args['owner'] and OWNER_DIFF_RE.search(line):\n            out += line + '\\n'\n        if run_uid == 0 and not self.file_args['group'] and GROUP_DIFF_RE.search(line):\n            out += line + '\\n'\n        if not self.file_args['mode'] and MODE_DIFF_RE.search(line):\n            out += line + '\\n'\n        differ_regexes = [\n            MOD_TIME_DIFF_RE, MISSING_FILE_RE, INVALID_OWNER_RE,\n            INVALID_GROUP_RE, SYMLINK_DIFF_RE, CONTENT_DIFF_RE,\n            SIZE_DIFF_RE\n        ]\n        for regex in differ_regexes:\n            if regex.search(line):\n                out += line + '\\n'\n\n    if out:\n        unarchived = False\n    return dict(unarchived=unarchived, rc=rc, out=out, err=err, cmd=cmd)", "loc": 54}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "TgzArchive", "function_name": "unarchive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "dict", "get_best_parsable_locale", "quote", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unarchive(self):\n    cmd = [self.cmd_path, '--extract', '-C', self.b_dest]\n    if self.zipflag:\n        cmd.append(self.zipflag)\n    if self.opts:\n        cmd.extend(['--show-transformed-names'] + self.opts)\n    if self.file_args['owner']:\n        cmd.append('--owner=' + quote(self.file_args['owner']))\n    if self.file_args['group']:\n        cmd.append('--group=' + quote(self.file_args['group']))\n    if self.module.params['keep_newer']:\n        cmd.append('--keep-newer-files')\n    if self.excludes:\n        cmd.extend(['--exclude=' + f for f in self.excludes])\n    cmd.extend(['-f', self.src])\n    if self.include_files:\n        cmd.extend(self.include_files)\n    locale = get_best_parsable_locale(self.module)\n    rc, out, err = self.module.run_command(cmd, cwd=self.b_dest, environ_update=dict(LANG=locale, LC_ALL=locale, LC_MESSAGES=locale, LANGUAGE=locale))\n    return dict(cmd=cmd, rc=rc, out=out, err=err)", "loc": 20}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "TgzArchive", "function_name": "can_handle_archive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_bin_path", "self._get_tar_type", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def can_handle_archive(self):\n    # Prefer gtar (GNU tar) as it supports the compression options -z, -j and -J\n    try:\n        self.cmd_path = get_bin_path('gtar')\n    except ValueError:\n        # Fallback to tar\n        try:\n            self.cmd_path = get_bin_path('tar')\n        except ValueError:\n            return False, \"Unable to find required 'gtar' or 'tar' binary in the path\"\n\n    self.tar_type = self._get_tar_type()\n\n    if self.tar_type != 'gnu':\n        return False, 'Command \"%s\" detected as tar type %s. GNU tar required.' % (self.cmd_path, self.tar_type)\n\n    try:\n        if self.files_in_archive:\n            return True, None\n    except UnarchiveError as e:\n        return False, 'Command \"%s\" could not handle archive: %s' % (self.cmd_path, to_native(e))\n    # Errors and no files in archive assume that we weren't able to\n    # properly unarchive it\n    return False, 'Command \"%s\" found no files in archive. Empty archive files are not supported.' % self.cmd_path", "loc": 24}
{"file": "ansible\\lib\\ansible\\modules\\unarchive.py", "class_name": "ZipZArchive", "function_name": "can_handle_archive", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["out.lower", "self.module.run_command", "super", "super(ZipZArchive, self).can_handle_archive"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def can_handle_archive(self):\n    unzip_available, error_msg = super(ZipZArchive, self).can_handle_archive()\n\n    if not unzip_available:\n        return unzip_available, error_msg\n\n    # Ensure unzip -Z is available before we use it in is_unarchive\n    cmd = [self.zipinfo_cmd_path, self.zipinfoflag]\n    rc, out, err = self.module.run_command(cmd)\n    if 'zipinfo' in out.lower():\n        return True, None\n    return False, 'Command \"unzip -Z\" could not handle archive: %s' % err", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\uri.py", "class_name": null, "function_name": "kv_list", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "data.items", "isinstance", "list"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert data into a list of key-value tuples", "source_code": "def kv_list(data):\n    \"\"\" Convert data into a list of key-value tuples \"\"\"\n    if data is None:\n        return None\n\n    if isinstance(data, Sequence):\n        return list(data)\n\n    if isinstance(data, Mapping):\n        return list(data.items())\n\n    raise TypeError('cannot form-urlencode body, expect list or dict')", "loc": 12}
{"file": "ansible\\lib\\ansible\\modules\\uri.py", "class_name": null, "function_name": "form_urlencoded", "parameters": ["body"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "kv_list", "result.append", "to_text", "urlencode"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Convert data into a form-urlencoded string", "source_code": "def form_urlencoded(body):\n    \"\"\" Convert data into a form-urlencoded string \"\"\"\n    if isinstance(body, str):\n        return body\n\n    if isinstance(body, (Mapping, Sequence)):\n        result = []\n        # Turn a list of lists into a list of tuples that urlencode accepts\n        for key, values in kv_list(body):\n            if isinstance(values, str) or not isinstance(values, (Mapping, Sequence)):\n                values = [values]\n            for value in values:\n                if value is not None:\n                    result.append((to_text(key), to_text(value)))\n        return urlencode(result, doseq=True)\n\n    return body", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "check_password_encrypted", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_HASH_RE.search", "any", "bool", "len", "self.module.params['password'].split", "self.module.warn", "set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_password_encrypted(self):\n    # Darwin needs cleartext password, so skip validation\n    if self.module.params['password'] and self.platform != 'Darwin':\n        maybe_invalid = False\n\n        # Allow setting certain passwords in order to disable the account\n        if self.module.params['password'] in set(['*', '!', '*************']):\n            maybe_invalid = False\n        else:\n            # : for delimiter, * for disable user, ! for lock user\n            # these characters are invalid in the password\n            if any(char in self.module.params['password'] for char in ':*!'):\n                maybe_invalid = True\n            if '$' not in self.module.params['password']:\n                maybe_invalid = True\n            else:\n                fields = self.module.params['password'].split(\"$\")\n                if len(fields) >= 3:\n                    # contains character outside the crypto constraint\n                    if bool(_HASH_RE.search(fields[-1])):\n                        maybe_invalid = True\n                    # md5\n                    if fields[1] == '1' and len(fields[-1]) != 22:\n                        maybe_invalid = True\n                    # sha256\n                    if fields[1] == '5' and len(fields[-1]) != 43:\n                        maybe_invalid = True\n                    # sha512\n                    if fields[1] == '6' and len(fields[-1]) != 86:\n                        maybe_invalid = True\n                    # yescrypt\n                    if fields[1] == 'y' and len(fields[-1]) != 43:\n                        maybe_invalid = True\n                else:\n                    maybe_invalid = True\n        if maybe_invalid:\n            self.module.warn(\"The input password appears not to have been hashed. \"\n                             \"The 'password' argument must be encrypted for this module to work properly.\")", "loc": 38}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "execute_command", "parameters": ["self", "cmd", "use_unsafe_shell", "data", "obey_checkmode"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.module.debug", "self.module.run_command", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def execute_command(self, cmd, use_unsafe_shell=False, data=None, obey_checkmode=True):\n    if self.module.check_mode and obey_checkmode:\n        self.module.debug('In check mode, would have run: \"%s\"' % cmd)\n        return (0, '', '')\n    else:\n        # cast all args to strings ansible-modules-core/issues/4397\n        cmd = [str(x) for x in cmd]\n        return self.module.run_command(cmd, use_unsafe_shell=use_unsafe_shell, data=data)", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "remove_user_userdel", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user_userdel(self):\n    if self.local:\n        command_name = 'luserdel'\n    else:\n        command_name = 'userdel'\n\n    cmd = [self.module.get_bin_path(command_name, True)]\n    if self.force and not self.local:\n        cmd.append('-f')\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n\n    return self.execute_command(cmd)", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "group_exists", "parameters": ["self", "group"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["grp.getgrgid", "grp.getgrnam", "int"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_exists(self, group):\n    try:\n        # Try group as a gid first\n        grp.getgrgid(int(group))\n        return True\n    except (ValueError, KeyError):\n        try:\n            grp.getgrnam(group)\n            return True\n        except KeyError:\n            return False", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "group_info", "parameters": ["self", "group"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["grp.getgrgid", "grp.getgrnam", "int", "list", "self.group_exists"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def group_info(self, group):\n    if not self.group_exists(group):\n        return False\n    try:\n        # Try group as a gid first\n        return list(grp.getgrgid(int(group)))\n    except (ValueError, KeyError):\n        return list(grp.getgrnam(group))", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "get_groups_set", "parameters": ["self", "remove_existing", "names_only"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["group_names.add", "groups.copy", "groups.remove", "self.group_exists", "self.group_info", "self.groups.split", "self.module.fail_json", "self.user_info", "set", "x.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_groups_set(self, remove_existing=True, names_only=False):\n    if self.groups is None:\n        return None\n    info = self.user_info()\n    groups = set(x.strip() for x in self.groups.split(',') if x)\n    group_names = set()\n    for g in groups.copy():\n        if not self.group_exists(g):\n            self.module.fail_json(msg=\"Group %s does not exist\" % (g))\n        group_info = self.group_info(g)\n        if info and remove_existing and group_info[2] == info[3]:\n            groups.remove(g)\n        elif names_only:\n            group_names.add(group_info[0])\n    if names_only:\n        return group_names\n    return groups", "loc": 17}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "user_group_membership", "parameters": ["self", "exclude_primary"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["groups.append", "grp.getgrall", "self.get_pwd_info"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a list of groups the user belongs to", "source_code": "def user_group_membership(self, exclude_primary=True):\n    \"\"\" Return a list of groups the user belongs to \"\"\"\n    groups = []\n    info = self.get_pwd_info()\n    for group in grp.getgrall():\n        if self.name in group.gr_mem:\n            # Exclude the user's primary group by default\n            if not exclude_primary:\n                groups.append(group[0])\n            else:\n                if info[3] != group.gr_gid:\n                    groups.append(group[0])\n\n    return groups", "loc": 14}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "user_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.get_pwd_info", "self.user_exists", "self.user_password"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def user_info(self):\n    if not self.user_exists():\n        return False\n    info = self.get_pwd_info()\n    if len(info[1]) == 1 or len(info[1]) == 0:\n        info[1] = self.user_password()[0]\n    return info", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "set_password_expire", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "cmd.extend", "getspnam", "self.execute_command", "self.module.get_bin_path", "to_bytes"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_password_expire(self):\n    min_needs_change = self.password_expire_min is not None\n    max_needs_change = self.password_expire_max is not None\n    warn_needs_change = self.password_expire_warn is not None\n\n    if HAVE_SPWD:\n        try:\n            shadow_info = getspnam(to_bytes(self.name))\n        except ValueError:\n            return None, '', ''\n\n        min_needs_change &= self.password_expire_min != shadow_info.sp_min\n        max_needs_change &= self.password_expire_max != shadow_info.sp_max\n        warn_needs_change &= self.password_expire_warn != shadow_info.sp_warn\n\n    if not (min_needs_change or max_needs_change or warn_needs_change):\n        return (None, '', '')  # target state already reached\n\n    command_name = 'chage'\n    cmd = [self.module.get_bin_path(command_name, True)]\n    if min_needs_change:\n        cmd.extend([\"-m\", self.password_expire_min])\n    if max_needs_change:\n        cmd.extend([\"-M\", self.password_expire_max])\n    if warn_needs_change:\n        cmd.extend([\"-W\", self.password_expire_warn])\n    cmd.append(self.name)\n\n    return self.execute_command(cmd)", "loc": 29}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "User", "function_name": "user_password", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getspnam", "self.parse_shadow_file", "self.user_exists", "to_bytes", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def user_password(self):\n    passwd = ''\n    expires = ''\n    if HAVE_SPWD:\n        try:\n            shadow_info = getspnam(to_bytes(self.name))\n            passwd = to_native(shadow_info.sp_pwdp)\n            expires = shadow_info.sp_expire\n            return passwd, expires\n        except ValueError:\n            return passwd, expires\n\n    if not self.user_exists():\n        return passwd, expires\n    elif self.SHADOWFILE:\n        passwd, expires = self.parse_shadow_file()\n\n    return passwd, expires", "loc": 18}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "FreeBsdUser", "function_name": "remove_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user(self):\n    cmd = [\n        self.module.get_bin_path('pw', True),\n        'userdel',\n        '-n',\n        self.name\n    ]\n    if self.remove:\n        cmd.append('-r')\n\n    return self.execute_command(cmd)", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "FreeBsdUser", "function_name": "create_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "calendar.timegm", "cmd.append", "self._handle_lock", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json", "self.module.get_bin_path", "str", "time.gmtime"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user(self):\n    cmd = [\n        self.module.get_bin_path('pw', True),\n        'useradd',\n        '-n',\n        self.name,\n    ]\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.comment is not None:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        cmd.append('-g')\n        cmd.append(self.group)\n\n    if self.groups is not None:\n        groups = self.get_groups_set()\n        cmd.append('-G')\n        cmd.append(','.join(groups))\n\n    if self.create_home:\n        cmd.append('-m')\n\n        if self.skeleton is not None:\n            cmd.append('-k')\n            cmd.append(self.skeleton)\n\n        if self.umask is not None:\n            cmd.append('-K')\n            cmd.append('UMASK=' + self.umask)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.login_class is not None:\n        cmd.append('-L')\n        cmd.append(self.login_class)\n\n    if self.expires is not None:\n        cmd.append('-e')\n        if self.expires < time.gmtime(0):\n            cmd.append('0')\n        else:\n            cmd.append(str(calendar.timegm(self.expires)))\n\n    if self.uid_min is not None:\n        cmd.append('-K')\n        cmd.append('UID_MIN=' + str(self.uid_min))\n\n    if self.uid_max is not None:\n        cmd.append('-K')\n        cmd.append('UID_MAX=' + str(self.uid_max))\n\n    # system cannot be handled currently - should we error if its requested?\n    # create the user\n    (rc, out, err) = self.execute_command(cmd)\n\n    if rc is not None and rc != 0:\n        self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    # we have to set the password in a second command\n    if self.password is not None:\n        cmd = [\n            self.module.get_bin_path('chpass', True),\n            '-p',\n            self.password,\n            self.name\n        ]\n        _rc, _out, _err = self.execute_command(cmd)\n        if rc is None:\n            rc = _rc\n        out += _out\n        err += _err\n\n    # we have to lock/unlock the password in a distinct command\n    _rc, _out, _err = self._handle_lock()\n    if rc is None:\n        rc = _rc\n    out += _out\n    err += _err\n\n    return (rc, out, err)", "loc": 97}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "OpenBSDUser", "function_name": "create_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user(self):\n    cmd = [self.module.get_bin_path('useradd', True)]\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        cmd.append('-g')\n        cmd.append(self.group)\n\n    if self.groups is not None:\n        groups = self.get_groups_set()\n        cmd.append('-G')\n        cmd.append(','.join(groups))\n\n    if self.comment is not None:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.login_class is not None:\n        cmd.append('-L')\n        cmd.append(self.login_class)\n\n    if self.password is not None and self.password != '*':\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    if self.create_home:\n        cmd.append('-m')\n\n        if self.skeleton is not None:\n            cmd.append('-k')\n            cmd.append(self.skeleton)\n\n        if self.umask is not None:\n            cmd.append('-K')\n            cmd.append('UMASK=' + self.umask)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n    if self.uid_min is not None:\n        cmd.append('-K')\n        cmd.append('UID_MIN=' + str(self.uid_min))\n\n    if self.uid_max is not None:\n        cmd.append('-K')\n        cmd.append('UID_MAX=' + str(self.uid_max))\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 65}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "OpenBSDUser", "function_name": "remove_user_userdel", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user_userdel(self):\n    cmd = [self.module.get_bin_path('userdel', True)]\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "OpenBSDUser", "function_name": "modify_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "info[1].startswith", "int", "len", "line.split", "out.splitlines", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.group_info", "self.module.fail_json", "self.module.get_bin_path", "self.user_group_membership", "self.user_info", "set", "set(current_groups).symmetric_difference"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user(self):\n    cmd = [self.module.get_bin_path('usermod', True)]\n    info = self.user_info()\n\n    if self.uid is not None and info[2] != int(self.uid):\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        ginfo = self.group_info(self.group)\n        if info[3] != ginfo[2]:\n            cmd.append('-g')\n            cmd.append(self.group)\n\n    if self.groups is not None:\n        current_groups = self.user_group_membership()\n        groups_need_mod = False\n        groups_option = '-S'\n        groups = []\n\n        if self.groups == '':\n            if current_groups and not self.append:\n                groups_need_mod = True\n        else:\n            groups = self.get_groups_set(names_only=True)\n            group_diff = set(current_groups).symmetric_difference(groups)\n\n            if group_diff:\n                if self.append:\n                    for g in groups:\n                        if g in group_diff:\n                            groups_option = '-G'\n                            groups_need_mod = True\n                            break\n                else:\n                    groups_need_mod = True\n\n        if groups_need_mod:\n            cmd.append(groups_option)\n            cmd.append(','.join(groups))\n\n    if self.comment is not None and info[4] != self.comment:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None and info[5] != self.home:\n        if self.move_home:\n            cmd.append('-m')\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None and info[6] != self.shell:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n\n    if self.login_class is not None:\n        # find current login class\n        user_login_class = None\n        userinfo_cmd = [self.module.get_bin_path('userinfo', True), self.name]\n        (rc, out, err) = self.execute_command(userinfo_cmd, obey_checkmode=False)\n\n        for line in out.splitlines():\n            tokens = line.split()\n\n            if tokens[0] == 'class' and len(tokens) == 2:\n                user_login_class = tokens[1]\n\n        # act only if login_class change\n        if self.login_class != user_login_class:\n            cmd.append('-L')\n            cmd.append(self.login_class)\n\n    if self.password_lock and not info[1].startswith('*'):\n        cmd.append('-Z')\n    elif self.password_lock is False and info[1].startswith('*'):\n        cmd.append('-U')\n\n    if self.update_password == 'always' and self.password is not None \\\n            and self.password != '*' and info[1] != self.password:\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    # skip if no changes to be made\n    if len(cmd) == 1:\n        return (None, '', '')\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 97}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "NetBSDUser", "function_name": "create_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user(self):\n    cmd = [self.module.get_bin_path('useradd', True)]\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        cmd.append('-g')\n        cmd.append(self.group)\n\n    if self.groups is not None:\n        groups = self.get_groups_set()\n        if len(groups) > 16:\n            self.module.fail_json(msg=\"Too many groups (%d) NetBSD allows for 16 max.\" % len(groups))\n        cmd.append('-G')\n        cmd.append(','.join(groups))\n\n    if self.comment is not None:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.login_class is not None:\n        cmd.append('-L')\n        cmd.append(self.login_class)\n\n    if self.password is not None:\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n\n    if self.create_home:\n        cmd.append('-m')\n\n        if self.skeleton is not None:\n            cmd.append('-k')\n            cmd.append(self.skeleton)\n\n        if self.umask is not None:\n            cmd.append('-K')\n            cmd.append('UMASK=' + self.umask)\n\n    if self.uid_min is not None:\n        cmd.append('-K')\n        cmd.append('UID_MIN=' + str(self.uid_min))\n\n    if self.uid_max is not None:\n        cmd.append('-K')\n        cmd.append('UID_MAX=' + str(self.uid_max))\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 68}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "NetBSDUser", "function_name": "remove_user_userdel", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user_userdel(self):\n    cmd = [self.module.get_bin_path('userdel', True)]\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "NetBSDUser", "function_name": "modify_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "info[1].startswith", "int", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.group_info", "self.module.fail_json", "self.module.get_bin_path", "self.user_group_membership", "self.user_info", "set", "set(current_groups).symmetric_difference", "set(current_groups).union"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user(self):\n    cmd = [self.module.get_bin_path('usermod', True)]\n    info = self.user_info()\n\n    if self.uid is not None and info[2] != int(self.uid):\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        ginfo = self.group_info(self.group)\n        if info[3] != ginfo[2]:\n            cmd.append('-g')\n            cmd.append(self.group)\n\n    if self.groups is not None:\n        current_groups = self.user_group_membership()\n        groups_need_mod = False\n        groups = []\n\n        if self.groups == '':\n            if current_groups and not self.append:\n                groups_need_mod = True\n        else:\n            groups = self.get_groups_set(names_only=True)\n            group_diff = set(current_groups).symmetric_difference(groups)\n\n            if group_diff:\n                if self.append:\n                    for g in groups:\n                        if g in group_diff:\n                            groups = set(current_groups).union(groups)\n                            groups_need_mod = True\n                            break\n                else:\n                    groups_need_mod = True\n\n        if groups_need_mod:\n            if len(groups) > 16:\n                self.module.fail_json(msg=\"Too many groups (%d) NetBSD allows for 16 max.\" % len(groups))\n            cmd.append('-G')\n            cmd.append(','.join(groups))\n\n    if self.comment is not None and info[4] != self.comment:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None and info[5] != self.home:\n        if self.move_home:\n            cmd.append('-m')\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None and info[6] != self.shell:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.login_class is not None:\n        cmd.append('-L')\n        cmd.append(self.login_class)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n\n    if self.update_password == 'always' and self.password is not None and info[1] != self.password:\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    if self.password_lock and not info[1].startswith('*LOCKED*'):\n        cmd.append('-C yes')\n    elif self.password_lock is False and info[1].startswith('*LOCKED*'):\n        cmd.append('-C no')\n\n    # skip if no changes to be made\n    if len(cmd) == 1:\n        return (None, '', '')\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 84}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "SunOS", "function_name": "remove_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user(self):\n    cmd = [self.module.get_bin_path('userdel', True)]\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n\n    return self.execute_command(cmd)", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "SunOS", "function_name": "user_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._user_attr_info", "super", "super(SunOS, self).user_info"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def user_info(self):\n    info = super(SunOS, self).user_info()\n    if info:\n        info += self._user_attr_info()\n    return info", "loc": 5}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "DarwinUser", "function_name": "user_exists", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_dscl", "self.execute_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Check is SELF.NAME is a known user on the system.", "source_code": "def user_exists(self):\n    \"\"\"Check is SELF.NAME is a known user on the system.\"\"\"\n    cmd = self._get_dscl()\n    cmd += ['-read', '/Users/%s' % self.name, 'UniqueID']\n    (rc, out, err) = self.execute_command(cmd, obey_checkmode=False)\n    return rc == 0", "loc": 6}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "DarwinUser", "function_name": "modify_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._change_user_password", "self._get_dscl", "self._get_user_property", "self._make_group_numerical", "self._modify_group", "self._update_system_user", "self.execute_command", "self.module.fail_json", "to_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user(self):\n    changed = None\n    out = ''\n    err = ''\n\n    if self.group:\n        self._make_group_numerical()\n\n    for field in self.fields:\n        if field[0] in self.__dict__ and self.__dict__[field[0]]:\n            current = self._get_user_property(field[1])\n            if current is None or current != to_text(self.__dict__[field[0]]):\n                cmd = self._get_dscl()\n                cmd += ['-create', '/Users/%s' % self.name, field[1], self.__dict__[field[0]]]\n                (rc, _out, _err) = self.execute_command(cmd)\n                if rc != 0:\n                    self.module.fail_json(\n                        msg='Cannot update property \"%s\" for user \"%s\".'\n                            % (field[0], self.name), err=err, out=out, rc=rc)\n                changed = rc\n                out += _out\n                err += _err\n    if self.update_password == 'always' and self.password is not None:\n        (rc, _out, _err) = self._change_user_password()\n        out += _out\n        err += _err\n        changed = rc\n\n    if self.groups:\n        (rc, _out, _err, _changed) = self._modify_group()\n        out += _out\n        err += _err\n\n        if _changed is True:\n            changed = rc\n\n    rc = self._update_system_user()\n    if rc == 0:\n        changed = rc\n\n    return (changed, out, err)", "loc": 41}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "AIX", "function_name": "remove_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user(self):\n    cmd = [self.module.get_bin_path('userdel', True)]\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n\n    return self.execute_command(cmd)", "loc": 7}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "AIX", "function_name": "create_user_useradd", "parameters": ["self", "command_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json", "self.module.get_bin_path", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user_useradd(self, command_name='useradd'):\n    cmd = [self.module.get_bin_path(command_name, True)]\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        cmd.append('-g')\n        cmd.append(self.group)\n\n    if self.groups is not None and len(self.groups):\n        groups = self.get_groups_set()\n        cmd.append('-G')\n        cmd.append(','.join(groups))\n\n    if self.comment is not None:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.create_home:\n        cmd.append('-m')\n\n        if self.skeleton is not None:\n            cmd.append('-k')\n            cmd.append(self.skeleton)\n\n        if self.umask is not None:\n            cmd.append('-K')\n            cmd.append('UMASK=' + self.umask)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n    if self.uid_min is not None:\n        cmd.append('-K')\n        cmd.append('UID_MIN=' + str(self.uid_min))\n\n    if self.uid_max is not None:\n        cmd.append('-K')\n        cmd.append('UID_MAX=' + str(self.uid_max))\n\n    cmd.append(self.name)\n    (rc, out, err) = self.execute_command(cmd)\n\n    # set password with chpasswd\n    if self.password is not None:\n        cmd = []\n        cmd.append(self.module.get_bin_path('chpasswd', True))\n        cmd.append('-e')\n        cmd.append('-c')\n        self.execute_command(cmd, data=\"%s:%s\" % (self.name, self.password))\n\n    return (rc, out, err)", "loc": 64}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "AIX", "function_name": "modify_user_usermod", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "int", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.group_info", "self.module.fail_json", "self.module.get_bin_path", "self.user_group_membership", "self.user_info", "set", "set(current_groups).symmetric_difference"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user_usermod(self):\n    cmd = [self.module.get_bin_path('usermod', True)]\n    info = self.user_info()\n\n    if self.uid is not None and info[2] != int(self.uid):\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        ginfo = self.group_info(self.group)\n        if info[3] != ginfo[2]:\n            cmd.append('-g')\n            cmd.append(self.group)\n\n    if self.groups is not None:\n        current_groups = self.user_group_membership()\n        groups_need_mod = False\n        groups = []\n\n        if self.groups == '':\n            if current_groups and not self.append:\n                groups_need_mod = True\n        else:\n            groups = self.get_groups_set(names_only=True)\n            group_diff = set(current_groups).symmetric_difference(groups)\n\n            if group_diff:\n                if self.append:\n                    for g in groups:\n                        if g in group_diff:\n                            groups_need_mod = True\n                            break\n                else:\n                    groups_need_mod = True\n\n        if groups_need_mod:\n            cmd.append('-G')\n            cmd.append(','.join(groups))\n\n    if self.comment is not None and info[4] != self.comment:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None and info[5] != self.home:\n        if self.move_home:\n            cmd.append('-m')\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None and info[6] != self.shell:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.inactive is not None:\n        cmd.append('-f')\n        cmd.append(self.inactive)\n\n    # skip if no changes to be made\n    if len(cmd) == 1:\n        (rc, out, err) = (None, '', '')\n    else:\n        cmd.append(self.name)\n        (rc, out, err) = self.execute_command(cmd)\n\n    # set password with chpasswd\n    if self.update_password == 'always' and self.password is not None and info[1] != self.password:\n        cmd = []\n        cmd.append(self.module.get_bin_path('chpasswd', True))\n        cmd.append('-e')\n        cmd.append('-c')\n        (rc2, out2, err2) = self.execute_command(cmd, data=\"%s:%s\" % (self.name, self.password))\n    else:\n        (rc2, out2, err2) = (None, '', '')\n\n    if rc is not None:\n        return (rc, out + out2, err + err2)\n    else:\n        return (rc2, out + out2, err + err2)", "loc": 80}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "HPUX", "function_name": "create_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user(self):\n    cmd = ['/usr/sam/lbin/useradd.sam']\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        cmd.append('-g')\n        cmd.append(self.group)\n\n    if self.groups is not None and len(self.groups):\n        groups = self.get_groups_set()\n        cmd.append('-G')\n        cmd.append(','.join(groups))\n\n    if self.comment is not None:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-d')\n        cmd.append(self.home)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.password is not None:\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    if self.create_home:\n        cmd.append('-m')\n    else:\n        cmd.append('-M')\n\n    if self.system:\n        cmd.append('-r')\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 47}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "HPUX", "function_name": "remove_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user(self):\n    cmd = ['/usr/sam/lbin/userdel.sam']\n    if self.force:\n        cmd.append('-F')\n    if self.remove:\n        cmd.append('-r')\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 8}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "HPUX", "function_name": "modify_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "cmd.append", "int", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.group_info", "self.module.fail_json", "self.user_group_membership", "self.user_info", "set", "set(current_groups).symmetric_difference"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user(self):\n    cmd = ['/usr/sam/lbin/usermod.sam']\n    info = self.user_info()\n\n    if self.uid is not None and info[2] != int(self.uid):\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n        if self.non_unique:\n            cmd.append('-o')\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg=\"Group %s does not exist\" % self.group)\n        ginfo = self.group_info(self.group)\n        if info[3] != ginfo[2]:\n            cmd.append('-g')\n            cmd.append(self.group)\n\n    if self.groups is not None:\n        current_groups = self.user_group_membership()\n        groups_need_mod = False\n        groups = []\n\n        if self.groups == '':\n            if current_groups and not self.append:\n                groups_need_mod = True\n        else:\n            groups = self.get_groups_set(remove_existing=False, names_only=True)\n            group_diff = set(current_groups).symmetric_difference(groups)\n\n            if group_diff:\n                if self.append:\n                    for g in groups:\n                        if g in group_diff:\n                            groups_need_mod = True\n                            break\n                else:\n                    groups_need_mod = True\n\n        if groups_need_mod:\n            cmd.append('-G')\n            new_groups = groups\n            if self.append:\n                new_groups = groups | set(current_groups)\n            cmd.append(','.join(new_groups))\n\n    if self.comment is not None and info[4] != self.comment:\n        cmd.append('-c')\n        cmd.append(self.comment)\n\n    if self.home is not None and info[5] != self.home:\n        cmd.append('-d')\n        cmd.append(self.home)\n        if self.move_home:\n            cmd.append('-m')\n\n    if self.shell is not None and info[6] != self.shell:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if self.update_password == 'always' and self.password is not None and info[1] != self.password:\n        cmd.append('-F')\n        cmd.append('-p')\n        cmd.append(self.password)\n\n    # skip if no changes to be made\n    if len(cmd) == 1:\n        return (None, '', '')\n\n    cmd.append(self.name)\n    return self.execute_command(cmd)", "loc": 72}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "BusyBox", "function_name": "create_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Group {0} does not exist'.format", "'{name}:{password}'.format", "cmd.append", "len", "self.execute_command", "self.get_groups_set", "self.group_exists", "self.module.fail_json", "self.module.get_bin_path", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_user(self):\n    cmd = [self.module.get_bin_path('adduser', True)]\n\n    cmd.append('-D')\n\n    if self.uid is not None:\n        cmd.append('-u')\n        cmd.append(self.uid)\n\n    if self.group is not None:\n        if not self.group_exists(self.group):\n            self.module.fail_json(msg='Group {0} does not exist'.format(self.group))\n        cmd.append('-G')\n        cmd.append(self.group)\n\n    if self.comment is not None:\n        cmd.append('-g')\n        cmd.append(self.comment)\n\n    if self.home is not None:\n        cmd.append('-h')\n        cmd.append(self.home)\n\n    if self.shell is not None:\n        cmd.append('-s')\n        cmd.append(self.shell)\n\n    if not self.create_home:\n        cmd.append('-H')\n\n    if self.skeleton is not None:\n        cmd.append('-k')\n        cmd.append(self.skeleton)\n\n    if self.umask is not None:\n        cmd.append('-K')\n        cmd.append('UMASK=' + self.umask)\n\n    if self.system:\n        cmd.append('-S')\n\n    if self.uid_min is not None:\n        cmd.append('-K')\n        cmd.append('UID_MIN=' + str(self.uid_min))\n\n    if self.uid_max is not None:\n        cmd.append('-K')\n        cmd.append('UID_MAX=' + str(self.uid_max))\n\n    cmd.append(self.name)\n\n    rc, out, err = self.execute_command(cmd)\n\n    if rc is not None and rc != 0:\n        self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    if self.password is not None:\n        cmd = [self.module.get_bin_path('chpasswd', True)]\n        cmd.append('--encrypted')\n        data = '{name}:{password}'.format(name=self.name, password=self.password)\n        rc, out, err = self.execute_command(cmd, data=data)\n\n        if rc is not None and rc != 0:\n            self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    # Add to additional groups\n    if self.groups is not None and len(self.groups):\n        groups = self.get_groups_set()\n        add_cmd_bin = self.module.get_bin_path('adduser', True)\n        for group in groups:\n            cmd = [add_cmd_bin, self.name, group]\n            rc, out, err = self.execute_command(cmd)\n            if rc is not None and rc != 0:\n                self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    return rc, out, err", "loc": 76}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "BusyBox", "function_name": "remove_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.append", "self.execute_command", "self.module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_user(self):\n\n    cmd = [\n        self.module.get_bin_path('deluser', True),\n        self.name\n    ]\n\n    if self.remove:\n        cmd.append('--remove-home')\n\n    return self.execute_command(cmd)", "loc": 11}
{"file": "ansible\\lib\\ansible\\modules\\user.py", "class_name": "BusyBox", "function_name": "modify_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{name}:{password}'.format", "cmd.append", "len", "self.execute_command", "self.get_groups_set", "self.module.fail_json", "self.module.get_bin_path", "self.user_group_membership", "self.user_info", "set", "set(current_groups).symmetric_difference"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def modify_user(self):\n    current_groups = self.user_group_membership()\n    groups = []\n    rc = None\n    out = ''\n    err = ''\n    info = self.user_info()\n    add_cmd_bin = self.module.get_bin_path('adduser', True)\n    remove_cmd_bin = self.module.get_bin_path('delgroup', True)\n\n    # Manage group membership\n    if self.groups is not None and len(self.groups):\n        groups = self.get_groups_set()\n        group_diff = set(current_groups).symmetric_difference(groups)\n\n        if group_diff:\n            for g in groups:\n                if g in group_diff:\n                    add_cmd = [add_cmd_bin, self.name, g]\n                    rc, out, err = self.execute_command(add_cmd)\n                    if rc is not None and rc != 0:\n                        self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n            for g in group_diff:\n                if g not in groups and not self.append:\n                    remove_cmd = [remove_cmd_bin, self.name, g]\n                    rc, out, err = self.execute_command(remove_cmd)\n                    if rc is not None and rc != 0:\n                        self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    # Manage password\n    if self.update_password == 'always' and self.password is not None and info[1] != self.password:\n        cmd = [self.module.get_bin_path('chpasswd', True)]\n        cmd.append('--encrypted')\n        data = '{name}:{password}'.format(name=self.name, password=self.password)\n        rc, out, err = self.execute_command(cmd, data=data)\n\n        if rc is not None and rc != 0:\n            self.module.fail_json(name=self.name, msg=err, rc=rc)\n\n    return rc, out, err", "loc": 41}
{"file": "ansible\\lib\\ansible\\modules\\wait_for.py", "class_name": "TCPConnectionInfo", "function_name": "get_active_connections_count", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "hasattr", "local_ip.startswith", "p.connections", "p.get_connections", "psutil.process_iter"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_active_connections_count(self):\n    active_connections = 0\n    for p in psutil.process_iter():\n        try:\n            if hasattr(p, 'get_connections'):\n                connections = p.get_connections(kind='inet')\n            else:\n                connections = p.connections(kind='inet')\n        except psutil.Error:\n            # Process is Zombie or other error state\n            continue\n        for conn in connections:\n            if conn.status not in self.module.params['active_connection_states']:\n                continue\n            if hasattr(conn, 'local_address'):\n                (local_ip, local_port) = conn.local_address\n            else:\n                (local_ip, local_port) = conn.laddr\n            if self.port != local_port:\n                continue\n            if hasattr(conn, 'remote_address'):\n                (remote_ip, remote_port) = conn.remote_address\n            else:\n                (remote_ip, remote_port) = conn.raddr\n            if (conn.family, remote_ip) in self.exclude_ips:\n                continue\n            if any((\n                (conn.family, local_ip) in self.ips,\n                (conn.family, self.match_all_ips[conn.family]) in self.ips,\n                local_ip.startswith(self.ipv4_mapped_ipv6_address['prefix']) and\n                    (conn.family, self.ipv4_mapped_ipv6_address['match_all']) in self.ips,\n            )):\n                active_connections += 1\n    return active_connections", "loc": 34}
{"file": "ansible\\lib\\ansible\\modules\\yum_repository.py", "class_name": "YumRepo", "function_name": "add", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "isinstance", "self.module.deprecate", "self.params.items", "self.remove", "self.repofile.add_section", "self.repofile.set", "sorted", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add(self):\n    self.remove()\n    self.repofile.add_section(self.section)\n\n    for key, value in sorted(self.params.items()):\n        if value is None:\n            continue\n        if key == 'async':\n            self.module.deprecate(\n                \"'async' parameter is deprecated as it has been removed on systems supported by ansible-core\",\n                version='2.22',\n            )\n        elif key in {\n            \"deltarpm_metadata_percentage\",\n            \"gpgcakey\",\n            \"http_caching\",\n            \"keepalive\",\n            \"metadata_expire_filter\",\n            \"mirrorlist_expire\",\n            \"protect\",\n            \"ssl_check_cert_permissions\",\n            \"ui_repoid_vars\",\n        }:\n            self.module.deprecate(\n                f\"'{key}' parameter is deprecated as it has no effect with dnf \"\n                \"as an underlying package manager.\",\n                version='2.22'\n            )\n        if isinstance(value, bool):\n            value = str(int(value))\n        self.repofile.set(self.section, key, value)", "loc": 31}
{"file": "ansible\\lib\\ansible\\modules\\yum_repository.py", "class_name": "YumRepo", "function_name": "dump", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.repofile.items", "self.repofile.sections", "sorted"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def dump(self):\n    repo_string = \"\"\n\n    for section in sorted(self.repofile.sections()):\n        repo_string += \"[%s]\\n\" % section\n\n        for key, value in sorted(self.repofile.items(section)):\n            repo_string += \"%s = %s\\n\" % (key, value)\n\n        repo_string += \"\\n\"\n\n    return repo_string", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "rate_limit_argument_spec", "parameters": ["spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["arg_spec.update", "dict"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Creates an argument spec for working with rate limiting", "source_code": "def rate_limit_argument_spec(spec=None):\n    \"\"\"Creates an argument spec for working with rate limiting\"\"\"\n    arg_spec = (dict(\n        rate=dict(type='int'),\n        rate_limit=dict(type='int'),\n    ))\n    if spec:\n        arg_spec.update(spec)\n    return arg_spec", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "retry_argument_spec", "parameters": ["spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["arg_spec.update", "dict"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Creates an argument spec for working with retrying", "source_code": "def retry_argument_spec(spec=None):\n    \"\"\"Creates an argument spec for working with retrying\"\"\"\n    arg_spec = (dict(\n        retries=dict(type='int'),\n        retry_pause=dict(type='float', default=1),\n    ))\n    if spec:\n        arg_spec.update(spec)\n    return arg_spec", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "basic_auth_argument_spec", "parameters": ["spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["arg_spec.update", "dict"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def basic_auth_argument_spec(spec=None):\n    arg_spec = (dict(\n        api_username=dict(type='str'),\n        api_password=dict(type='str', no_log=True),\n        api_url=dict(type='str'),\n        validate_certs=dict(type='bool', default=True)\n    ))\n    if spec:\n        arg_spec.update(spec)\n    return arg_spec", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "rate_limit", "parameters": ["rate", "rate_limit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f", "float", "real_time", "time.sleep"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "rate limiting decorator", "source_code": "def rate_limit(rate=None, rate_limit=None):\n    \"\"\"rate limiting decorator\"\"\"\n    minrate = None\n    if rate is not None and rate_limit is not None:\n        minrate = float(rate_limit) / float(rate)\n\n    def wrapper(f):\n        last = [0.0]\n\n        def ratelimited(*args, **kwargs):\n            if sys.version_info >= (3, 8):\n                real_time = time.process_time\n            else:\n                real_time = time.clock\n            if minrate is not None:\n                elapsed = real_time() - last[0]\n                left = minrate - elapsed\n                if left > 0:\n                    time.sleep(left)\n                last[0] = real_time()\n            ret = f(*args, **kwargs)\n            return ret\n\n        return ratelimited\n    return wrapper", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "retry", "parameters": ["retries", "retry_pause"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "f", "time.sleep"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Retry decorator", "source_code": "def retry(retries=None, retry_pause=1):\n    \"\"\"Retry decorator\"\"\"\n    def wrapper(f):\n\n        def retried(*args, **kwargs):\n            retry_count = 0\n            if retries is not None:\n                ret = None\n                while True:\n                    retry_count += 1\n                    if retry_count >= retries:\n                        raise Exception(\"Retry limit exceeded: %d\" % retries)\n                    try:\n                        ret = f(*args, **kwargs)\n                    except Exception:\n                        pass\n                    if ret:\n                        break\n                    time.sleep(retry_pause)\n                return ret\n\n        return retried\n    return wrapper", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "generate_jittered_backoff", "parameters": ["retries", "delay_base", "delay_threshold"], "param_types": {}, "return_type": null, "param_doc": {"retries": "The number of delays to generate.", "delay_base": "The base time in seconds used to calculate the exponential backoff.", "delay_threshold": "The maximum time in seconds for any delay."}, "return_doc": "", "raises_doc": [], "called_functions": ["min", "range", "secrets.randbelow"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "The \"Full Jitter\" backoff strategy. Ref: https://www.awsarchitectureblog.com/2015/03/backoff.html", "source_code": "def generate_jittered_backoff(retries=10, delay_base=3, delay_threshold=60):\n    \"\"\"The \"Full Jitter\" backoff strategy.\n\n    Ref: https://www.awsarchitectureblog.com/2015/03/backoff.html\n\n    :param retries: The number of delays to generate.\n    :param delay_base: The base time in seconds used to calculate the exponential backoff.\n    :param delay_threshold: The maximum time in seconds for any delay.\n    \"\"\"\n    for retry in range(0, retries):\n        yield secrets.randbelow(min(delay_threshold, delay_base * 2 ** retry))", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "retry_with_delays_and_condition", "parameters": ["backoff_iterator", "should_retry_error"], "param_types": {}, "return_type": null, "param_doc": {"backoff_iterator": "An iterable of delays in seconds.", "should_retry_error": "A callable that takes an exception of the decorated function and decides whether to retry or not (returns a bool)."}, "return_doc": "", "raises_doc": [], "called_functions": ["_emit_isolated_iterator_copies", "call_retryable_function", "copy.copy", "functools.partial", "functools.wraps", "itertools.tee", "next", "should_retry_error", "time.sleep"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Generic retry decorator.", "source_code": "def retry_with_delays_and_condition(backoff_iterator, should_retry_error=None):\n    \"\"\"Generic retry decorator.\n\n    :param backoff_iterator: An iterable of delays in seconds.\n    :param should_retry_error: A callable that takes an exception of the decorated function and decides whether to retry or not (returns a bool).\n    \"\"\"\n    def _emit_isolated_iterator_copies(original_iterator):  # type: (t.Iterable[t.Any]) -> t.Generator\n        # Ref: https://stackoverflow.com/a/30232619/595220\n        _copiable_iterator, _first_iterator_copy = itertools.tee(original_iterator)\n        yield _first_iterator_copy\n        while True:\n            yield copy.copy(_copiable_iterator)\n    backoff_iterator_generator = _emit_isolated_iterator_copies(backoff_iterator)\n    del backoff_iterator  # prevent accidental use elsewhere\n\n    if should_retry_error is None:\n        should_retry_error = retry_never\n\n    def function_wrapper(function):\n        @functools.wraps(function)\n        def run_function(*args, **kwargs):\n            \"\"\"This assumes the function has not already been called.\n            If backoff_iterator is empty, we should still run the function a single time with no delay.\n            \"\"\"\n            call_retryable_function = functools.partial(function, *args, **kwargs)\n\n            for delay in next(backoff_iterator_generator):\n                try:\n                    return call_retryable_function()\n                except Exception as e:\n                    if not should_retry_error(e):\n                        raise\n                time.sleep(delay)\n\n            # Only or final attempt\n            return call_retryable_function()\n        return run_function\n    return function_wrapper", "loc": 38}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "wrapper", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f", "real_time", "time.sleep"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(f):\n    last = [0.0]\n\n    def ratelimited(*args, **kwargs):\n        if sys.version_info >= (3, 8):\n            real_time = time.process_time\n        else:\n            real_time = time.clock\n        if minrate is not None:\n            elapsed = real_time() - last[0]\n            left = minrate - elapsed\n            if left > 0:\n                time.sleep(left)\n            last[0] = real_time()\n        ret = f(*args, **kwargs)\n        return ret\n\n    return ratelimited", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "wrapper", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "f", "time.sleep"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(f):\n\n    def retried(*args, **kwargs):\n        retry_count = 0\n        if retries is not None:\n            ret = None\n            while True:\n                retry_count += 1\n                if retry_count >= retries:\n                    raise Exception(\"Retry limit exceeded: %d\" % retries)\n                try:\n                    ret = f(*args, **kwargs)\n                except Exception:\n                    pass\n                if ret:\n                    break\n                time.sleep(retry_pause)\n            return ret\n\n    return retried", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "function_wrapper", "parameters": ["function"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["call_retryable_function", "functools.partial", "functools.wraps", "next", "should_retry_error", "time.sleep"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def function_wrapper(function):\n    @functools.wraps(function)\n    def run_function(*args, **kwargs):\n        \"\"\"This assumes the function has not already been called.\n        If backoff_iterator is empty, we should still run the function a single time with no delay.\n        \"\"\"\n        call_retryable_function = functools.partial(function, *args, **kwargs)\n\n        for delay in next(backoff_iterator_generator):\n            try:\n                return call_retryable_function()\n            except Exception as e:\n                if not should_retry_error(e):\n                    raise\n            time.sleep(delay)\n\n        # Only or final attempt\n        return call_retryable_function()\n    return run_function", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "ratelimited", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f", "real_time", "time.sleep"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ratelimited(*args, **kwargs):\n    if sys.version_info >= (3, 8):\n        real_time = time.process_time\n    else:\n        real_time = time.clock\n    if minrate is not None:\n        elapsed = real_time() - last[0]\n        left = minrate - elapsed\n        if left > 0:\n            time.sleep(left)\n        last[0] = real_time()\n    ret = f(*args, **kwargs)\n    return ret", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "retried", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "f", "time.sleep"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def retried(*args, **kwargs):\n    retry_count = 0\n    if retries is not None:\n        ret = None\n        while True:\n            retry_count += 1\n            if retry_count >= retries:\n                raise Exception(\"Retry limit exceeded: %d\" % retries)\n            try:\n                ret = f(*args, **kwargs)\n            except Exception:\n                pass\n            if ret:\n                break\n            time.sleep(retry_pause)\n        return ret", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\api.py", "class_name": null, "function_name": "run_function", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["call_retryable_function", "functools.partial", "functools.wraps", "next", "should_retry_error", "time.sleep"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "This assumes the function has not already been called. If backoff_iterator is empty, we should still run the function a single time with no delay.", "source_code": "def run_function(*args, **kwargs):\n    \"\"\"This assumes the function has not already been called.\n    If backoff_iterator is empty, we should still run the function a single time with no delay.\n    \"\"\"\n    call_retryable_function = functools.partial(function, *args, **kwargs)\n\n    for delay in next(backoff_iterator_generator):\n        try:\n            return call_retryable_function()\n        except Exception as e:\n            if not should_retry_error(e):\n                raise\n        time.sleep(delay)\n\n    # Only or final attempt\n    return call_retryable_function()", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": null, "function_name": "heuristic_log_sanitize", "parameters": ["data", "no_log_values"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "data.index", "data.rindex", "len", "output.insert", "remove_values", "to_native"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Remove strings that look like passwords from log messages", "source_code": "def heuristic_log_sanitize(data, no_log_values=None):\n    \"\"\" Remove strings that look like passwords from log messages \"\"\"\n    # Currently filters:\n    # user:pass@foo/whatever and http://username:pass@wherever/foo\n    # This code has false positives and consumes parts of logs that are\n    # not passwds\n\n    # begin: start of a passwd containing string\n    # end: end of a passwd containing string\n    # sep: char between user and passwd\n    # prev_begin: where in the overall string to start a search for\n    #   a passwd\n    # sep_search_end: where in the string to end a search for the sep\n    data = to_native(data)\n\n    output = []\n    begin = len(data)\n    prev_begin = begin\n    sep = 1\n    while sep:\n        # Find the potential end of a passwd\n        try:\n            end = data.rindex('@', 0, begin)\n        except ValueError:\n            # No passwd in the rest of the data\n            output.insert(0, data[0:begin])\n            break\n\n        # Search for the beginning of a passwd\n        sep = None\n        sep_search_end = end\n        while not sep:\n            # URL-style username+password\n            try:\n                begin = data.rindex('://', 0, sep_search_end)\n            except ValueError:\n                # No url style in the data, check for ssh style in the\n                # rest of the string\n                begin = 0\n            # Search for separator\n            try:\n                sep = data.index(':', begin + 3, end)\n            except ValueError:\n                # No separator; choices:\n                if begin == 0:\n                    # Searched the whole string so there's no password\n                    # here.  Return the remaining data\n                    output.insert(0, data[0:prev_begin])\n                    break\n                # Search for a different beginning of the password field.\n                sep_search_end = begin\n                continue\n        if sep:\n            # Password was found; remove it.\n            output.insert(0, data[end:prev_begin])\n            output.insert(0, '********')\n            output.insert(0, data[begin:sep + 1])\n            prev_begin = begin\n\n    output = ''.join(output)\n    if no_log_values:\n        output = remove_values(output, no_log_values)\n    return output", "loc": 63}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": null, "function_name": "missing_required_lib", "parameters": ["library", "reason", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["platform.node"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def missing_required_lib(library, reason=None, url=None):\n    hostname = platform.node()\n    msg = \"Failed to import the required Python library (%s) on %s's Python %s.\" % (library, hostname, sys.executable)\n    if reason:\n        msg += \" This is required %s.\" % reason\n    if url:\n        msg += \" See %s for more info.\" % url\n\n    msg += (\" Please read the module documentation and install it in the appropriate location.\"\n            \" If the required library is installed, but Ansible is using the wrong Python interpreter,\"\n            \" please consult the documentation on ansible_python_interpreter\")\n    return msg", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "deprecate", "parameters": ["self", "msg", "version", "date", "collection_name"], "param_types": {"msg": "str", "version": "str | None", "date": "str | None", "collection_name": "str | None"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_deprecator.get_best_deprecator", "deprecate"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Record a deprecation warning to be returned with the module result. Most callers do not need to provide `collection_name` or `deprecator` -- but provide only one if needed. Specify `version` or `date`, but not both.", "source_code": "def deprecate(\n    self,\n    msg: str,\n    version: str | None = None,\n    date: str | None = None,\n    collection_name: str | None = None,\n    *,\n    deprecator: _messages.PluginInfo | None = None,\n    help_text: str | None = None,\n) -> None:\n    \"\"\"\n    Record a deprecation warning to be returned with the module result.\n    Most callers do not need to provide `collection_name` or `deprecator` -- but provide only one if needed.\n    Specify `version` or `date`, but not both.\n    If `date` is a string, it must be in the form `YYYY-MM-DD`.\n    \"\"\"\n    _skip_stackwalk = True\n\n    deprecate(  # pylint: disable=ansible-deprecated-date-not-permitted,ansible-deprecated-unnecessary-collection-name\n        msg=msg,\n        version=version,\n        date=date,\n        deprecator=_deprecator.get_best_deprecator(deprecator=deprecator, collection_name=collection_name),\n        help_text=help_text,\n    )", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "selinux_mls_enabled", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["selinux.is_selinux_mls_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_mls_enabled(self):\n    if self._selinux_mls_enabled is None:\n        self._selinux_mls_enabled = HAVE_SELINUX and selinux.is_selinux_mls_enabled() == 1\n\n    return self._selinux_mls_enabled", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "selinux_enabled", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["selinux.is_selinux_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_enabled(self):\n    if self._selinux_enabled is None:\n        self._selinux_enabled = HAVE_SELINUX and selinux.is_selinux_enabled() == 1\n\n    return self._selinux_enabled", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "selinux_initial_context", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._selinux_initial_context.append", "self.selinux_mls_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_initial_context(self):\n    if self._selinux_initial_context is None:\n        self._selinux_initial_context = [None, None, None]\n        if self.selinux_mls_enabled():\n            self._selinux_initial_context.append(None)\n\n    return self._selinux_initial_context", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "selinux_default_context", "parameters": ["self", "path", "mode"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ret[1].split", "self.selinux_enabled", "self.selinux_initial_context", "selinux.matchpathcon", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_default_context(self, path, mode=0):\n    context = self.selinux_initial_context()\n    if not self.selinux_enabled():\n        return context\n    try:\n        ret = selinux.matchpathcon(to_native(path, errors='surrogate_or_strict'), mode)\n    except OSError:\n        return context\n    if ret[0] == -1:\n        return context\n    # Limit split to 4 because the selevel, the last in the list,\n    # may contain ':' characters\n    context = ret[1].split(':', 3)\n    return context", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "selinux_context", "parameters": ["self", "path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ret[1].split", "self.fail_json", "self.selinux_enabled", "self.selinux_initial_context", "selinux.lgetfilecon_raw", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_context(self, path):\n    context = self.selinux_initial_context()\n    if not self.selinux_enabled():\n        return context\n    try:\n        ret = selinux.lgetfilecon_raw(to_native(path, errors='surrogate_or_strict'))\n    except OSError as ex:\n        self.fail_json(path=path, msg='Failed to retrieve selinux context.', exception=ex)\n    if ret[0] == -1:\n        return context\n    # Limit split to 4 because the selevel, the last in the list,\n    # may contain ':' characters\n    context = ret[1].split(':', 3)\n    return context", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "set_default_selinux_context", "parameters": ["self", "path", "changed"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.selinux_default_context", "self.selinux_enabled", "self.set_context_if_different"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_default_selinux_context(self, path, changed):\n    if not self.selinux_enabled():\n        return changed\n    context = self.selinux_default_context(path)\n    return self.set_context_if_different(path, context, False)", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "set_context_if_different", "parameters": ["self", "path", "context", "changed", "diff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["':'.join", "len", "list", "range", "self.check_file_absent_if_check_mode", "self.fail_json", "self.is_special_selinux_path", "self.selinux_context", "self.selinux_enabled", "selinux.lsetfilecon", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_context_if_different(self, path, context, changed, diff=None):\n\n    if not self.selinux_enabled():\n        return changed\n\n    if self.check_file_absent_if_check_mode(path):\n        return True\n\n    cur_context = self.selinux_context(path)\n    new_context = list(cur_context)\n    # Iterate over the current context instead of the\n    # argument context, which may have selevel.\n\n    (is_special_se, sp_context) = self.is_special_selinux_path(path)\n    if is_special_se:\n        new_context = sp_context\n    else:\n        for i in range(len(cur_context)):\n            if len(context) > i:\n                if context[i] is not None and context[i] != cur_context[i]:\n                    new_context[i] = context[i]\n                elif context[i] is None:\n                    new_context[i] = cur_context[i]\n\n    if cur_context != new_context:\n        if diff is not None:\n            if 'before' not in diff:\n                diff['before'] = {}\n            diff['before']['secontext'] = cur_context\n            if 'after' not in diff:\n                diff['after'] = {}\n            diff['after']['secontext'] = new_context\n\n        try:\n            if self.check_mode:\n                return True\n            rc = selinux.lsetfilecon(to_native(path), ':'.join(new_context))\n        except OSError as e:\n            self.fail_json(path=path, msg='invalid selinux context: %s' % to_native(e),\n                           new_context=new_context, cur_context=cur_context, input_was=context)\n        if rc != 0:\n            self.fail_json(path=path, msg='set selinux context failed')\n        changed = True\n    return changed", "loc": 44}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "get_file_attributes", "parameters": ["self", "path", "include_version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_attributes", "out.split", "res[0].strip", "res[attr_flags_idx].replace", "res[attr_flags_idx].replace('-', '').strip", "self.get_bin_path", "self.run_command"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_file_attributes(self, path, include_version=True):\n    output = {}\n    attrcmd = self.get_bin_path('lsattr', False)\n    if attrcmd:\n        flags = '-vd' if include_version else '-d'\n        attrcmd = [attrcmd, flags, path]\n        try:\n            rc, out, err = self.run_command(attrcmd)\n            if rc == 0:\n                res = out.split()\n                attr_flags_idx = 0\n                if include_version:\n                    attr_flags_idx = 1\n                    output['version'] = res[0].strip()\n                output['attr_flags'] = res[attr_flags_idx].replace('-', '').strip()\n                output['attributes'] = format_attributes(output['attr_flags'])\n        except Exception:\n            pass\n    return output", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "set_fs_attributes_if_different", "parameters": ["self", "file_args", "changed", "diff", "expand"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.set_attributes_if_different", "self.set_context_if_different", "self.set_group_if_different", "self.set_mode_if_different", "self.set_owner_if_different"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_fs_attributes_if_different(self, file_args, changed, diff=None, expand=True):\n    # set modes owners and context as needed\n    changed = self.set_context_if_different(\n        file_args['path'], file_args['secontext'], changed, diff\n    )\n    changed = self.set_owner_if_different(\n        file_args['path'], file_args['owner'], changed, diff, expand\n    )\n    changed = self.set_group_if_different(\n        file_args['path'], file_args['group'], changed, diff, expand\n    )\n    changed = self.set_mode_if_different(\n        file_args['path'], file_args['mode'], changed, diff, expand\n    )\n    changed = self.set_attributes_if_different(\n        file_args['path'], file_args['attributes'], changed, diff, expand\n    )\n    return changed", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "get_bin_path", "parameters": ["self", "arg", "required", "opt_dirs"], "param_types": {}, "return_type": null, "param_doc": {"arg": "The executable to find.", "required": "if the executable is not found and required is ``True``, fail_json", "opt_dirs": "optional list of directories to search in addition to ``PATH``"}, "return_doc": "if found return full path; otherwise return original arg, unless 'warning' then return None", "raises_doc": [], "called_functions": ["get_bin_path", "self.fail_json", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Find system executable in PATH.", "source_code": "def get_bin_path(self, arg, required=False, opt_dirs=None):\n    \"\"\"\n    Find system executable in PATH.\n\n    :param arg: The executable to find.\n    :param required: if the executable is not found and required is ``True``, fail_json\n    :param opt_dirs: optional list of directories to search in addition to ``PATH``\n    :returns: if found return full path; otherwise return original arg, unless 'warning' then return None\n    :raises: Sysexit: if arg is not found and required=True (via fail_json)\n    \"\"\"\n\n    bin_path = None\n    try:\n        bin_path = get_bin_path(arg=arg, opt_dirs=opt_dirs)\n    except ValueError as e:\n        if required:\n            self.fail_json(msg=to_text(e))\n\n    return bin_path", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "boolean", "parameters": ["self", "arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["boolean", "self.fail_json", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Convert the argument to a boolean", "source_code": "def boolean(self, arg):\n    \"\"\"Convert the argument to a boolean\"\"\"\n    if arg is None:\n        return arg\n\n    try:\n        return boolean(arg)\n    except TypeError as e:\n        self.fail_json(msg=to_native(e))", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "jsonify", "parameters": ["self", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_common_json._get_legacy_encoder", "json.dumps", "self.fail_json", "to_text"], "control_structures": ["Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def jsonify(self, data):\n    # deprecated: description='deprecate AnsibleModule.jsonify()' core_version='2.23'\n    # deprecate(\n    #     msg=\"The `AnsibleModule.jsonify' method is deprecated.\",\n    #     version=\"2.27\",\n    #     # help_text=\"\",  # DTFIX-FUTURE: fill in this help text\n    # )\n\n    try:\n        return json.dumps(data, cls=_common_json._get_legacy_encoder())\n    except UnicodeError as e:\n        self.fail_json(msg=to_text(e))", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "exit_json", "parameters": ["self"], "param_types": {}, "return_type": "t.NoReturn", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._return_formatted", "self.do_cleanup_files", "sys.exit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "return from the module, without error", "source_code": "def exit_json(self, **kwargs) -> t.NoReturn:\n    \"\"\" return from the module, without error \"\"\"\n    _skip_stackwalk = True\n\n    self.do_cleanup_files()\n    self._return_formatted(kwargs)\n    sys.exit(0)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "fail_json", "parameters": ["self", "msg"], "param_types": {"msg": "str"}, "return_type": "t.NoReturn", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_errors.EventFactory.from_exception", "_messages.ErrorSummary", "_messages.Event", "_messages.EventChain", "_traceback.is_traceback_enabled", "_traceback.maybe_capture_traceback", "_traceback.maybe_extract_traceback", "isinstance", "kwargs.update", "self._return_formatted", "self.do_cleanup_files", "str", "sys.exc_info", "sys.exit", "t.cast"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return from the module with an error message and optional exception/traceback detail. A traceback will only be included in the result if error traceback capturing has been enabled. When `exception` is an exception object, its message chain will be automatically combined with `msg` to create the final error message.", "source_code": "def fail_json(self, msg: str, *, exception: BaseException | str | None = _UNSET, **kwargs) -> t.NoReturn:\n    \"\"\"\n    Return from the module with an error message and optional exception/traceback detail.\n    A traceback will only be included in the result if error traceback capturing has been enabled.\n\n    When `exception` is an exception object, its message chain will be automatically combined with `msg` to create the final error message.\n    The message chain includes the exception's message as well as messages from any __cause__ exceptions.\n    The traceback from `exception` will be used for the formatted traceback.\n\n    When `exception` is a string, it will be used as the formatted traceback.\n\n    When `exception` is set to `None`, the current call stack will be used for the formatted traceback.\n\n    When `exception` is not specified, a formatted traceback will be retrieved from the current exception.\n    If no exception is pending, the current call stack will be used instead.\n    \"\"\"\n    _skip_stackwalk = True\n\n    msg = str(msg)  # coerce to str instead of raising an error due to an invalid type\n\n    kwargs.update(\n        failed=True,\n        msg=msg,\n    )\n\n    if isinstance(exception, BaseException):\n        # Include a `_messages.Event` in the result.\n        # The `msg` is included in the chain to ensure it is not lost when looking only at `exception` from the result.\n\n        kwargs.update(\n            exception=_messages.ErrorSummary(\n                event=_messages.Event(\n                    msg=msg,\n                    formatted_traceback=_traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.ERROR),\n                    chain=_messages.EventChain(\n                        msg_reason=_errors.MSG_REASON_DIRECT_CAUSE,\n                        traceback_reason=\"The above exception was the direct cause of the following error:\",\n                        event=_errors.EventFactory.from_exception(exception, _traceback.is_traceback_enabled(_traceback.TracebackEvent.ERROR)),\n                    ),\n                ),\n            ),\n        )\n    elif _traceback.is_traceback_enabled(_traceback.TracebackEvent.ERROR):\n        # Include only a formatted traceback string in the result.\n        # The controller will combine this with `msg` to create an `_messages.ErrorSummary`.\n\n        formatted_traceback: str | None\n\n        if isinstance(exception, str):\n            formatted_traceback = exception\n        elif exception is _UNSET and (current_exception := t.cast(t.Optional[BaseException], sys.exc_info()[1])):\n            formatted_traceback = _traceback.maybe_extract_traceback(current_exception, _traceback.TracebackEvent.ERROR)\n        else:\n            formatted_traceback = _traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.ERROR)\n\n        if formatted_traceback:\n            kwargs.update(exception=formatted_traceback)\n\n    self.do_cleanup_files()\n    self._return_formatted(kwargs)\n\n    sys.exit(1)", "loc": 62}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "fail_on_missing_params", "parameters": ["self", "required_params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["check_missing_parameters", "self.fail_json", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fail_on_missing_params(self, required_params=None):\n    if not required_params:\n        return\n    try:\n        check_missing_parameters(self.params, required_params)\n    except TypeError as e:\n        self.fail_json(msg=to_native(e))", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "md5", "parameters": ["self", "filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self.digest_from_file"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return MD5 hex digest of local file using digest_from_file(). Do not use this function unless you have no other choice for: 1) Optional backwards compatibility", "source_code": "def md5(self, filename):\n    \"\"\" Return MD5 hex digest of local file using digest_from_file().\n\n    Do not use this function unless you have no other choice for:\n        1) Optional backwards compatibility\n        2) Compatibility with a third party protocol\n\n    This function will not work on systems complying with FIPS-140-2.\n\n    Most uses of this function can use the module.sha1 function instead.\n    \"\"\"\n    if 'md5' not in AVAILABLE_HASH_ALGORITHMS:\n        raise ValueError('MD5 not available.  Possibly running in FIPS mode')\n    return self.digest_from_file(filename, 'md5')", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "preserved_copy", "parameters": ["self", "src", "dest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["current_attribs.get", "os.chown", "os.stat", "self.get_file_attributes", "self.selinux_context", "self.selinux_enabled", "self.set_attributes_if_different", "self.set_context_if_different", "shutil.copy2"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Copy a file with preserved ownership, permissions and context", "source_code": "def preserved_copy(self, src, dest):\n    \"\"\"Copy a file with preserved ownership, permissions and context\"\"\"\n\n    # shutil.copy2(src, dst)\n    #   Similar to shutil.copy(), but metadata is copied as well - in fact,\n    #   this is just shutil.copy() followed by copystat(). This is similar\n    #   to the Unix command cp -p.\n\n    # shutil.copystat(src, dst)\n    #   Copy the permission bits, last access time, last modification time,\n    #   and flags from src to dst. The file contents, owner, and group are\n    #   unaffected. src and dst are path names given as strings.\n\n    shutil.copy2(src, dest)\n\n    # Set the context\n    if self.selinux_enabled():\n        context = self.selinux_context(src)\n        self.set_context_if_different(dest, context, False)\n\n    # chown it\n    try:\n        dest_stat = os.stat(src)\n        tmp_stat = os.stat(dest)\n        if dest_stat and (tmp_stat.st_uid != dest_stat.st_uid or tmp_stat.st_gid != dest_stat.st_gid):\n            os.chown(dest, dest_stat.st_uid, dest_stat.st_gid)\n    except OSError as e:\n        if e.errno != errno.EPERM:\n            raise\n\n    # Set the attributes\n    current_attribs = self.get_file_attributes(src, include_version=False)\n    current_attribs = current_attribs.get('attr_flags', '')\n    self.set_attributes_if_different(dest, current_attribs, True)", "loc": 34}
{"file": "ansible\\lib\\ansible\\module_utils\\basic.py", "class_name": "AnsibleModule", "function_name": "get_buffer_size", "parameters": ["fd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fcntl.fcntl"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_buffer_size(fd):\n    try:\n        # 1032 == FZ_GETPIPE_SZ\n        buffer_size = fcntl.fcntl(fd, 1032)\n    except Exception:\n        try:\n            # not as exact as above, but should be good enough for most platforms that fail the previous call\n            buffer_size = select.PIPE_BUF\n        except Exception:\n            buffer_size = 9000  # use logical default JIC\n\n    return buffer_size", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\connection.py", "class_name": null, "function_name": "write_to_stream", "parameters": ["stream", "obj"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "pickle.dumps", "stream.write"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Write a length+newline-prefixed pickled object to a stream.", "source_code": "def write_to_stream(stream, obj):\n    \"\"\"Write a length+newline-prefixed pickled object to a stream.\"\"\"\n    src = pickle.dumps(obj)\n\n    stream.write(b'%d\\n' % len(src))\n    stream.write(src)", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\connection.py", "class_name": null, "function_name": "recv_data", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "s.recv", "struct.unpack", "to_bytes"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def recv_data(s):\n    header_len = 8  # size of a packed unsigned long long\n    data = to_bytes(\"\")\n    while len(data) < header_len:\n        d = s.recv(header_len - len(data))\n        if not d:\n            return None\n        data += d\n    data_len = struct.unpack('!Q', data[:header_len])[0]\n    data = data[header_len:]\n    while len(data) < data_len:\n        d = s.recv(data_len - len(data))\n        if not d:\n            return None\n        data += d\n    return data", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\connection.py", "class_name": null, "function_name": "exec_command", "parameters": ["module", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Connection", "connection.exec_command", "getattr", "to_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_command(module, command):\n    connection = Connection(module._socket_path)\n    try:\n        out = connection.exec_command(command)\n    except ConnectionError as exc:\n        code = getattr(exc, 'code', 1)\n        message = getattr(exc, 'err', exc)\n        return code, '', to_text(message, errors='surrogate_then_replace')\n    return 0, out, ''", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\connection.py", "class_name": null, "function_name": "request_builder", "parameters": ["method_"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["str", "uuid.uuid4"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def request_builder(method_, *args, **kwargs):\n    reqid = str(uuid.uuid4())\n    req = {'jsonrpc': '2.0', 'method': method_, 'id': reqid}\n    req['params'] = (args, kwargs)\n\n    return req", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\connection.py", "class_name": "Connection", "function_name": "send", "parameters": ["self", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ConnectionError", "recv_data", "send_data", "sf.close", "sf.connect", "socket.socket", "to_bytes", "to_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def send(self, data):\n    try:\n        sf = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sf.connect(self.socket_path)\n\n        send_data(sf, to_bytes(data))\n        response = recv_data(sf)\n\n    except OSError as ex:\n        sf.close()\n        raise ConnectionError(\n            f'Unable to connect to socket {self.socket_path!r}. See the socket path issue category in '\n            'Network Debug and Troubleshooting Guide.',\n        ) from ex\n\n    sf.close()\n\n    return to_text(response, errors='surrogate_or_strict')", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\datatag.py", "class_name": null, "function_name": "deprecate_value", "parameters": ["value", "msg"], "param_types": {"value": "_T", "msg": "str"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_deprecator.get_best_deprecator", "_tags.Deprecated", "_traceback.maybe_capture_traceback", "deprecated.tag"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return `value` tagged with the given deprecation details. The types `None` and `bool` cannot be deprecated and are returned unmodified.", "source_code": "def deprecate_value(\n    value: _T,\n    msg: str,\n    *,\n    version: str | None = None,\n    date: str | None = None,\n    collection_name: str | None = None,\n    deprecator: _messages.PluginInfo | None = None,\n    help_text: str | None = None,\n) -> _T:\n    \"\"\"\n    Return `value` tagged with the given deprecation details.\n    The types `None` and `bool` cannot be deprecated and are returned unmodified.\n    Raises a `TypeError` if `value` is not a supported type.\n    Most callers do not need to provide `collection_name` or `deprecator` -- but provide only one if needed.\n    Specify `version` or `date`, but not both.\n    If `date` is provided, it should be in the form `YYYY-MM-DD`.\n    \"\"\"\n    _skip_stackwalk = True\n\n    deprecated = _tags.Deprecated(\n        msg=msg,\n        help_text=help_text,\n        date=date,\n        version=version,\n        deprecator=_deprecator.get_best_deprecator(deprecator=deprecator, collection_name=collection_name),\n        formatted_traceback=_traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.DEPRECATED_VALUE),\n    )\n\n    return deprecated.tag(value)", "loc": 30}
{"file": "ansible\\lib\\ansible\\module_utils\\service.py", "class_name": null, "function_name": "get_sysv_script", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["name.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "This function will return the expected path for an init script corresponding to the service name supplied. :arg name: name or path of the service to test for", "source_code": "def get_sysv_script(name):\n    \"\"\"\n    This function will return the expected path for an init script\n    corresponding to the service name supplied.\n\n    :arg name: name or path of the service to test for\n    \"\"\"\n    if name.startswith('/'):\n        result = name\n    else:\n        result = '/etc/init.d/%s' % name\n\n    return result", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\service.py", "class_name": null, "function_name": "get_ps", "parameters": ["module", "pattern"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.get_bin_path", "module.run_command", "platform.system", "psout.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Last resort to find a service by trying to match pattern to programs in memory", "source_code": "def get_ps(module, pattern):\n    \"\"\"\n    Last resort to find a service by trying to match pattern to programs in memory\n    \"\"\"\n    found = False\n    if platform.system() == 'SunOS':\n        flags = '-ef'\n    else:\n        flags = 'auxww'\n    psbin = module.get_bin_path('ps', True)\n\n    (rc, psout, pserr) = module.run_command([psbin, flags])\n    if rc == 0:\n        for line in psout.splitlines():\n            if pattern in line:\n                # FIXME: should add logic to prevent matching 'self', though that should be extremely rare\n                found = True\n                break\n    return found", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\service.py", "class_name": null, "function_name": "fail_if_missing", "parameters": ["module", "found", "service", "msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.fail_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "This function will return an error or exit gracefully depending on check mode status and if the service is missing or not. :arg module: is an AnsibleModule object, used for it's utility methods", "source_code": "def fail_if_missing(module, found, service, msg=''):\n    \"\"\"\n    This function will return an error or exit gracefully depending on check mode status\n    and if the service is missing or not.\n\n    :arg module: is an AnsibleModule object, used for it's utility methods\n    :arg found: boolean indicating if services were found or not\n    :arg service: name of service\n    :kw msg: extra info to append to error/success msg when missing\n    \"\"\"\n    if not found:\n        module.fail_json(msg='Could not find the requested service %s: %s' % (service, msg))", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\service.py", "class_name": null, "function_name": "daemonize", "parameters": ["module", "cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fds.remove", "fork_process", "isinstance", "list", "module.fail_json", "os._exit", "os.close", "os.pipe", "os.read", "os.waitpid", "os.write", "out.fileno", "p.poll", "p.wait", "pickle.dumps", "pickle.loads", "run_cmd.append", "select.select", "shlex.split", "subprocess.Popen", "to_bytes", "to_text"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Execute a command while detaching as a daemon, returns rc, stdout, and stderr. :arg module: is an AnsibleModule object, used for it's utility methods :arg cmd: is a list or string representing the command and options to run", "source_code": "def daemonize(module, cmd):\n    \"\"\"\n    Execute a command while detaching as a daemon, returns rc, stdout, and stderr.\n\n    :arg module: is an AnsibleModule object, used for it's utility methods\n    :arg cmd: is a list or string representing the command and options to run\n\n    This is complex because daemonization is hard for people.\n    What we do is daemonize a part of this module, the daemon runs the command,\n    picks up the return code and output, and returns it to the main process.\n    \"\"\"\n\n    # init some vars\n    chunk = 4096  # FIXME: pass in as arg?\n    errors = 'surrogate_or_strict'\n\n    # start it!\n    try:\n        pipe = os.pipe()\n        pid = fork_process()\n    except (OSError, RuntimeError):\n        module.fail_json(msg=\"Error while attempting to fork.\")\n    except Exception as exc:\n        module.fail_json(msg=to_text(exc))\n\n    # we don't do any locking as this should be a unique module/process\n    if pid == 0:\n        os.close(pipe[0])\n\n        if not isinstance(cmd, list):\n            cmd = shlex.split(to_text(cmd, errors=errors))\n\n        # make sure we always use byte strings\n        run_cmd = []\n        for c in cmd:\n            run_cmd.append(to_bytes(c, errors=errors))\n\n        # execute the command in forked process\n        p = subprocess.Popen(run_cmd, shell=False, stdout=subprocess.PIPE, stderr=subprocess.PIPE, preexec_fn=lambda: os.close(pipe[1]))\n        fds = [p.stdout, p.stderr]\n\n        # loop reading output till it is done\n        output = {p.stdout: b\"\", p.stderr: b\"\"}\n        while fds:\n            rfd, wfd, efd = select.select(fds, [], fds, 1)\n            if (rfd + wfd + efd) or p.poll() is None:\n                for out in list(fds):\n                    if out in rfd:\n                        data = os.read(out.fileno(), chunk)\n                        if data:\n                            output[out] += to_bytes(data, errors=errors)\n                        else:\n                            fds.remove(out)\n            else:\n                break\n\n        # even after fds close, we might want to wait for pid to die\n        p.wait()\n\n        # Return a pickled data of parent\n        return_data = pickle.dumps([p.returncode, to_text(output[p.stdout]), to_text(output[p.stderr])], protocol=pickle.HIGHEST_PROTOCOL)\n        os.write(pipe[1], to_bytes(return_data, errors=errors))\n\n        # clean up\n        os.close(pipe[1])\n        os._exit(0)\n\n    elif pid == -1:\n        module.fail_json(msg=\"Unable to fork, no exception thrown, probably due to lack of resources, check logs.\")\n\n    else:\n        # in parent\n        os.close(pipe[1])\n        os.waitpid(pid, 0)\n\n        # Grab response data after child finishes\n        return_data = b\"\"\n        while True:\n            rfd, wfd, efd = select.select([pipe[0]], [], [pipe[0]])\n            if pipe[0] in rfd:\n                data = os.read(pipe[0], chunk)\n                if not data:\n                    break\n                return_data += to_bytes(data, errors=errors)\n\n        return pickle.loads(to_bytes(return_data, errors=errors))", "loc": 86}
{"file": "ansible\\lib\\ansible\\module_utils\\service.py", "class_name": null, "function_name": "check_ps", "parameters": ["module", "pattern"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.get_bin_path", "module.run_command", "out.split", "platform.system"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_ps(module, pattern):\n\n    # Set ps flags\n    if platform.system() == 'SunOS':\n        psflags = '-ef'\n    else:\n        psflags = 'auxww'\n\n    # Find ps binary\n    psbin = module.get_bin_path('ps', True)\n\n    (rc, out, err) = module.run_command('%s %s' % (psbin, psflags))\n    # If rc is 0, set running as appropriate\n    if rc == 0:\n        for line in out.split('\\n'):\n            if pattern in line:\n                return True\n    return False", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\splitter.py", "class_name": null, "function_name": "unquote", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_quoted"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "removes first and last quotes from a string, if the string starts and ends with the same quotes", "source_code": "def unquote(data):\n    \"\"\" removes first and last quotes from a string, if the string starts and ends with the same quotes \"\"\"\n    if is_quoted(data):\n        return data[1:-1]\n    return data", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "generic_urlparse", "parameters": ["parts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ParseResultDottedDict", "parts._asdict", "result.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generic_urlparse(parts):\n    \"\"\"\n    Returns a dictionary of url parts as parsed by urlparse,\n    but accounts for the fact that older versions of that\n    library do not support named attributes (ie. .netloc)\n\n    This method isn't of much use any longer, but is kept\n    in a minimal state for backwards compat.\n    \"\"\"\n    result = ParseResultDottedDict(parts._asdict())\n    result.update({\n        'username': parts.username,\n        'password': parts.password,\n        'hostname': parts.hostname,\n        'port': parts.port,\n    })\n    return result", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "parse_content_type", "parameters": ["response"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["content_type.split", "get_param", "get_param('charset') or 'utf-8'.split", "get_type", "get_type() or 'application/octet-stream'.split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_content_type(response):\n    get_type = response.headers.get_content_type\n    get_param = response.headers.get_param\n    content_type = (get_type() or 'application/octet-stream').split(',')[0]\n    main_type, sub_type = content_type.split('/')\n    charset = (get_param('charset') or 'utf-8').split(',')[0]\n    return content_type, main_type, sub_type, charset", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "make_context", "parameters": ["cafile", "cadata", "capath", "ciphers", "validate_certs", "client_cert", "client_key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["':'.join", "TypeError", "bytearray", "cadata.extend", "context.load_cert_chain", "context.load_verify_locations", "context.set_ciphers", "get_ca_certs", "hasattr", "is_sequence", "map", "ssl.create_default_context"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_context(cafile=None, cadata=None, capath=None, ciphers=None, validate_certs=True, client_cert=None,\n                 client_key=None):\n    if ciphers is None:\n        ciphers = []\n\n    if not is_sequence(ciphers):\n        raise TypeError('Ciphers must be a list. Got %s.' % ciphers.__class__.__name__)\n\n    context = ssl.create_default_context(cafile=cafile)\n\n    if not validate_certs:\n        context.options |= ssl.OP_NO_SSLv3\n        context.check_hostname = False\n        context.verify_mode = ssl.CERT_NONE\n\n    # If cafile is passed, we are only using that for verification,\n    # don't add additional ca certs\n    if validate_certs and not cafile:\n        if not cadata:\n            cadata = bytearray()\n        cadata.extend(get_ca_certs(capath=capath)[0])\n        if cadata:\n            context.load_verify_locations(cadata=cadata)\n\n    if ciphers:\n        context.set_ciphers(':'.join(map(to_native, ciphers)))\n\n    if client_cert:\n        # TLS 1.3 needs this to be set to True to allow post handshake cert\n        # authentication. This functionality was added in Python 3.8 and was\n        # backported to 3.6.7, and 3.7.1 so needs a check for now.\n        if hasattr(context, \"post_handshake_auth\"):\n            context.post_handshake_auth = True\n\n        context.load_cert_chain(client_cert, keyfile=client_key)\n\n    return context", "loc": 37}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "getpeercert", "parameters": ["response", "binary_form"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["socket.getpeercert"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Attempt to get the peer certificate of the response from urlopen.", "source_code": "def getpeercert(response, binary_form=False):\n    \"\"\" Attempt to get the peer certificate of the response from urlopen. \"\"\"\n    socket = response.fp.raw._sock\n\n    try:\n        return socket.getpeercert(binary_form)\n    except AttributeError:\n        pass  # Not HTTPS", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "get_channel_binding_cert_hash", "parameters": ["certificate_der"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["default_backend", "digest.finalize", "digest.update", "hashes.Hash", "hashes.SHA256", "x509.load_der_x509_certificate"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Gets the channel binding app data for a TLS connection using the peer cert.", "source_code": "def get_channel_binding_cert_hash(certificate_der):\n    \"\"\" Gets the channel binding app data for a TLS connection using the peer cert. \"\"\"\n    if not HAS_CRYPTOGRAPHY:\n        return\n\n    # Logic documented in RFC 5929 section 4 https://tools.ietf.org/html/rfc5929#section-4\n    cert = x509.load_der_x509_certificate(certificate_der, default_backend())\n\n    hash_algorithm = None\n    try:\n        hash_algorithm = cert.signature_hash_algorithm\n    except UnsupportedAlgorithm:\n        pass\n\n    # If the signature hash algorithm is unknown/unsupported or md5/sha1 we must use SHA256.\n    if not hash_algorithm or hash_algorithm.name in ('md5', 'sha1'):\n        hash_algorithm = hashes.SHA256()\n\n    digest = hashes.Hash(hash_algorithm, default_backend())\n    digest.update(certificate_der)\n    return digest.finalize()", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "set_multipart_encoding", "parameters": ["encoding"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "encoders_dict.get", "encoders_dict.keys", "repr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Takes an string with specific encoding type for multipart data. Will return reference to function from email.encoders library. If given string key doesn't exist it will raise a ValueError", "source_code": "def set_multipart_encoding(encoding):\n    \"\"\"Takes an string with specific encoding type for multipart data.\n    Will return reference to function from email.encoders library.\n    If given string key doesn't exist it will raise a ValueError\"\"\"\n    encoders_dict = {\n        \"base64\": email.encoders.encode_base64,\n        \"7or8bit\": email.encoders.encode_7or8bit\n    }\n    if encoders_dict.get(encoding):\n        return encoders_dict.get(encoding)\n    else:\n        raise ValueError(\"multipart_encoding must be one of %s.\" % repr(encoders_dict.keys()))", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": null, "function_name": "basic_auth_header", "parameters": ["username", "password"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["base64.b64encode", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Takes a username and password and returns a byte string suitable for using as value of an Authorization header to do basic auth.", "source_code": "def basic_auth_header(username, password):\n    \"\"\"Takes a username and password and returns a byte string suitable for\n    using as value of an Authorization header to do basic auth.\n    \"\"\"\n    if password is None:\n        password = ''\n    return b\"Basic %s\" % base64.b64encode(to_bytes(\"%s:%s\" % (username, password), errors='surrogate_or_strict'))", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": "UnixHTTPConnection", "function_name": "connect", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "self.sock.connect", "self.sock.settimeout", "socket.socket"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def connect(self):\n    self.sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n    try:\n        self.sock.connect(self._unix_socket)\n    except OSError as e:\n        raise OSError('Invalid Socket File (%s): %s' % (self._unix_socket, e))\n    if self.timeout is not socket._GLOBAL_DEFAULT_TIMEOUT:\n        self.sock.settimeout(self.timeout)", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\urls.py", "class_name": "GzipDecodedReader", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gzip.GzipFile.close", "self._io.close"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    try:\n        gzip.GzipFile.close(self)\n    finally:\n        self._io.close()", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\yumdnf.py", "class_name": "YumDnf", "function_name": "listify_comma_sep_strings_in_list", "parameters": ["self", "some_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["e.strip", "element.split", "new_list.extend", "remove_from_original_list.append", "some_list.extend", "some_list.remove"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "method to accept a list of strings as the parameter, find any strings in that list that are comma separated, remove them from the list and add their comma separated elements to the original list", "source_code": "def listify_comma_sep_strings_in_list(self, some_list):\n    \"\"\"\n    method to accept a list of strings as the parameter, find any strings\n    in that list that are comma separated, remove them from the list and add\n    their comma separated elements to the original list\n    \"\"\"\n    new_list = []\n    remove_from_original_list = []\n    for element in some_list:\n        if ',' in element:\n            remove_from_original_list.append(element)\n            new_list.extend([e.strip() for e in element.split(',')])\n\n    for element in remove_from_original_list:\n        some_list.remove(element)\n\n    some_list.extend(new_list)\n\n    if some_list == [\"\"]:\n        return []\n\n    return some_list", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\arg_spec.py", "class_name": "ModuleArgumentSpecValidator", "function_name": "validate", "parameters": ["self", "parameters"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Both option {option} and its alias {alias} are set.'.format", "d.get", "deprecate", "deprecator_from_collection_name", "super", "super(ModuleArgumentSpecValidator, self).validate", "warn"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def validate(self, parameters):\n    result = super(ModuleArgumentSpecValidator, self).validate(parameters)\n\n    for d in result._deprecations:\n        # DTFIX-FUTURE: pass an actual deprecator instead of one derived from collection_name\n        deprecate(  # pylint: disable=ansible-deprecated-date-not-permitted,ansible-deprecated-unnecessary-collection-name\n            msg=d['msg'],\n            version=d.get('version'),\n            date=d.get('date'),\n            deprecator=deprecator_from_collection_name(d.get('collection_name')),\n        )\n\n    for w in result._warnings:\n        warn('Both option {option} and its alias {alias} are set.'.format(option=w['option'], alias=w['alias']))\n\n    return result", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\collections.py", "class_name": null, "function_name": "is_iterable", "parameters": ["seq", "include_strings"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_string", "iter"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Identify whether the input is an iterable.", "source_code": "def is_iterable(seq, include_strings=False):\n    \"\"\"Identify whether the input is an iterable.\"\"\"\n    if not include_strings and is_string(seq):\n        return False\n\n    try:\n        iter(seq)\n        return True\n    except TypeError:\n        return False", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\collections.py", "class_name": null, "function_name": "is_sequence", "parameters": ["seq", "include_strings"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_string", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Identify whether the input is a sequence. Strings and bytes are not sequences here, unless ``include_string`` is ``True``.", "source_code": "def is_sequence(seq, include_strings=False):\n    \"\"\"Identify whether the input is a sequence.\n\n    Strings and bytes are not sequences here,\n    unless ``include_string`` is ``True``.\n\n    Non-indexable things are never of a sequence type.\n    \"\"\"\n    if not include_strings and is_string(seq):\n        return False\n\n    return isinstance(seq, Sequence)", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\collections.py", "class_name": null, "function_name": "count", "parameters": ["seq"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "_warnings.deprecate", "counters.get", "dict", "is_iterable"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def count(seq):\n    \"\"\"Returns a dictionary with the number of appearances of each element of the iterable.\n\n    Resembles the collections.Counter class functionality. It is meant to be used when the\n    code is run on Python 2.6.* where collections.Counter is not available. It should be\n    deprecated and replaced when support for Python < 2.7 is dropped.\n    \"\"\"\n    _warnings.deprecate(\n        msg=\"The `ansible.module_utils.common.collections.count` function is deprecated.\",\n        version=\"2.23\",\n        help_text=\"Use `collections.Counter` from the Python standard library instead.\",\n    )\n    if not is_iterable(seq):\n        raise Exception('Argument provided  is not an iterable')\n    counters = dict()\n    for elem in seq:\n        counters[elem] = counters.get(elem, 0) + 1\n    return counters", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\collections.py", "class_name": "ImmutableDict", "function_name": "difference", "parameters": ["self", "subtractive_iterable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A copy of the ImmutableDict with keys from the subtractive_iterable removed", "raises_doc": [], "called_functions": ["ImmutableDict", "frozenset", "self._store.keys"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create an ImmutableDict as a combination of the original minus keys in subtractive_iterable :arg subtractive_iterable: Any iterable containing keys that should not be present in the new ImmutableDict", "source_code": "def difference(self, subtractive_iterable):\n    \"\"\"\n    Create an ImmutableDict as a combination of the original minus keys in subtractive_iterable\n\n    :arg subtractive_iterable: Any iterable containing keys that should not be present in the\n        new ImmutableDict\n    :return: A copy of the ImmutableDict with keys from the subtractive_iterable removed\n    \"\"\"\n    remove_keys = frozenset(subtractive_iterable)\n    keys = (k for k in self._store.keys() if k not in remove_keys)\n    return ImmutableDict((k, self._store[k]) for k in keys)", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "camel_dict_to_snake_dict", "parameters": ["camel_dict", "reversible", "ignore_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_camel_to_snake", "camel_dict.items", "camel_dict_to_snake_dict", "checked_list.append", "isinstance", "value_is_list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "reversible allows two way conversion of a camelized dict such that snake_dict_to_camel_dict(camel_dict_to_snake_dict(x)) == x This is achieved through mapping e.g. HTTPEndpoint to h_t_t_p_endpoint", "source_code": "def camel_dict_to_snake_dict(camel_dict, reversible=False, ignore_list=()):\n    \"\"\"\n    reversible allows two way conversion of a camelized dict\n    such that snake_dict_to_camel_dict(camel_dict_to_snake_dict(x)) == x\n\n    This is achieved through mapping e.g. HTTPEndpoint to h_t_t_p_endpoint\n    where the default would be simply http_endpoint, which gets turned into\n    HttpEndpoint if recamelized.\n\n    ignore_list is used to avoid converting a sub-tree of a dict. This is\n    particularly important for tags, where keys are case-sensitive. We convert\n    the 'Tags' key but nothing below.\n    \"\"\"\n\n    def value_is_list(camel_list):\n\n        checked_list = []\n        for item in camel_list:\n            if isinstance(item, dict):\n                checked_list.append(camel_dict_to_snake_dict(item, reversible))\n            elif isinstance(item, list):\n                checked_list.append(value_is_list(item))\n            else:\n                checked_list.append(item)\n\n        return checked_list\n\n    snake_dict = {}\n    for k, v in camel_dict.items():\n        if isinstance(v, dict) and k not in ignore_list:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = camel_dict_to_snake_dict(v, reversible)\n        elif isinstance(v, list) and k not in ignore_list:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = value_is_list(v)\n        else:\n            snake_dict[_camel_to_snake(k, reversible=reversible)] = v\n\n    return snake_dict", "loc": 37}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "snake_dict_to_camel_dict", "parameters": ["snake_dict", "capitalize_first"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_snake_to_camel", "camelize", "isinstance", "len", "new_type.append", "range", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Perhaps unexpectedly, snake_dict_to_camel_dict returns dromedaryCase rather than true CamelCase. Passing capitalize_first=True returns CamelCase. The default remains False as that was the original implementation", "source_code": "def snake_dict_to_camel_dict(snake_dict, capitalize_first=False):\n    \"\"\"\n    Perhaps unexpectedly, snake_dict_to_camel_dict returns dromedaryCase\n    rather than true CamelCase. Passing capitalize_first=True returns\n    CamelCase. The default remains False as that was the original implementation\n    \"\"\"\n\n    def camelize(complex_type, capitalize_first=False):\n        if complex_type is None:\n            return\n        new_type = type(complex_type)()\n        if isinstance(complex_type, dict):\n            for key in complex_type:\n                new_type[_snake_to_camel(key, capitalize_first)] = camelize(complex_type[key], capitalize_first)\n        elif isinstance(complex_type, list):\n            for i in range(len(complex_type)):\n                new_type.append(camelize(complex_type[i], capitalize_first))\n        else:\n            return complex_type\n        return new_type\n\n    return camelize(snake_dict, capitalize_first)", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "dict_merge", "parameters": ["a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["b.items", "deepcopy", "dict_merge", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "recursively merges dicts. not just simple a['key'] = b['key'], if both a and b have a key whose value is a dict then dict_merge is called on both values and the result stored in the returned dictionary.", "source_code": "def dict_merge(a, b):\n    \"\"\"recursively merges dicts. not just simple a['key'] = b['key'], if\n    both a and b have a key whose value is a dict then dict_merge is called\n    on both values and the result stored in the returned dictionary.\"\"\"\n    if not isinstance(b, dict):\n        return b\n    result = deepcopy(a)\n    for k, v in b.items():\n        if k in result and isinstance(result[k], dict):\n            result[k] = dict_merge(result[k], v)\n        else:\n            result[k] = deepcopy(v)\n    return result", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "recursive_diff", "parameters": ["dict1", "dict2"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Tuple of dictionaries of differences or ``None`` if there are no differences.", "raises_doc": [], "called_functions": ["TypeError", "all", "dict", "dict1.items", "dict1.keys", "dict2.items", "dict2.keys", "isinstance", "recursive_diff", "set", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Recursively diff two dictionaries", "source_code": "def recursive_diff(dict1, dict2):\n    \"\"\"Recursively diff two dictionaries\n\n    Raises ``TypeError`` for incorrect argument type.\n\n    :arg dict1: Dictionary to compare against.\n    :arg dict2: Dictionary to compare with ``dict1``.\n    :return: Tuple of dictionaries of differences or ``None`` if there are no differences.\n    \"\"\"\n\n    if not all((isinstance(item, MutableMapping) for item in (dict1, dict2))):\n        raise TypeError(\"Unable to diff 'dict1' %s and 'dict2' %s. \"\n                        \"Both must be a dictionary.\" % (type(dict1), type(dict2)))\n\n    left = dict((k, v) for (k, v) in dict1.items() if k not in dict2)\n    right = dict((k, v) for (k, v) in dict2.items() if k not in dict1)\n    for k in (set(dict1.keys()) & set(dict2.keys())):\n        if isinstance(dict1[k], dict) and isinstance(dict2[k], dict):\n            result = recursive_diff(dict1[k], dict2[k])\n            if result:\n                left[k] = result[0]\n                right[k] = result[1]\n        elif dict1[k] != dict2[k]:\n            left[k] = dict1[k]\n            right[k] = dict2[k]\n    if left or right:\n        return left, right\n    return None", "loc": 28}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "value_is_list", "parameters": ["camel_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["camel_dict_to_snake_dict", "checked_list.append", "isinstance", "value_is_list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def value_is_list(camel_list):\n\n    checked_list = []\n    for item in camel_list:\n        if isinstance(item, dict):\n            checked_list.append(camel_dict_to_snake_dict(item, reversible))\n        elif isinstance(item, list):\n            checked_list.append(value_is_list(item))\n        else:\n            checked_list.append(item)\n\n    return checked_list", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\dict_transformations.py", "class_name": null, "function_name": "camelize", "parameters": ["complex_type", "capitalize_first"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_snake_to_camel", "camelize", "isinstance", "len", "new_type.append", "range", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def camelize(complex_type, capitalize_first=False):\n    if complex_type is None:\n        return\n    new_type = type(complex_type)()\n    if isinstance(complex_type, dict):\n        for key in complex_type:\n            new_type[_snake_to_camel(key, capitalize_first)] = camelize(complex_type[key], capitalize_first)\n    elif isinstance(complex_type, list):\n        for i in range(len(complex_type)):\n            new_type.append(camelize(complex_type[i], capitalize_first))\n    else:\n        return complex_type\n    return new_type", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\locale.py", "class_name": null, "function_name": "get_best_parsable_locale", "parameters": ["module", "preferences", "raise_on_locale"], "param_types": {}, "return_type": null, "param_doc": {"module": "an AnsibleModule instance", "preferences": "A list of preferred locales, in order of preference", "raise_on_locale": "boolean that determines if we raise exception or not"}, "return_doc": "The first matched preferred locale or 'C' which is the default", "raises_doc": [], "called_functions": ["RuntimeWarning", "module.debug", "module.get_bin_path", "module.run_command", "out.strip", "out.strip().splitlines", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Attempts to return the best possible locale for parsing output in English useful for scraping output with i18n tools. When this raises an exception and the caller wants to continue, it should use the 'C' locale.", "source_code": "def get_best_parsable_locale(module, preferences=None, raise_on_locale=False):\n    \"\"\"\n        Attempts to return the best possible locale for parsing output in English\n        useful for scraping output with i18n tools. When this raises an exception\n        and the caller wants to continue, it should use the 'C' locale.\n\n        :param module: an AnsibleModule instance\n        :param preferences: A list of preferred locales, in order of preference\n        :param raise_on_locale: boolean that determines if we raise exception or not\n                                due to locale CLI issues\n        :returns: The first matched preferred locale or 'C' which is the default\n    \"\"\"\n\n    found = 'C'  # default posix, its ascii but always there\n    try:\n        locale = module.get_bin_path(\"locale\")\n        if not locale:\n            # not using required=true as that forces fail_json\n            raise RuntimeWarning(\"Could not find 'locale' tool\")\n\n        available = []\n\n        if preferences is None:\n            # new POSIX standard or English cause those are messages core team expects\n            # yes, the last 2 are the same but some systems are weird\n            preferences = ['C.utf8', 'C.UTF-8', 'en_US.utf8', 'en_US.UTF-8', 'C', 'POSIX']\n\n        rc, out, err = module.run_command([locale, '-a'])\n\n        if rc == 0:\n            if out:\n                available = out.strip().splitlines()\n            else:\n                raise RuntimeWarning(\"No output from locale, rc=%s: %s\" % (rc, to_native(err)))\n        else:\n            raise RuntimeWarning(\"Unable to get locale information, rc=%s: %s\" % (rc, to_native(err)))\n\n        if available:\n            for pref in preferences:\n                if pref in available:\n                    found = pref\n                    break\n\n    except RuntimeWarning as e:\n        if raise_on_locale:\n            raise\n        else:\n            module.debug('Failed to get locale information: %s' % to_native(e))\n\n    module.debug('Matched preferred locale to: %s' % found)\n\n    return found", "loc": 52}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "is_netmask", "parameters": ["val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "len", "str", "str(val).split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_netmask(val):\n    parts = str(val).split('.')\n    if not len(parts) == 4:\n        return False\n    for part in parts:\n        try:\n            if int(part) not in VALID_MASKS:\n                raise ValueError\n        except ValueError:\n            return False\n    return True", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "is_masklen", "parameters": ["val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_masklen(val):\n    try:\n        return 0 <= int(val) <= 32\n    except ValueError:\n        return False", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_netmask", "parameters": ["val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "inet_ntoa", "int", "is_masklen", "pack", "range"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "converts a masklen to a netmask", "source_code": "def to_netmask(val):\n    \"\"\" converts a masklen to a netmask \"\"\"\n    if not is_masklen(val):\n        raise ValueError('invalid value for masklen')\n\n    bits = 0\n    for i in range(32 - int(val), 32):\n        bits |= (1 << i)\n\n    return inet_ntoa(pack('>I', bits))", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_masklen", "parameters": ["val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "bin", "bin(int(x)).count", "bits.append", "int", "is_netmask", "list", "sum", "val.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "converts a netmask to a masklen", "source_code": "def to_masklen(val):\n    \"\"\" converts a netmask to a masklen \"\"\"\n    if not is_netmask(val):\n        raise ValueError('invalid value for netmask: %s' % val)\n\n    bits = list()\n    for x in val.split('.'):\n        octet = bin(int(x)).count('1')\n        bits.append(octet)\n\n    return sum(bits)", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_subnet", "parameters": ["addr", "mask", "dotted_notation"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "addr.split", "int", "is_masklen", "list", "mask.split", "network.append", "str", "to_masklen", "to_netmask", "zip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "converts an addr / mask pair to a subnet in cidr notation", "source_code": "def to_subnet(addr, mask, dotted_notation=False):\n    \"\"\" converts an addr / mask pair to a subnet in cidr notation \"\"\"\n    try:\n        if not is_masklen(mask):\n            raise ValueError\n        cidr = int(mask)\n        mask = to_netmask(mask)\n    except ValueError:\n        cidr = to_masklen(mask)\n\n    addr = addr.split('.')\n    mask = mask.split('.')\n\n    network = list()\n    for s_addr, s_mask in zip(addr, mask):\n        network.append(str(int(s_addr) & int(s_mask)))\n\n    if dotted_notation:\n        return '%s %s' % ('.'.join(network), to_netmask(cidr))\n    return '%s/%s' % ('.'.join(network), cidr)", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_ipv6_subnet", "parameters": ["addr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["addr.split", "found_groups.append", "ipv6_prefix.split", "len", "network_addr.endswith", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "IPv6 addresses are eight groupings. The first four groupings (64 bits) comprise the subnet address.", "source_code": "def to_ipv6_subnet(addr):\n    \"\"\" IPv6 addresses are eight groupings. The first four groupings (64 bits) comprise the subnet address. \"\"\"\n\n    # https://tools.ietf.org/rfc/rfc2374.txt\n\n    # Split by :: to identify omitted zeros\n    ipv6_prefix = addr.split('::')[0]\n\n    # Get the first four groups, or as many as are found + ::\n    found_groups = []\n    for group in ipv6_prefix.split(':'):\n        found_groups.append(group)\n        if len(found_groups) == 4:\n            break\n    if len(found_groups) < 4:\n        found_groups.append('::')\n\n    # Concatenate network address parts\n    network_addr = ''\n    for group in found_groups:\n        if group != '::':\n            network_addr += str(group)\n        network_addr += str(':')\n\n    # Ensure network address ends with ::\n    if not network_addr.endswith('::'):\n        network_addr += str(':')\n    return network_addr", "loc": 28}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_ipv6_network", "parameters": ["addr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["addr.split", "found_groups.append", "ipv6_prefix.split", "len", "network_addr.endswith", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "IPv6 addresses are eight groupings. The first three groupings (48 bits) comprise the network address.", "source_code": "def to_ipv6_network(addr):\n    \"\"\" IPv6 addresses are eight groupings. The first three groupings (48 bits) comprise the network address. \"\"\"\n\n    # Split by :: to identify omitted zeros\n    ipv6_prefix = addr.split('::')[0]\n\n    # Get the first three groups, or as many as are found + ::\n    found_groups = []\n    for group in ipv6_prefix.split(':'):\n        found_groups.append(group)\n        if len(found_groups) == 3:\n            break\n    if len(found_groups) < 3:\n        found_groups.append('::')\n\n    # Concatenate network address parts\n    network_addr = ''\n    for group in found_groups:\n        if group != '::':\n            network_addr += str(group)\n        network_addr += str(':')\n\n    # Ensure network address ends with ::\n    if not network_addr.endswith('::'):\n        network_addr += str(':')\n    return network_addr", "loc": 26}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "to_bits", "parameters": ["val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bin", "bin(int(octet))[2:].zfill", "int", "val.split"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "converts a netmask to bits", "source_code": "def to_bits(val):\n    \"\"\" converts a netmask to bits \"\"\"\n    bits = ''\n    for octet in val.split('.'):\n        bits += bin(int(octet))[2:].zfill(8)\n    return bits", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\network.py", "class_name": null, "function_name": "is_mac", "parameters": ["mac_address"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "mac_addr_regex.match", "mac_address.lower", "re.compile"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Validate MAC address for given string", "source_code": "def is_mac(mac_address):\n    \"\"\"\n    Validate MAC address for given string\n    Args:\n        mac_address: string to validate as MAC address\n\n    Returns: (Boolean) True if string is valid MAC address, otherwise False\n    \"\"\"\n    mac_addr_regex = re.compile('[0-9a-f]{2}([-:])[0-9a-f]{2}(\\\\1[0-9a-f]{2}){4}$')\n    return bool(mac_addr_regex.match(mac_address.lower()))", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\parameters.py", "class_name": null, "function_name": "env_fallback", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Load value from environment variable", "source_code": "def env_fallback(*args, **kwargs):\n    \"\"\"Load value from environment variable\"\"\"\n\n    for arg in args:\n        if arg in os.environ:\n            return os.environ[arg]\n    raise AnsibleFallbackNotFound", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\parameters.py", "class_name": null, "function_name": "set_fallbacks", "parameters": ["argument_spec", "parameters"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argument_spec.items", "fallback_strategy", "isinstance", "no_log_values.add", "set", "value.get"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_fallbacks(argument_spec, parameters):\n    no_log_values = set()\n    for param, value in argument_spec.items():\n        fallback = value.get('fallback', (None,))\n        fallback_strategy = fallback[0]\n        fallback_args = []\n        fallback_kwargs = {}\n        if param not in parameters and fallback_strategy is not None:\n            for item in fallback[1:]:\n                if isinstance(item, dict):\n                    fallback_kwargs = item\n                else:\n                    fallback_args = item\n            try:\n                fallback_value = fallback_strategy(*fallback_args, **fallback_kwargs)\n            except AnsibleFallbackNotFound:\n                continue\n            else:\n                if value.get('no_log', False) and fallback_value:\n                    no_log_values.add(fallback_value)\n                parameters[param] = fallback_value\n\n    return no_log_values", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\parameters.py", "class_name": null, "function_name": "sanitize_keys", "parameters": ["obj", "no_log_strings", "ignore_keys"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "An object with sanitized keys.", "raises_doc": [], "called_functions": ["TypeError", "_remove_values_conditions", "_sanitize_keys_conditions", "deferred_removals.popleft", "deque", "frozenset", "isinstance", "new_data.add", "new_data.append", "old_data.items", "old_key.startswith", "to_native"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Sanitize the keys in a container object by removing ``no_log`` values from key names. This is a companion function to the :func:`remove_values` function. Similar to that function, we make use of ``deferred_removals`` to avoid hitting maximum recursion depth in cases of large data structures. :arg obj: The container object to sanitize. Non-container objects are returned unmodified. :arg no_log_strings: A set of string values we do not want logged. :kwarg ignore_keys: A set of string values of keys to not sanitize.", "source_code": "def sanitize_keys(obj, no_log_strings, ignore_keys=frozenset()):\n    \"\"\"Sanitize the keys in a container object by removing ``no_log`` values from key names.\n\n    This is a companion function to the :func:`remove_values` function. Similar to that function,\n    we make use of ``deferred_removals`` to avoid hitting maximum recursion depth in cases of\n    large data structures.\n\n    :arg obj: The container object to sanitize. Non-container objects are returned unmodified.\n    :arg no_log_strings: A set of string values we do not want logged.\n    :kwarg ignore_keys: A set of string values of keys to not sanitize.\n\n    :returns: An object with sanitized keys.\n    \"\"\"\n\n    deferred_removals = deque()\n\n    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n    new_value = _sanitize_keys_conditions(obj, no_log_strings, ignore_keys, deferred_removals)\n\n    while deferred_removals:\n        old_data, new_data = deferred_removals.popleft()\n\n        if isinstance(new_data, Mapping):\n            for old_key, old_elem in old_data.items():\n                if old_key in ignore_keys or old_key.startswith('_ansible'):\n                    new_data[old_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n                else:\n                    # Sanitize the old key. We take advantage of the sanitizing code in\n                    # _remove_values_conditions() rather than recreating it here.\n                    new_key = _remove_values_conditions(old_key, no_log_strings, None)\n                    new_data[new_key] = _sanitize_keys_conditions(old_elem, no_log_strings, ignore_keys, deferred_removals)\n        else:\n            for elem in old_data:\n                new_elem = _sanitize_keys_conditions(elem, no_log_strings, ignore_keys, deferred_removals)\n                if isinstance(new_data, MutableSequence):\n                    new_data.append(new_elem)\n                elif isinstance(new_data, MutableSet):\n                    new_data.add(new_elem)\n                else:\n                    raise TypeError('Unknown container type encountered when removing private values from keys')\n\n    return new_value", "loc": 42}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\parameters.py", "class_name": null, "function_name": "remove_values", "parameters": ["value", "no_log_strings"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "_remove_values_conditions", "deferred_removals.popleft", "deque", "isinstance", "new_data.add", "new_data.append", "old_data.items", "to_native"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Remove strings in ``no_log_strings`` from value. If value is a container type, then remove a lot more. Use of ``deferred_removals`` exists, rather than a pure recursive solution,", "source_code": "def remove_values(value, no_log_strings):\n    \"\"\"Remove strings in ``no_log_strings`` from value.\n\n    If value is a container type, then remove a lot more.\n\n    Use of ``deferred_removals`` exists, rather than a pure recursive solution,\n    because of the potential to hit the maximum recursion depth when dealing with\n    large amounts of data (see `issue #24560 <https://github.com/ansible/ansible/issues/24560>`_).\n    \"\"\"\n\n    deferred_removals = deque()\n\n    no_log_strings = [to_native(s, errors='surrogate_or_strict') for s in no_log_strings]\n    new_value = _remove_values_conditions(value, no_log_strings, deferred_removals)\n\n    while deferred_removals:\n        old_data, new_data = deferred_removals.popleft()\n        if isinstance(new_data, Mapping):\n            for old_key, old_elem in old_data.items():\n                new_elem = _remove_values_conditions(old_elem, no_log_strings, deferred_removals)\n                new_data[old_key] = new_elem\n        else:\n            for elem in old_data:\n                new_elem = _remove_values_conditions(elem, no_log_strings, deferred_removals)\n                if isinstance(new_data, MutableSequence):\n                    new_data.append(new_elem)\n                elif isinstance(new_data, MutableSet):\n                    new_data.add(new_elem)\n                else:\n                    raise TypeError('Unknown container type encountered when removing private values from output')\n\n    return new_value", "loc": 32}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\respawn.py", "class_name": null, "function_name": "respawn_module", "parameters": ["interpreter_path"], "param_types": {}, "return_type": "t.NoReturn", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "_respawn.create_payload", "has_respawned", "os.close", "os.pipe", "os.write", "subprocess.call", "sys.exit", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Respawn the currently-running Ansible Python module under the specified Python interpreter. Ansible modules that require libraries that are typically available only under well-known interpreters (eg, ``apt``, ``dnf``) can use bespoke logic to determine the libraries they need are not", "source_code": "def respawn_module(interpreter_path) -> t.NoReturn:\n    \"\"\"\n    Respawn the currently-running Ansible Python module under the specified Python interpreter.\n\n    Ansible modules that require libraries that are typically available only under well-known interpreters\n    (eg, ``apt``, ``dnf``) can use bespoke logic to determine the libraries they need are not\n    available, then call `respawn_module` to re-execute the current module under a different interpreter\n    and exit the current process when the new subprocess has completed. The respawned process inherits only\n    stdout/stderr from the current process.\n\n    Only a single respawn is allowed. ``respawn_module`` will fail on nested respawns. Modules are encouraged\n    to call `has_respawned()` to defensively guide behavior before calling ``respawn_module``, and to ensure\n    that the target interpreter exists, as ``respawn_module`` will not fail gracefully.\n\n    :arg interpreter_path: path to a Python interpreter to respawn the current module\n    \"\"\"\n\n    if has_respawned():\n        raise Exception('module has already been respawned')\n\n    # FUTURE: we need a safe way to log that a respawn has occurred for forensic/debug purposes\n    payload = _respawn.create_payload()\n    stdin_read, stdin_write = os.pipe()\n    os.write(stdin_write, to_bytes(payload))\n    os.close(stdin_write)\n    rc = subprocess.call([interpreter_path, '--'], stdin=stdin_read)\n    sys.exit(rc)  # pylint: disable=ansible-bad-function", "loc": 27}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\sys_info.py", "class_name": null, "function_name": "get_distribution", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Name of the distribution the module is running on", "raises_doc": [], "called_functions": ["distro.id", "distro.id().capitalize", "platform.system"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the name of the distribution the module is running on. :rtype: NativeString or None", "source_code": "def get_distribution():\n    \"\"\"\n    Return the name of the distribution the module is running on.\n\n    :rtype: NativeString or None\n    :returns: Name of the distribution the module is running on\n\n    This function attempts to determine what distribution the code is running\n    on and return a string representing that value. If the platform is Linux\n    and the distribution cannot be determined, it returns ``OtherLinux``.\n    \"\"\"\n    distribution = distro.id().capitalize()\n\n    if platform.system() == 'Linux':\n        if distribution == 'Amzn':\n            distribution = 'Amazon'\n        elif distribution == 'Rhel':\n            distribution = 'Redhat'\n        elif not distribution:\n            distribution = 'OtherLinux'\n\n    return distribution", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\sys_info.py", "class_name": null, "function_name": "get_distribution_version", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A string representation of the version of the distribution. If it", "raises_doc": [], "called_functions": ["distro.id", "distro.version", "frozenset", "u'.'.join", "version_best.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get the version of the distribution the code is running on :rtype: NativeString or None", "source_code": "def get_distribution_version():\n    \"\"\"\n    Get the version of the distribution the code is running on\n\n    :rtype: NativeString or None\n    :returns: A string representation of the version of the distribution. If it\n    cannot determine the version, it returns an empty string. If this is not run on\n    a Linux machine it returns None.\n    \"\"\"\n    version = None\n\n    needs_best_version = frozenset((\n        u'centos',\n        u'debian',\n    ))\n\n    version = distro.version()\n    distro_id = distro.id()\n\n    if version is not None:\n        if distro_id in needs_best_version:\n            version_best = distro.version(best=True)\n\n            # CentoOS maintainers believe only the major version is appropriate\n            # but Ansible users desire minor version information, e.g., 7.5.\n            # https://github.com/ansible/ansible/issues/50141#issuecomment-449452781\n            if distro_id == u'centos':\n                version = u'.'.join(version_best.split(u'.')[:2])\n\n            # Debian does not include minor version in /etc/os-release.\n            # Bug report filed upstream requesting this be added to /etc/os-release\n            # https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=931197\n            if distro_id == u'debian':\n                version = version_best\n\n    else:\n        version = u''\n\n    return version", "loc": 39}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\sys_info.py", "class_name": null, "function_name": "get_distribution_codename", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A string representation of the distribution's codename or None if not a Linux distro", "raises_doc": [], "called_functions": ["distro.codename", "distro.id", "distro.lsb_release_info", "distro.os_release_info", "lsb_release_info.get", "os_release_info.get", "platform.system"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the code name for this Linux Distribution :rtype: NativeString or None", "source_code": "def get_distribution_codename():\n    \"\"\"\n    Return the code name for this Linux Distribution\n\n    :rtype: NativeString or None\n    :returns: A string representation of the distribution's codename or None if not a Linux distro\n    \"\"\"\n    codename = None\n    if platform.system() == 'Linux':\n        # Until this gets merged and we update our bundled copy of distro:\n        # https://github.com/nir0s/distro/pull/230\n        # Fixes Fedora 28+ not having a code name and Ubuntu Xenial Xerus needing to be \"xenial\"\n        os_release_info = distro.os_release_info()\n        codename = os_release_info.get('version_codename')\n\n        if codename is None:\n            codename = os_release_info.get('ubuntu_codename')\n\n        if codename is None and distro.id() == 'ubuntu':\n            lsb_release_info = distro.lsb_release_info()\n            codename = lsb_release_info.get('codename')\n\n        if codename is None:\n            codename = distro.codename()\n            if codename == u'':\n                codename = None\n\n    return codename", "loc": 28}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\sys_info.py", "class_name": null, "function_name": "get_platform_subclass", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A class that implements the functionality on this platform", "raises_doc": [], "called_functions": ["get_all_subclasses", "get_distribution", "platform.system"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Finds a subclass implementing desired functionality on the platform the code is running on :arg cls: Class to find an appropriate subclass for", "source_code": "def get_platform_subclass(cls):\n    \"\"\"\n    Finds a subclass implementing desired functionality on the platform the code is running on\n\n    :arg cls: Class to find an appropriate subclass for\n    :returns: A class that implements the functionality on this platform\n\n    Some Ansible modules have different implementations depending on the platform they run on.  This\n    function is used to select between the various implementations and choose one.  You can look at\n    the implementation of the Ansible :ref:`User module<user_module>` module for an example of how to use this.\n\n    This function replaces ``basic.load_platform_subclass()``.  When you port code, you need to\n    change the callers to be explicit about instantiating the class.  For instance, code in the\n    Ansible User module changed from::\n\n    .. code-block:: python\n\n        # Old\n        class User:\n            def __new__(cls, args, kwargs):\n                return load_platform_subclass(User, args, kwargs)\n\n        # New\n        class User:\n            def __new__(cls, *args, **kwargs):\n                new_cls = get_platform_subclass(User)\n                return super(cls, new_cls).__new__(new_cls)\n    \"\"\"\n    this_platform = platform.system()\n    distribution = get_distribution()\n\n    subclass = None\n\n    # get the most specific superclass for this platform\n    if distribution is not None:\n        for sc in get_all_subclasses(cls):\n            if sc.distribution is not None and sc.distribution == distribution and sc.platform == this_platform:\n                subclass = sc\n    if subclass is None:\n        for sc in get_all_subclasses(cls):\n            if sc.platform == this_platform and sc.distribution is None:\n                subclass = sc\n    if subclass is None:\n        subclass = cls\n\n    return subclass", "loc": 46}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "count_terms", "parameters": ["terms", "parameters"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "An integer that is the number of occurrences of the terms values", "raises_doc": [], "called_functions": ["is_iterable", "len", "set", "set(terms).intersection"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Count the number of occurrences of a key in a given dictionary :arg terms: String or iterable of values to check :arg parameters: Dictionary of parameters", "source_code": "def count_terms(terms, parameters):\n    \"\"\"Count the number of occurrences of a key in a given dictionary\n\n    :arg terms: String or iterable of values to check\n    :arg parameters: Dictionary of parameters\n\n    :returns: An integer that is the number of occurrences of the terms values\n        in the provided dictionary.\n    \"\"\"\n\n    if not is_iterable(terms):\n        terms = [terms]\n\n    return len(set(terms).intersection(parameters))", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "safe_eval", "parameters": ["value", "locals", "include_exceptions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["deprecate", "isinstance", "literal_eval", "re.search"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def safe_eval(value, locals=None, include_exceptions=False):\n    deprecate(\n        \"The safe_eval function should not be used.\",\n        version=\"2.21\",\n    )\n    # do not allow method calls to modules\n    if not isinstance(value, str):\n        # already templated to a datavaluestructure, perhaps?\n        if include_exceptions:\n            return (value, None)\n        return value\n    if re.search(r'\\w\\.\\w+\\(', value):\n        if include_exceptions:\n            return (value, None)\n        return value\n    # do not allow imports\n    if re.search(r'import \\w+', value):\n        if include_exceptions:\n            return (value, None)\n        return value\n    try:\n        result = literal_eval(value)\n        if include_exceptions:\n            return (result, None)\n        else:\n            return result\n    except Exception as e:\n        if include_exceptions:\n            return (value, e)\n        return value", "loc": 30}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_mutually_exclusive", "parameters": ["terms", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "'{0} found in {1}'.format", "'|'.join", "TypeError", "count_terms", "results.append", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check mutually exclusive terms against argument parameters Accepts a single list or list of lists that are groups of terms that should be mutually exclusive with one another :arg terms: List of mutually exclusive parameters :arg parameters: Dictionary of parameters :kwarg options_context: List of strings of parent key names if ``terms`` are in a sub spec.", "source_code": "def check_mutually_exclusive(terms, parameters, options_context=None):\n    \"\"\"Check mutually exclusive terms against argument parameters\n\n    Accepts a single list or list of lists that are groups of terms that should be\n    mutually exclusive with one another\n\n    :arg terms: List of mutually exclusive parameters\n    :arg parameters: Dictionary of parameters\n    :kwarg options_context: List of strings of parent key names if ``terms`` are\n        in a sub spec.\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n    \"\"\"\n\n    results = []\n    if terms is None:\n        return results\n\n    for check in terms:\n        count = count_terms(check, parameters)\n        if count > 1:\n            results.append(check)\n\n    if results:\n        full_list = ['|'.join(check) for check in results]\n        msg = \"parameters are mutually exclusive: %s\" % ', '.join(full_list)\n        if options_context:\n            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\n        raise TypeError(to_native(msg))\n\n    return results", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_required_one_of", "parameters": ["terms", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "'{0} found in {1}'.format", "TypeError", "count_terms", "results.append", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check each list of terms to ensure at least one exists in the given module parameters Accepts a list of lists or tuples :arg terms: List of lists of terms to check. For each list of terms, at least one is required. :arg parameters: Dictionary of parameters :kwarg options_context: List of strings of parent key names if ``terms`` are in a sub spec.", "source_code": "def check_required_one_of(terms, parameters, options_context=None):\n    \"\"\"Check each list of terms to ensure at least one exists in the given module\n    parameters\n\n    Accepts a list of lists or tuples\n\n    :arg terms: List of lists of terms to check. For each list of terms, at\n        least one is required.\n    :arg parameters: Dictionary of parameters\n    :kwarg options_context: List of strings of parent key names if ``terms`` are\n        in a sub spec.\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n    \"\"\"\n\n    results = []\n    if terms is None:\n        return results\n\n    for term in terms:\n        count = count_terms(term, parameters)\n        if count == 0:\n            results.append(term)\n\n    if results:\n        for term in results:\n            msg = \"one of the following is required: %s\" % ', '.join(term)\n            if options_context:\n                msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\n            raise TypeError(to_native(msg))\n\n    return results", "loc": 32}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_required_together", "parameters": ["terms", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "'{0} found in {1}'.format", "TypeError", "count_terms", "len", "results.append", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check each list of terms to ensure every parameter in each list exists in the given parameters. Accepts a list of lists or tuples. :arg terms: List of lists of terms to check. Each list should include parameters that are all required when at least one is specified in the parameters. :arg parameters: Dictionary of parameters :kwarg options_context: List of strings of parent key names if ``terms`` are in a sub spec.", "source_code": "def check_required_together(terms, parameters, options_context=None):\n    \"\"\"Check each list of terms to ensure every parameter in each list exists\n    in the given parameters.\n\n    Accepts a list of lists or tuples.\n\n    :arg terms: List of lists of terms to check. Each list should include\n        parameters that are all required when at least one is specified\n        in the parameters.\n    :arg parameters: Dictionary of parameters\n    :kwarg options_context: List of strings of parent key names if ``terms`` are\n        in a sub spec.\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n    \"\"\"\n\n    results = []\n    if terms is None:\n        return results\n\n    for term in terms:\n        counts = [count_terms(field, parameters) for field in term]\n        non_zero = [c for c in counts if c > 0]\n        if len(non_zero) > 0:\n            if 0 in counts:\n                results.append(term)\n    if results:\n        for term in results:\n            msg = \"parameters are required together: %s\" % ', '.join(term)\n            if options_context:\n                msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\n            raise TypeError(to_native(msg))\n\n    return results", "loc": 34}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_required_by", "parameters": ["requirements", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty dictionary or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "TypeError", "isinstance", "requirements.items", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "For each key in requirements, check the corresponding list to see if they exist in parameters. Accepts a single string or list of values for each key. :arg requirements: Dictionary of requirements :arg parameters: Dictionary of parameters :kwarg options_context: List of strings of parent key names if ``requirements`` are in a sub spec.", "source_code": "def check_required_by(requirements, parameters, options_context=None):\n    \"\"\"For each key in requirements, check the corresponding list to see if they\n    exist in parameters.\n\n    Accepts a single string or list of values for each key.\n\n    :arg requirements: Dictionary of requirements\n    :arg parameters: Dictionary of parameters\n    :kwarg options_context: List of strings of parent key names if ``requirements`` are\n        in a sub spec.\n\n    :returns: Empty dictionary or raises :class:`TypeError` if the check fails.\n    \"\"\"\n\n    result = {}\n    if requirements is None:\n        return result\n\n    for (key, value) in requirements.items():\n        if key not in parameters or parameters[key] is None:\n            continue\n        # Support strings (single-item lists)\n        if isinstance(value, str):\n            value = [value]\n\n        if missing := [required for required in value if required not in parameters or parameters[required] is None]:\n            msg = f\"missing parameter(s) required by '{key}': {', '.join(missing)}\"\n            if options_context:\n                msg = f\"{msg} found in {' -> '.join(options_context)}\"\n            raise TypeError(to_native(msg))\n    return result", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_required_arguments", "parameters": ["argument_spec", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "'{0} found in {1}'.format", "TypeError", "argument_spec.items", "missing.append", "sorted", "to_native", "v.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check all parameters in argument_spec and return a list of parameters that are required but not present in parameters.", "source_code": "def check_required_arguments(argument_spec, parameters, options_context=None):\n    \"\"\"Check all parameters in argument_spec and return a list of parameters\n    that are required but not present in parameters.\n\n    Raises :class:`TypeError` if the check fails\n\n    :arg argument_spec: Argument spec dictionary containing all parameters\n        and their specification\n    :arg parameters: Dictionary of parameters\n    :kwarg options_context: List of strings of parent key names if ``argument_spec`` are\n        in a sub spec.\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n    \"\"\"\n\n    missing = []\n    if argument_spec is None:\n        return missing\n\n    for (k, v) in argument_spec.items():\n        required = v.get('required', False)\n        if required and k not in parameters:\n            missing.append(k)\n\n    if missing:\n        msg = \"missing required arguments: %s\" % \", \".join(sorted(missing))\n        if options_context:\n            msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\n        raise TypeError(to_native(msg))\n\n    return missing", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_required_if", "parameters": ["requirements", "parameters", "options_context"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["' -> '.join", "', '.join", "'{0} found in {1}'.format", "TypeError", "count_terms", "len", "missing['missing'].append", "results.append", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check parameters that are conditionally required", "source_code": "def check_required_if(requirements, parameters, options_context=None):\n    \"\"\"Check parameters that are conditionally required\n\n    Raises :class:`TypeError` if the check fails\n\n    :arg requirements: List of lists specifying a parameter, value, parameters\n        required when the given parameter is the specified value, and optionally\n        a boolean indicating any or all parameters are required.\n\n    :Example:\n\n    .. code-block:: python\n\n        required_if=[\n            ['state', 'present', ('path',), True],\n            ['someint', 99, ('bool_param', 'string_param')],\n        ]\n\n    :arg parameters: Dictionary of parameters\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n        The results attribute of the exception contains a list of dictionaries.\n        Each dictionary is the result of evaluating each item in requirements.\n        Each return dictionary contains the following keys:\n\n            :key missing: List of parameters that are required but missing\n            :key requires: 'any' or 'all'\n            :key parameter: Parameter name that has the requirement\n            :key value: Original value of the parameter\n            :key requirements: Original required parameters\n\n        :Example:\n\n        .. code-block:: python\n\n            [\n                {\n                    'parameter': 'someint',\n                    'value': 99\n                    'requirements': ('bool_param', 'string_param'),\n                    'missing': ['string_param'],\n                    'requires': 'all',\n                }\n            ]\n\n    :kwarg options_context: List of strings of parent key names if ``requirements`` are\n        in a sub spec.\n    \"\"\"\n    results = []\n    if requirements is None:\n        return results\n\n    for req in requirements:\n        missing = {}\n        missing['missing'] = []\n        max_missing_count = 0\n        is_one_of = False\n        if len(req) == 4:\n            key, val, requirements, is_one_of = req\n        else:\n            key, val, requirements = req\n\n        # is_one_of is True at least one requirement should be\n        # present, else all requirements should be present.\n        if is_one_of:\n            max_missing_count = len(requirements)\n            missing['requires'] = 'any'\n        else:\n            missing['requires'] = 'all'\n\n        if key in parameters and parameters[key] == val:\n            for check in requirements:\n                count = count_terms(check, parameters)\n                if count == 0:\n                    missing['missing'].append(check)\n        if len(missing['missing']) and len(missing['missing']) >= max_missing_count:\n            missing['parameter'] = key\n            missing['value'] = val\n            missing['requirements'] = requirements\n            results.append(missing)\n\n    if results:\n        for missing in results:\n            msg = \"%s is %s but %s of the following are missing: %s\" % (\n                missing['parameter'], missing['value'], missing['requires'], ', '.join(missing['missing']))\n            if options_context:\n                msg = \"{0} found in {1}\".format(msg, \" -> \".join(options_context))\n            raise TypeError(to_native(msg))\n\n    return results", "loc": 90}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_missing_parameters", "parameters": ["parameters", "required_parameters"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Empty list or raises :class:`TypeError` if the check fails.", "raises_doc": [], "called_functions": ["', '.join", "TypeError", "missing_params.append", "parameters.get", "to_native"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "This is for checking for required params when we can not check via argspec because we need more information than is simply given in the argspec.", "source_code": "def check_missing_parameters(parameters, required_parameters=None):\n    \"\"\"This is for checking for required params when we can not check via\n    argspec because we need more information than is simply given in the argspec.\n\n    Raises :class:`TypeError` if any required parameters are missing\n\n    :arg parameters: Dictionary of parameters\n    :arg required_parameters: List of parameters to look for in the given parameters.\n\n    :returns: Empty list or raises :class:`TypeError` if the check fails.\n    \"\"\"\n    missing_params = []\n    if required_parameters is None:\n        return missing_params\n\n    for param in required_parameters:\n        if not parameters.get(param):\n            missing_params.append(param)\n\n    if missing_params:\n        msg = \"missing required arguments: %s\" % ', '.join(missing_params)\n        raise TypeError(to_native(msg))\n\n    return missing_params", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_str", "parameters": ["value", "allow_conversion", "param", "prefix"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Original value if it is a string, the value converted to a string", "raises_doc": [], "called_functions": ["\"'{0!r}' is not a string and conversion is not allowed\".format", "TypeError", "isinstance", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Verify that the value is a string or convert to a string. Since unexpected changes can sometimes happen when converting to a string, ``allow_conversion`` controls whether or not the value will be converted or a TypeError will be raised if the value is not a string and would be converted :arg value: Value to validate or convert to a string :arg allow_conversion: Whether to convert the string and return it or raise a TypeError", "source_code": "def check_type_str(value, allow_conversion=True, param=None, prefix=''):\n    \"\"\"Verify that the value is a string or convert to a string.\n\n    Since unexpected changes can sometimes happen when converting to a string,\n    ``allow_conversion`` controls whether or not the value will be converted or a\n    TypeError will be raised if the value is not a string and would be converted\n\n    :arg value: Value to validate or convert to a string\n    :arg allow_conversion: Whether to convert the string and return it or raise\n        a TypeError\n\n    :returns: Original value if it is a string, the value converted to a string\n        if allow_conversion=True, or raises a TypeError if allow_conversion=False.\n    \"\"\"\n    if isinstance(value, str):\n        return value\n\n    if value is None:\n        return ''  # approximate pre-2.19 templating None->empty str equivalency here for backward compatibility\n\n    if allow_conversion:\n        return to_native(value, errors='surrogate_or_strict')\n\n    msg = \"'{0!r}' is not a string and conversion is not allowed\".format(value)\n    raise TypeError(to_native(msg))", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_list", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Original value if it is already a list, single item list if a", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "str", "type", "value.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Verify that the value is a list or convert to a list A comma separated string will be split into a list. Raises a :class:`TypeError` if unable to convert to a list. :arg value: Value to validate or convert to a list", "source_code": "def check_type_list(value):\n    \"\"\"Verify that the value is a list or convert to a list\n\n    A comma separated string will be split into a list. Raises a :class:`TypeError`\n    if unable to convert to a list.\n\n    :arg value: Value to validate or convert to a list\n\n    :returns: Original value if it is already a list, single item list if a\n        float, int, or string without commas, or a multi-item list if a\n        comma-delimited string.\n    \"\"\"\n    if isinstance(value, list):\n        return value\n\n    # DTFIX-FUTURE: deprecate legacy comma split functionality, eventually replace with `_check_type_list_strict`\n    if isinstance(value, str):\n        return value.split(\",\")\n    elif isinstance(value, int) or isinstance(value, float):\n        return [str(value)]\n\n    raise TypeError('%s cannot be converted to a list' % type(value))", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_dict", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "value converted to a dictionary", "raises_doc": [], "called_functions": ["''.join", "TypeError", "dict", "field_buffer.append", "fields.append", "isinstance", "json.loads", "literal_eval", "type", "value.startswith", "value.strip", "x.split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Verify that value is a dict or convert it to a dict and return it.", "source_code": "def check_type_dict(value):\n    \"\"\"Verify that value is a dict or convert it to a dict and return it.\n\n    Raises :class:`TypeError` if unable to convert to a dict\n\n    :arg value: Dict or string to convert to a dict. Accepts ``k1=v2, k2=v2`` or ``k1=v2 k2=v2``.\n\n    :returns: value converted to a dictionary\n    \"\"\"\n    if isinstance(value, dict):\n        return value\n\n    if isinstance(value, str):\n        if value.startswith(\"{\"):\n            try:\n                return json.loads(value)\n            except Exception:\n                try:\n                    result = literal_eval(value)\n                except Exception:\n                    pass\n                else:\n                    if isinstance(result, dict):\n                        return result\n                raise TypeError('unable to evaluate string as dictionary')\n        elif '=' in value:\n            fields = []\n            field_buffer = []\n            in_quote = False\n            in_escape = False\n            for c in value.strip():\n                if in_escape:\n                    field_buffer.append(c)\n                    in_escape = False\n                elif c == '\\\\':\n                    in_escape = True\n                elif not in_quote and c in ('\\'', '\"'):\n                    in_quote = c\n                elif in_quote and in_quote == c:\n                    in_quote = False\n                elif not in_quote and c in (',', ' '):\n                    field = ''.join(field_buffer)\n                    if field:\n                        fields.append(field)\n                    field_buffer = []\n                else:\n                    field_buffer.append(c)\n\n            field = ''.join(field_buffer)\n            if field:\n                fields.append(field)\n            try:\n                return dict(x.split(\"=\", 1) for x in fields)\n            except ValueError:\n                # no \"=\" to split on: \"k1=v1, k2\"\n                raise TypeError('unable to evaluate string in the \"key=value\" format as dictionary')\n        else:\n            raise TypeError(\"dictionary requested, could not parse JSON or key=value\")\n\n    raise TypeError('%s cannot be converted to a dict' % type(value))", "loc": 60}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_bool", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Boolean True or False", "raises_doc": [], "called_functions": ["TypeError", "boolean", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Verify that the value is a bool or convert it to a bool and return it.", "source_code": "def check_type_bool(value):\n    \"\"\"Verify that the value is a bool or convert it to a bool and return it.\n\n    Raises :class:`TypeError` if unable to convert to a bool\n\n    :arg value: String, int, or float to convert to bool. Valid booleans include:\n         '1', 'on', 1, '0', 0, 'n', 'f', 'false', 'true', 'y', 't', 'yes', 'no', 'off'\n\n    :returns: Boolean True or False\n    \"\"\"\n    if isinstance(value, bool):\n        return value\n\n    if isinstance(value, str) or isinstance(value, (int, float)):\n        return boolean(value)\n\n    raise TypeError('%s cannot be converted to a bool' % type(value))", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_int", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "int of given value", "raises_doc": [], "called_functions": ["TypeError", "ValueError", "decimal.Decimal", "int", "isinstance"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Verify that the value is an integer and return it or convert the value to an integer and return it", "source_code": "def check_type_int(value):\n    \"\"\"Verify that the value is an integer and return it or convert the value\n    to an integer and return it\n\n    Raises :class:`TypeError` if unable to convert to an int\n\n    :arg value: String or int to convert of verify\n\n    :return: int of given value\n    \"\"\"\n    if not isinstance(value, int):\n        try:\n            if (decimal_value := decimal.Decimal(value)) != (int_value := int(decimal_value)):\n                raise ValueError(\"Significant decimal part found\")\n            else:\n                value = int_value\n        except (decimal.DecimalException, TypeError, ValueError) as e:\n            raise TypeError(f'\"{value!r}\" cannot be converted to an int') from e\n    return value", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_float", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "float of given value.", "raises_doc": [], "called_functions": ["TypeError", "float", "isinstance", "type"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Verify that value is a float or convert it to a float and return it", "source_code": "def check_type_float(value):\n    \"\"\"Verify that value is a float or convert it to a float and return it\n\n    Raises :class:`TypeError` if unable to convert to a float\n\n    :arg value: float, int, str, or bytes to verify or convert and return.\n\n    :returns: float of given value.\n    \"\"\"\n    if not isinstance(value, float):\n        try:\n            value = float(value)\n        except (TypeError, ValueError) as e:\n            raise TypeError(f'{type(value)} cannot be converted to a float')\n    return value", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_bytes", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "human_to_bytes", "type"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Convert a human-readable string value to bytes", "source_code": "def check_type_bytes(value):\n    \"\"\"Convert a human-readable string value to bytes\n\n    Raises :class:`TypeError` if unable to convert the value\n    \"\"\"\n    try:\n        return human_to_bytes(value)\n    except ValueError:\n        raise TypeError('%s cannot be converted to a Byte value' % type(value))", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_bits", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "human_to_bytes", "type"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Convert a human-readable string bits value to bits in integer. Example: ``check_type_bits('1Mb')`` returns integer 1048576.", "source_code": "def check_type_bits(value):\n    \"\"\"Convert a human-readable string bits value to bits in integer.\n\n    Example: ``check_type_bits('1Mb')`` returns integer 1048576.\n\n    Raises :class:`TypeError` if unable to convert the value.\n    \"\"\"\n    try:\n        return human_to_bytes(value, isbits=True)\n    except ValueError:\n        raise TypeError('%s cannot be converted to a Bit value' % type(value))", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\validation.py", "class_name": null, "function_name": "check_type_jsonarg", "parameters": ["value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "_common_json._get_legacy_encoder", "isinstance", "json.dumps", "type", "value.strip"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "JSON serialize dict/list/tuple, strip str and bytes. Previously required for cases where Ansible/Jinja classic-mode literal eval pass could inadvertently deserialize objects.", "source_code": "def check_type_jsonarg(value):\n    \"\"\"\n    JSON serialize dict/list/tuple, strip str and bytes.\n    Previously required for cases where Ansible/Jinja classic-mode literal eval pass could inadvertently deserialize objects.\n    \"\"\"\n    # deprecated: description='deprecate jsonarg type support' core_version='2.23'\n    # deprecate(\n    #     msg=\"The `jsonarg` type is deprecated.\",\n    #     version=\"2.27\",\n    #     help_text=\"JSON string arguments should use `str`; structures can be explicitly serialized as JSON with the `to_json` filter.\",\n    # )\n\n    if isinstance(value, (str, bytes)):\n        return value.strip()\n\n    if isinstance(value, (list, tuple, dict)):\n        return json.dumps(value, cls=_common_json._get_legacy_encoder(), _decode_bytes=True)\n\n    raise TypeError('%s cannot be converted to a json string' % type(value))", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\warnings.py", "class_name": null, "function_name": "warn", "parameters": ["warning"], "param_types": {"warning": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_display.warning", "_internal.import_controller_module", "_internal.import_controller_module('ansible.utils.display').Display", "_messages.Event", "_messages.WarningSummary", "_traceback.maybe_capture_traceback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Record a warning to be returned with the module result.", "source_code": "def warn(\n    warning: str,\n    *,\n    help_text: str | None = None,\n    obj: object | None = None,\n) -> None:\n    \"\"\"Record a warning to be returned with the module result.\"\"\"\n    _skip_stackwalk = True\n\n    if _internal.is_controller:\n        _display = _internal.import_controller_module('ansible.utils.display').Display()\n        _display.warning(\n            msg=warning,\n            help_text=help_text,\n            obj=obj,\n        )\n\n        return\n\n    warning = _messages.WarningSummary(\n        event=_messages.Event(\n            msg=warning,\n            help_text=help_text,\n            formatted_traceback=_traceback.maybe_capture_traceback(warning, _traceback.TracebackEvent.WARNING),\n        ),\n    )\n\n    _global_warnings[warning] = None", "loc": 28}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\warnings.py", "class_name": null, "function_name": "error_as_warning", "parameters": ["msg", "exception"], "param_types": {"msg": "str | None", "exception": "BaseException"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_display.error_as_warning", "_errors.EventFactory.from_exception", "_internal.import_controller_module", "_internal.import_controller_module('ansible.utils.display').Display", "_messages.Event", "_messages.EventChain", "_messages.WarningSummary", "_traceback.is_traceback_enabled", "_traceback.maybe_capture_traceback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Display an exception as a warning.", "source_code": "def error_as_warning(\n    msg: str | None,\n    exception: BaseException,\n    *,\n    help_text: str | None = None,\n    obj: object = None,\n) -> None:\n    \"\"\"Display an exception as a warning.\"\"\"\n    _skip_stackwalk = True\n\n    if _internal.is_controller:\n        _display = _internal.import_controller_module('ansible.utils.display').Display()\n        _display.error_as_warning(\n            msg=msg,\n            exception=exception,\n            help_text=help_text,\n            obj=obj,\n        )\n\n        return\n\n    event = _errors.EventFactory.from_exception(exception, _traceback.is_traceback_enabled(_traceback.TracebackEvent.WARNING))\n\n    warning = _messages.WarningSummary(\n        event=_messages.Event(\n            msg=msg,\n            help_text=help_text,\n            formatted_traceback=_traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.WARNING),\n            chain=_messages.EventChain(\n                msg_reason=_errors.MSG_REASON_DIRECT_CAUSE,\n                traceback_reason=_errors.TRACEBACK_REASON_EXCEPTION_DIRECT_WARNING,\n                event=event,\n            ),\n        ),\n    )\n\n    _global_warnings[warning] = None", "loc": 37}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\warnings.py", "class_name": null, "function_name": "deprecate", "parameters": ["msg", "version", "date", "collection_name"], "param_types": {"msg": "str", "version": "str | None", "date": "str | None", "collection_name": "str | None"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_deprecator.get_best_deprecator", "_display.deprecated", "_internal.import_controller_module", "_internal.import_controller_module('ansible.utils.display').Display", "_messages.DeprecationSummary", "_messages.Event", "_traceback.maybe_capture_traceback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Record a deprecation warning. The `obj` argument is only useful in a controller context; it is ignored for target-side callers. Most callers do not need to provide `collection_name` or `deprecator` -- but provide only one if needed.", "source_code": "def deprecate(\n    msg: str,\n    version: str | None = None,\n    date: str | None = None,\n    collection_name: str | None = None,\n    *,\n    deprecator: _messages.PluginInfo | None = None,\n    help_text: str | None = None,\n    obj: object | None = None,\n) -> None:\n    \"\"\"\n    Record a deprecation warning.\n    The `obj` argument is only useful in a controller context; it is ignored for target-side callers.\n    Most callers do not need to provide `collection_name` or `deprecator` -- but provide only one if needed.\n    Specify `version` or `date`, but not both.\n    If `date` is a string, it must be in the form `YYYY-MM-DD`.\n    \"\"\"\n    _skip_stackwalk = True\n\n    deprecator = _deprecator.get_best_deprecator(deprecator=deprecator, collection_name=collection_name)\n\n    if _internal.is_controller:\n        _display = _internal.import_controller_module('ansible.utils.display').Display()\n        _display.deprecated(\n            msg=msg,\n            version=version,\n            date=date,\n            help_text=help_text,\n            obj=obj,\n            # skip passing collection_name; get_best_deprecator already accounted for it when present\n            deprecator=deprecator,\n        )\n\n        return\n\n    warning = _messages.DeprecationSummary(\n        event=_messages.Event(\n            msg=msg,\n            help_text=help_text,\n            formatted_traceback=_traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.DEPRECATED),\n        ),\n        version=version,\n        date=date,\n        deprecator=deprecator,\n    )\n\n    _global_deprecations[warning] = None", "loc": 47}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\_utils.py", "class_name": null, "function_name": "get_all_subclasses", "parameters": ["cls"], "param_types": {"cls": "type[_Type]"}, "return_type": "set[type[_Type]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["inspect.isabstract", "parent.__subclasses__", "queue.append", "queue.pop", "set", "subclasses.add"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Recursively find all subclasses of a given type, including abstract classes by default.", "source_code": "def get_all_subclasses(cls: type[_Type], *, include_abstract: bool = True, consider_self: bool = False) -> set[type[_Type]]:\n    \"\"\"Recursively find all subclasses of a given type, including abstract classes by default.\"\"\"\n    subclasses: set[type[_Type]] = {cls} if consider_self else set()\n    queue: list[type[_Type]] = [cls]\n\n    while queue:\n        parent = queue.pop()\n\n        for child in parent.__subclasses__():\n            if child in subclasses:\n                continue\n\n            queue.append(child)\n            subclasses.add(child)\n\n    if not include_abstract:\n        subclasses = {sc for sc in subclasses if not inspect.isabstract(sc)}\n\n    return subclasses", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\converters.py", "class_name": null, "function_name": "to_text", "parameters": ["obj", "encoding", "errors", "nonstring"], "param_types": {"obj": "_T", "encoding": "str", "errors": "str | None", "nonstring": "_NonStringAll"}, "return_type": "_T | str", "param_doc": {}, "return_doc": "Typically this returns a text string.  If a nonstring object is", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "obj.decode", "repr", "str", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Make sure that a string is a text string :arg obj: An object to make sure is a text string.  In most cases this will be either a text string or a byte string.  However, with ``nonstring='simplerepr'``, this can be used as a traceback-free version of ``str(obj)``. :kwarg encoding: The encoding to use to transform from a byte string to a text string.  Defaults to using 'utf-8'. :kwarg errors: The error handler to use if the byte string is not decodable using the specified encoding.  Any valid `codecs error handler <https://docs.python.org/3/library/codecs.html#codec-base-classes>`_ may be specified.   We support three additional error strategies specifically aimed at helping people to port code: :surrogate_or_strict: Will use surrogateescape if it is a valid handler, otherwise it will use strict :surrogate_or_replace: Will use surrogateescape if it is a valid handler, otherwise it will use replace. :surrogate_then_replace: Does the same as surrogate_or_replace but `was added for symmetry with the error handlers in :func:`ansible.module_utils.common.text.converters.to_bytes` (Added in Ansible 2.3) Because surrogateescape was added in Python3 this usually means that Python3 will use `surrogateescape` and Python2 will use the fallback error handler. Note that the code checks for surrogateescape when the module is imported.  If you have a backport of `surrogateescape` for python2, be sure to register the error handler prior to importing this module. The default until Ansible-2.2 was `surrogate_or_replace` In Ansible-2.3 this defaults to `surrogate_then_replace` for symmetry with :func:`ansible.module_utils.common.text.converters.to_bytes` . :kwarg nonstring: The strategy to use if a nonstring is specified in ``obj``.  Default is 'simplerepr'.  Valid values are: :simplerepr: The default.  This takes the ``str`` of the object and then returns the text version of that string. :empty: Return an empty text string :passthru: Return the object passed in :strict: Raise a :exc:`TypeError`", "source_code": "def to_text(\n    obj: _T,\n    encoding: str = 'utf-8',\n    errors: str | None = None,\n    nonstring: _NonStringAll = 'simplerepr'\n) -> _T | str:\n    \"\"\"Make sure that a string is a text string\n\n    :arg obj: An object to make sure is a text string.  In most cases this\n        will be either a text string or a byte string.  However, with\n        ``nonstring='simplerepr'``, this can be used as a traceback-free\n        version of ``str(obj)``.\n    :kwarg encoding: The encoding to use to transform from a byte string to\n        a text string.  Defaults to using 'utf-8'.\n    :kwarg errors: The error handler to use if the byte string is not\n        decodable using the specified encoding.  Any valid `codecs error\n        handler <https://docs.python.org/3/library/codecs.html#codec-base-classes>`_\n        may be specified.   We support three additional error strategies\n        specifically aimed at helping people to port code:\n\n            :surrogate_or_strict: Will use surrogateescape if it is a valid\n                handler, otherwise it will use strict\n            :surrogate_or_replace: Will use surrogateescape if it is a valid\n                handler, otherwise it will use replace.\n            :surrogate_then_replace: Does the same as surrogate_or_replace but\n                `was added for symmetry with the error handlers in\n                :func:`ansible.module_utils.common.text.converters.to_bytes` (Added in Ansible 2.3)\n\n        Because surrogateescape was added in Python3 this usually means that\n        Python3 will use `surrogateescape` and Python2 will use the fallback\n        error handler. Note that the code checks for surrogateescape when the\n        module is imported.  If you have a backport of `surrogateescape` for\n        python2, be sure to register the error handler prior to importing this\n        module.\n\n        The default until Ansible-2.2 was `surrogate_or_replace`\n        In Ansible-2.3 this defaults to `surrogate_then_replace` for symmetry\n        with :func:`ansible.module_utils.common.text.converters.to_bytes` .\n    :kwarg nonstring: The strategy to use if a nonstring is specified in\n        ``obj``.  Default is 'simplerepr'.  Valid values are:\n\n        :simplerepr: The default.  This takes the ``str`` of the object and\n            then returns the text version of that string.\n        :empty: Return an empty text string\n        :passthru: Return the object passed in\n        :strict: Raise a :exc:`TypeError`\n\n    :returns: Typically this returns a text string.  If a nonstring object is\n        passed in this may be a different type depending on the strategy\n        specified by nonstring.  This will never return a byte string.\n        From Ansible-2.3 onwards, the default is `surrogate_then_replace`.\n\n    .. version_changed:: 2.3\n\n        Added the surrogate_then_replace error handler and made it the default error handler.\n    \"\"\"\n    if isinstance(obj, str):\n        return obj\n\n    if errors in _COMPOSED_ERROR_HANDLERS:\n        if HAS_SURROGATEESCAPE:\n            errors = 'surrogateescape'\n        elif errors == 'surrogate_or_strict':\n            errors = 'strict'\n        else:\n            errors = 'replace'\n\n    if isinstance(obj, bytes):\n        # Note: We don't need special handling for surrogate_then_replace\n        # because all bytes will either be made into surrogates or are valid\n        # to decode.\n        return obj.decode(encoding, errors)\n\n    # Note: We do these last even though we have to call to_text again on the\n    # value because we're optimizing the common case\n    if nonstring == 'simplerepr':\n        try:\n            value = str(obj)\n        except UnicodeError:\n            try:\n                value = repr(obj)\n            except UnicodeError:\n                # Giving up\n                return ''\n    elif nonstring == 'passthru':\n        return obj\n    elif nonstring == 'empty':\n        return ''\n    elif nonstring == 'strict':\n        raise TypeError('obj must be a string type')\n    else:\n        raise TypeError('Invalid value %s for to_text\\'s nonstring parameter' % nonstring)\n\n    return to_text(value, encoding=encoding, errors=errors)", "loc": 94}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\converters.py", "class_name": null, "function_name": "jsonify", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_common_json._get_legacy_encoder", "json.dumps"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def jsonify(data, **kwargs):\n    from ansible.module_utils.common import json as _common_json\n    # from ansible.module_utils.common.warnings import deprecate\n\n    # deprecated: description='deprecate jsonify()' core_version='2.23'\n    # deprecate(\n    #     msg=\"The `jsonify` function is deprecated.\",\n    #     version=\"2.27\",\n    #     # help_text=\"\",  # DTFIX-FUTURE: fill in this help text\n    # )\n\n    return json.dumps(data, cls=_common_json._get_legacy_encoder(), _decode_bytes=True, **kwargs)", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\converters.py", "class_name": null, "function_name": "container_to_bytes", "parameters": ["d", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["container_to_bytes", "d.items", "dict", "isinstance", "to_bytes", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Recursively convert dict keys and values to byte str Specialized for json return because this only handles, lists, tuples, and dict container types (the containers that the json module returns)", "source_code": "def container_to_bytes(d, encoding='utf-8', errors='surrogate_or_strict'):\n    \"\"\" Recursively convert dict keys and values to byte str\n\n        Specialized for json return because this only handles, lists, tuples,\n        and dict container types (the containers that the json module returns)\n    \"\"\"\n    # DTFIX-FUTURE: deprecate\n\n    if isinstance(d, str):\n        return to_bytes(d, encoding=encoding, errors=errors)\n    elif isinstance(d, dict):\n        return dict(container_to_bytes(o, encoding, errors) for o in d.items())\n    elif isinstance(d, list):\n        return [container_to_bytes(o, encoding, errors) for o in d]\n    elif isinstance(d, tuple):\n        return tuple(container_to_bytes(o, encoding, errors) for o in d)\n    else:\n        return d", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\converters.py", "class_name": null, "function_name": "container_to_text", "parameters": ["d", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["container_to_text", "d.items", "dict", "isinstance", "to_text", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Recursively convert dict keys and values to text str Specialized for json return because this only handles, lists, tuples, and dict container types (the containers that the json module returns)", "source_code": "def container_to_text(d, encoding='utf-8', errors='surrogate_or_strict'):\n    \"\"\"Recursively convert dict keys and values to text str\n\n    Specialized for json return because this only handles, lists, tuples,\n    and dict container types (the containers that the json module returns)\n    \"\"\"\n    # DTFIX-FUTURE: deprecate\n\n    if isinstance(d, bytes):\n        # Warning, can traceback\n        return to_text(d, encoding=encoding, errors=errors)\n    elif isinstance(d, dict):\n        return dict(container_to_text(o, encoding, errors) for o in d.items())\n    elif isinstance(d, list):\n        return [container_to_text(o, encoding, errors) for o in d]\n    elif isinstance(d, tuple):\n        return tuple(container_to_text(o, encoding, errors) for o in d)\n    else:\n        return d", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\formatters.py", "class_name": null, "function_name": "lenient_lowercase", "parameters": ["lst"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lowered.append", "value.lower"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Lowercase elements of a list. If an element is not a string, pass it through untouched.", "source_code": "def lenient_lowercase(lst):\n    \"\"\"Lowercase elements of a list.\n\n    If an element is not a string, pass it through untouched.\n    \"\"\"\n    lowered = []\n    for value in lst:\n        try:\n            lowered.append(value.lower())\n        except AttributeError:\n            lowered.append(value)\n    return lowered", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\formatters.py", "class_name": null, "function_name": "human_to_bytes", "parameters": ["number", "default_unit", "isbits"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "SIZE_RANGES.keys", "VALID_UNITS.get", "ValueError", "float", "int", "len", "m.group", "re.search", "round", "str", "unit.lower", "unit[0].upper"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Convert number in string format into bytes (ex: '2K' => 2048) or using unit argument. example: human_to_bytes('10M') <=> human_to_bytes(10, 'M'). When isbits is False (default), converts bytes from a human-readable format to integer.", "source_code": "def human_to_bytes(number, default_unit=None, isbits=False):\n    \"\"\"Convert number in string format into bytes (ex: '2K' => 2048) or using unit argument.\n\n    example: human_to_bytes('10M') <=> human_to_bytes(10, 'M').\n\n    When isbits is False (default), converts bytes from a human-readable format to integer.\n        example: human_to_bytes('1MB') returns 1048576 (int).\n        The function expects 'B' (uppercase) as a byte identifier passed\n        as a part of 'name' param string or 'unit', e.g. 'MB'/'KB'/etc.\n        (except when the identifier is single 'b', it is perceived as a byte identifier too).\n        if 'Mb'/'Kb'/... is passed, the ValueError will be rased.\n\n    When isbits is True, converts bits from a human-readable format to integer.\n        example: human_to_bytes('1Mb', isbits=True) returns 8388608 (int) -\n        string bits representation was passed and return as a number or bits.\n        The function expects 'b' (lowercase) as a bit identifier, e.g. 'Mb'/'Kb'/etc.\n        if 'MB'/'KB'/... is passed, the ValueError will be rased.\n    \"\"\"\n    m = re.search(r'^([0-9]*\\.?[0-9]+)(?:\\s*([A-Za-z]+))?\\s*$', str(number))\n\n    if m is None:\n        raise ValueError(\"human_to_bytes() can't interpret following string: %s\" % str(number))\n    try:\n        num = float(m.group(1))\n    except Exception:\n        raise ValueError(\"human_to_bytes() can't interpret following number: %s (original input string: %s)\" % (m.group(1), number))\n\n    unit = m.group(2)\n    if unit is None:\n        unit = default_unit\n\n    if unit is None:\n        # No unit given, returning raw number\n        return int(round(num))\n    range_key = unit[0].upper()\n    try:\n        limit = SIZE_RANGES[range_key]\n    except Exception:\n        raise ValueError(\"human_to_bytes() failed to convert %s (unit = %s). The suffix must be one of %s\" % (number, unit, \", \".join(SIZE_RANGES.keys())))\n\n    # default value\n    unit_class = 'B'\n    unit_class_name = 'byte'\n    # handling bits case\n    if isbits:\n        unit_class = 'b'\n        unit_class_name = 'bit'\n    # check unit value if more than one character (KB, MB)\n    if len(unit) > 1:\n        expect_message = 'expect %s%s or %s' % (range_key, unit_class, range_key)\n        if range_key == 'B':\n            expect_message = 'expect %s or %s' % (unit_class, unit_class_name)\n        unit_group = VALID_UNITS.get(range_key, None)\n        if unit_group is None:\n            raise ValueError(f\"human_to_bytes() can't interpret a valid unit for {range_key}\")\n        isbits_flag = 1 if isbits else 0\n        if unit.lower() == unit_group[isbits_flag][0]:\n            pass\n        elif unit != unit_group[isbits_flag][1]:\n            raise ValueError(\"human_to_bytes() failed to convert %s. Value is not a valid string (%s)\" % (number, expect_message))\n\n    return int(round(num * limit))", "loc": 62}
{"file": "ansible\\lib\\ansible\\module_utils\\common\\text\\formatters.py", "class_name": null, "function_name": "bytes_to_human", "parameters": ["size", "isbits", "unit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SIZE_RANGES.items", "sorted", "unit.upper"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def bytes_to_human(size, isbits=False, unit=None):\n    base = 'Bytes'\n    if isbits:\n        base = 'bits'\n    suffix = ''\n\n    for suffix, limit in sorted(SIZE_RANGES.items(), key=lambda item: -item[1]):\n        if (unit is None and size >= limit) or unit is not None and unit.upper() == suffix[0]:\n            break\n\n    if limit != 1:\n        suffix += base[0]\n    else:\n        suffix = base\n\n    return '%.2f %s' % (size / limit, suffix)", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\selinux.py", "class_name": null, "function_name": "selinux_getpolicytype", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_selinux_lib.freecon", "_selinux_lib.selinux_getpolicytype", "byref", "c_char_p", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selinux_getpolicytype():\n    con = c_char_p()\n    try:\n        rc = _selinux_lib.selinux_getpolicytype(byref(con))\n        return [rc, to_native(con.value)]\n    finally:\n        _selinux_lib.freecon(con)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\selinux.py", "class_name": null, "function_name": "lgetfilecon_raw", "parameters": ["path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_selinux_lib.freecon", "_selinux_lib.lgetfilecon_raw", "byref", "c_char_p", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def lgetfilecon_raw(path):\n    con = c_char_p()\n    try:\n        rc = _selinux_lib.lgetfilecon_raw(path, byref(con))\n        return [rc, to_native(con.value)]\n    finally:\n        _selinux_lib.freecon(con)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\selinux.py", "class_name": null, "function_name": "matchpathcon", "parameters": ["path", "mode"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_selinux_lib.freecon", "_selinux_lib.matchpathcon", "byref", "c_char_p", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def matchpathcon(path, mode):\n    con = c_char_p()\n    try:\n        rc = _selinux_lib.matchpathcon(path, mode, byref(con))\n        return [rc, to_native(con.value)]\n    finally:\n        _selinux_lib.freecon(con)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\selinux.py", "class_name": "_to_char_p", "function_name": "from_param", "parameters": ["cls", "strvalue"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_param(cls, strvalue):\n    if strvalue is not None and not isinstance(strvalue, binary_char_type):\n        strvalue = to_bytes(strvalue)\n\n    return strvalue", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\version.py", "class_name": "StrictVersion", "function_name": "parse", "parameters": ["self", "vstring"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "int", "map", "match.group", "self.version_re.match", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(self, vstring):\n    match = self.version_re.match(vstring)\n    if not match:\n        raise ValueError(\"invalid version number '%s'\" % vstring)\n\n    (major, minor, patch, prerelease, prerelease_num) = \\\n        match.group(1, 2, 4, 5, 6)\n\n    if patch:\n        self.version = tuple(map(int, [major, minor, patch]))\n    else:\n        self.version = tuple(map(int, [major, minor])) + (0,)\n\n    if prerelease:\n        self.prerelease = (prerelease[0], int(prerelease_num))\n    else:\n        self.prerelease = None", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\compat\\version.py", "class_name": "LooseVersion", "function_name": "parse", "parameters": ["self", "vstring"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "int", "self.component_re.split"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(self, vstring):\n    # I've given up on thinking I can reconstruct the version string\n    # from the parsed tuple -- so I just store the string here for\n    # use by __str__\n    self.vstring = vstring\n    components = [x for x in self.component_re.split(vstring) if x and x != '.']\n    for i, obj in enumerate(components):\n        try:\n            components[i] = int(obj)\n        except ValueError:\n            pass\n\n    self.version = components", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": null, "function_name": "linux_distribution", "parameters": ["full_distribution_name"], "param_types": {"full_distribution_name": "bool"}, "return_type": "Tuple[str, str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_distro.linux_distribution", "warnings.warn"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": ".. deprecated:: 1.6.0 :func:`distro.linux_distribution()` is deprecated. It should only be used as a compatibility shim with Python's", "source_code": "def linux_distribution(full_distribution_name: bool = True) -> Tuple[str, str, str]:\n    \"\"\"\n    .. deprecated:: 1.6.0\n\n        :func:`distro.linux_distribution()` is deprecated. It should only be\n        used as a compatibility shim with Python's\n        :py:func:`platform.linux_distribution()`. Please use :func:`distro.id`,\n        :func:`distro.version` and :func:`distro.name` instead.\n\n    Return information about the current OS distribution as a tuple\n    ``(id_name, version, codename)`` with items as follows:\n\n    * ``id_name``:  If *full_distribution_name* is false, the result of\n      :func:`distro.id`. Otherwise, the result of :func:`distro.name`.\n\n    * ``version``:  The result of :func:`distro.version`.\n\n    * ``codename``:  The extra item (usually in parentheses) after the\n      os-release version number, or the result of :func:`distro.codename`.\n\n    The interface of this function is compatible with the original\n    :py:func:`platform.linux_distribution` function, supporting a subset of\n    its parameters.\n\n    The data it returns may not exactly be the same, because it uses more data\n    sources than the original function, and that may lead to different data if\n    the OS distribution is not consistent across multiple data sources it\n    provides (there are indeed such distributions ...).\n\n    Another reason for differences is the fact that the :func:`distro.id`\n    method normalizes the distro ID string to a reliable machine-readable value\n    for a number of popular OS distributions.\n    \"\"\"\n    warnings.warn(\n        \"distro.linux_distribution() is deprecated. It should only be used as a \"\n        \"compatibility shim with Python's platform.linux_distribution(). Please use \"\n        \"distro.id(), distro.version() and distro.name() instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return _distro.linux_distribution(full_distribution_name)", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LinuxDistribution", "argparse.ArgumentParser", "dist.codename", "dist.info", "dist.name", "dist.version", "json.dumps", "logger.addHandler", "logger.info", "logger.setLevel", "logging.StreamHandler", "logging.getLogger", "parser.add_argument", "parser.parse_args"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def main() -> None:\n    logger = logging.getLogger(__name__)\n    logger.setLevel(logging.DEBUG)\n    logger.addHandler(logging.StreamHandler(sys.stdout))\n\n    parser = argparse.ArgumentParser(description=\"OS distro info tool\")\n    parser.add_argument(\n        \"--json\", \"-j\", help=\"Output in machine readable format\", action=\"store_true\"\n    )\n\n    parser.add_argument(\n        \"--root-dir\",\n        \"-r\",\n        type=str,\n        dest=\"root_dir\",\n        help=\"Path to the root filesystem directory (defaults to /)\",\n    )\n\n    args = parser.parse_args()\n\n    if args.root_dir:\n        dist = LinuxDistribution(\n            include_lsb=False,\n            include_uname=False,\n            include_oslevel=False,\n            root_dir=args.root_dir,\n        )\n    else:\n        dist = _distro\n\n    if args.json:\n        logger.info(json.dumps(dist.info(), indent=4, sort_keys=True))\n    else:\n        logger.info(\"Name: %s\", dist.name(pretty=True))\n        distribution_version = dist.version(pretty=True)\n        logger.info(\"Version: %s\", distribution_version)\n        distribution_codename = dist.codename()\n        logger.info(\"Codename: %s\", distribution_codename)", "loc": 38}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "linux_distribution", "parameters": ["self", "full_distribution_name"], "param_types": {"full_distribution_name": "bool"}, "return_type": "Tuple[str, str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._os_release_info.get", "self.codename", "self.id", "self.name", "self.version"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return information about the OS distribution that is compatible with Python's :func:`platform.linux_distribution`, supporting a subset of its parameters.", "source_code": "def linux_distribution(\n    self, full_distribution_name: bool = True\n) -> Tuple[str, str, str]:\n    \"\"\"\n    Return information about the OS distribution that is compatible\n    with Python's :func:`platform.linux_distribution`, supporting a subset\n    of its parameters.\n\n    For details, see :func:`distro.linux_distribution`.\n    \"\"\"\n    return (\n        self.name() if full_distribution_name else self.id(),\n        self.version(),\n        self._os_release_info.get(\"release_codename\") or self.codename(),\n    )", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "id", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["distro_id.lower", "distro_id.lower().replace", "normalize", "self.distro_release_attr", "self.lsb_release_attr", "self.os_release_attr", "self.uname_attr", "table.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the distro ID of the OS distribution, as a string. For details, see :func:`distro.id`.", "source_code": "def id(self) -> str:\n    \"\"\"Return the distro ID of the OS distribution, as a string.\n\n    For details, see :func:`distro.id`.\n    \"\"\"\n\n    def normalize(distro_id: str, table: Dict[str, str]) -> str:\n        distro_id = distro_id.lower().replace(\" \", \"_\")\n        return table.get(distro_id, distro_id)\n\n    distro_id = self.os_release_attr(\"id\")\n    if distro_id:\n        return normalize(distro_id, NORMALIZED_OS_ID)\n\n    distro_id = self.lsb_release_attr(\"distributor_id\")\n    if distro_id:\n        return normalize(distro_id, NORMALIZED_LSB_ID)\n\n    distro_id = self.distro_release_attr(\"id\")\n    if distro_id:\n        return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n    distro_id = self.uname_attr(\"id\")\n    if distro_id:\n        return normalize(distro_id, NORMALIZED_DISTRO_ID)\n\n    return \"\"", "loc": 27}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "name", "parameters": ["self", "pretty"], "param_types": {"pretty": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.distro_release_attr", "self.lsb_release_attr", "self.os_release_attr", "self.uname_attr", "self.version"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the name of the OS distribution, as a string. For details, see :func:`distro.name`.", "source_code": "def name(self, pretty: bool = False) -> str:\n    \"\"\"\n    Return the name of the OS distribution, as a string.\n\n    For details, see :func:`distro.name`.\n    \"\"\"\n    name = (\n        self.os_release_attr(\"name\")\n        or self.lsb_release_attr(\"distributor_id\")\n        or self.distro_release_attr(\"name\")\n        or self.uname_attr(\"name\")\n    )\n    if pretty:\n        name = self.os_release_attr(\"pretty_name\") or self.lsb_release_attr(\n            \"description\"\n        )\n        if not name:\n            name = self.distro_release_attr(\"name\") or self.uname_attr(\"name\")\n            version = self.version(pretty=True)\n            if version:\n                name = f\"{name} {version}\"\n    return name or \"\"", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "version", "parameters": ["self", "pretty", "best"], "param_types": {"pretty": "bool", "best": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parse_distro_release_content", "self._parse_distro_release_content(self.lsb_release_attr('description')).get", "self._parse_distro_release_content(self.os_release_attr('pretty_name')).get", "self.codename", "self.distro_release_attr", "self.id", "self.like", "self.like().split", "self.lsb_release_attr", "self.os_release_attr", "self.oslevel_info", "self.uname_attr", "self.uname_attr('id').startswith", "v.count", "version.count", "versions.append", "versions.insert"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return the version of the OS distribution, as a string. For details, see :func:`distro.version`.", "source_code": "def version(self, pretty: bool = False, best: bool = False) -> str:\n    \"\"\"\n    Return the version of the OS distribution, as a string.\n\n    For details, see :func:`distro.version`.\n    \"\"\"\n    versions = [\n        self.os_release_attr(\"version_id\"),\n        self.lsb_release_attr(\"release\"),\n        self.distro_release_attr(\"version_id\"),\n        self._parse_distro_release_content(self.os_release_attr(\"pretty_name\")).get(\n            \"version_id\", \"\"\n        ),\n        self._parse_distro_release_content(\n            self.lsb_release_attr(\"description\")\n        ).get(\"version_id\", \"\"),\n        self.uname_attr(\"release\"),\n    ]\n    if self.uname_attr(\"id\").startswith(\"aix\"):\n        # On AIX platforms, prefer oslevel command output.\n        versions.insert(0, self.oslevel_info())\n    elif self.id() == \"debian\" or \"debian\" in self.like().split():\n        # On Debian-like, add debian_version file content to candidates list.\n        versions.append(self._debian_version)\n    version = \"\"\n    if best:\n        # This algorithm uses the last version in priority order that has\n        # the best precision. If the versions are not in conflict, that\n        # does not matter; otherwise, using the last one instead of the\n        # first one might be considered a surprise.\n        for v in versions:\n            if v.count(\".\") > version.count(\".\") or version == \"\":\n                version = v\n    else:\n        for v in versions:\n            if v != \"\":\n                version = v\n                break\n    if pretty and version and self.codename():\n        version = f\"{version} ({self.codename()})\"\n    return version", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "version_parts", "parameters": ["self", "best"], "param_types": {"best": "bool"}, "return_type": "Tuple[str, str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["matches.groups", "re.compile", "self.version", "version_regex.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the version of the OS distribution, as a tuple of version numbers. For details, see :func:`distro.version_parts`.", "source_code": "def version_parts(self, best: bool = False) -> Tuple[str, str, str]:\n    \"\"\"\n    Return the version of the OS distribution, as a tuple of version\n    numbers.\n\n    For details, see :func:`distro.version_parts`.\n    \"\"\"\n    version_str = self.version(best=best)\n    if version_str:\n        version_regex = re.compile(r\"(\\d+)\\.?(\\d+)?\\.?(\\d+)?\")\n        matches = version_regex.match(version_str)\n        if matches:\n            major, minor, build_number = matches.groups()\n            return major, minor or \"\", build_number or \"\"\n    return \"\", \"\", \"\"", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "codename", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.distro_release_attr", "self.lsb_release_attr"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return the codename of the OS distribution. For details, see :func:`distro.codename`.", "source_code": "def codename(self) -> str:\n    \"\"\"\n    Return the codename of the OS distribution.\n\n    For details, see :func:`distro.codename`.\n    \"\"\"\n    try:\n        # Handle os_release specially since distros might purposefully set\n        # this to empty string to have no codename\n        return self._os_release_info[\"codename\"]\n    except KeyError:\n        return (\n            self.lsb_release_attr(\"codename\")\n            or self.distro_release_attr(\"codename\")\n            or \"\"\n        )", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\distro\\_distro.py", "class_name": "LinuxDistribution", "function_name": "info", "parameters": ["self", "pretty", "best"], "param_types": {"pretty": "bool", "best": "bool"}, "return_type": "InfoDict", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["InfoDict", "VersionDict", "self.build_number", "self.codename", "self.id", "self.like", "self.major_version", "self.minor_version", "self.version"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return certain machine-readable information about the OS distribution. For details, see :func:`distro.info`.", "source_code": "def info(self, pretty: bool = False, best: bool = False) -> InfoDict:\n    \"\"\"\n    Return certain machine-readable information about the OS\n    distribution.\n\n    For details, see :func:`distro.info`.\n    \"\"\"\n    return InfoDict(\n        id=self.id(),\n        version=self.version(pretty, best),\n        version_parts=VersionDict(\n            major=self.major_version(best),\n            minor=self.minor_version(best),\n            build_number=self.build_number(best),\n        ),\n        like=self.like(),\n        codename=self.codename(),\n    )", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\ansible_collector.py", "class_name": null, "function_name": "get_ansible_collector", "parameters": ["all_collector_classes", "namespace", "filter_spec", "gather_subset", "gather_timeout", "minimal_gather_subset"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFactCollector", "CollectorMetaDataCollector", "collector.collector_classes_from_gather_subset", "collector_class", "collectors.append", "frozenset"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ansible_collector(all_collector_classes,\n                          namespace=None,\n                          filter_spec=None,\n                          gather_subset=None,\n                          gather_timeout=None,\n                          minimal_gather_subset=None):\n\n    filter_spec = filter_spec or []\n    gather_subset = gather_subset or ['all']\n    gather_timeout = gather_timeout or timeout.DEFAULT_GATHER_TIMEOUT\n    minimal_gather_subset = minimal_gather_subset or frozenset()\n\n    collector_classes = \\\n        collector.collector_classes_from_gather_subset(\n            all_collector_classes=all_collector_classes,\n            minimal_gather_subset=minimal_gather_subset,\n            gather_subset=gather_subset,\n            gather_timeout=gather_timeout)\n\n    collectors = []\n    for collector_class in collector_classes:\n        collector_obj = collector_class(namespace=namespace)\n        collectors.append(collector_obj)\n\n    # Add a collector that knows what gather_subset we used so it it can provide a fact\n    collector_meta_data_collector = \\\n        CollectorMetaDataCollector(gather_subset=gather_subset,\n                                   module_setup=True)\n    collectors.append(collector_meta_data_collector)\n\n    fact_collector = \\\n        AnsibleFactCollector(collectors=collectors,\n                             filter_spec=filter_spec,\n                             namespace=namespace)\n\n    return fact_collector", "loc": 36}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\ansible_collector.py", "class_name": "AnsibleFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.update", "collector_obj.collect_with_namespace", "facts_dict.update", "info_dict.copy", "repr", "self._filter", "sys.stderr.write"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    collected_facts = collected_facts or {}\n\n    facts_dict = {}\n\n    for collector_obj in self.collectors:\n        info_dict = {}\n\n        try:\n\n            # Note: this collects with namespaces, so collected_facts also includes namespaces\n            info_dict = collector_obj.collect_with_namespace(module=module,\n                                                             collected_facts=collected_facts)\n        except Exception as e:\n            sys.stderr.write(repr(e))\n            sys.stderr.write('\\n')\n\n        # shallow copy of the new facts to pass to each collector in collected_facts so facts\n        # can reference other facts they depend on.\n        collected_facts.update(info_dict.copy())\n\n        # NOTE: If we want complicated fact dict merging, this is where it would hook in\n        facts_dict.update(self._filter(info_dict, self.filter_spec))\n\n    return facts_dict", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\ansible_collector.py", "class_name": "CollectorMetaDataCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    # NOTE: deprecate/remove once DT lands\n    # we can return this data, but should not be top level key\n    meta_facts = {'gather_subset': self.gather_subset}\n\n    # NOTE: this is just a boolean indicator that 'facts were gathered'\n    # and should be moved to the 'gather_facts' action plugin\n    # probably revised to handle modules/subsets combos\n    if self.module_setup:\n        meta_facts['module_setup'] = self.module_setup\n    return meta_facts", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "get_collector_names", "parameters": ["valid_subsets", "minimal_gather_subset", "gather_subset", "aliases_map", "platform_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "TypeError", "additional_subsets.add", "additional_subsets.difference_update", "additional_subsets.update", "aliases_map.get", "defaultdict", "exclude_subsets.add", "exclude_subsets.update", "explicitly_added.add", "frozenset", "gather_subset_with_min.extend", "set", "sorted", "subset_id.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "return a set of FactCollector names based on gather_subset spec. gather_subset is a spec describing which facts to gather. valid_subsets is a frozenset of potential matches for gather_subset ('all', 'network') etc", "source_code": "def get_collector_names(valid_subsets=None,\n                        minimal_gather_subset=None,\n                        gather_subset=None,\n                        aliases_map=None,\n                        platform_info=None):\n    \"\"\"return a set of FactCollector names based on gather_subset spec.\n\n    gather_subset is a spec describing which facts to gather.\n    valid_subsets is a frozenset of potential matches for gather_subset ('all', 'network') etc\n    minimal_gather_subsets is a frozenset of matches to always use, even for gather_subset='!all'\n    \"\"\"\n\n    # Retrieve module parameters\n    gather_subset = gather_subset or ['all']\n\n    # the list of everything that 'all' expands to\n    valid_subsets = valid_subsets or frozenset()\n\n    # if provided, minimal_gather_subset is always added, even after all negations\n    minimal_gather_subset = minimal_gather_subset or frozenset()\n\n    aliases_map = aliases_map or defaultdict(set)\n\n    # Retrieve all facts elements\n    additional_subsets = set()\n    exclude_subsets = set()\n\n    # total always starts with the min set, then\n    # adds of the additions in gather_subset, then\n    # excludes all of the excludes, then add any explicitly\n    # requested subsets.\n    gather_subset_with_min = ['min']\n    gather_subset_with_min.extend(gather_subset)\n\n    # subsets we mention in gather_subset explicitly, except for 'all'/'min'\n    explicitly_added = set()\n\n    for subset in gather_subset_with_min:\n        subset_id = subset\n        if subset_id == 'min':\n            additional_subsets.update(minimal_gather_subset)\n            continue\n        if subset_id == 'all':\n            additional_subsets.update(valid_subsets)\n            continue\n        if subset_id.startswith('!'):\n            subset = subset[1:]\n            if subset == 'min':\n                exclude_subsets.update(minimal_gather_subset)\n                continue\n            if subset == 'all':\n                exclude_subsets.update(valid_subsets - minimal_gather_subset)\n                continue\n            exclude = True\n        else:\n            exclude = False\n\n        if exclude:\n            # include 'devices', 'dmi' etc for '!hardware'\n            exclude_subsets.update(aliases_map.get(subset, set()))\n            exclude_subsets.add(subset)\n        else:\n            # NOTE: this only considers adding an unknown gather subsetup an error. Asking to\n            #       exclude an unknown gather subset is ignored.\n            if subset_id not in valid_subsets:\n                raise TypeError(\"Bad subset '%s' given to Ansible. gather_subset options allowed: all, %s\" %\n                                (subset, \", \".join(sorted(valid_subsets))))\n\n            explicitly_added.add(subset)\n            additional_subsets.add(subset)\n\n    if not additional_subsets:\n        additional_subsets.update(valid_subsets)\n\n    additional_subsets.difference_update(exclude_subsets - explicitly_added)\n\n    return additional_subsets", "loc": 77}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "find_collectors_for_platform", "parameters": ["all_collector_classes", "compat_platforms"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all_collector_class.platform_match", "found_collectors.add", "found_collectors_names.add", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_collectors_for_platform(all_collector_classes, compat_platforms):\n    found_collectors = set()\n    found_collectors_names = set()\n\n    # start from specific platform, then try generic\n    for compat_platform in compat_platforms:\n        platform_match = None\n        for all_collector_class in all_collector_classes:\n\n            # ask the class if it is compatible with the platform info\n            platform_match = all_collector_class.platform_match(compat_platform)\n\n            if not platform_match:\n                continue\n\n            primary_name = all_collector_class.name\n\n            if primary_name not in found_collectors_names:\n                found_collectors.add(all_collector_class)\n                found_collectors_names.add(all_collector_class.name)\n\n    return found_collectors", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "build_fact_id_to_collector_map", "parameters": ["collectors_for_platform"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aliases_map[primary_name].add", "defaultdict", "fact_id_to_collector_map[fact_id].append", "fact_id_to_collector_map[primary_name].append"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_fact_id_to_collector_map(collectors_for_platform):\n    fact_id_to_collector_map = defaultdict(list)\n    aliases_map = defaultdict(set)\n\n    for collector_class in collectors_for_platform:\n        primary_name = collector_class.name\n\n        fact_id_to_collector_map[primary_name].append(collector_class)\n\n        for fact_id in collector_class._fact_ids:\n            fact_id_to_collector_map[fact_id].append(collector_class)\n            aliases_map[primary_name].add(fact_id)\n\n    return fact_id_to_collector_map, aliases_map", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "select_collector_classes", "parameters": ["collector_names", "all_fact_subsets"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all_fact_subsets.get", "seen_collector_classes.add", "selected_collector_classes.append", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select_collector_classes(collector_names, all_fact_subsets):\n    seen_collector_classes = set()\n\n    selected_collector_classes = []\n\n    for collector_name in collector_names:\n        collector_classes = all_fact_subsets.get(collector_name, [])\n        for collector_class in collector_classes:\n            if collector_class not in seen_collector_classes:\n                selected_collector_classes.append(collector_class)\n                seen_collector_classes.add(collector_class)\n\n    return selected_collector_classes", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "find_unresolved_requires", "parameters": ["collector_names", "all_fact_subsets"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_requires_by_collector_name", "set", "unresolved.add"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Find any collector names that have unresolved requires", "source_code": "def find_unresolved_requires(collector_names, all_fact_subsets):\n    \"\"\"Find any collector names that have unresolved requires\n\n    Returns a list of collector names that correspond to collector\n    classes whose .requires_facts() are not in collector_names.\n    \"\"\"\n    unresolved = set()\n\n    for collector_name in collector_names:\n        required_facts = _get_requires_by_collector_name(collector_name, all_fact_subsets)\n        for required_fact in required_facts:\n            if required_fact not in collector_names:\n                unresolved.add(required_fact)\n\n    return unresolved", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "resolve_requires", "parameters": ["unresolved_requires", "all_fact_subsets"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "UnresolvedFactDep", "failed.append", "new_names.add", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve_requires(unresolved_requires, all_fact_subsets):\n    new_names = set()\n    failed = []\n    for unresolved in unresolved_requires:\n        if unresolved in all_fact_subsets:\n            new_names.add(unresolved)\n        else:\n            failed.append(unresolved)\n\n    if failed:\n        raise UnresolvedFactDep('unresolved fact dep %s' % ','.join(failed))\n    return new_names", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "build_dep_data", "parameters": ["collector_names", "all_fact_subsets"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collector_deps.add", "defaultdict", "set"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_dep_data(collector_names, all_fact_subsets):\n    dep_map = defaultdict(set)\n    for collector_name in collector_names:\n        collector_deps = set()\n        for collector in all_fact_subsets[collector_name]:\n            for dep in collector.required_facts:\n                collector_deps.add(dep)\n        dep_map[collector_name] = collector_deps\n    return dep_map", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "tsort", "parameters": ["dep_map"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CycleFoundInFactDeps", "dep_map.copy", "list", "sorted_list.append", "unsorted_map.items"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def tsort(dep_map):\n    sorted_list = []\n\n    unsorted_map = dep_map.copy()\n\n    while unsorted_map:\n        acyclic = False\n        for node, edges in list(unsorted_map.items()):\n            for edge in edges:\n                if edge in unsorted_map:\n                    break\n            else:\n                acyclic = True\n                del unsorted_map[node]\n                sorted_list.append((node, edges))\n\n        if not acyclic:\n            raise CycleFoundInFactDeps('Unable to tsort deps, there was a cycle in the graph. sorted=%s' % sorted_list)\n\n    return sorted_list", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": null, "function_name": "collector_classes_from_gather_subset", "parameters": ["all_collector_classes", "valid_subsets", "minimal_gather_subset", "gather_subset", "gather_timeout", "platform_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_solve_deps", "all_fact_subsets.keys", "build_dep_data", "build_fact_id_to_collector_map", "defaultdict", "find_collectors_for_platform", "frozenset", "get_collector_names", "platform.system", "select_collector_classes", "tsort"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "return a list of collector classes that match the args", "source_code": "def collector_classes_from_gather_subset(all_collector_classes=None,\n                                         valid_subsets=None,\n                                         minimal_gather_subset=None,\n                                         gather_subset=None,\n                                         gather_timeout=None,\n                                         platform_info=None):\n    \"\"\"return a list of collector classes that match the args\"\"\"\n\n    # use gather_name etc to get the list of collectors\n\n    all_collector_classes = all_collector_classes or []\n\n    minimal_gather_subset = minimal_gather_subset or frozenset()\n\n    platform_info = platform_info or {'system': platform.system()}\n\n    gather_timeout = gather_timeout or timeout.DEFAULT_GATHER_TIMEOUT\n\n    # tweak the modules GATHER_TIMEOUT\n    timeout.GATHER_TIMEOUT = gather_timeout\n\n    valid_subsets = valid_subsets or frozenset()\n\n    # maps alias names like 'hardware' to the list of names that are part of hardware\n    # like 'devices' and 'dmi'\n    aliases_map = defaultdict(set)\n\n    compat_platforms = [platform_info, {'system': 'Generic'}]\n\n    collectors_for_platform = find_collectors_for_platform(all_collector_classes, compat_platforms)\n\n    # all_facts_subsets maps the subset name ('hardware') to the class that provides it.\n\n    # TODO: name collisions here? are there facts with the same name as a gather_subset (all, network, hardware, virtual, ohai, facter)\n    all_fact_subsets, aliases_map = build_fact_id_to_collector_map(collectors_for_platform)\n\n    all_valid_subsets = frozenset(all_fact_subsets.keys())\n\n    # expand any fact_id/collectorname/gather_subset term ('all', 'env', etc) to the list of names that represents\n    collector_names = get_collector_names(valid_subsets=all_valid_subsets,\n                                          minimal_gather_subset=minimal_gather_subset,\n                                          gather_subset=gather_subset,\n                                          aliases_map=aliases_map,\n                                          platform_info=platform_info)\n\n    complete_collector_names = _solve_deps(collector_names, all_fact_subsets)\n\n    dep_map = build_dep_data(complete_collector_names, all_fact_subsets)\n\n    ordered_deps = tsort(dep_map)\n    ordered_collector_names = [x[0] for x in ordered_deps]\n\n    selected_collector_classes = select_collector_classes(ordered_collector_names,\n                                                          all_fact_subsets)\n\n    return selected_collector_classes", "loc": 56}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\collector.py", "class_name": "BaseFactCollector", "function_name": "collect_with_namespace", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._transform_dict_keys", "self.collect"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect_with_namespace(self, module=None, collected_facts=None):\n    # collect, then transform the key names if needed\n    facts_dict = self.collect(module=module, collected_facts=collected_facts)\n    if self.namespace:\n        facts_dict = self._transform_dict_keys(facts_dict)\n    return facts_dict", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\compat.py", "class_name": null, "function_name": "ansible_facts", "parameters": ["module", "gather_subset"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PrefixFactNamespace", "ansible_collector.get_ansible_collector", "fact_collector.collect", "frozenset", "module.params.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Compat api for ansible 2.0/2.2/2.3 module_utils.facts.ansible_facts method 2.3/2.3 expects a gather_subset arg. 2.0/2.1 does not except a gather_subset arg", "source_code": "def ansible_facts(module, gather_subset=None):\n    \"\"\"Compat api for ansible 2.0/2.2/2.3 module_utils.facts.ansible_facts method\n\n    2.3/2.3 expects a gather_subset arg.\n    2.0/2.1 does not except a gather_subset arg\n\n    So make gather_subsets an optional arg, defaulting to configured DEFAULT_GATHER_TIMEOUT\n\n    'module' should be an instance of an AnsibleModule.\n\n    returns a dict mapping the bare fact name ('default_ipv4' with no 'ansible_' namespace) to\n    the fact value.\n    \"\"\"\n\n    gather_subset = gather_subset or module.params.get('gather_subset', ['all'])\n    gather_timeout = module.params.get('gather_timeout', 10)\n    filter_spec = module.params.get('filter', '*')\n\n    minimal_gather_subset = frozenset(['apparmor', 'caps', 'cmdline', 'date_time',\n                                       'distribution', 'dns', 'env', 'fips', 'local',\n                                       'lsb', 'pkg_mgr', 'platform', 'python', 'selinux',\n                                       'service_mgr', 'ssh_pub_keys', 'user'])\n\n    all_collector_classes = default_collectors.collectors\n\n    # don't add a prefix\n    namespace = PrefixFactNamespace(namespace_name='ansible', prefix='')\n\n    fact_collector = \\\n        ansible_collector.get_ansible_collector(all_collector_classes=all_collector_classes,\n                                                namespace=namespace,\n                                                filter_spec=filter_spec,\n                                                gather_subset=gather_subset,\n                                                gather_timeout=gather_timeout,\n                                                minimal_gather_subset=minimal_gather_subset)\n\n    facts_dict = fact_collector.collect(module=module)\n\n    return facts_dict", "loc": 39}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\packages.py", "class_name": "PkgMgr", "function_name": "get_packages", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["installed_packages[name].append", "self.__class__.__name__.lower", "self.get_package_details", "self.list_installed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_packages(self):\n    # Take all of the above and return a dictionary of lists of dictionaries (package = list of installed versions)\n\n    installed_packages = {}\n    for package in self.list_installed():\n        package_details = self.get_package_details(package)\n        if 'source' not in package_details:\n            package_details['source'] = self.__class__.__name__.lower()\n        name = package_details['name']\n        if name not in installed_packages:\n            installed_packages[name] = [package_details]\n        else:\n            installed_packages[name].append(package_details)\n    return installed_packages", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\packages.py", "class_name": "LibMgr", "function_name": "is_available", "parameters": ["self", "handle_exceptions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "__import__", "missing_required_lib"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_available(self, handle_exceptions=True):\n    found = False\n    try:\n        self._lib = __import__(self.LIB)\n        found = True\n    except ImportError:\n        if not handle_exceptions:\n            raise Exception(missing_required_lib(self.LIB))\n    return found", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\packages.py", "class_name": "RespawningLibMgr", "function_name": "is_available", "parameters": ["self", "handle_exceptions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "get_bin_path", "has_respawned", "missing_required_lib", "probe_interpreters_for_module", "respawn_module", "super", "super(RespawningLibMgr, self).is_available"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_available(self, handle_exceptions=True):\n    if super(RespawningLibMgr, self).is_available():\n        return True\n\n    for binary in self.CLI_BINARIES:\n        try:\n            bin_path = get_bin_path(binary)\n        except ValueError:\n            # Not an interesting exception to raise, just a speculative probe\n            continue\n        else:\n            # It looks like this package manager is installed\n            if not has_respawned():\n                # See if respawning will help\n                interpreter_path = probe_interpreters_for_module(self.INTERPRETERS, self.LIB)\n                if interpreter_path:\n                    respawn_module(interpreter_path)\n                    # The module will exit when the respawned copy completes\n\n            if not handle_exceptions:\n                raise Exception(f'Found executable at {bin_path}. {missing_required_lib(self.LIB)}')\n\n    if not handle_exceptions:\n        raise Exception(missing_required_lib(self.LIB))\n\n    return False", "loc": 26}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\packages.py", "class_name": "CLIMgr", "function_name": "is_available", "parameters": ["self", "handle_exceptions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_bin_path"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_available(self, handle_exceptions=True):\n    found = False\n    try:\n        self._cli = get_bin_path(self.CLI)\n        found = True\n    except ValueError:\n        if not handle_exceptions:\n            raise\n    return found", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\sysctl.py", "class_name": null, "function_name": "get_sysctl", "parameters": ["module", "prefixes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.extend", "dict", "line.startswith", "line.strip", "module.error_as_warning", "module.get_bin_path", "module.run_command", "out.splitlines", "re.split", "value.strip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_sysctl(module, prefixes):\n\n    sysctl = dict()\n    sysctl_cmd = module.get_bin_path('sysctl')\n    if sysctl_cmd is not None:\n\n        cmd = [sysctl_cmd]\n        cmd.extend(prefixes)\n\n        try:\n            rc, out, err = module.run_command(cmd)\n        except OSError as ex:\n            module.error_as_warning('Unable to read sysctl.', exception=ex)\n            rc = 1\n\n        if rc == 0:\n            key = ''\n            value = ''\n            for line in out.splitlines():\n                if not line.strip():\n                    continue\n\n                if line.startswith(' '):\n                    # handle multiline values, they will not have a starting key\n                    # Add the newline back in so people can split on it to parse\n                    # lines if they need to.\n                    value += '\\n' + line\n                    continue\n\n                if key:\n                    sysctl[key] = value.strip()\n\n                try:\n                    (key, value) = re.split(r'\\s?=\\s?|: ', line, maxsplit=1)\n                except Exception as ex:\n                    module.error_as_warning(f'Unable to split sysctl line {line!r}.', exception=ex)\n\n            if key:\n                sysctl[key] = value.strip()\n\n    return sysctl", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\timeout.py", "class_name": null, "function_name": "timeout", "parameters": ["seconds", "error_message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TimeoutError", "callable", "decorator", "globals", "globals().get", "mp.ThreadPool", "pool.apply_async", "pool.close", "pool.terminate", "res.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Timeout decorator to expire after a set number of seconds.  This raises an ansible.module_utils.facts.TimeoutError if the timeout is hit before the function completes.", "source_code": "def timeout(seconds=None, error_message=\"Timer expired\"):\n    \"\"\"\n    Timeout decorator to expire after a set number of seconds.  This raises an\n    ansible.module_utils.facts.TimeoutError if the timeout is hit before the\n    function completes.\n    \"\"\"\n    def decorator(func):\n        def wrapper(*args, **kwargs):\n            timeout_value = seconds\n            if timeout_value is None:\n                timeout_value = globals().get('GATHER_TIMEOUT') or DEFAULT_GATHER_TIMEOUT\n\n            pool = mp.ThreadPool(processes=1)\n            res = pool.apply_async(func, args, kwargs)\n            pool.close()\n            try:\n                return res.get(timeout_value)\n            except multiprocessing.TimeoutError:\n                # This is an ansible.module_utils.common.facts.timeout.TimeoutError\n                raise TimeoutError(f'{error_message} after {timeout_value} seconds')\n            finally:\n                pool.terminate()\n\n        return wrapper\n\n    # If we were called as @timeout, then the first parameter will be the\n    # function we are to wrap instead of the number of seconds.  Detect this\n    # and correct it by setting seconds to our default value and return the\n    # inner decorator function manually wrapped around the function\n    if callable(seconds):\n        func = seconds\n        seconds = None\n        return decorator(func)\n\n    # If we were called as @timeout([...]) then python itself will take\n    # care of wrapping the inner decorator around the function\n\n    return decorator", "loc": 38}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\timeout.py", "class_name": null, "function_name": "decorator", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TimeoutError", "globals", "globals().get", "mp.ThreadPool", "pool.apply_async", "pool.close", "pool.terminate", "res.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decorator(func):\n    def wrapper(*args, **kwargs):\n        timeout_value = seconds\n        if timeout_value is None:\n            timeout_value = globals().get('GATHER_TIMEOUT') or DEFAULT_GATHER_TIMEOUT\n\n        pool = mp.ThreadPool(processes=1)\n        res = pool.apply_async(func, args, kwargs)\n        pool.close()\n        try:\n            return res.get(timeout_value)\n        except multiprocessing.TimeoutError:\n            # This is an ansible.module_utils.common.facts.timeout.TimeoutError\n            raise TimeoutError(f'{error_message} after {timeout_value} seconds')\n        finally:\n            pool.terminate()\n\n    return wrapper", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\timeout.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TimeoutError", "globals", "globals().get", "mp.ThreadPool", "pool.apply_async", "pool.close", "pool.terminate", "res.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    timeout_value = seconds\n    if timeout_value is None:\n        timeout_value = globals().get('GATHER_TIMEOUT') or DEFAULT_GATHER_TIMEOUT\n\n    pool = mp.ThreadPool(processes=1)\n    res = pool.apply_async(func, args, kwargs)\n    pool.close()\n    try:\n        return res.get(timeout_value)\n    except multiprocessing.TimeoutError:\n        # This is an ansible.module_utils.common.facts.timeout.TimeoutError\n        raise TimeoutError(f'{error_message} after {timeout_value} seconds')\n    finally:\n        pool.terminate()", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\utils.py", "class_name": null, "function_name": "get_file_lines", "parameters": ["path", "strip", "line_sep"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.rstrip", "data.rstrip(line_sep).split", "data.split", "data.splitlines", "get_file_content", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "get list of lines from file", "source_code": "def get_file_lines(path, strip=True, line_sep=None):\n    \"\"\"get list of lines from file\"\"\"\n    data = get_file_content(path, strip=strip)\n    if data:\n        if line_sep is None:\n            ret = data.splitlines()\n        else:\n            if len(line_sep) == 1:\n                ret = data.rstrip(line_sep).split(line_sep)\n            else:\n                ret = data.split(line_sep)\n    else:\n        ret = []\n    return ret", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\utils.py", "class_name": null, "function_name": "get_mount_size", "parameters": ["mountpoint"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.statvfs"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_size(mountpoint):\n    mount_size = {}\n\n    try:\n        statvfs_result = os.statvfs(mountpoint)\n        mount_size['size_total'] = statvfs_result.f_frsize * statvfs_result.f_blocks\n        mount_size['size_available'] = statvfs_result.f_frsize * (statvfs_result.f_bavail)\n\n        # Block total/available/used\n        mount_size['block_size'] = statvfs_result.f_bsize\n        mount_size['block_total'] = statvfs_result.f_blocks\n        mount_size['block_available'] = statvfs_result.f_bavail\n        mount_size['block_used'] = mount_size['block_total'] - mount_size['block_available']\n\n        # Inode total/available/used\n        mount_size['inode_total'] = statvfs_result.f_files\n        mount_size['inode_available'] = statvfs_result.f_favail\n        mount_size['inode_used'] = mount_size['inode_total'] - mount_size['inode_available']\n    except OSError:\n        pass\n\n    return mount_size", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hardware_facts.update", "self.get_cpu_facts", "self.get_device_facts", "self.get_dmi_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_uptime_facts", "self.get_vgs_facts"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n\n    cpu_facts = self.get_cpu_facts()\n    memory_facts = self.get_memory_facts()\n    dmi_facts = self.get_dmi_facts()\n    vgs_facts = self.get_vgs_facts()\n    mount_facts = self.get_mount_facts()\n    devices_facts = self.get_device_facts()\n    uptime_facts = self.get_uptime_facts()\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(dmi_facts)\n    hardware_facts.update(vgs_facts)\n    hardware_facts.update(mount_facts)\n    hardware_facts.update(devices_facts)\n    hardware_facts.update(uptime_facts)\n\n    return hardware_facts", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_cpu_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "line.split", "out.split", "out.splitlines", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self):\n    cpu_facts = {}\n    cpu_facts['processor'] = []\n\n    # FIXME: not clear how to detect multi-sockets\n    cpu_facts['processor_count'] = 1\n    rc, out, err = self.module.run_command(\n        \"/usr/sbin/lsdev -Cc processor\"\n    )\n    if out:\n        i = 0\n        for line in out.splitlines():\n\n            if 'Available' in line:\n                if i == 0:\n                    data = line.split(' ')\n                    cpudev = data[0]\n\n                i += 1\n        cpu_facts['processor_cores'] = int(i)\n\n        rc, out, err = self.module.run_command(\n            \"/usr/sbin/lsattr -El \" + cpudev + \" -a type\"\n        )\n\n        data = out.split(' ')\n        cpu_facts['processor'] = [data[1]]\n\n        cpu_facts['processor_threads_per_core'] = 1\n        rc, out, err = self.module.run_command(\n            \"/usr/sbin/lsattr -El \" + cpudev + \" -a smt_threads\"\n        )\n        if out:\n            data = out.split(' ')\n            cpu_facts['processor_threads_per_core'] = int(data[1])\n        cpu_facts['processor_vcpus'] = (\n            cpu_facts['processor_cores'] * cpu_facts['processor_threads_per_core']\n        )\n\n    return cpu_facts", "loc": 40}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[0].rstrip", "data[1].rstrip", "int", "line.split", "lines[1].split", "out.splitlines", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n    pagesize = 4096\n    rc, out, err = self.module.run_command(\"/usr/bin/vmstat -v\")\n    for line in out.splitlines():\n        data = line.split()\n        if 'memory pages' in line:\n            pagecount = int(data[0])\n        if 'free pages' in line:\n            freecount = int(data[0])\n    memory_facts['memtotal_mb'] = pagesize * pagecount // 1024 // 1024\n    memory_facts['memfree_mb'] = pagesize * freecount // 1024 // 1024\n    # Get swapinfo.  swapinfo output looks like:\n    # Device          1M-blocks     Used    Avail Capacity\n    # /dev/ada0p3        314368        0   314368     0%\n    #\n    rc, out, err = self.module.run_command(\"/usr/sbin/lsps -s\")\n    if out:\n        lines = out.splitlines()\n        data = lines[1].split()\n        swaptotal_mb = int(data[0].rstrip('MB'))\n        percused = int(data[1].rstrip('%'))\n        memory_facts['swaptotal_mb'] = swaptotal_mb\n        memory_facts['swapfree_mb'] = int(swaptotal_mb * (100 - percused) / 100)\n\n    return memory_facts", "loc": 26}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "lines[0].replace", "lines[0].replace(':', '-').split", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    uptime_facts = {}\n    # On AIX, there are no options to get the uptime directly in seconds.\n    # Your options are to parse the output of \"who\", \"uptime\", or \"ps\".\n    # Only \"ps\" always provides a field with seconds.\n    ps_bin = self.module.get_bin_path(\"ps\")\n    if ps_bin is None:\n        return uptime_facts\n\n    ps_cmd = [ps_bin, \"-p\", \"1\", \"-o\", \"etime=\"]\n\n    rc, out, err = self.module.run_command(ps_cmd)\n    if rc != 0:\n        return uptime_facts\n\n    # Parse out\n    if out:\n        lines = out.splitlines()\n        data = lines[0].replace(':', '-').split('-')\n        try:\n            days = int(data[0])\n            hours = int(data[1])\n            minutes = int(data[2])\n            seconds = int(data[3])\n        except (IndexError, ValueError):\n            return uptime_facts\n        # Calculate uptime in seconds\n        uptime_seconds = (days * 86400) + (hours * 3600) + (minutes * 60) + seconds\n        uptime_facts['uptime_seconds'] = int(uptime_seconds)\n\n    return uptime_facts", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_dmi_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[1].strip", "line.split", "out.split", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dmi_facts(self):\n    dmi_facts = {}\n\n    rc, out, err = self.module.run_command(\"/usr/sbin/lsattr -El sys0 -a fwversion\")\n    data = out.split()\n    dmi_facts['firmware_version'] = data[1].strip('IBM,')\n    lsconf_path = self.module.get_bin_path(\"lsconf\")\n    if lsconf_path:\n        rc, out, err = self.module.run_command(lsconf_path)\n        if rc == 0 and out:\n            for line in out.splitlines():\n                data = line.split(':')\n                if 'Machine Serial Number' in line:\n                    dmi_facts['product_serial'] = data[1].strip()\n                if 'LPAR Info' in line:\n                    dmi_facts['lpar_info'] = data[1].strip()\n                if 'System Model' in line:\n                    dmi_facts['product_name'] = data[1].strip()\n    return dmi_facts", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_vgs_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.group", "n.group", "re.finditer", "re.search", "re.search('PP SIZE:\\\\s+(\\\\d+\\\\s+\\\\S+)', out).group", "self.module.get_bin_path", "self.module.run_command", "vgs_facts['vgs'][m.group(1)].append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Get vg and pv Facts rootvg: PV_NAME           PV STATE          TOTAL PPs   FREE PPs    FREE DISTRIBUTION", "source_code": "def get_vgs_facts(self):\n    \"\"\"\n    Get vg and pv Facts\n    rootvg:\n    PV_NAME           PV STATE          TOTAL PPs   FREE PPs    FREE DISTRIBUTION\n    hdisk0            active            546         0           00..00..00..00..00\n    hdisk1            active            546         113         00..00..00..21..92\n    realsyncvg:\n    PV_NAME           PV STATE          TOTAL PPs   FREE PPs    FREE DISTRIBUTION\n    hdisk74           active            1999        6           00..00..00..00..06\n    testvg:\n    PV_NAME           PV STATE          TOTAL PPs   FREE PPs    FREE DISTRIBUTION\n    hdisk105          active            999         838         200..39..199..200..200\n    hdisk106          active            999         599         200..00..00..199..200\n    \"\"\"\n\n    vgs_facts = {}\n    lsvg_path = self.module.get_bin_path(\"lsvg\")\n    xargs_path = self.module.get_bin_path(\"xargs\")\n    cmd = \"%s -o | %s %s -p\" % (lsvg_path, xargs_path, lsvg_path)\n    if lsvg_path and xargs_path:\n        rc, out, err = self.module.run_command(cmd, use_unsafe_shell=True)\n        if rc == 0 and out:\n            vgs_facts['vgs'] = {}\n            for m in re.finditer(r'(\\S+):\\n.*FREE DISTRIBUTION(\\n(\\S+)\\s+(\\w+)\\s+(\\d+)\\s+(\\d+).*)+', out):\n                vgs_facts['vgs'][m.group(1)] = []\n                pp_size = 0\n                cmd = \"%s %s\" % (lsvg_path, m.group(1))\n                rc, out, err = self.module.run_command(cmd)\n                if rc == 0 and out:\n                    pp_size = re.search(r'PP SIZE:\\s+(\\d+\\s+\\S+)', out).group(1)\n                    for n in re.finditer(r'(\\S+)\\s+(\\w+)\\s+(\\d+)\\s+(\\d+).*', m.group(0)):\n                        pv_info = {'pv_name': n.group(1),\n                                   'pv_state': n.group(2),\n                                   'total_pps': n.group(3),\n                                   'free_pps': n.group(4),\n                                   'pp_size': pp_size\n                                   }\n                        vgs_facts['vgs'][m.group(1)].append(pv_info)\n\n    return vgs_facts", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fields.append", "get_mount_size", "len", "line.split", "mount_info.update", "mount_out.split", "mounts.append", "re.match", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n    mount_facts = {}\n\n    mount_facts['mounts'] = []\n\n    mounts = []\n\n    # AIX does not have mtab but mount command is only source of info (or to use\n    # api calls to get same info)\n    mount_path = self.module.get_bin_path('mount')\n    if mount_path:\n        rc, mount_out, err = self.module.run_command(mount_path)\n        if mount_out:\n            for line in mount_out.split('\\n'):\n                fields = line.split()\n                if len(fields) != 0 and fields[0] != 'node' and fields[0][0] != '-' and re.match('^/.*|^[a-zA-Z].*|^[0-9].*', fields[0]):\n                    if re.match('^/', fields[0]):\n                        # normal mount\n                        mount = fields[1]\n                        mount_info = {'mount': mount,\n                                      'device': fields[0],\n                                      'fstype': fields[2],\n                                      'options': fields[6],\n                                      'time': '%s %s %s' % (fields[3], fields[4], fields[5])}\n                        mount_info.update(get_mount_size(mount))\n                    else:\n                        # nfs or cifs based mount\n                        # in case of nfs if no mount options are provided on command line\n                        # add into fields empty string...\n                        if len(fields) < 8:\n                            fields.append(\"\")\n\n                        mount_info = {'mount': fields[2],\n                                      'device': '%s:%s' % (fields[0], fields[1]),\n                                      'fstype': fields[3],\n                                      'options': fields[7],\n                                      'time': '%s %s %s' % (fields[4], fields[5], fields[6])}\n\n                    mounts.append(mount_info)\n\n    mount_facts['mounts'] = mounts\n\n    return mount_facts", "loc": 43}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\aix.py", "class_name": "AIXHardware", "function_name": "get_device_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "attr.split", "line.split", "out_lsattr.splitlines", "out_lsdev.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_device_facts(self):\n    device_facts = {}\n    device_facts['devices'] = {}\n\n    lsdev_cmd = self.module.get_bin_path('lsdev')\n    lsattr_cmd = self.module.get_bin_path('lsattr')\n    if lsdev_cmd and lsattr_cmd:\n        rc, out_lsdev, err = self.module.run_command(lsdev_cmd)\n\n        for line in out_lsdev.splitlines():\n            field = line.split()\n\n            device_attrs = {}\n            device_name = field[0]\n            device_state = field[1]\n            device_type = field[2:]\n            lsattr_cmd_args = [lsattr_cmd, '-E', '-l', device_name]\n            rc, out_lsattr, err = self.module.run_command(lsattr_cmd_args)\n            for attr in out_lsattr.splitlines():\n                attr_fields = attr.split()\n                attr_name = attr_fields[0]\n                attr_parameter = attr_fields[1]\n                device_attrs[attr_name] = attr_parameter\n\n            device_facts['devices'][device_name] = {\n                'state': device_state,\n                'type': ' '.join(device_type),\n                'attributes': device_attrs\n            }\n\n    return device_facts", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\base.py", "class_name": "HardwareCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["facts_obj.populate", "self._fact_class"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    collected_facts = collected_facts or {}\n    if not module:\n        return {}\n\n    # Network munges cached_facts by side effect, so give it a copy\n    facts_obj = self._fact_class(module)\n\n    facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n    return facts_dict", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_sysctl", "hardware_facts.update", "self.get_cpu_facts", "self.get_mac_facts", "self.get_memory_facts", "self.get_uptime_facts"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n\n    self.sysctl = get_sysctl(self.module, ['hw', 'machdep', 'kern', 'hw.model'])\n    mac_facts = self.get_mac_facts()\n    cpu_facts = self.get_cpu_facts()\n    memory_facts = self.get_memory_facts()\n    uptime_facts = self.get_uptime_facts()\n\n    hardware_facts.update(mac_facts)\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(uptime_facts)\n\n    return hardware_facts", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "get_system_profile", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "dict", "key.strip", "line.split", "out.splitlines", "self.module.run_command", "value.strip", "value.strip().split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_system_profile(self):\n    rc, out, err = self.module.run_command([\"/usr/sbin/system_profiler\", \"SPHardwareDataType\"])\n    if rc != 0:\n        return dict()\n    system_profile = dict()\n    for line in out.splitlines():\n        if ': ' in line:\n            (key, value) = line.split(': ', 1)\n            system_profile[key.strip()] = ' '.join(value.strip().split())\n    return system_profile", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "get_mac_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mac_facts(self):\n    mac_facts = {}\n    if 'hw.model' in self.sysctl:\n        mac_facts['model'] = mac_facts['product_name'] = self.sysctl['hw.model']\n    mac_facts['osversion'] = self.sysctl['kern.osversion']\n    mac_facts['osrevision'] = self.sysctl['kern.osrevision']\n\n    return mac_facts", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "get_cpu_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_system_profile", "self.sysctl.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self):\n    cpu_facts = {}\n    if 'machdep.cpu.brand_string' in self.sysctl:  # Intel\n        cpu_facts['processor'] = self.sysctl['machdep.cpu.brand_string']\n        cpu_facts['processor_cores'] = self.sysctl['machdep.cpu.core_count']\n    else:  # PowerPC\n        system_profile = self.get_system_profile()\n        cpu_facts['processor'] = '%s @ %s' % (system_profile['Processor Name'], system_profile['Processor Speed'])\n        cpu_facts['processor_cores'] = self.sysctl['hw.physicalcpu']\n    cpu_facts['processor_vcpus'] = self.sysctl.get('hw.logicalcpu') or self.sysctl.get('hw.ncpu') or ''\n\n    return cpu_facts", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "int", "line.rstrip", "line.rstrip('.').split", "memory_stats.get", "memory_stats.items", "out.splitlines", "self.module.get_bin_path", "self.module.run_command", "self.sysctl.get", "v.lstrip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {\n        'memtotal_mb': int(self.sysctl['hw.memsize']) // 1024 // 1024,\n        'memfree_mb': 0,\n    }\n\n    total_used = 0\n    page_size = int(self.sysctl.get('hw.pagesize', 4096))\n\n    vm_stat_command = self.module.get_bin_path('vm_stat')\n    if vm_stat_command is None:\n        return memory_facts\n\n    if vm_stat_command:\n        rc, out, err = self.module.run_command(vm_stat_command)\n        if rc == 0:\n            # Free = Total - (Wired + active + inactive)\n            # Get a generator of tuples from the command output so we can later\n            # turn it into a dictionary\n            memory_stats = (line.rstrip('.').split(':', 1) for line in out.splitlines())\n\n            # Strip extra left spaces from the value\n            memory_stats = dict((k, v.lstrip()) for k, v in memory_stats)\n\n            for k, v in memory_stats.items():\n                try:\n                    memory_stats[k] = int(v)\n                except ValueError:\n                    # Most values convert cleanly to integer values but if the field does\n                    # not convert to an integer, just leave it alone.\n                    pass\n\n            if memory_stats.get('Pages wired down'):\n                total_used += memory_stats['Pages wired down'] * page_size\n            if memory_stats.get('Pages active'):\n                total_used += memory_stats['Pages active'] * page_size\n            if memory_stats.get('Pages inactive'):\n                total_used += memory_stats['Pages inactive'] * page_size\n\n            memory_facts['memfree_mb'] = memory_facts['memtotal_mb'] - (total_used // 1024 // 1024)\n\n    return memory_facts", "loc": 42}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\darwin.py", "class_name": "DarwinHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "len", "self.module.get_bin_path", "self.module.run_command", "struct.calcsize", "struct.unpack", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n\n    # On Darwin, the default format is annoying to parse.\n    # Use -b to get the raw value and decode it.\n    sysctl_cmd = self.module.get_bin_path('sysctl')\n    if not sysctl_cmd:\n        return {}\n\n    cmd = [sysctl_cmd, '-b', 'kern.boottime']\n\n    # We need to get raw bytes, not UTF-8.\n    rc, out, err = self.module.run_command(cmd, encoding=None)\n\n    # kern.boottime returns seconds and microseconds as two 64-bits\n    # fields, but we are only interested in the first field.\n    struct_format = '@L'\n    struct_size = struct.calcsize(struct_format)\n    if rc != 0 or len(out) < struct_size:\n        return {}\n\n    (kern_boottime, ) = struct.unpack(struct_format, out[:struct_size])\n\n    return {\n        'uptime_seconds': int(time.time() - kern_boottime),\n    }", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hardware_facts.update", "self.get_cpu_facts", "self.get_device_facts", "self.get_dmi_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_uptime_facts"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n\n    cpu_facts = self.get_cpu_facts()\n    memory_facts = self.get_memory_facts()\n    uptime_facts = self.get_uptime_facts()\n    dmi_facts = self.get_dmi_facts()\n    device_facts = self.get_device_facts()\n\n    mount_facts = {}\n    try:\n        mount_facts = self.get_mount_facts()\n    except TimeoutError:\n        pass\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(uptime_facts)\n    hardware_facts.update(dmi_facts)\n    hardware_facts.update(device_facts)\n    hardware_facts.update(mount_facts)\n\n    return hardware_facts", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "get_cpu_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cpu.strip", "cpu_facts['processor'].append", "dmesg_boot.splitlines", "get_file_content", "line.split", "out.strip", "re.sub", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self):\n    cpu_facts = {}\n    cpu_facts['processor'] = []\n    sysctl = self.module.get_bin_path('sysctl')\n    if sysctl:\n        rc, out, err = self.module.run_command(\"%s -n hw.ncpu\" % sysctl, check_rc=False)\n        cpu_facts['processor_count'] = out.strip()\n\n    dmesg_boot = get_file_content(FreeBSDHardware.DMESG_BOOT)\n    if not dmesg_boot:\n        try:\n            rc, dmesg_boot, err = self.module.run_command(self.module.get_bin_path(\"dmesg\"), check_rc=False)\n        except Exception:\n            dmesg_boot = ''\n\n    for line in dmesg_boot.splitlines():\n        if 'CPU:' in line:\n            cpu = re.sub(r'CPU:\\s+', r\"\", line)\n            cpu_facts['processor'].append(cpu.strip())\n        if 'Logical CPUs per core' in line:\n            cpu_facts['processor_cores'] = line.split()[4]\n\n    return cpu_facts", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "len", "line.split", "lines.pop", "lines[-1].split", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n\n    sysctl = self.module.get_bin_path('sysctl')\n    if sysctl:\n        rc, out, err = self.module.run_command(\"%s vm.stats\" % sysctl, check_rc=False)\n        for line in out.splitlines():\n            data = line.split()\n            if 'vm.stats.vm.v_page_size' in line:\n                pagesize = int(data[1])\n            if 'vm.stats.vm.v_page_count' in line:\n                pagecount = int(data[1])\n            if 'vm.stats.vm.v_free_count' in line:\n                freecount = int(data[1])\n        memory_facts['memtotal_mb'] = pagesize * pagecount // 1024 // 1024\n        memory_facts['memfree_mb'] = pagesize * freecount // 1024 // 1024\n\n    swapinfo = self.module.get_bin_path('swapinfo')\n    if swapinfo:\n        # Get swapinfo.  swapinfo output looks like:\n        # Device          1M-blocks     Used    Avail Capacity\n        # /dev/ada0p3        314368        0   314368     0%\n        #\n        rc, out, err = self.module.run_command(\"%s -k\" % swapinfo)\n        lines = out.splitlines()\n        if len(lines[-1]) == 0:\n            lines.pop()\n        data = lines[-1].split()\n        if data[0] != 'Device':\n            memory_facts['swaptotal_mb'] = int(data[1]) // 1024\n            memory_facts['swapfree_mb'] = int(data[3]) // 1024\n\n    return memory_facts", "loc": 33}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "len", "self.module.get_bin_path", "self.module.run_command", "struct.calcsize", "struct.unpack", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    # On FreeBSD, the default format is annoying to parse.\n    # Use -b to get the raw value and decode it.\n    sysctl_cmd = self.module.get_bin_path('sysctl')\n    cmd = [sysctl_cmd, '-b', 'kern.boottime']\n\n    # We need to get raw bytes, not UTF-8.\n    rc, out, err = self.module.run_command(cmd, encoding=None)\n\n    # kern.boottime returns seconds and microseconds as two 64-bits\n    # fields, but we are only interested in the first field.\n    struct_format = '@L'\n    struct_size = struct.calcsize(struct_format)\n    if rc != 0 or len(out) < struct_size:\n        return {}\n\n    (kern_boottime, ) = struct.unpack(struct_format, out[:struct_size])\n\n    return {\n        'uptime_seconds': int(time.time() - kern_boottime),\n    }", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fstab.splitlines", "get_file_content", "get_mount_size", "line.startswith", "line.strip", "mount_facts['mounts'].append", "mount_info.update", "re.sub", "re.sub('\\\\s+', ' ', line).split", "timeout"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n    mount_facts = {}\n\n    mount_facts['mounts'] = []\n    fstab = get_file_content('/etc/fstab')\n    if fstab:\n        for line in fstab.splitlines():\n            if line.startswith('#') or line.strip() == '':\n                continue\n            fields = re.sub(r'\\s+', ' ', line).split()\n            mount_statvfs_info = get_mount_size(fields[1])\n            mount_info = {'mount': fields[1],\n                          'device': fields[0],\n                          'fstype': fields[2],\n                          'options': fields[3]}\n            mount_info.update(mount_statvfs_info)\n            mount_facts['mounts'].append(mount_info)\n\n    return mount_facts", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\freebsd.py", "class_name": "FreeBSDHardware", "function_name": "get_dmi_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "DMI_DICT.items", "DMI_DICT.keys", "dict.fromkeys", "json.dumps", "line.startswith", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "learn dmi facts from system Use dmidecode executable if available", "source_code": "def get_dmi_facts(self):\n    \"\"\" learn dmi facts from system\n\n    Use dmidecode executable if available\"\"\"\n\n    dmi_facts = {}\n\n    # Fall back to using dmidecode, if available\n    dmi_bin = self.module.get_bin_path('dmidecode')\n    DMI_DICT = {\n        'bios_date': 'bios-release-date',\n        'bios_vendor': 'bios-vendor',\n        'bios_version': 'bios-version',\n        'board_asset_tag': 'baseboard-asset-tag',\n        'board_name': 'baseboard-product-name',\n        'board_serial': 'baseboard-serial-number',\n        'board_vendor': 'baseboard-manufacturer',\n        'board_version': 'baseboard-version',\n        'chassis_asset_tag': 'chassis-asset-tag',\n        'chassis_serial': 'chassis-serial-number',\n        'chassis_vendor': 'chassis-manufacturer',\n        'chassis_version': 'chassis-version',\n        'form_factor': 'chassis-type',\n        'product_name': 'system-product-name',\n        'product_serial': 'system-serial-number',\n        'product_uuid': 'system-uuid',\n        'product_version': 'system-version',\n        'system_vendor': 'system-manufacturer',\n    }\n    if dmi_bin is None:\n        dmi_facts = dict.fromkeys(\n            DMI_DICT.keys(),\n            'NA'\n        )\n        return dmi_facts\n\n    for (k, v) in DMI_DICT.items():\n        (rc, out, err) = self.module.run_command('%s -s %s' % (dmi_bin, v))\n        if rc == 0:\n            # Strip out commented lines (specific dmidecode output)\n            # FIXME: why add the fact and then test if it is json?\n            dmi_facts[k] = ''.join([line for line in out.splitlines() if not line.startswith('#')])\n            try:\n                json.dumps(dmi_facts[k])\n            except UnicodeDecodeError:\n                dmi_facts[k] = 'NA'\n        else:\n            dmi_facts[k] = 'NA'\n\n    return dmi_facts", "loc": 50}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\hpux.py", "class_name": "HPUXHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hardware_facts.update", "self.get_cpu_facts", "self.get_hw_facts", "self.get_memory_facts"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n\n    # TODO: very inefficient calls to machinfo,\n    # should just make one and then deal with finding the data (see facts/sysctl)\n    # but not going to change unless there is hp/ux for testing\n    cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n    memory_facts = self.get_memory_facts()\n    hw_facts = self.get_hw_facts()\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(hw_facts)\n\n    return hardware_facts", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\hpux.py", "class_name": "HPUXHardware", "function_name": "get_cpu_facts", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.get", "int", "len", "out.strip", "out.strip().split", "re.search", "re.search('.*(Intel.*)', out).groups", "re.search('.*(Intel.*)', out).groups()[0].strip", "re.sub", "re.sub(' +', ' ', out).strip", "re.sub(' +', ' ', out).strip().split", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self, collected_facts=None):\n    cpu_facts = {}\n    collected_facts = collected_facts or {}\n\n    if collected_facts.get('ansible_architecture') in ['9000/800', '9000/785']:\n        rc, out, err = self.module.run_command(\"ioscan -FkCprocessor | wc -l\", use_unsafe_shell=True)\n        cpu_facts['processor_count'] = int(out.strip())\n    # Working with machinfo mess\n    elif collected_facts.get('ansible_architecture') == 'ia64':\n        if collected_facts.get('ansible_distribution_version') == \"B.11.23\":\n            rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep 'Number of CPUs'\", use_unsafe_shell=True)\n            if out:\n                cpu_facts['processor_count'] = int(out.strip().split('=')[1])\n            rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep 'processor family'\", use_unsafe_shell=True)\n            if out:\n                cpu_facts['processor'] = re.search('.*(Intel.*)', out).groups()[0].strip()\n            rc, out, err = self.module.run_command(\"ioscan -FkCprocessor | wc -l\", use_unsafe_shell=True)\n            cpu_facts['processor_cores'] = int(out.strip())\n        if collected_facts.get('ansible_distribution_version') == \"B.11.31\":\n            # if machinfo return cores strings release B.11.31 > 1204\n            rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep core | wc -l\", use_unsafe_shell=True)\n            if out.strip() == '0':\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep Intel\", use_unsafe_shell=True)\n                cpu_facts['processor_count'] = int(out.strip().split(\" \")[0])\n                # If hyperthreading is active divide cores by 2\n                rc, out, err = self.module.run_command(\"/usr/sbin/psrset | grep LCPU\", use_unsafe_shell=True)\n                data = re.sub(' +', ' ', out).strip().split(' ')\n                if len(data) == 1:\n                    hyperthreading = 'OFF'\n                else:\n                    hyperthreading = data[1]\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep logical\", use_unsafe_shell=True)\n                data = out.strip().split(\" \")\n                if hyperthreading == 'ON':\n                    cpu_facts['processor_cores'] = int(data[0]) / 2\n                else:\n                    if len(data) == 1:\n                        cpu_facts['processor_cores'] = cpu_facts['processor_count']\n                    else:\n                        cpu_facts['processor_cores'] = int(data[0])\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep Intel |cut -d' ' -f4-\", use_unsafe_shell=True)\n                cpu_facts['processor'] = out.strip()\n            else:\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | egrep 'socket[s]?$' | tail -1\", use_unsafe_shell=True)\n                cpu_facts['processor_count'] = int(out.strip().split(\" \")[0])\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep -e '[0-9] core' | tail -1\", use_unsafe_shell=True)\n                cpu_facts['processor_cores'] = int(out.strip().split(\" \")[0])\n                rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep Intel\", use_unsafe_shell=True)\n                cpu_facts['processor'] = out.strip()\n\n    return cpu_facts", "loc": 51}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\hpux.py", "class_name": "HPUXHardware", "function_name": "get_memory_facts", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.get", "int", "os.access", "out.strip", "out.strip().splitlines", "re.search", "re.search('.*Physical: ([0-9]*) Kbytes.*', out).groups", "re.search('.*Physical: ([0-9]*) Kbytes.*', out).groups()[0].strip", "re.search('Memory[\\\\ :=]*([0-9]*).*MB.*', out).groups", "re.search('Memory[\\\\ :=]*([0-9]*).*MB.*', out).groups()[0].strip", "re.sub", "re.sub(' +', ' ', line).split", "re.sub(' +', ' ', line).split(' ')[3].strip", "re.sub(' +', ' ', out).split", "re.sub(' +', ' ', out).split(' ')[5].strip", "self.module.run_command"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self, collected_facts=None):\n    memory_facts = {}\n    collected_facts = collected_facts or {}\n\n    pagesize = 4096\n    rc, out, err = self.module.run_command(\"/usr/bin/vmstat | tail -1\", use_unsafe_shell=True)\n    data = int(re.sub(' +', ' ', out).split(' ')[5].strip())\n    memory_facts['memfree_mb'] = pagesize * data // 1024 // 1024\n    if collected_facts.get('ansible_architecture') in ['9000/800', '9000/785']:\n        try:\n            rc, out, err = self.module.run_command(\"grep Physical /var/adm/syslog/syslog.log\")\n            data = re.search('.*Physical: ([0-9]*) Kbytes.*', out).groups()[0].strip()\n            memory_facts['memtotal_mb'] = int(data) // 1024\n        except AttributeError:\n            # For systems where memory details aren't sent to syslog or the log has rotated, use parsed\n            # adb output. Unfortunately /dev/kmem doesn't have world-read, so this only works as root.\n            if os.access(\"/dev/kmem\", os.R_OK):\n                rc, out, err = self.module.run_command(\"echo 'phys_mem_pages/D' | adb -k /stand/vmunix /dev/kmem | tail -1 | awk '{print $2}'\",\n                                                       use_unsafe_shell=True)\n                if not err:\n                    data = out\n                    memory_facts['memtotal_mb'] = int(data) / 256\n    else:\n        rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo | grep Memory\", use_unsafe_shell=True)\n        data = re.search(r'Memory[\\ :=]*([0-9]*).*MB.*', out).groups()[0].strip()\n        memory_facts['memtotal_mb'] = int(data)\n    rc, out, err = self.module.run_command(\"/usr/sbin/swapinfo -m -d -f -q\")\n    memory_facts['swaptotal_mb'] = int(out.strip())\n    rc, out, err = self.module.run_command(\"/usr/sbin/swapinfo -m -d -f | egrep '^dev|^fs'\", use_unsafe_shell=True)\n    swap = 0\n    for line in out.strip().splitlines():\n        swap += int(re.sub(' +', ' ', line).split(' ')[3].strip())\n    memory_facts['swapfree_mb'] = swap\n\n    return memory_facts", "loc": 35}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\hpux.py", "class_name": "HPUXHardware", "function_name": "get_hw_facts", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.get", "out.split", "out.split(separator)[1].strip", "out.strip", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_hw_facts(self, collected_facts=None):\n    hw_facts = {}\n    collected_facts = collected_facts or {}\n\n    rc, out, err = self.module.run_command(\"model\")\n    hw_facts['model'] = out.strip()\n    if collected_facts.get('ansible_architecture') == 'ia64':\n        separator = ':'\n        if collected_facts.get('ansible_distribution_version') == \"B.11.23\":\n            separator = '='\n        rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo |grep -i 'Firmware revision' | grep -v BMC\", use_unsafe_shell=True)\n        hw_facts['firmware_version'] = out.split(separator)[1].strip()\n        rc, out, err = self.module.run_command(\"/usr/contrib/bin/machinfo |grep -i 'Machine serial number' \", use_unsafe_shell=True)\n        if rc == 0 and out:\n            hw_facts['product_serial'] = out.split(separator)[1].strip()\n\n    return hw_facts", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\hurd.py", "class_name": "HurdHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hardware_facts.update", "self.get_memory_facts", "self.get_mount_facts", "self.get_uptime_facts"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n    uptime_facts = self.get_uptime_facts()\n    memory_facts = self.get_memory_facts()\n\n    mount_facts = {}\n    try:\n        mount_facts = self.get_mount_facts()\n    except TimeoutError:\n        pass\n\n    hardware_facts.update(uptime_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(mount_facts)\n\n    return hardware_facts", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_best_parsable_locale", "hardware_facts.update", "self.get_cpu_facts", "self.get_device_facts", "self.get_dmi_facts", "self.get_lvm_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_sysinfo_facts", "self.get_uptime_facts", "self.module.warn"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n    locale = get_best_parsable_locale(self.module)\n    self.module.run_command_environ_update = {'LANG': locale, 'LC_ALL': locale, 'LC_NUMERIC': locale}\n\n    cpu_facts = self.get_cpu_facts(collected_facts=collected_facts)\n    memory_facts = self.get_memory_facts()\n    dmi_facts = self.get_dmi_facts()\n    sysinfo_facts = self.get_sysinfo_facts()\n    device_facts = self.get_device_facts()\n    uptime_facts = self.get_uptime_facts()\n    lvm_facts = self.get_lvm_facts()\n\n    mount_facts = {}\n    try:\n        mount_facts = self.get_mount_facts()\n    except timeout.TimeoutError:\n        self.module.warn(\"No mount facts were gathered due to timeout.\")\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(dmi_facts)\n    hardware_facts.update(sysinfo_facts)\n    hardware_facts.update(device_facts)\n    hardware_facts.update(uptime_facts)\n    hardware_facts.update(lvm_facts)\n    hardware_facts.update(mount_facts)\n\n    return hardware_facts", "loc": 29}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[1].strip", "data[1].strip().split", "get_file_lines", "int", "key.lower", "line.split", "memstats.get", "os.access"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n    if not os.access(\"/proc/meminfo\", os.R_OK):\n        return memory_facts\n\n    memstats = {}\n    for line in get_file_lines(\"/proc/meminfo\"):\n        data = line.split(\":\", 1)\n        key = data[0]\n        if key in self.ORIGINAL_MEMORY_FACTS:\n            val = data[1].strip().split(' ')[0]\n            memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n\n        if key in self.MEMORY_FACTS:\n            val = data[1].strip().split(' ')[0]\n            memstats[key.lower()] = int(val) // 1024\n\n    if None not in (memstats.get('memtotal'), memstats.get('memfree')):\n        memstats['real:used'] = memstats['memtotal'] - memstats['memfree']\n    if None not in (memstats.get('cached'), memstats.get('memfree'), memstats.get('buffers')):\n        memstats['nocache:free'] = memstats['cached'] + memstats['memfree'] + memstats['buffers']\n    if None not in (memstats.get('memtotal'), memstats.get('nocache:free')):\n        memstats['nocache:used'] = memstats['memtotal'] - memstats['nocache:free']\n    if None not in (memstats.get('swaptotal'), memstats.get('swapfree')):\n        memstats['swap:used'] = memstats['swaptotal'] - memstats['swapfree']\n\n    memory_facts['memory_mb'] = {\n        'real': {\n            'total': memstats.get('memtotal'),\n            'used': memstats.get('real:used'),\n            'free': memstats.get('memfree'),\n        },\n        'nocache': {\n            'free': memstats.get('nocache:free'),\n            'used': memstats.get('nocache:used'),\n        },\n        'swap': {\n            'total': memstats.get('swaptotal'),\n            'free': memstats.get('swapfree'),\n            'used': memstats.get('swap:used'),\n            'cached': memstats.get('swapcached'),\n        },\n    }\n\n    return memory_facts", "loc": 45}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_mount_info", "parameters": ["self", "mount", "device", "uuids"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_mount_size", "self._udevadm_uuid", "uuids.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_info(self, mount, device, uuids):\n\n    mount_size = get_mount_size(mount)\n\n    # _udevadm_uuid is a fallback for versions of lsblk <= 2.23 that don't have --paths\n    # see _run_lsblk() above\n    # https://github.com/ansible/ansible/issues/36077\n    uuid = uuids.get(device, self._udevadm_uuid(device))\n\n    return mount_size, uuid", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_futures.DaemonThreadPoolExecutor", "device.startswith", "executor.shutdown", "executor.submit", "int", "list", "mounts.append", "res.done", "res.exception", "res.result", "results[mount]['info'].update", "self.MTAB_BIND_MOUNT_RE.match", "self._find_bind_mounts", "self._lsblk_uuid", "self._mtab_entries", "self._replace_octal_escapes", "self.module.debug", "self.module.warn", "time.monotonic", "time.sleep", "to_text", "traceback.format_exc", "type"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n\n    mounts = []\n\n    # gather system lists\n    bind_mounts = self._find_bind_mounts()\n    uuids = self._lsblk_uuid()\n    mtab_entries = self._mtab_entries()\n\n    # start threads to query each mount\n    results = {}\n    executor = _futures.DaemonThreadPoolExecutor()\n    maxtime = timeout.GATHER_TIMEOUT or timeout.DEFAULT_GATHER_TIMEOUT\n    for fields in mtab_entries:\n        # Transform octal escape sequences\n        fields = [self._replace_octal_escapes(field) for field in fields]\n\n        device, mount, fstype, options = fields[0], fields[1], fields[2], fields[3]\n        dump, passno = int(fields[4]), int(fields[5])\n\n        if not device.startswith(('/', '\\\\')) and ':/' not in device or fstype == 'none':\n            continue\n\n        mount_info = {'mount': mount,\n                      'device': device,\n                      'fstype': fstype,\n                      'options': options,\n                      'dump': dump,\n                      'passno': passno}\n\n        if mount in bind_mounts:\n            # only add if not already there, we might have a plain /etc/mtab\n            if not self.MTAB_BIND_MOUNT_RE.match(options):\n                mount_info['options'] += \",bind\"\n\n        results[mount] = {'info': mount_info, 'timelimit': time.monotonic() + maxtime}\n        results[mount]['extra'] = executor.submit(self.get_mount_info, mount, device, uuids)\n\n    # done with spawning new workers, start gc\n    executor.shutdown()\n\n    while results:  # wait for workers and get results\n        for mount in list(results):\n            done = False\n            res = results[mount]['extra']\n            try:\n                if res.done():\n                    done = True\n                    if res.exception() is None:\n                        mount_size, uuid = res.result()\n                        if mount_size:\n                            results[mount]['info'].update(mount_size)\n                        results[mount]['info']['uuid'] = uuid or 'N/A'\n                    else:\n                        # failed, try to find out why, if 'res.successful' we know there are no exceptions\n                        results[mount]['info']['note'] = f'Could not get extra information: {res.exception()}'\n\n                elif time.monotonic() > results[mount]['timelimit']:\n                    done = True\n                    self.module.warn(\"Timeout exceeded when getting mount info for %s\" % mount)\n                    results[mount]['info']['note'] = 'Could not get extra information due to timeout'\n            except Exception as e:\n                import traceback\n                done = True\n                results[mount]['info'] = 'N/A'\n                self.module.warn(\"Error prevented getting extra info for mount %s: [%s] %s.\" % (mount, type(e), to_text(e)))\n                self.module.debug(traceback.format_exc())\n\n            if done:\n                # move results outside and make loop only handle pending\n                mounts.append(results[mount]['info'])\n                del results[mount]\n\n        # avoid cpu churn, sleep between retrying for loop with remaining mounts\n        time.sleep(0.1)\n\n    return {'mounts': mounts}", "loc": 77}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_all_device_owners", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collections.defaultdict", "dict", "glob.glob", "list", "path.split", "retval.items", "retval[target].add", "sorted"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_device_owners(self):\n    try:\n        retval = collections.defaultdict(set)\n        for path in glob.glob('/sys/block/*/slaves/*'):\n            elements = path.split('/')\n            device = elements[3]\n            target = elements[5]\n            retval[target].add(device)\n        return dict((k, list(sorted(v))) for (k, v) in retval.items())\n    except OSError:\n        return {}", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_all_device_links", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_all_device_owners", "self.get_device_links"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_device_links(self):\n    return {\n        'ids': self.get_device_links('/dev/disk/by-id'),\n        'uuids': self.get_device_links('/dev/disk/by-uuid'),\n        'labels': self.get_device_links('/dev/disk/by-label'),\n        'masters': self.get_all_device_owners(),\n    }", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "get_file_content", "int", "uptime_file_content.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    uptime_facts = {}\n    uptime_file_content = get_file_content('/proc/uptime')\n    if uptime_file_content:\n        uptime_seconds_string = uptime_file_content.split(' ')[0]\n        uptime_facts['uptime_seconds'] = int(float(uptime_seconds_string))\n\n    return uptime_facts", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\linux.py", "class_name": "LinuxHardware", "function_name": "get_lvm_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lv_line.strip", "lv_line.strip().split", "lv_lines.splitlines", "os.getuid", "pv_line.strip", "pv_line.strip().split", "pv_lines.splitlines", "self._find_mapper_device_name", "self.module.get_bin_path", "self.module.run_command", "self.module.warn", "vg_line.strip", "vg_line.strip().split", "vg_lines.splitlines"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Get LVM Facts if running as root and lvm utils are available", "source_code": "def get_lvm_facts(self):\n    \"\"\" Get LVM Facts if running as root and lvm utils are available \"\"\"\n\n    lvm_facts = {'lvm': 'N/A'}\n    vgs_cmd = self.module.get_bin_path('vgs')\n    if vgs_cmd is None:\n        return lvm_facts\n\n    if os.getuid() == 0:\n        lvm_util_options = '--noheadings --nosuffix --units g --separator ,'\n\n        # vgs fields: VG #PV #LV #SN Attr VSize VFree\n        vgs = {}\n        rc, vg_lines, err = self.module.run_command('%s %s' % (vgs_cmd, lvm_util_options))\n        for vg_line in vg_lines.splitlines():\n            items = vg_line.strip().split(',')\n            vgs[items[0]] = {\n                'size_g': items[-2],\n                'free_g': items[-1],\n                'num_lvs': items[2],\n                'num_pvs': items[1],\n                'lvs': {},\n            }\n\n        lvs_path = self.module.get_bin_path('lvs')\n        # lvs fields:\n        # LV VG Attr LSize Pool Origin Data% Move Log Copy% Convert\n        lvs = {}\n        if lvs_path:\n            rc, lv_lines, err = self.module.run_command('%s %s' % (lvs_path, lvm_util_options))\n            for lv_line in lv_lines.splitlines():\n                items = lv_line.strip().split(',')\n                vg_name = items[1]\n                lv_name = items[0]\n                # The LV name is only unique per VG, so the top level fact lvs can be misleading.\n                # TODO: deprecate lvs in favor of vgs\n                lvs[lv_name] = {'size_g': items[3], 'vg': vg_name}\n                try:\n                    vgs[vg_name]['lvs'][lv_name] = {'size_g': items[3]}\n                except KeyError:\n                    self.module.warn(\n                        \"An LVM volume group was created while gathering LVM facts, \"\n                        \"and is not included in ansible_facts['vgs'].\"\n                    )\n\n        pvs_path = self.module.get_bin_path('pvs')\n        # pvs fields: PV VG #Fmt #Attr PSize PFree\n        pvs = {}\n        if pvs_path:\n            rc, pv_lines, err = self.module.run_command('%s %s' % (pvs_path, lvm_util_options))\n            for pv_line in pv_lines.splitlines():\n                items = pv_line.strip().split(',')\n                pvs[self._find_mapper_device_name(items[0])] = {\n                    'size_g': items[4],\n                    'free_g': items[5],\n                    'vg': items[1]}\n\n        lvm_facts['lvm'] = {'lvs': lvs, 'vgs': vgs, 'pvs': pvs}\n\n    return lvm_facts", "loc": 60}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_sysctl", "hardware_facts.update", "self.get_cpu_facts", "self.get_dmi_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_uptime_facts"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n    self.sysctl = get_sysctl(self.module, ['machdep'])\n    cpu_facts = self.get_cpu_facts()\n    memory_facts = self.get_memory_facts()\n\n    mount_facts = {}\n    try:\n        mount_facts = self.get_mount_facts()\n    except TimeoutError:\n        pass\n\n    dmi_facts = self.get_dmi_facts()\n    uptime_facts = self.get_uptime_facts()\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(mount_facts)\n    hardware_facts.update(dmi_facts)\n    hardware_facts.update(uptime_facts)\n\n    return hardware_facts", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "get_cpu_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cpu_facts['processor'].append", "data[0].strip", "data[1].strip", "get_file_lines", "int", "len", "line.split", "os.access", "reduce", "sockets.values"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self):\n    cpu_facts = {}\n\n    i = 0\n    physid = 0\n    sockets = {}\n    if not os.access(\"/proc/cpuinfo\", os.R_OK):\n        return cpu_facts\n    cpu_facts['processor'] = []\n    for line in get_file_lines(\"/proc/cpuinfo\"):\n        data = line.split(\":\", 1)\n        key = data[0].strip()\n        # model name is for Intel arch, Processor (mind the uppercase P)\n        # works for some ARM devices, like the Sheevaplug.\n        if key == 'model name' or key == 'Processor':\n            if 'processor' not in cpu_facts:\n                cpu_facts['processor'] = []\n            cpu_facts['processor'].append(data[1].strip())\n            i += 1\n        elif key == 'physical id':\n            physid = data[1].strip()\n            if physid not in sockets:\n                sockets[physid] = 1\n        elif key == 'cpu cores':\n            sockets[physid] = int(data[1].strip())\n    if len(sockets) > 0:\n        cpu_facts['processor_count'] = len(sockets)\n        cpu_facts['processor_cores'] = reduce(lambda x, y: x + y, sockets.values())\n    else:\n        cpu_facts['processor_count'] = i\n        cpu_facts['processor_cores'] = 'NA'\n\n    return cpu_facts", "loc": 33}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[1].strip", "data[1].strip().split", "get_file_lines", "int", "key.lower", "line.split", "os.access"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n    if not os.access(\"/proc/meminfo\", os.R_OK):\n        return memory_facts\n    for line in get_file_lines(\"/proc/meminfo\"):\n        data = line.split(\":\", 1)\n        key = data[0]\n        if key in NetBSDHardware.MEMORY_FACTS:\n            val = data[1].strip().split(' ')[0]\n            memory_facts[\"%s_mb\" % key.lower()] = int(val) // 1024\n\n    return memory_facts", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fstab.splitlines", "get_file_content", "get_mount_size", "line.startswith", "line.strip", "mount_facts['mounts'].append", "mount_info.update", "re.sub", "re.sub('\\\\s+', ' ', line).split", "timeout"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n    mount_facts = {}\n\n    mount_facts['mounts'] = []\n    fstab = get_file_content('/etc/fstab')\n\n    if not fstab:\n        return mount_facts\n\n    for line in fstab.splitlines():\n        if line.startswith('#') or line.strip() == '':\n            continue\n        fields = re.sub(r'\\s+', ' ', line).split()\n        mount_statvfs_info = get_mount_size(fields[1])\n        mount_info = {'mount': fields[1],\n                      'device': fields[0],\n                      'fstype': fields[2],\n                      'options': fields[3]}\n        mount_info.update(mount_statvfs_info)\n        mount_facts['mounts'].append(mount_info)\n    return mount_facts", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "get_dmi_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dmi_facts(self):\n    dmi_facts = {}\n    # We don't use dmidecode(8) here because:\n    # - it would add dependency on an external package\n    # - dmidecode(8) can only be ran as root\n    # So instead we rely on sysctl(8) to provide us the information on a\n    # best-effort basis. As a bonus we also get facts on non-amd64/i386\n    # platforms this way.\n    sysctl_to_dmi = {\n        'machdep.dmi.system-product': 'product_name',\n        'machdep.dmi.system-version': 'product_version',\n        'machdep.dmi.system-uuid': 'product_uuid',\n        'machdep.dmi.system-serial': 'product_serial',\n        'machdep.dmi.system-vendor': 'system_vendor',\n    }\n\n    for mib in sysctl_to_dmi:\n        if mib in self.sysctl:\n            dmi_facts[sysctl_to_dmi[mib]] = self.sysctl[mib]\n\n    return dmi_facts", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\netbsd.py", "class_name": "NetBSDHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "kern_boottime.isdigit", "out.strip", "self.module.get_bin_path", "self.module.run_command", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    # On NetBSD, we need to call sysctl with -n to get this value as an int.\n    sysctl_cmd = self.module.get_bin_path('sysctl')\n    if sysctl_cmd is None:\n        return {}\n\n    cmd = [sysctl_cmd, '-n', 'kern.boottime']\n\n    rc, out, err = self.module.run_command(cmd)\n\n    if rc != 0:\n        return {}\n\n    kern_boottime = out.strip()\n    if not kern_boottime.isdigit():\n        return {}\n\n    return {\n        'uptime_seconds': int(time.time() - int(kern_boottime)),\n    }", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_sysctl", "hardware_facts.update", "self.get_device_facts", "self.get_dmi_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_processor_facts", "self.get_uptime_facts"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n    self.sysctl = get_sysctl(self.module, ['hw'])\n\n    hardware_facts.update(self.get_processor_facts())\n    hardware_facts.update(self.get_memory_facts())\n    hardware_facts.update(self.get_device_facts())\n    hardware_facts.update(self.get_dmi_facts())\n    hardware_facts.update(self.get_uptime_facts())\n\n    # storage devices notoriously prone to hang/block so they are under a timeout\n    try:\n        hardware_facts.update(self.get_mount_facts())\n    except timeout.TimeoutError:\n        pass\n\n    return hardware_facts", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fstab.splitlines", "get_file_content", "get_mount_size", "line.startswith", "line.strip", "mount_facts['mounts'].append", "mount_info.update", "re.sub", "re.sub('\\\\s+', ' ', line).split", "timeout.timeout"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n    mount_facts = {}\n\n    mount_facts['mounts'] = []\n    fstab = get_file_content('/etc/fstab')\n    if fstab:\n        for line in fstab.splitlines():\n            if line.startswith('#') or line.strip() == '':\n                continue\n            fields = re.sub(r'\\s+', ' ', line).split()\n            if fields[1] == 'none' or fields[3] == 'xx':\n                continue\n            mount_statvfs_info = get_mount_size(fields[1])\n            mount_info = {'mount': fields[1],\n                          'device': fields[0],\n                          'fstype': fields[2],\n                          'options': fields[3]}\n            mount_info.update(mount_statvfs_info)\n            mount_facts['mounts'].append(mount_info)\n    return mount_facts", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[-2].translate", "data[1].translate", "int", "ord", "out.splitlines", "out.splitlines()[-1].split", "self.module.run_command", "to_text", "to_text(out, errors='surrogate_or_strict').split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n    # Get free memory. vmstat output looks like:\n    #  procs    memory       page                    disks    traps          cpu\n    #  r b w    avm     fre  flt  re  pi  po  fr  sr wd0 fd0  int   sys   cs us sy id\n    #  0 0 0  47512   28160   51   0   0   0   0   0   1   0  116    89   17  0  1 99\n    rc, out, err = self.module.run_command(\"/usr/bin/vmstat\")\n    if rc == 0:\n        memory_facts['memfree_mb'] = int(out.splitlines()[-1].split()[4]) // 1024\n        memory_facts['memtotal_mb'] = int(self.sysctl['hw.physmem']) // 1024 // 1024\n\n    # Get swapctl info. swapctl output looks like:\n    # total: 69268 1K-blocks allocated, 0 used, 69268 available\n    # And for older OpenBSD:\n    # total: 69268k bytes allocated = 0k used, 69268k available\n    rc, out, err = self.module.run_command(\"/sbin/swapctl -sk\")\n    if rc == 0:\n        swaptrans = {ord(u'k'): None,\n                     ord(u'm'): None,\n                     ord(u'g'): None}\n        data = to_text(out, errors='surrogate_or_strict').split()\n        memory_facts['swapfree_mb'] = int(data[-2].translate(swaptrans)) // 1024\n        memory_facts['swaptotal_mb'] = int(data[1].translate(swaptrans)) // 1024\n\n    return memory_facts", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "kern_boottime.isdigit", "out.strip", "self.module.get_bin_path", "self.module.run_command", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    # On openbsd, we need to call it with -n to get this value as an int.\n    sysctl_cmd = self.module.get_bin_path('sysctl')\n    if sysctl_cmd is None:\n        return {}\n\n    cmd = [sysctl_cmd, '-n', 'kern.boottime']\n\n    rc, out, err = self.module.run_command(cmd)\n\n    if rc != 0:\n        return {}\n\n    kern_boottime = out.strip()\n    if not kern_boottime.isdigit():\n        return {}\n\n    return {\n        'uptime_seconds': int(time.time() - int(kern_boottime)),\n    }", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_processor_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "processor.append", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_processor_facts(self):\n    cpu_facts = {}\n    processor = []\n    for i in range(int(self.sysctl['hw.ncpuonline'])):\n        processor.append(self.sysctl['hw.model'])\n\n    cpu_facts['processor'] = processor\n    # The following is partly a lie because there is no reliable way to\n    # determine the number of physical CPUs in the system. We can only\n    # query the number of logical CPUs, which hides the number of cores.\n    # On amd64/i386 we could try to inspect the smt/core/package lines in\n    # dmesg, however even those have proven to be unreliable.\n    # So take a shortcut and report the logical number of processors in\n    # 'processor_count' and 'processor_cores' and leave it at that.\n    cpu_facts['processor_count'] = self.sysctl['hw.ncpuonline']\n    cpu_facts['processor_cores'] = self.sysctl['hw.ncpuonline']\n\n    return cpu_facts", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_device_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["devices.extend", "self.sysctl['hw.disknames'].split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_device_facts(self):\n    device_facts = {}\n    devices = []\n    devices.extend(self.sysctl['hw.disknames'].split(','))\n    device_facts['devices'] = devices\n\n    return device_facts", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\openbsd.py", "class_name": "OpenBSDHardware", "function_name": "get_dmi_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dmi_facts(self):\n    dmi_facts = {}\n    # We don't use dmidecode(8) here because:\n    # - it would add dependency on an external package\n    # - dmidecode(8) can only be ran as root\n    # So instead we rely on sysctl(8) to provide us the information on a\n    # best-effort basis. As a bonus we also get facts on non-amd64/i386\n    # platforms this way.\n    sysctl_to_dmi = {\n        'hw.product': 'product_name',\n        'hw.version': 'product_version',\n        'hw.uuid': 'product_uuid',\n        'hw.serialno': 'product_serial',\n        'hw.vendor': 'system_vendor',\n    }\n\n    for mib in sysctl_to_dmi:\n        if mib in self.sysctl:\n            dmi_facts[sysctl_to_dmi[mib]] = self.sysctl[mib]\n\n    return dmi_facts", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_best_parsable_locale", "hardware_facts.update", "self.get_cpu_facts", "self.get_device_facts", "self.get_dmi_facts", "self.get_memory_facts", "self.get_mount_facts", "self.get_uptime_facts"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    hardware_facts = {}\n\n    # FIXME: could pass to run_command(environ_update), but it also tweaks the env\n    #        of the parent process instead of altering an env provided to Popen()\n    # Use C locale for hardware collection helpers to avoid locale specific number formatting (#24542)\n    locale = get_best_parsable_locale(self.module)\n    self.module.run_command_environ_update = {'LANG': locale, 'LC_ALL': locale, 'LC_NUMERIC': locale}\n\n    cpu_facts = self.get_cpu_facts()\n    memory_facts = self.get_memory_facts()\n    dmi_facts = self.get_dmi_facts()\n    device_facts = self.get_device_facts()\n    uptime_facts = self.get_uptime_facts()\n\n    mount_facts = {}\n    try:\n        mount_facts = self.get_mount_facts()\n    except timeout.TimeoutError:\n        pass\n\n    hardware_facts.update(cpu_facts)\n    hardware_facts.update(memory_facts)\n    hardware_facts.update(dmi_facts)\n    hardware_facts.update(device_facts)\n    hardware_facts.update(uptime_facts)\n    hardware_facts.update(mount_facts)\n\n    return hardware_facts", "loc": 29}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_cpu_facts", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.get", "cpu_facts['processor'].append", "data[0].strip", "data[1].strip", "len", "line.split", "out.splitlines", "reduce", "self.module.run_command", "sockets.values"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cpu_facts(self, collected_facts=None):\n    physid = 0\n    sockets = {}\n\n    cpu_facts = {}\n    collected_facts = collected_facts or {}\n\n    rc, out, err = self.module.run_command(\"/usr/bin/kstat cpu_info\")\n\n    cpu_facts['processor'] = []\n\n    for line in out.splitlines():\n        if len(line) < 1:\n            continue\n\n        data = line.split(None, 1)\n        key = data[0].strip()\n\n        # \"brand\" works on Solaris 10 & 11. \"implementation\" for Solaris 9.\n        if key == 'module:':\n            brand = ''\n        elif key == 'brand':\n            brand = data[1].strip()\n        elif key == 'clock_MHz':\n            clock_mhz = data[1].strip()\n        elif key == 'implementation':\n            processor = brand or data[1].strip()\n            # Add clock speed to description for SPARC CPU\n            # FIXME\n            if collected_facts.get('ansible_machine') != 'i86pc':\n                processor += \" @ \" + clock_mhz + \"MHz\"\n            if 'ansible_processor' not in collected_facts:\n                cpu_facts['processor'] = []\n            cpu_facts['processor'].append(processor)\n        elif key == 'chip_id':\n            physid = data[1].strip()\n            if physid not in sockets:\n                sockets[physid] = 1\n            else:\n                sockets[physid] += 1\n\n    # Counting cores on Solaris can be complicated.\n    # https://blogs.oracle.com/mandalika/entry/solaris_show_me_the_cpu\n    # Treat 'processor_count' as physical sockets and 'processor_cores' as\n    # virtual CPUs visible to Solaris. Not a true count of cores for modern SPARC as\n    # these processors have: sockets -> cores -> threads/virtual CPU.\n    if len(sockets) > 0:\n        cpu_facts['processor_count'] = len(sockets)\n        cpu_facts['processor_cores'] = reduce(lambda x, y: x + y, sockets.values())\n    else:\n        cpu_facts['processor_cores'] = 'NA'\n        cpu_facts['processor_count'] = len(cpu_facts['processor'])\n\n    return cpu_facts", "loc": 54}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_memory_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "line.split", "out.split", "out.splitlines", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_memory_facts(self):\n    memory_facts = {}\n\n    rc, out, err = self.module.run_command([\"/usr/sbin/prtconf\"])\n\n    for line in out.splitlines():\n        if 'Memory size' in line:\n            memory_facts['memtotal_mb'] = int(line.split()[2])\n\n    rc, out, err = self.module.run_command(\"/usr/sbin/swap -s\")\n\n    allocated = int(out.split()[1][:-1])\n    reserved = int(out.split()[5][:-1])\n    used = int(out.split()[8][:-1])\n    free = int(out.split()[10][:-1])\n\n    memory_facts['swapfree_mb'] = free // 1024\n    memory_facts['swaptotal_mb'] = (free + used) // 1024\n    memory_facts['swap_allocated_mb'] = allocated // 1024\n    memory_facts['swap_reserved_mb'] = reserved // 1024\n\n    return memory_facts", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_mount_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fstab.splitlines", "get_file_content", "get_mount_size", "line.split", "mount_facts['mounts'].append", "mount_info.update", "timeout.timeout"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_mount_facts(self):\n    mount_facts = {}\n    mount_facts['mounts'] = []\n\n    # For a detailed format description see mnttab(4)\n    #   special mount_point fstype options time\n    fstab = get_file_content('/etc/mnttab')\n\n    if fstab:\n        for line in fstab.splitlines():\n            fields = line.split('\\t')\n            mount_statvfs_info = get_mount_size(fields[1])\n            mount_info = {'mount': fields[1],\n                          'device': fields[0],\n                          'fstype': fields[2],\n                          'options': fields[3],\n                          'time': fields[4]}\n            mount_info.update(mount_statvfs_info)\n            mount_facts['mounts'].append(mount_info)\n\n    return mount_facts", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_dmi_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'|'.join", "found.group", "map", "out.split", "platform.rstrip", "re.match", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dmi_facts(self):\n    dmi_facts = {}\n\n    # On Solaris 8 the prtdiag wrapper is absent from /usr/sbin,\n    # but that's okay, because we know where to find the real thing:\n    rc, platform, err = self.module.run_command('/usr/bin/uname -i')\n    platform_sbin = '/usr/platform/' + platform.rstrip() + '/sbin'\n\n    prtdiag_path = self.module.get_bin_path(\n        \"prtdiag\",\n        opt_dirs=[platform_sbin]\n    )\n    if prtdiag_path is None:\n        return dmi_facts\n\n    rc, out, err = self.module.run_command(prtdiag_path)\n    # rc returns 1\n    if out:\n        system_conf = out.split('\\n')[0]\n\n        # If you know of any other manufacturers whose names appear in\n        # the first line of prtdiag's output, please add them here:\n        vendors = [\n            \"Fujitsu\",\n            \"Oracle Corporation\",\n            \"QEMU\",\n            \"Sun Microsystems\",\n            \"VMware, Inc.\",\n        ]\n        vendor_regexp = \"|\".join(map(re.escape, vendors))\n        system_conf_regexp = (r'System Configuration:\\s+'\n                              + r'(' + vendor_regexp + r')\\s+'\n                              + r'(?:sun\\w+\\s+)?'\n                              + r'(.+)')\n\n        found = re.match(system_conf_regexp, system_conf)\n        if found:\n            dmi_facts['system_vendor'] = found.group(1)\n            dmi_facts['product_name'] = found.group(2)\n\n    return dmi_facts", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_device_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bytes_to_human", "cmd.append", "disk_stats.get", "float", "frozenset", "line.split", "line.startswith", "out.split", "self.module.run_command", "text.split", "value.rstrip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_device_facts(self):\n    # Device facts are derived for sdderr kstats. This code does not use the\n    # full output, but rather queries for specific stats.\n    # Example output:\n    # sderr:0:sd0,err:Hard Errors     0\n    # sderr:0:sd0,err:Illegal Request 6\n    # sderr:0:sd0,err:Media Error     0\n    # sderr:0:sd0,err:Predictive Failure Analysis     0\n    # sderr:0:sd0,err:Product VBOX HARDDISK   9\n    # sderr:0:sd0,err:Revision        1.0\n    # sderr:0:sd0,err:Serial No       VB0ad2ec4d-074a\n    # sderr:0:sd0,err:Size    53687091200\n    # sderr:0:sd0,err:Soft Errors     0\n    # sderr:0:sd0,err:Transport Errors        0\n    # sderr:0:sd0,err:Vendor  ATA\n\n    device_facts = {}\n    device_facts['devices'] = {}\n\n    disk_stats = {\n        'Product': 'product',\n        'Revision': 'revision',\n        'Serial No': 'serial',\n        'Size': 'size',\n        'Vendor': 'vendor',\n        'Hard Errors': 'hard_errors',\n        'Soft Errors': 'soft_errors',\n        'Transport Errors': 'transport_errors',\n        'Media Error': 'media_errors',\n        'Predictive Failure Analysis': 'predictive_failure_analysis',\n        'Illegal Request': 'illegal_request',\n    }\n\n    cmd = ['/usr/bin/kstat', '-p']\n\n    for ds in disk_stats:\n        cmd.append('sderr:::%s' % ds)\n\n    d = {}\n    rc, out, err = self.module.run_command(cmd)\n    if rc != 0:\n        return device_facts\n\n    sd_instances = frozenset(line.split(':')[1] for line in out.split('\\n') if line.startswith('sderr'))\n    for instance in sd_instances:\n        lines = (line for line in out.split('\\n') if ':' in line and line.split(':')[1] == instance)\n        for line in lines:\n            text, value = line.split('\\t')\n            stat = text.split(':')[3]\n\n            if stat == 'Size':\n                d[disk_stats.get(stat)] = bytes_to_human(float(value))\n            else:\n                d[disk_stats.get(stat)] = value.rstrip()\n\n        diskname = 'sd' + instance\n        device_facts['devices'][diskname] = d\n        d = {}\n\n    return device_facts", "loc": 60}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\hardware\\sunos.py", "class_name": "SunOSHardware", "function_name": "get_uptime_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "out.split", "self.module.run_command", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uptime_facts(self):\n    uptime_facts = {}\n    # sample kstat output:\n    # unix:0:system_misc:boot_time    1548249689\n    rc, out, err = self.module.run_command('/usr/bin/kstat -p unix:0:system_misc:boot_time')\n\n    if rc != 0:\n        return\n\n    # uptime = $current_time - $boot_time\n    uptime_facts['uptime_seconds'] = int(time.time() - int(out.split('\\t')[1]))\n\n    return uptime_facts", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\aix.py", "class_name": "AIXNetwork", "function_name": "get_default_interfaces", "parameters": ["self", "route_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "len", "line.split", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_default_interfaces(self, route_path):\n    interface = dict(v4={}, v6={})\n\n    netstat_path = self.module.get_bin_path('netstat')\n    if netstat_path is None:\n        return interface['v4'], interface['v6']\n\n    rc, out, err = self.module.run_command([netstat_path, '-nr'])\n\n    lines = out.splitlines()\n    for line in lines:\n        words = line.split()\n        if len(words) > 1 and words[0] == 'default':\n            if '.' in words[1]:\n                interface['v4']['gateway'] = words[1]\n                interface['v4']['interface'] = words[5]\n            elif ':' in words[1]:\n                interface['v6']['gateway'] = words[1]\n                interface['v6']['interface'] = words[5]\n\n    return interface['v4'], interface['v6']", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\aix.py", "class_name": "AIXNetwork", "function_name": "get_interfaces_info", "parameters": ["self", "ifconfig_path", "ifconfig_options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["buff.group", "dict", "line.split", "out.splitlines", "re.match", "self.module.get_bin_path", "self.module.run_command", "self.parse_ether_line", "self.parse_inet6_line", "self.parse_inet_line", "self.parse_interface_line", "self.parse_lladdr_line", "self.parse_media_line", "self.parse_nd6_line", "self.parse_options_line", "self.parse_status_line", "self.parse_unknown_line", "uname_out.split", "words[0].startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_interfaces_info(self, ifconfig_path, ifconfig_options='-a'):\n    interfaces = {}\n    current_if = {}\n    ips = dict(\n        all_ipv4_addresses=[],\n        all_ipv6_addresses=[],\n    )\n\n    uname_rc = uname_out = uname_err = None\n    uname_path = self.module.get_bin_path('uname')\n    if uname_path:\n        uname_rc, uname_out, uname_err = self.module.run_command([uname_path, '-W'])\n\n    rc, out, err = self.module.run_command([ifconfig_path, ifconfig_options])\n\n    for line in out.splitlines():\n\n        if line:\n            words = line.split()\n\n            # only this condition differs from GenericBsdIfconfigNetwork\n            if re.match(r'^\\w*\\d*:', line):\n                current_if = self.parse_interface_line(words)\n                interfaces[current_if['device']] = current_if\n            elif words[0].startswith('options='):\n                self.parse_options_line(words, current_if, ips)\n            elif words[0] == 'nd6':\n                self.parse_nd6_line(words, current_if, ips)\n            elif words[0] == 'ether':\n                self.parse_ether_line(words, current_if, ips)\n            elif words[0] == 'media:':\n                self.parse_media_line(words, current_if, ips)\n            elif words[0] == 'status:':\n                self.parse_status_line(words, current_if, ips)\n            elif words[0] == 'lladdr':\n                self.parse_lladdr_line(words, current_if, ips)\n            elif words[0] == 'inet':\n                self.parse_inet_line(words, current_if, ips)\n            elif words[0] == 'inet6':\n                self.parse_inet6_line(words, current_if, ips)\n            else:\n                self.parse_unknown_line(words, current_if, ips)\n\n        # don't bother with wpars it does not work\n        # zero means not in wpar\n        if not uname_rc and uname_out.split()[0] == '0':\n\n            if current_if['macaddress'] == 'unknown' and re.match('^en', current_if['device']):\n                entstat_path = self.module.get_bin_path('entstat')\n                if entstat_path:\n                    rc, out, err = self.module.run_command([entstat_path, current_if['device']])\n                    if rc != 0:\n                        break\n                    for line in out.splitlines():\n                        if not line:\n                            pass\n                        buff = re.match('^Hardware Address: (.*)', line)\n                        if buff:\n                            current_if['macaddress'] = buff.group(1)\n\n                        buff = re.match('^Device Type:', line)\n                        if buff and re.match('.*Ethernet', line):\n                            current_if['type'] = 'ether'\n\n            # device must have mtu attribute in ODM\n            if 'mtu' not in current_if:\n                lsattr_path = self.module.get_bin_path('lsattr')\n                if lsattr_path:\n                    rc, out, err = self.module.run_command([lsattr_path, '-El', current_if['device']])\n                    if rc != 0:\n                        break\n                    for line in out.splitlines():\n                        if line:\n                            words = line.split()\n                            if words[0] == 'mtu':\n                                current_if['mtu'] = words[1]\n    return interfaces, ips", "loc": 77}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\base.py", "class_name": "NetworkCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["facts_obj.populate", "self._fact_class"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    collected_facts = collected_facts or {}\n    if not module:\n        return {}\n\n    # Network munges cached_facts by side effect, so give it a copy\n    facts_obj = self._fact_class(module)\n\n    facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n    return facts_dict", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\darwin.py", "class_name": "DarwinNetwork", "function_name": "parse_media_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.get_options"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_media_line(self, words, current_if, ips):\n    # not sure if this is useful - we also drop information\n    current_if['media'] = 'Unknown'  # Mac does not give us this\n    current_if['media_select'] = words[1]\n    if len(words) > 2:\n        # MacOSX sets the media to '<unknown type>' for bridge interface\n        # and parsing splits this into two words; this if/else helps\n        if words[1] == '<unknown' and words[2] == 'type>':\n            current_if['media_select'] = 'Unknown'\n            current_if['media_type'] = 'unknown type'\n        else:\n            current_if['media_type'] = words[2][1:-1]\n    if len(words) > 3:\n        current_if['media_options'] = self.get_options(words[3])", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\fc_wwn.py", "class_name": "FcWwnInitiatorFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data[-1].rstrip", "data[-1].strip", "fc_facts['fibre_channel_wwn'].append", "fcinfo_out.splitlines", "fcmsutil_out.splitlines", "get_file_lines", "glob.glob", "ioscan_out.splitlines", "line.rstrip", "line.split", "line.strip", "lscfg_out.splitlines", "lsdev_out.splitlines", "module.get_bin_path", "module.run_command", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Example contents /sys/class/fc_host/*/port_name: 0x21000014ff52a9bb", "source_code": "def collect(self, module=None, collected_facts=None):\n    \"\"\"\n    Example contents /sys/class/fc_host/*/port_name:\n\n    0x21000014ff52a9bb\n\n    \"\"\"\n\n    fc_facts = {}\n    fc_facts['fibre_channel_wwn'] = []\n    if sys.platform.startswith('linux'):\n        for fcfile in glob.glob('/sys/class/fc_host/*/port_name'):\n            for line in get_file_lines(fcfile):\n                fc_facts['fibre_channel_wwn'].append(line.rstrip()[2:])\n    elif sys.platform.startswith('sunos'):\n        # on solaris 10 or solaris 11 should use `fcinfo hba-port`\n        # TBD (not implemented): on solaris 9 use `prtconf -pv`\n        cmd = module.get_bin_path('fcinfo')\n        if cmd:\n            cmd = cmd + \" hba-port\"\n            rc, fcinfo_out, err = module.run_command(cmd)\n            # fcinfo hba-port  | grep \"Port WWN\"\n            # HBA Port WWN: 10000090fa1658de\n            if rc == 0 and fcinfo_out:\n                for line in fcinfo_out.splitlines():\n                    if 'Port WWN' in line:\n                        data = line.split(' ')\n                        fc_facts['fibre_channel_wwn'].append(data[-1].rstrip())\n    elif sys.platform.startswith('aix'):\n        cmd = module.get_bin_path('lsdev')\n        lscfg_cmd = module.get_bin_path('lscfg')\n        if cmd and lscfg_cmd:\n            # get list of available fibre-channel devices (fcs)\n            cmd = cmd + \" -Cc adapter -l fcs*\"\n            rc, lsdev_out, err = module.run_command(cmd)\n            if rc == 0 and lsdev_out:\n                for line in lsdev_out.splitlines():\n                    # if device is available (not in defined state), get its WWN\n                    if 'Available' in line:\n                        data = line.split(' ')\n                        cmd = lscfg_cmd + \" -vl %s\" % data[0]\n                        rc, lscfg_out, err = module.run_command(cmd)\n                        # example output\n                        # lscfg -vpl fcs3 | grep \"Network Address\"\n                        #        Network Address.............10000090FA551509\n                        if rc == 0 and lscfg_out:\n                            for line in lscfg_out.splitlines():\n                                if 'Network Address' in line:\n                                    data = line.split('.')\n                                    fc_facts['fibre_channel_wwn'].append(data[-1].rstrip())\n    elif sys.platform.startswith('hp-ux'):\n        cmd = module.get_bin_path('ioscan')\n        fcmsu_cmd = module.get_bin_path(\n            'fcmsutil',\n            opt_dirs=['/opt/fcms/bin'],\n        )\n        # go ahead if we have both commands available\n        if cmd and fcmsu_cmd:\n            # ioscan / get list of available fibre-channel devices (fcd)\n            cmd = cmd + \" -fnC FC\"\n            rc, ioscan_out, err = module.run_command(cmd)\n            if rc == 0 and ioscan_out:\n                for line in ioscan_out.splitlines():\n                    line = line.strip()\n                    if '/dev/fcd' in line:\n                        dev = line.split(' ')\n                        # get device information\n                        cmd = fcmsu_cmd + \" %s\" % dev[0]\n                        rc, fcmsutil_out, err = module.run_command(cmd)\n                        # lookup the following line\n                        #             N_Port Port World Wide Name = 0x50060b00006975ec\n                        if rc == 0 and fcmsutil_out:\n                            for line in fcmsutil_out.splitlines():\n                                if 'N_Port Port World Wide Name' in line:\n                                    data = line.split('=')\n                                    fc_facts['fibre_channel_wwn'].append(data[-1].strip())\n    return fc_facts", "loc": 77}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["interfaces.keys", "list", "self.detect_type_media", "self.get_default_interfaces", "self.get_interfaces_info", "self.merge_default_interface", "self.module.get_bin_path", "sorted"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    network_facts = {}\n    ifconfig_path = self.module.get_bin_path('ifconfig')\n\n    if ifconfig_path is None:\n        return network_facts\n\n    route_path = self.module.get_bin_path('route')\n\n    if route_path is None:\n        return network_facts\n\n    default_ipv4, default_ipv6 = self.get_default_interfaces(route_path)\n    interfaces, ips = self.get_interfaces_info(ifconfig_path)\n    interfaces = self.detect_type_media(interfaces)\n\n    self.merge_default_interface(default_ipv4, interfaces, 'ipv4')\n    self.merge_default_interface(default_ipv6, interfaces, 'ipv6')\n    network_facts['interfaces'] = sorted(list(interfaces.keys()))\n\n    for iface in interfaces:\n        network_facts[iface] = interfaces[iface]\n\n    network_facts['default_ipv4'] = default_ipv4\n    network_facts['default_ipv6'] = default_ipv6\n    network_facts['all_ipv4_addresses'] = ips['all_ipv4_addresses']\n    network_facts['all_ipv6_addresses'] = ips['all_ipv6_addresses']\n\n    return network_facts", "loc": 29}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "detect_type_media", "parameters": ["self", "interfaces"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["interfaces[iface]['media'].lower"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def detect_type_media(self, interfaces):\n    for iface in interfaces:\n        if 'media' in interfaces[iface]:\n            if 'ether' in interfaces[iface]['media'].lower():\n                interfaces[iface]['type'] = 'ether'\n    return interfaces", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "get_default_interfaces", "parameters": ["self", "route_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "len", "line.strip", "line.strip().split", "out.splitlines", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_default_interfaces(self, route_path):\n\n    # Use the commands:\n    #     route -n get default\n    #     route -n get -inet6 default\n    # to find out the default outgoing interface, address, and gateway\n\n    command = dict(v4=[route_path, '-n', 'get', 'default'],\n                   v6=[route_path, '-n', 'get', '-inet6', 'default'])\n\n    interface = dict(v4={}, v6={})\n\n    for v in 'v4', 'v6':\n\n        if v == 'v6' and not socket.has_ipv6:\n            continue\n        rc, out, err = self.module.run_command(command[v])\n        if not out:\n            # v6 routing may result in\n            #   RTNETLINK answers: Invalid argument\n            continue\n        for line in out.splitlines():\n            words = line.strip().split(': ')\n            # Collect output from route command\n            if len(words) > 1:\n                if words[0] == 'interface':\n                    interface[v]['interface'] = words[1]\n                if words[0] == 'gateway':\n                    interface[v]['gateway'] = words[1]\n                # help pick the right interface address on OpenBSD\n                if words[0] == 'if address':\n                    interface[v]['address'] = words[1]\n                # help pick the right interface address on NetBSD\n                if words[0] == 'local addr':\n                    interface[v]['address'] = words[1]\n\n    return interface['v4'], interface['v6']", "loc": 37}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "get_interfaces_info", "parameters": ["self", "ifconfig_path", "ifconfig_options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "len", "line.split", "out.splitlines", "re.match", "self.module.run_command", "self.parse_ether_line", "self.parse_inet6_line", "self.parse_inet_line", "self.parse_interface_line", "self.parse_lladdr_line", "self.parse_media_line", "self.parse_nd6_line", "self.parse_options_line", "self.parse_status_line", "self.parse_tunnel_line", "self.parse_unknown_line", "words[0].startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_interfaces_info(self, ifconfig_path, ifconfig_options='-a'):\n    interfaces = {}\n    current_if = {}\n    ips = dict(\n        all_ipv4_addresses=[],\n        all_ipv6_addresses=[],\n    )\n    # FreeBSD, DragonflyBSD, NetBSD, OpenBSD and macOS all implicitly add '-a'\n    # when running the command 'ifconfig'.\n    # Solaris must explicitly run the command 'ifconfig -a'.\n    rc, out, err = self.module.run_command([ifconfig_path, ifconfig_options])\n\n    for line in out.splitlines():\n\n        if line:\n            words = line.split()\n\n            if words[0] == 'pass':\n                continue\n            elif re.match(r'^\\S', line) and len(words) > 3:\n                current_if = self.parse_interface_line(words)\n                interfaces[current_if['device']] = current_if\n            elif words[0].startswith('options='):\n                self.parse_options_line(words, current_if, ips)\n            elif words[0] == 'nd6':\n                self.parse_nd6_line(words, current_if, ips)\n            elif words[0] == 'ether':\n                self.parse_ether_line(words, current_if, ips)\n            elif words[0] == 'media:':\n                self.parse_media_line(words, current_if, ips)\n            elif words[0] == 'status:':\n                self.parse_status_line(words, current_if, ips)\n            elif words[0] == 'lladdr':\n                self.parse_lladdr_line(words, current_if, ips)\n            elif words[0] == 'inet':\n                self.parse_inet_line(words, current_if, ips)\n            elif words[0] == 'inet6':\n                self.parse_inet6_line(words, current_if, ips)\n            elif words[0] == 'tunnel':\n                self.parse_tunnel_line(words, current_if, ips)\n            else:\n                self.parse_unknown_line(words, current_if, ips)\n\n    return interfaces, ips", "loc": 44}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "parse_interface_line", "parameters": ["self", "words"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.get_options"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_interface_line(self, words):\n    device = words[0][0:-1]\n    current_if = {'device': device, 'ipv4': [], 'ipv6': [], 'type': 'unknown'}\n    current_if['flags'] = self.get_options(words[1])\n    if 'LOOPBACK' in current_if['flags']:\n        current_if['type'] = 'loopback'\n    current_if['macaddress'] = 'unknown'    # will be overwritten later\n\n    if len(words) >= 5:  # Newer FreeBSD versions\n        current_if['metric'] = words[3]\n        current_if['mtu'] = words[5]\n    else:\n        current_if['mtu'] = words[3]\n\n    return current_if", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "parse_media_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.get_options"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_media_line(self, words, current_if, ips):\n    # not sure if this is useful - we also drop information\n    current_if['media'] = words[1]\n    if len(words) > 2:\n        current_if['media_select'] = words[2]\n    if len(words) > 3:\n        current_if['media_type'] = words[3][1:]\n    if len(words) > 4:\n        current_if['media_options'] = self.get_options(words[4])", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "parse_inet_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["address['address'].split", "current_if['ipv4'].append", "int", "ips['all_ipv4_addresses'].append", "len", "netmask.startswith", "re.match", "socket.inet_aton", "socket.inet_ntoa", "struct.pack", "struct.unpack", "words.index", "words[1].startswith"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_inet_line(self, words, current_if, ips):\n    # netbsd show aliases like this\n    #  lo0: flags=8049<UP,LOOPBACK,RUNNING,MULTICAST> mtu 33184\n    #         inet 127.0.0.1 netmask 0xff000000\n    #         inet alias 127.1.1.1 netmask 0xff000000\n    if words[1] == 'alias':\n        del words[1]\n\n    address = {'address': words[1]}\n    # cidr style ip address (eg, 127.0.0.1/24) in inet line\n    # used in netbsd ifconfig -e output after 7.1\n    if '/' in address['address']:\n        ip_address, cidr_mask = address['address'].split('/')\n\n        address['address'] = ip_address\n\n        netmask_length = int(cidr_mask)\n        netmask_bin = (1 << 32) - (1 << 32 >> int(netmask_length))\n        address['netmask'] = socket.inet_ntoa(struct.pack('!L', netmask_bin))\n\n        if len(words) > 5:\n            address['broadcast'] = words[3]\n\n    else:\n        # Don't just assume columns, use \"netmask\" as the index for the prior column\n        try:\n            netmask_idx = words.index('netmask') + 1\n        except ValueError:\n            netmask_idx = 3\n\n        # deal with hex netmask\n        if re.match('([0-9a-f]){8}$', words[netmask_idx]):\n            netmask = '0x' + words[netmask_idx]\n        else:\n            netmask = words[netmask_idx]\n\n        if netmask.startswith('0x'):\n            address['netmask'] = socket.inet_ntoa(struct.pack('!L', int(netmask, base=16)))\n        else:\n            # otherwise assume this is a dotted quad\n            address['netmask'] = netmask\n    # calculate the network\n    address_bin = struct.unpack('!L', socket.inet_aton(address['address']))[0]\n    netmask_bin = struct.unpack('!L', socket.inet_aton(address['netmask']))[0]\n    address['network'] = socket.inet_ntoa(struct.pack('!L', address_bin & netmask_bin))\n    if 'broadcast' not in address:\n        # broadcast may be given or we need to calculate\n        try:\n            broadcast_idx = words.index('broadcast') + 1\n        except ValueError:\n            address['broadcast'] = socket.inet_ntoa(struct.pack('!L', address_bin | (~netmask_bin & 0xffffffff)))\n        else:\n            address['broadcast'] = words[broadcast_idx]\n\n    # add to our list of addresses\n    if not words[1].startswith('127.'):\n        ips['all_ipv4_addresses'].append(address['address'])\n    current_if['ipv4'].append(address)", "loc": 58}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "parse_inet6_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["address['address'].split", "current_if['ipv6'].append", "ips['all_ipv6_addresses'].append", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_inet6_line(self, words, current_if, ips):\n    address = {'address': words[1]}\n\n    # using cidr style addresses, ala NetBSD ifconfig post 7.1\n    if '/' in address['address']:\n        ip_address, cidr_mask = address['address'].split('/')\n\n        address['address'] = ip_address\n        address['prefix'] = cidr_mask\n\n        if len(words) > 5:\n            address['scope'] = words[5]\n    else:\n        if (len(words) >= 4) and (words[2] == 'prefixlen'):\n            address['prefix'] = words[3]\n        if (len(words) >= 6) and (words[4] == 'scopeid'):\n            address['scope'] = words[5]\n\n    localhost6 = ['::1', '::1/128', 'fe80::1%lo0']\n    if address['address'] not in localhost6:\n        ips['all_ipv6_addresses'].append(address['address'])\n    current_if['ipv6'].append(address)", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "get_options", "parameters": ["self", "option_string"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["option_csv.split", "option_string.find", "option_string.rfind"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_options(self, option_string):\n    start = option_string.find('<') + 1\n    end = option_string.rfind('>')\n    if (start > 0) and (end > 0) and (end > start + 1):\n        option_csv = option_string[start:end]\n        return option_csv.split(',')\n    else:\n        return []", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\generic_bsd.py", "class_name": "GenericBsdIfconfigNetwork", "function_name": "merge_default_interface", "parameters": ["self", "defaults", "interfaces", "ip_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def merge_default_interface(self, defaults, interfaces, ip_type):\n    if 'interface' not in defaults:\n        return\n    if not defaults['interface'] in interfaces:\n        return\n    ifinfo = interfaces[defaults['interface']]\n    # copy all the interface values across except addresses\n    for item in ifinfo:\n        if item != 'ipv4' and item != 'ipv6':\n            defaults[item] = ifinfo[item]\n\n    ipinfo = []\n    if 'address' in defaults:\n        ipinfo = [x for x in ifinfo[ip_type] if x['address'] == defaults['address']]\n\n    if len(ipinfo) == 0:\n        ipinfo = ifinfo[ip_type]\n\n    if len(ipinfo) > 0:\n        for item in ipinfo[0]:\n            defaults[item] = ipinfo[0][item]", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\hpux.py", "class_name": "HPUXNetwork", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["interfaces.keys", "network_facts.update", "self.get_default_interfaces", "self.get_interfaces_info", "self.module.get_bin_path"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    network_facts = {}\n    netstat_path = self.module.get_bin_path(\n        'netstat',\n        opt_dirs=['/usr/bin']\n    )\n\n    if netstat_path is None:\n        return network_facts\n\n    default_interfaces_facts = self.get_default_interfaces()\n    network_facts.update(default_interfaces_facts)\n\n    interfaces = self.get_interfaces_info()\n    network_facts['interfaces'] = interfaces.keys()\n    for iface in interfaces:\n        network_facts[iface] = interfaces[iface]\n\n    return network_facts", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\hpux.py", "class_name": "HPUXNetwork", "function_name": "get_default_interfaces", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "out.splitlines", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_default_interfaces(self):\n    default_interfaces = {}\n    netstat_path = self.module.get_bin_path(\n        'netstat',\n        opt_dirs=['/usr/bin']\n    )\n\n    if netstat_path is None:\n        return default_interfaces\n    rc, out, err = self.module.run_command(\"%s -nr\" % netstat_path)\n    lines = out.splitlines()\n    for line in lines:\n        words = line.split()\n        if len(words) > 1:\n            if words[0] == 'default':\n                default_interfaces['default_interface'] = words[4]\n                default_interfaces['default_gateway'] = words[1]\n\n    return default_interfaces", "loc": 19}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\hpux.py", "class_name": "HPUXNetwork", "function_name": "get_interfaces_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "line.split", "out.splitlines", "range", "self.module.get_bin_path", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_interfaces_info(self):\n    interfaces = {}\n    netstat_path = self.module.get_bin_path(\n        'netstat',\n        opt_dirs=['/usr/bin']\n    )\n\n    if netstat_path is None:\n        return interfaces\n    rc, out, err = self.module.run_command(\"%s -niw\" % netstat_path)\n    lines = out.splitlines()\n    for line in lines:\n        words = line.split()\n        for i in range(len(words) - 1):\n            if words[i][:3] == 'lan':\n                device = words[i]\n                interfaces[device] = {'device': device}\n                address = words[i + 3]\n                interfaces[device]['ipv4'] = {'address': address}\n                network = words[i + 2]\n                interfaces[device]['ipv4'] = {'network': network,\n                                              'interface': device,\n                                              'address': address}\n    return interfaces", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\hurd.py", "class_name": "HurdPfinetNetwork", "function_name": "assign_network_facts", "parameters": ["self", "network_facts", "fsysopts_path", "socket_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["i.split", "i.startswith", "network_facts['interfaces'].append", "network_facts[current_if]['ipv6'].append", "out.split", "self.module.run_command", "v.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def assign_network_facts(self, network_facts, fsysopts_path, socket_path):\n    rc, out, err = self.module.run_command([fsysopts_path, '-L', socket_path])\n    # FIXME: build up a interfaces datastructure, then assign into network_facts\n    network_facts['interfaces'] = []\n    for i in out.split():\n        if '=' in i and i.startswith('--'):\n            k, v = i.split('=', 1)\n            # remove '--'\n            k = k[2:]\n            if k == 'interface':\n                # remove /dev/ from /dev/eth0\n                v = v[5:]\n                network_facts['interfaces'].append(v)\n                network_facts[v] = {\n                    'active': True,\n                    'device': v,\n                    'ipv4': {},\n                    'ipv6': [],\n                }\n                current_if = v\n            elif k == 'address':\n                network_facts[current_if]['ipv4']['address'] = v\n            elif k == 'netmask':\n                network_facts[current_if]['ipv4']['netmask'] = v\n            elif k == 'address6':\n                address, prefix = v.split('/')\n                network_facts[current_if]['ipv6'].append({\n                    'address': address,\n                    'prefix': prefix,\n                })\n    return network_facts", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\iscsi.py", "class_name": "IscsiInitiatorNetworkCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_file_content", "get_file_content('/etc/iscsi/initiatorname.iscsi', '').splitlines", "line.split", "line.split(':', 1)[1].rstrip", "line.split()[1].rstrip", "line.startswith", "line.strip", "module.get_bin_path", "module.run_command", "self.findstr", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Example of contents of /etc/iscsi/initiatorname.iscsi: ## DO NOT EDIT OR REMOVE THIS FILE! ## If you remove this file, the iSCSI daemon will not start.", "source_code": "def collect(self, module=None, collected_facts=None):\n    \"\"\"\n    Example of contents of /etc/iscsi/initiatorname.iscsi:\n\n    ## DO NOT EDIT OR REMOVE THIS FILE!\n    ## If you remove this file, the iSCSI daemon will not start.\n    ## If you change the InitiatorName, existing access control lists\n    ## may reject this initiator.  The InitiatorName must be unique\n    ## for each iSCSI initiator.  Do NOT duplicate iSCSI InitiatorNames.\n    InitiatorName=iqn.1993-08.org.debian:01:44a42c8ddb8b\n\n    Example of output from the AIX lsattr command:\n\n    # lsattr -E -l iscsi0\n    disc_filename  /etc/iscsi/targets            Configuration file                            False\n    disc_policy    file                          Discovery Policy                              True\n    initiator_name iqn.localhost.hostid.7f000002 iSCSI Initiator Name                          True\n    isns_srvnames  auto                          iSNS Servers IP Addresses                     True\n    isns_srvports                                iSNS Servers Port Numbers                     True\n    max_targets    16                            Maximum Targets Allowed                       True\n    num_cmd_elems  200                           Maximum number of commands to queue to driver True\n\n    Example of output from the HP-UX iscsiutil command:\n\n    #iscsiutil -l\n    Initiator Name             : iqn.1986-03.com.hp:mcel_VMhost3.1f355cf6-e2db-11e0-a999-b44c0aef5537\n    Initiator Alias            :\n\n    Authentication Method      : None\n    CHAP Method                : CHAP_UNI\n    Initiator CHAP Name        :\n    CHAP Secret                :\n    NAS Hostname               :\n    NAS Secret                 :\n    Radius Server Hostname     :\n    Header Digest              : None, CRC32C (default)\n    Data Digest                : None, CRC32C (default)\n    SLP Scope list for iSLPD   :\n    \"\"\"\n\n    iscsi_facts = {}\n    iscsi_facts['iscsi_iqn'] = \"\"\n    if sys.platform.startswith('linux') or sys.platform.startswith('sunos'):\n        for line in get_file_content('/etc/iscsi/initiatorname.iscsi', '').splitlines():\n            if line.startswith('#') or line.startswith(';') or line.strip() == '':\n                continue\n            if line.startswith('InitiatorName='):\n                iscsi_facts['iscsi_iqn'] = line.split('=', 1)[1]\n                break\n    elif sys.platform.startswith('aix'):\n        cmd = module.get_bin_path('lsattr')\n        if cmd is None:\n            return iscsi_facts\n\n        cmd += \" -E -l iscsi0\"\n        rc, out, err = module.run_command(cmd)\n        if rc == 0 and out:\n            line = self.findstr(out, 'initiator_name')\n            iscsi_facts['iscsi_iqn'] = line.split()[1].rstrip()\n\n    elif sys.platform.startswith('hp-ux'):\n        cmd = module.get_bin_path(\n            'iscsiutil',\n            opt_dirs=['/opt/iscsi/bin']\n        )\n        if cmd is None:\n            return iscsi_facts\n\n        cmd += \" -l\"\n        rc, out, err = module.run_command(cmd)\n        if out:\n            line = self.findstr(out, 'Initiator Name')\n            iscsi_facts['iscsi_iqn'] = line.split(\":\", 1)[1].rstrip()\n\n    return iscsi_facts", "loc": 75}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\iscsi.py", "class_name": "IscsiInitiatorNetworkCollector", "function_name": "findstr", "parameters": ["self", "text", "match"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["text.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def findstr(self, text, match):\n    for line in text.splitlines():\n        if match in line:\n            found = line\n    return found", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": "LinuxNetwork", "function_name": "populate", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["interfaces.keys", "self.get_default_interfaces", "self.get_interfaces_info", "self.get_locally_reachable_ips", "self.module.get_bin_path"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def populate(self, collected_facts=None):\n    network_facts = {}\n    ip_path = self.module.get_bin_path('ip')\n    if ip_path is None:\n        return network_facts\n    default_ipv4, default_ipv6 = self.get_default_interfaces(ip_path,\n                                                             collected_facts=collected_facts)\n    interfaces, ips = self.get_interfaces_info(ip_path, default_ipv4, default_ipv6)\n    network_facts['interfaces'] = interfaces.keys()\n    for iface in interfaces:\n        network_facts[iface] = interfaces[iface]\n    network_facts['default_ipv4'] = default_ipv4\n    network_facts['default_ipv6'] = default_ipv6\n    network_facts['all_ipv4_addresses'] = ips['all_ipv4_addresses']\n    network_facts['all_ipv6_addresses'] = ips['all_ipv6_addresses']\n    network_facts['locally_reachable_ips'] = self.get_locally_reachable_ips(ip_path)\n    return network_facts", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": "LinuxNetwork", "function_name": "get_locally_reachable_ips", "parameters": ["self", "ip_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "line.split", "locally_reachable_ips['ipv4'].append", "locally_reachable_ips['ipv6'].append", "output.splitlines", "parse_locally_reachable_ips", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_locally_reachable_ips(self, ip_path):\n    locally_reachable_ips = dict(\n        ipv4=[],\n        ipv6=[],\n    )\n\n    def parse_locally_reachable_ips(output):\n        for line in output.splitlines():\n            if not line:\n                continue\n            words = line.split()\n            if words[0] != 'local':\n                continue\n            address = words[1]\n            if \":\" in address:\n                if address not in locally_reachable_ips['ipv6']:\n                    locally_reachable_ips['ipv6'].append(address)\n            else:\n                if address not in locally_reachable_ips['ipv4']:\n                    locally_reachable_ips['ipv4'].append(address)\n\n    args = [ip_path, '-4', 'route', 'show', 'table', 'local']\n    rc, routes, dummy = self.module.run_command(args)\n    if rc == 0:\n        parse_locally_reachable_ips(routes)\n    args = [ip_path, '-6', 'route', 'show', 'table', 'local']\n    rc, routes, dummy = self.module.run_command(args)\n    if rc == 0:\n        parse_locally_reachable_ips(routes)\n\n    return locally_reachable_ips", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": "LinuxNetwork", "function_name": "get_default_interfaces", "parameters": ["self", "ip_path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collected_facts.get", "collected_facts.get('ansible_distribution_version', '').startswith", "dict", "len", "out.splitlines", "out.splitlines()[0].split", "range", "self.module.run_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_default_interfaces(self, ip_path, collected_facts=None):\n    collected_facts = collected_facts or {}\n    # Use the commands:\n    #     ip -4 route get 8.8.8.8                     -> Google public DNS\n    #     ip -6 route get 2404:6800:400a:800::1012    -> ipv6.google.com\n    # to find out the default outgoing interface, address, and gateway\n    command = dict(\n        v4=[ip_path, '-4', 'route', 'get', '8.8.8.8'],\n        v6=[ip_path, '-6', 'route', 'get', '2404:6800:400a:800::1012']\n    )\n    interface = dict(v4={}, v6={})\n\n    for v in 'v4', 'v6':\n        if (v == 'v6' and collected_facts.get('ansible_os_family') == 'RedHat' and\n                collected_facts.get('ansible_distribution_version', '').startswith('4.')):\n            continue\n        if v == 'v6' and not socket.has_ipv6:\n            continue\n        rc, out, err = self.module.run_command(command[v], errors='surrogate_then_replace')\n        if not out:\n            # v6 routing may result in\n            #   RTNETLINK answers: Invalid argument\n            continue\n        words = out.splitlines()[0].split()\n        # A valid output starts with the queried address on the first line\n        if len(words) > 0 and words[0] == command[v][-1]:\n            for i in range(len(words) - 1):\n                if words[i] == 'dev':\n                    interface[v]['interface'] = words[i + 1]\n                elif words[i] == 'src':\n                    interface[v]['address'] = words[i + 1]\n                elif words[i] == 'via' and words[i + 1] != command[v][-1]:\n                    interface[v]['gateway'] = words[i + 1]\n    return interface['v4'], interface['v6']", "loc": 34}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": "LinuxNetwork", "function_name": "get_ethtool_data", "parameters": ["self", "device"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "key.strip", "key.strip().replace", "line.endswith", "line.split", "m.groups", "m.lower", "re.findall", "re.search", "self.module.get_bin_path", "self.module.run_command", "stdout.strip", "stdout.strip().splitlines", "value.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ethtool_data(self, device):\n\n    data = {}\n    ethtool_path = self.module.get_bin_path(\"ethtool\")\n    # FIXME: exit early on falsey ethtool_path and un-indent\n    if ethtool_path:\n        args = [ethtool_path, '-k', device]\n        rc, stdout, stderr = self.module.run_command(args, errors='surrogate_then_replace')\n        # FIXME: exit early on falsey if we can\n        if rc == 0:\n            features = {}\n            for line in stdout.strip().splitlines():\n                if not line or line.endswith(\":\"):\n                    continue\n                key, value = line.split(\": \")\n                if not value:\n                    continue\n                features[key.strip().replace('-', '_')] = value.strip()\n            data['features'] = features\n\n        args = [ethtool_path, '-T', device]\n        rc, stdout, stderr = self.module.run_command(args, errors='surrogate_then_replace')\n        if rc == 0:\n            data['timestamping'] = [m.lower() for m in re.findall(r'SOF_TIMESTAMPING_(\\w+)', stdout)]\n            data['hw_timestamp_filters'] = [m.lower() for m in re.findall(r'HWTSTAMP_FILTER_(\\w+)', stdout)]\n            m = re.search(r'PTP Hardware Clock: (\\d+)', stdout)\n            if m:\n                data['phc_index'] = int(m.groups()[0])\n\n    return data", "loc": 30}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": null, "function_name": "parse_locally_reachable_ips", "parameters": ["output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.split", "locally_reachable_ips['ipv4'].append", "locally_reachable_ips['ipv6'].append", "output.splitlines"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_locally_reachable_ips(output):\n    for line in output.splitlines():\n        if not line:\n            continue\n        words = line.split()\n        if words[0] != 'local':\n            continue\n        address = words[1]\n        if \":\" in address:\n            if address not in locally_reachable_ips['ipv6']:\n                locally_reachable_ips['ipv6'].append(address)\n        else:\n            if address not in locally_reachable_ips['ipv4']:\n                locally_reachable_ips['ipv4'].append(address)", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\linux.py", "class_name": null, "function_name": "parse_ip_output", "parameters": ["output", "secondary"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["address.startswith", "int", "interfaces[device].get", "interfaces[device]['ipv4_secondaries'].append", "interfaces[device]['ipv6'].append", "interfaces[iface]['ipv4_secondaries'].append", "ips['all_ipv4_addresses'].append", "ips['all_ipv6_addresses'].append", "len", "line.split", "output.splitlines", "socket.inet_aton", "socket.inet_ntoa", "struct.pack", "struct.unpack", "words[1].split", "words[3].split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_ip_output(output, secondary=False):\n    for line in output.splitlines():\n        if not line:\n            continue\n        words = line.split()\n        broadcast = ''\n        if words[0] == 'inet':\n            if '/' in words[1]:\n                address, netmask_length = words[1].split('/')\n                if len(words) > 3:\n                    if words[2] == 'brd':\n                        broadcast = words[3]\n            else:\n                # pointopoint interfaces do not have a prefix\n                address = words[1]\n                netmask_length = \"32\"\n            address_bin = struct.unpack('!L', socket.inet_aton(address))[0]\n            netmask_bin = (1 << 32) - (1 << 32 >> int(netmask_length))\n            netmask = socket.inet_ntoa(struct.pack('!L', netmask_bin))\n            network = socket.inet_ntoa(struct.pack('!L', address_bin & netmask_bin))\n            iface = words[-1]\n            # NOTE: device is ref to outside scope\n            # NOTE: interfaces is also ref to outside scope\n            if iface != device:\n                interfaces[iface] = {}\n            if not secondary and \"ipv4\" not in interfaces[iface]:\n                interfaces[iface]['ipv4'] = {'address': address,\n                                             'broadcast': broadcast,\n                                             'netmask': netmask,\n                                             'network': network,\n                                             'prefix': netmask_length,\n                                             }\n            else:\n                if \"ipv4_secondaries\" not in interfaces[iface]:\n                    interfaces[iface][\"ipv4_secondaries\"] = []\n                interfaces[iface][\"ipv4_secondaries\"].append({\n                    'address': address,\n                    'broadcast': broadcast,\n                    'netmask': netmask,\n                    'network': network,\n                    'prefix': netmask_length,\n                })\n\n            # add this secondary IP to the main device\n            if secondary:\n                if \"ipv4_secondaries\" not in interfaces[device]:\n                    interfaces[device][\"ipv4_secondaries\"] = []\n                if device != iface:\n                    interfaces[device][\"ipv4_secondaries\"].append({\n                        'address': address,\n                        'broadcast': broadcast,\n                        'netmask': netmask,\n                        'network': network,\n                        'prefix': netmask_length,\n                    })\n\n            # NOTE: default_ipv4 is ref to outside scope\n            # If this is the default address, update default_ipv4\n            if 'address' in default_ipv4 and default_ipv4['address'] == address:\n                default_ipv4['broadcast'] = broadcast\n                default_ipv4['netmask'] = netmask\n                default_ipv4['network'] = network\n                default_ipv4['prefix'] = netmask_length\n                # NOTE: macaddress is ref from outside scope\n                default_ipv4['macaddress'] = macaddress\n                default_ipv4['mtu'] = interfaces[device]['mtu']\n                default_ipv4['type'] = interfaces[device].get(\"type\", \"unknown\")\n                default_ipv4['alias'] = words[-1]\n            if not address.startswith('127.'):\n                ips['all_ipv4_addresses'].append(address)\n        elif words[0] == 'inet6':\n            if 'peer' == words[2]:\n                address = words[1]\n                dummy, prefix = words[3].split('/')\n                scope = words[5]\n            else:\n                address, prefix = words[1].split('/')\n                scope = words[3]\n            if 'ipv6' not in interfaces[device]:\n                interfaces[device]['ipv6'] = []\n            interfaces[device]['ipv6'].append({\n                'address': address,\n                'prefix': prefix,\n                'scope': scope\n            })\n            # If this is the default address, update default_ipv6\n            if 'address' in default_ipv6 and default_ipv6['address'] == address:\n                default_ipv6['prefix'] = prefix\n                default_ipv6['scope'] = scope\n                default_ipv6['macaddress'] = macaddress\n                default_ipv6['mtu'] = interfaces[device]['mtu']\n                default_ipv6['type'] = interfaces[device].get(\"type\", \"unknown\")\n            if not address == '::1':\n                ips['all_ipv6_addresses'].append(address)", "loc": 94}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\netbsd.py", "class_name": "NetBSDNetwork", "function_name": "parse_media_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "words[3].split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_media_line(self, words, current_if, ips):\n    # example of line:\n    # $ ifconfig\n    # ne0: flags=8863<UP,BROADCAST,NOTRAILERS,RUNNING,SIMPLEX,MULTICAST> mtu 1500\n    #    ec_capabilities=1<VLAN_MTU>\n    #    ec_enabled=0\n    #    address: 00:20:91:45:00:78\n    #    media: Ethernet 10baseT full-duplex\n    #    inet 192.168.156.29 netmask 0xffffff00 broadcast 192.168.156.255\n    current_if['media'] = words[1]\n    if len(words) > 2:\n        current_if['media_type'] = words[2]\n    if len(words) > 3:\n        current_if['media_options'] = words[3].split(',')", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\nvme.py", "class_name": "NvmeInitiatorNetworkCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_file_content", "get_file_content('/etc/nvme/hostnqn', '').splitlines", "line.startswith", "line.strip", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Currently NVMe is only supported in some Linux distributions. If NVMe is configured on the host then a file will have been created during the NVMe driver installation. This file holds the unique NQN", "source_code": "def collect(self, module=None, collected_facts=None):\n    \"\"\"\n    Currently NVMe is only supported in some Linux distributions.\n    If NVMe is configured on the host then a file will have been created\n    during the NVMe driver installation. This file holds the unique NQN\n    of the host.\n\n    Example of contents of /etc/nvme/hostnqn:\n\n    # cat /etc/nvme/hostnqn\n    nqn.2014-08.org.nvmexpress:fc_lif:uuid:2cd61a74-17f9-4c22-b350-3020020c458d\n\n    \"\"\"\n\n    nvme_facts = {}\n    nvme_facts['hostnqn'] = \"\"\n    if sys.platform.startswith('linux'):\n        for line in get_file_content('/etc/nvme/hostnqn', '').splitlines():\n            if line.startswith('#') or line.startswith(';') or line.strip() == '':\n                continue\n            if line.startswith('nqn.'):\n                nvme_facts['hostnqn'] = line\n                break\n    return nvme_facts", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\sunos.py", "class_name": "SunOSNetwork", "function_name": "get_interfaces_info", "parameters": ["self", "ifconfig_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combined_facts.keys", "combined_facts.update", "dict", "len", "line.split", "out.splitlines", "re.match", "self.module.run_command", "self.parse_ether_line", "self.parse_inet6_line", "self.parse_inet_line", "self.parse_interface_line", "self.parse_lladdr_line", "self.parse_media_line", "self.parse_nd6_line", "self.parse_options_line", "self.parse_status_line", "self.parse_unknown_line", "words[0].startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_interfaces_info(self, ifconfig_path):\n    interfaces = {}\n    current_if = {}\n    ips = dict(\n        all_ipv4_addresses=[],\n        all_ipv6_addresses=[],\n    )\n    rc, out, err = self.module.run_command([ifconfig_path, '-a'])\n\n    for line in out.splitlines():\n\n        if line:\n            words = line.split()\n\n            if re.match(r'^\\S', line) and len(words) > 3:\n                current_if = self.parse_interface_line(words, current_if, interfaces)\n                interfaces[current_if['device']] = current_if\n            elif words[0].startswith('options='):\n                self.parse_options_line(words, current_if, ips)\n            elif words[0] == 'nd6':\n                self.parse_nd6_line(words, current_if, ips)\n            elif words[0] == 'ether':\n                self.parse_ether_line(words, current_if, ips)\n            elif words[0] == 'media:':\n                self.parse_media_line(words, current_if, ips)\n            elif words[0] == 'status:':\n                self.parse_status_line(words, current_if, ips)\n            elif words[0] == 'lladdr':\n                self.parse_lladdr_line(words, current_if, ips)\n            elif words[0] == 'inet':\n                self.parse_inet_line(words, current_if, ips)\n            elif words[0] == 'inet6':\n                self.parse_inet6_line(words, current_if, ips)\n            else:\n                self.parse_unknown_line(words, current_if, ips)\n\n    # 'parse_interface_line' and 'parse_inet*_line' leave two dicts in the\n    # ipv4/ipv6 lists which is ugly and hard to read.\n    # This quick hack merges the dictionaries. Purely cosmetic.\n    for iface in interfaces:\n        for v in 'ipv4', 'ipv6':\n            combined_facts = {}\n            for facts in interfaces[iface][v]:\n                combined_facts.update(facts)\n            if len(combined_facts.keys()) > 0:\n                interfaces[iface][v] = [combined_facts]\n\n    return interfaces, ips", "loc": 48}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\sunos.py", "class_name": "SunOSNetwork", "function_name": "parse_interface_line", "parameters": ["self", "words", "current_if", "interfaces"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["current_if[v].append", "self.get_options"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_interface_line(self, words, current_if, interfaces):\n    device = words[0][0:-1]\n    if device not in interfaces:\n        current_if = {'device': device, 'ipv4': [], 'ipv6': [], 'type': 'unknown'}\n    else:\n        current_if = interfaces[device]\n    flags = self.get_options(words[1])\n    v = 'ipv4'\n    if 'IPv6' in flags:\n        v = 'ipv6'\n    if 'LOOPBACK' in flags:\n        current_if['type'] = 'loopback'\n    current_if[v].append({'flags': flags, 'mtu': words[3]})\n    current_if['macaddress'] = 'unknown'    # will be overwritten later\n    return current_if", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\network\\sunos.py", "class_name": "SunOSNetwork", "function_name": "parse_ether_line", "parameters": ["self", "words", "current_if", "ips"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["words[1].split"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_ether_line(self, words, current_if, ips):\n    macaddress = ''\n    for octet in words[1].split(':'):\n        octet = ('0' + octet)[-2:None]\n        macaddress += (octet + ':')\n    current_if['macaddress'] = macaddress[0:-1]", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\facter.py", "class_name": "FacterFactCollector", "function_name": "find_facter", "parameters": ["self", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.get_bin_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_facter(self, module):\n    facter_path = module.get_bin_path(\n        'facter',\n        opt_dirs=['/opt/puppetlabs/bin']\n    )\n    cfacter_path = module.get_bin_path(\n        'cfacter',\n        opt_dirs=['/opt/puppetlabs/bin']\n    )\n\n    # Prefer to use cfacter if available\n    if cfacter_path is not None:\n        facter_path = cfacter_path\n\n    return facter_path", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\facter.py", "class_name": "FacterFactCollector", "function_name": "run_facter", "parameters": ["self", "module", "facter_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_facter(self, module, facter_path):\n    # if facter is installed, and we can use --json because\n    # ruby-json is ALSO installed, include facter data in the JSON\n    rc, out, err = module.run_command(facter_path + \" --puppet --json\")\n\n    # for some versions of facter, --puppet returns an error if puppet is not present,\n    # try again w/o it, other errors should still appear and be sent back\n    if rc != 0:\n        rc, out, err = module.run_command(facter_path + \" --json\")\n\n    return rc, out, err", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\facter.py", "class_name": "FacterFactCollector", "function_name": "get_facter_output", "parameters": ["self", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.find_facter", "self.run_facter"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_facter_output(self, module):\n    facter_path = self.find_facter(module)\n    if not facter_path:\n        return None\n\n    rc, out, err = self.run_facter(module, facter_path)\n\n    if rc != 0:\n        return None\n\n    return out", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\facter.py", "class_name": "FacterFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.loads", "module.warn", "self.get_facter_output"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    # Note that this mirrors previous facter behavior, where there isn't\n    # a 'ansible_facter' key in the main fact dict, but instead, 'facter_whatever'\n    # items are added to the main dict.\n    facter_dict = {}\n\n    if not module:\n        return facter_dict\n\n    facter_output = self.get_facter_output(module)\n\n    # TODO: if we fail, should we add a empty facter key or nothing?\n    if facter_output is None:\n        return facter_dict\n\n    try:\n        facter_dict = json.loads(facter_output)\n    except Exception:\n        module.warn(\"Failed to parse facter facts\")\n\n    return facter_dict", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\ohai.py", "class_name": "OhaiFactCollector", "function_name": "get_ohai_output", "parameters": ["self", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.find_ohai", "self.run_ohai"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ohai_output(self, module):\n    ohai_path = self.find_ohai(module)\n    if not ohai_path:\n        return None\n\n    rc, out, err = self.run_ohai(module, ohai_path)\n    if rc != 0:\n        return None\n\n    return out", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\other\\ohai.py", "class_name": "OhaiFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.loads", "module.warn", "self.get_ohai_output"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    ohai_facts = {}\n    if not module:\n        return ohai_facts\n\n    ohai_output = self.get_ohai_output(module)\n\n    if ohai_output is None:\n        return ohai_facts\n\n    try:\n        ohai_facts = json.loads(ohai_output)\n    except Exception:\n        module.warn(\"Failed to gather ohai facts\")\n\n    return ohai_facts", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\caps.py", "class_name": "SystemCapabilitiesFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["i.strip", "len", "line.split", "line.split(':')[1].strip", "line.split('=')[1].split", "line.startswith", "module.error_as_warning", "module.get_bin_path", "module.run_command", "out.splitlines"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n\n    rc = -1\n    facts_dict = {'system_capabilities_enforced': 'N/A',\n                  'system_capabilities': 'N/A'}\n    if module:\n        capsh_path = module.get_bin_path('capsh')\n        if capsh_path:\n            # NOTE: -> get_caps_data()/parse_caps_data() for easier mocking -akl\n            try:\n                rc, out, err = module.run_command([capsh_path, \"--print\"], errors='surrogate_then_replace', handle_exceptions=False)\n            except OSError as ex:\n                module.error_as_warning('Could not query system capabilities.', exception=ex)\n\n        if rc == 0:\n            enforced_caps = []\n            enforced = 'NA'\n            for line in out.splitlines():\n                if len(line) < 1:\n                    continue\n                if line.startswith('Current:'):\n                    if line.split(':')[1].strip() == '=ep':\n                        enforced = 'False'\n                    else:\n                        enforced = 'True'\n                        enforced_caps = [i.strip() for i in line.split('=')[1].split(',')]\n\n            facts_dict['system_capabilities_enforced'] = enforced\n            facts_dict['system_capabilities'] = enforced_caps\n\n    return facts_dict", "loc": 31}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\chroot.py", "class_name": null, "function_name": "is_chroot", "parameters": ["module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["module.get_bin_path", "module.run_command", "os.environ.get", "os.stat"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_chroot(module=None):\n\n    is_chroot = None\n\n    if os.environ.get('debian_chroot', False):\n        is_chroot = True\n    else:\n        my_root = os.stat('/')\n        try:\n            # check if my file system is the root one\n            proc_root = os.stat('/proc/1/root/.')\n            is_chroot = my_root.st_ino != proc_root.st_ino or my_root.st_dev != proc_root.st_dev\n        except Exception:\n            # I'm not root or no proc, fallback to checking it is inode #2\n            fs_root_ino = 2\n\n            if module is not None:\n                stat_path = module.get_bin_path('stat')\n                if stat_path:\n                    cmd = [stat_path, '-f', '--format=%T', '/']\n                    rc, out, err = module.run_command(cmd)\n                    if 'btrfs' in out:\n                        fs_root_ino = 256\n                    elif 'xfs' in out:\n                        fs_root_ino = 128\n\n            is_chroot = (my_root.st_ino != fs_root_ino)\n\n    return is_chroot", "loc": 29}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\cmdline.py", "class_name": "CmdLineFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_proc_cmdline", "self._parse_proc_cmdline", "self._parse_proc_cmdline_facts"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    cmdline_facts = {}\n\n    data = self._get_proc_cmdline()\n\n    if not data:\n        return cmdline_facts\n\n    cmdline_facts['cmdline'] = self._parse_proc_cmdline(data)\n    cmdline_facts['proc_cmdline'] = self._parse_proc_cmdline_facts(data)\n\n    return cmdline_facts", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\date_time.py", "class_name": "DateTimeFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.datetime.fromtimestamp", "int", "now.strftime", "str", "time.strftime", "time.time", "utcnow.strftime"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    facts_dict = {}\n    date_time_facts = {}\n\n    # Store the timestamp once, then get local and UTC versions from that\n    epoch_ts = time.time()\n    now = datetime.datetime.fromtimestamp(epoch_ts)\n    utcnow = datetime.datetime.fromtimestamp(\n        epoch_ts,\n        tz=datetime.timezone.utc,\n    )\n\n    date_time_facts['year'] = now.strftime('%Y')\n    date_time_facts['month'] = now.strftime('%m')\n    date_time_facts['weekday'] = now.strftime('%A')\n    date_time_facts['weekday_number'] = now.strftime('%w')\n    date_time_facts['weeknumber'] = now.strftime('%W')\n    date_time_facts['day'] = now.strftime('%d')\n    date_time_facts['hour'] = now.strftime('%H')\n    date_time_facts['minute'] = now.strftime('%M')\n    date_time_facts['second'] = now.strftime('%S')\n    date_time_facts['epoch'] = now.strftime('%s')\n    # epoch returns float or string in some non-linux environments\n    if date_time_facts['epoch'] == '' or date_time_facts['epoch'][0] == '%':\n        date_time_facts['epoch'] = str(int(epoch_ts))\n    # epoch_int always returns integer format of epoch\n    date_time_facts['epoch_int'] = str(int(now.strftime('%s')))\n    if date_time_facts['epoch_int'] == '' or date_time_facts['epoch_int'][0] == '%':\n        date_time_facts['epoch_int'] = str(int(epoch_ts))\n    date_time_facts['date'] = now.strftime('%Y-%m-%d')\n    date_time_facts['time'] = now.strftime('%H:%M:%S')\n    date_time_facts['iso8601_micro'] = utcnow.strftime(\"%Y-%m-%dT%H:%M:%S.%fZ\")\n    date_time_facts['iso8601'] = utcnow.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n    date_time_facts['iso8601_basic'] = now.strftime(\"%Y%m%dT%H%M%S%f\")\n    date_time_facts['iso8601_basic_short'] = now.strftime(\"%Y%m%dT%H%M%S\")\n    date_time_facts['tz'] = time.strftime(\"%Z\")\n    date_time_facts['tz_dst'] = time.tzname[1]\n    date_time_facts['tz_offset'] = time.strftime(\"%z\")\n\n    facts_dict['date_time'] = date_time_facts\n    return facts_dict", "loc": 41}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": null, "function_name": "get_uname", "parameters": ["module", "flags"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.extend", "flags.split", "isinstance", "module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_uname(module, flags=('-v')):\n    if isinstance(flags, str):\n        flags = flags.split()\n    command = ['uname']\n    command.extend(flags)\n    rc, out, err = module.run_command(command)\n    if rc == 0:\n        return out\n    return None", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "process_dist_files", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ddict.get", "dist_file_facts.update", "self._get_dist_file_content", "self._guess_distribution", "self._parse_dist_file"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_dist_files(self):\n    # Try to handle the exceptions now ...\n    # self.facts['distribution_debug'] = []\n    dist_file_facts = {}\n\n    dist_guess = self._guess_distribution()\n    dist_file_facts.update(dist_guess)\n\n    for ddict in self.OSDIST_LIST:\n        name = ddict['name']\n        path = ddict['path']\n        allow_empty = ddict.get('allowempty', False)\n\n        has_dist_file, dist_file_content = self._get_dist_file_content(path, allow_empty=allow_empty)\n\n        # but we allow_empty. For example, ArchLinux with an empty /etc/arch-release and a\n        # /etc/os-release with a different name\n        if has_dist_file and allow_empty:\n            dist_file_facts['distribution'] = name\n            dist_file_facts['distribution_file_path'] = path\n            dist_file_facts['distribution_file_variety'] = name\n            break\n\n        if not has_dist_file:\n            # keep looking\n            continue\n\n        parsed_dist_file, parsed_dist_file_facts = self._parse_dist_file(name, dist_file_content, path, dist_file_facts)\n\n        # finally found the right os dist file and were able to parse it\n        if parsed_dist_file:\n            dist_file_facts['distribution'] = name\n            dist_file_facts['distribution_file_path'] = path\n            # distribution and file_variety are the same here, but distribution\n            # will be changed/mapped to a more specific name.\n            # ie, dist=Fedora, file_variety=RedHat\n            dist_file_facts['distribution_file_variety'] = name\n            dist_file_facts['distribution_file_parsed'] = parsed_dist_file\n            dist_file_facts.update(parsed_dist_file_facts)\n            break\n\n    return dist_file_facts", "loc": 42}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Slackware", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Slackware(self, name, data, path, collected_facts):\n    slackware_facts = {}\n    if 'Slackware' not in data:\n        return False, slackware_facts  # TODO: remove\n    slackware_facts['distribution'] = name\n    version = re.findall(r'\\w+[.]\\w+\\+?', data)\n    if version:\n        slackware_facts['distribution_version'] = version[0]\n    return True, slackware_facts", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Amazon", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.split", "distribution_version.split", "len", "n.isdigit", "re.search", "version.group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Amazon(self, name, data, path, collected_facts):\n    amazon_facts = {}\n    if 'Amazon' not in data:\n        return False, amazon_facts\n    amazon_facts['distribution'] = 'Amazon'\n    if path == '/etc/os-release':\n        version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n        if version:\n            distribution_version = version.group(1)\n            amazon_facts['distribution_version'] = distribution_version\n            version_data = distribution_version.split(\".\")\n            if len(version_data) > 1:\n                major, minor = version_data\n            else:\n                major, minor = version_data[0], 'NA'\n\n            amazon_facts['distribution_major_version'] = major\n            amazon_facts['distribution_minor_version'] = minor\n    else:\n        version = [n for n in data.split() if n.isdigit()]\n        version = version[0] if version else 'NA'\n        amazon_facts['distribution_version'] = version\n\n    return True, amazon_facts", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_OpenWrt", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "release.groups", "version.groups"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_OpenWrt(self, name, data, path, collected_facts):\n    openwrt_facts = {}\n    if 'OpenWrt' not in data:\n        return False, openwrt_facts  # TODO: remove\n    openwrt_facts['distribution'] = name\n    version = re.search('DISTRIB_RELEASE=\"(.*)\"', data)\n    if version:\n        openwrt_facts['distribution_version'] = version.groups()[0]\n    release = re.search('DISTRIB_CODENAME=\"(.*)\"', data)\n    if release:\n        openwrt_facts['distribution_release'] = release.groups()[0]\n    return True, openwrt_facts", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Debian", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "get_file_lines", "line.strip", "m.groups", "out.strip", "re.search", "release.groups", "self.module.get_bin_path", "self.module.run_command", "version.group", "version.group(1).split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Debian(self, name, data, path, collected_facts):\n    debian_facts = {}\n    if any(distro in data for distro in ('Debian', 'Raspbian')):\n        debian_facts['distribution'] = 'Debian'\n        release = re.search(r\"PRETTY_NAME=[^(]+ \\(?([^)]+?)\\)\", data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n\n        # Last resort: try to find release from tzdata as either lsb is missing or this is very old debian\n        if collected_facts['distribution_release'] == 'NA' and 'Debian' in data:\n            dpkg_cmd = self.module.get_bin_path('dpkg')\n            if dpkg_cmd:\n                cmd = \"%s --status tzdata|grep Provides|cut -f2 -d'-'\" % dpkg_cmd\n                rc, out, err = self.module.run_command(cmd)\n                if rc == 0:\n                    debian_facts['distribution_release'] = out.strip()\n        debian_version_path = '/etc/debian_version'\n        distdata = get_file_lines(debian_version_path)\n        for line in distdata:\n            m = re.search(r'(\\d+)\\.(\\d+)', line.strip())\n            if m:\n                debian_facts['distribution_minor_version'] = m.groups()[1]\n    elif 'Ubuntu' in data:\n        debian_facts['distribution'] = 'Ubuntu'\n        # nothing else to do, Ubuntu gets correct info from python functions\n    elif 'SteamOS' in data:\n        debian_facts['distribution'] = 'SteamOS'\n        # nothing else to do, SteamOS gets correct info from python functions\n    elif path in ('/etc/lsb-release', '/etc/os-release') and ('Kali' in data or 'Parrot' in data):\n        if 'Kali' in data:\n            # Kali does not provide /etc/lsb-release anymore\n            debian_facts['distribution'] = 'Kali'\n        elif 'Parrot' in data:\n            debian_facts['distribution'] = 'Parrot'\n        release = re.search('DISTRIB_RELEASE=(.*)', data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n    elif 'Devuan' in data:\n        debian_facts['distribution'] = 'Devuan'\n        release = re.search(r\"PRETTY_NAME=\\\"?[^(\\\"]+ \\(?([^) \\\"]+)\\)?\", data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n        version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n        if version:\n            debian_facts['distribution_version'] = version.group(1)\n            debian_facts['distribution_major_version'] = version.group(1)\n    elif 'Cumulus' in data:\n        debian_facts['distribution'] = 'Cumulus Linux'\n        version = re.search(r\"VERSION_ID=(.*)\", data)\n        if version:\n            major, _minor, _dummy_ver = version.group(1).split(\".\")\n            debian_facts['distribution_version'] = version.group(1)\n            debian_facts['distribution_major_version'] = major\n\n        release = re.search(r'VERSION=\"(.*)\"', data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n    elif \"Mint\" in data:\n        debian_facts['distribution'] = 'Linux Mint'\n        version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n        if version:\n            debian_facts['distribution_version'] = version.group(1)\n            debian_facts['distribution_major_version'] = version.group(1).split('.')[0]\n    elif 'UOS' in data or 'Uos' in data or 'uos' in data:\n        debian_facts['distribution'] = 'Uos'\n        release = re.search(r\"VERSION_CODENAME=\\\"?([^\\\"]+)\\\"?\", data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n        version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n        if version:\n            debian_facts['distribution_version'] = version.group(1)\n            debian_facts['distribution_major_version'] = version.group(1).split('.')[0]\n    elif 'Deepin' in data or 'deepin' in data:\n        debian_facts['distribution'] = 'Deepin'\n        release = re.search(r\"VERSION_CODENAME=\\\"?([^\\\"]+)\\\"?\", data)\n        if release:\n            debian_facts['distribution_release'] = release.groups()[0]\n        version = re.search(r\"VERSION_ID=\\\"(.*)\\\"\", data)\n        if version:\n            debian_facts['distribution_version'] = version.group(1)\n            debian_facts['distribution_major_version'] = version.group(1).split('.')[0]\n    elif 'LMDE' in data:\n        debian_facts['distribution'] = 'Linux Mint Debian Edition'\n    else:\n        return False, debian_facts\n\n    return True, debian_facts", "loc": 87}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Mandriva", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "release.groups", "version.groups"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Mandriva(self, name, data, path, collected_facts):\n    mandriva_facts = {}\n    if 'Mandriva' in data:\n        mandriva_facts['distribution'] = 'Mandriva'\n        version = re.search('DISTRIB_RELEASE=\"(.*)\"', data)\n        if version:\n            mandriva_facts['distribution_version'] = version.groups()[0]\n        release = re.search('DISTRIB_CODENAME=\"(.*)\"', data)\n        if release:\n            mandriva_facts['distribution_release'] = release.groups()[0]\n        mandriva_facts['distribution'] = name\n    else:\n        return False, mandriva_facts\n\n    return True, mandriva_facts", "loc": 15}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_NA", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.splitlines", "distribution.group", "distribution.group(1).strip", "re.search", "version.group", "version.group(1).strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_NA(self, name, data, path, collected_facts):\n    na_facts = {}\n    for line in data.splitlines():\n        distribution = re.search(\"^NAME=(.*)\", line)\n        if distribution and name == 'NA':\n            na_facts['distribution'] = distribution.group(1).strip('\"')\n        version = re.search(\"^VERSION=(.*)\", line)\n        if version and collected_facts['distribution_version'] == 'NA':\n            na_facts['distribution_version'] = version.group(1).strip('\"')\n    return True, na_facts", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Coreos", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["distro.lower", "get_distribution", "re.search", "release.group", "release.group(1).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Coreos(self, name, data, path, collected_facts):\n    coreos_facts = {}\n    # FIXME: pass in ro copy of facts for this kind of thing\n    distro = get_distribution()\n\n    if distro.lower() == 'coreos':\n        if not data:\n            # include fix from #15230, #15228\n            # TODO: verify this is ok for above bugs\n            return False, coreos_facts\n        release = re.search(\"^GROUP=(.*)\", data)\n        if release:\n            coreos_facts['distribution_release'] = release.group(1).strip('\"')\n    else:\n        return False, coreos_facts  # TODO: remove if tested without this\n\n    return True, coreos_facts", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_Flatcar", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["distro.lower", "get_distribution", "re.search", "version.group", "version.group(1).strip", "version.group(1).strip('\"').split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_Flatcar(self, name, data, path, collected_facts):\n    flatcar_facts = {}\n    distro = get_distribution()\n\n    if distro.lower() != 'flatcar':\n        return False, flatcar_facts\n\n    if not data:\n        return False, flatcar_facts\n\n    version = re.search(\"VERSION=(.*)\", data)\n    if version:\n        flatcar_facts['distribution_major_version'] = version.group(1).strip('\"').split('.')[0]\n        flatcar_facts['distribution_version'] = version.group(1).strip('\"')\n\n    return True, flatcar_facts", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_ClearLinux", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["name.lower", "pname.groups", "re.search", "release.groups", "version.groups"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_ClearLinux(self, name, data, path, collected_facts):\n    clear_facts = {}\n    if \"clearlinux\" not in name.lower():\n        return False, clear_facts\n\n    pname = re.search('NAME=\"(.*)\"', data)\n    if pname:\n        if 'Clear Linux' not in pname.groups()[0]:\n            return False, clear_facts\n        clear_facts['distribution'] = pname.groups()[0]\n    version = re.search('VERSION_ID=(.*)', data)\n    if version:\n        clear_facts['distribution_major_version'] = version.groups()[0]\n        clear_facts['distribution_version'] = version.groups()[0]\n    release = re.search('ID=(.*)', data)\n    if release:\n        clear_facts['distribution_release'] = release.groups()[0]\n    return True, clear_facts", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFiles", "function_name": "parse_distribution_file_CentOS", "parameters": ["self", "name", "data", "path", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_distribution_file_CentOS(self, name, data, path, collected_facts):\n    centos_facts = {}\n\n    if 'CentOS Stream' in data:\n        centos_facts['distribution_release'] = 'Stream'\n        return True, centos_facts\n\n    if \"TencentOS Server\" in data:\n        centos_facts['distribution'] = 'TencentOS'\n        return True, centos_facts\n\n    return False, centos_facts", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DistributionFiles", "distfunc", "distribution_facts.update", "distribution_files.process_dist_files", "getattr", "platform.release", "platform.system", "platform.version", "self.OS_FAMILY.get", "system.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_facts(self):\n    distribution_facts = {}\n\n    # The platform module provides information about the running\n    # system/distribution. Use this as a baseline and fix buggy systems\n    # afterwards\n    system = platform.system()\n    distribution_facts['distribution'] = system\n    distribution_facts['distribution_release'] = platform.release()\n    distribution_facts['distribution_version'] = platform.version()\n\n    systems_implemented = ('AIX', 'HP-UX', 'Darwin', 'FreeBSD', 'OpenBSD', 'SunOS', 'DragonFly', 'NetBSD')\n\n    if system in systems_implemented:\n        cleanedname = system.replace('-', '')\n        distfunc = getattr(self, 'get_distribution_' + cleanedname)\n        dist_func_facts = distfunc()\n        distribution_facts.update(dist_func_facts)\n    elif system == 'Linux':\n\n        distribution_files = DistributionFiles(module=self.module)\n\n        # linux_distribution_facts = LinuxDistribution(module).get_distribution_facts()\n        dist_file_facts = distribution_files.process_dist_files()\n\n        distribution_facts.update(dist_file_facts)\n\n    distro = distribution_facts['distribution']\n    # look for an os family alias for the 'distribution', if there isn't one, use 'distribution'\n    distribution_facts['os_family'] = self.OS_FAMILY.get(distro, None) or distro\n\n    return distribution_facts", "loc": 32}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_AIX", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "out.split", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_AIX(self):\n    aix_facts = {}\n    rc, out, err = self.module.run_command(\"/usr/bin/oslevel\")\n    data = out.split('.')\n    aix_facts['distribution_major_version'] = data[0]\n    if len(data) > 1:\n        aix_facts['distribution_version'] = '%s.%s' % (data[0], data[1])\n        aix_facts['distribution_release'] = data[1]\n    else:\n        aix_facts['distribution_version'] = data[0]\n    return aix_facts", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_HPUX", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.groups", "re.search", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_HPUX(self):\n    hpux_facts = {}\n    rc, out, err = self.module.run_command(r\"/usr/sbin/swlist |egrep 'HPUX.*OE.*[AB].[0-9]+\\.[0-9]+'\", use_unsafe_shell=True)\n    data = re.search(r'HPUX.*OE.*([AB].[0-9]+\\.[0-9]+)\\.([0-9]+).*', out)\n    if data:\n        hpux_facts['distribution_version'] = data.groups()[0]\n        hpux_facts['distribution_release'] = data.groups()[1]\n    return hpux_facts", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_Darwin", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.split", "out.split", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_Darwin(self):\n    darwin_facts = {}\n    darwin_facts['distribution'] = 'MacOSX'\n    rc, out, err = self.module.run_command(\"/usr/bin/sw_vers -productVersion\")\n    data = out.split()[-1]\n    if data:\n        darwin_facts['distribution_major_version'] = data.split('.')[0]\n        darwin_facts['distribution_version'] = data\n    return darwin_facts", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_FreeBSD", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.group", "platform.release", "platform.version", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_FreeBSD(self):\n    freebsd_facts = {}\n    freebsd_facts['distribution_release'] = platform.release()\n    data = re.search(r'(\\d+)\\.(\\d+)-(RELEASE|STABLE|CURRENT|RC|PRERELEASE).*', freebsd_facts['distribution_release'])\n    if 'trueos' in platform.version():\n        freebsd_facts['distribution'] = 'TrueOS'\n    if data:\n        freebsd_facts['distribution_major_version'] = data.group(1)\n        freebsd_facts['distribution_version'] = '%s.%s' % (data.group(1), data.group(2))\n    return freebsd_facts", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_OpenBSD", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match.groups", "platform.release", "re.match", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_OpenBSD(self):\n    openbsd_facts = {}\n    openbsd_facts['distribution_version'] = platform.release()\n    rc, out, err = self.module.run_command(\"/sbin/sysctl -n kern.version\")\n    match = re.match(r'OpenBSD\\s[0-9]+.[0-9]+-(\\S+)\\s.*', out)\n    if match:\n        openbsd_facts['distribution_release'] = match.groups()[0]\n    else:\n        openbsd_facts['distribution_release'] = 'release'\n    return openbsd_facts", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_DragonFly", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match.group", "match.groups", "platform.release", "re.search", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_DragonFly(self):\n    dragonfly_facts = {\n        'distribution_release': platform.release()\n    }\n    rc, out, dummy = self.module.run_command(\"/sbin/sysctl -n kern.version\")\n    match = re.search(r'v(\\d+)\\.(\\d+)\\.(\\d+)-(RELEASE|STABLE|CURRENT).*', out)\n    if match:\n        dragonfly_facts['distribution_major_version'] = match.group(1)\n        dragonfly_facts['distribution_version'] = '%s.%s.%s' % match.groups()[:3]\n    return dragonfly_facts", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_NetBSD", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match.group", "match.groups", "platform.release", "platform_release.split", "re.match", "self.module.run_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_NetBSD(self):\n    netbsd_facts = {}\n    platform_release = platform.release()\n    netbsd_facts['distribution_release'] = platform_release\n    rc, out, dummy = self.module.run_command(\"/sbin/sysctl -n kern.version\")\n    match = re.match(r'NetBSD\\s(\\d+)\\.(\\d+)\\s\\((GENERIC)\\).*', out)\n    if match:\n        netbsd_facts['distribution_major_version'] = match.group(1)\n        netbsd_facts['distribution_version'] = '%s.%s' % match.groups()[:2]\n    else:\n        netbsd_facts['distribution_major_version'] = platform_release.split('.')[0]\n        netbsd_facts['distribution_version'] = platform_release\n    return netbsd_facts", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "Distribution", "function_name": "get_distribution_SunOS", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_file_exists", "data.replace", "data.split", "data.split()[-1].lstrip", "data.strip", "dict", "get_file_content", "get_file_content('/etc/product').splitlines", "get_file_content('/etc/release').splitlines", "get_uname", "l.split", "product_data.get", "product_data.get('Image').split", "sunos_facts.get", "uname_r.split", "uname_r.split('.')[1].rstrip", "uname_v.splitlines", "uname_v.splitlines()[0].strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution_SunOS(self):\n    sunos_facts = {}\n\n    data = get_file_content('/etc/release').splitlines()[0]\n\n    if 'Solaris' in data:\n        # for solaris 10 uname_r will contain 5.10, for solaris 11 it will have 5.11\n        uname_r = get_uname(self.module, flags=['-r'])\n        ora_prefix = ''\n        if 'Oracle Solaris' in data:\n            data = data.replace('Oracle ', '')\n            ora_prefix = 'Oracle '\n        sunos_facts['distribution'] = data.split()[0]\n        sunos_facts['distribution_version'] = data.split()[1]\n        sunos_facts['distribution_release'] = ora_prefix + data\n        sunos_facts['distribution_major_version'] = uname_r.split('.')[1].rstrip()\n        return sunos_facts\n\n    uname_v = get_uname(self.module, flags=['-v'])\n    distribution_version = None\n\n    if 'SmartOS' in data:\n        sunos_facts['distribution'] = 'SmartOS'\n        if _file_exists('/etc/product'):\n            product_data = dict([l.split(': ', 1) for l in get_file_content('/etc/product').splitlines() if ': ' in l])\n            if 'Image' in product_data:\n                distribution_version = product_data.get('Image').split()[-1]\n    elif 'OpenIndiana' in data:\n        sunos_facts['distribution'] = 'OpenIndiana'\n    elif 'OmniOS' in data:\n        sunos_facts['distribution'] = 'OmniOS'\n        distribution_version = data.split()[-1]\n    elif uname_v is not None and 'NexentaOS_' in uname_v:\n        sunos_facts['distribution'] = 'Nexenta'\n        distribution_version = data.split()[-1].lstrip('v')\n\n    if sunos_facts.get('distribution', '') in ('SmartOS', 'OpenIndiana', 'OmniOS', 'Nexenta'):\n        sunos_facts['distribution_release'] = data.strip()\n        if distribution_version is not None:\n            sunos_facts['distribution_version'] = distribution_version\n        elif uname_v is not None:\n            sunos_facts['distribution_version'] = uname_v.splitlines()[0].strip()\n        return sunos_facts\n\n    return sunos_facts", "loc": 45}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\distribution.py", "class_name": "DistributionFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Distribution", "distribution.get_distribution_facts"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    collected_facts = collected_facts or {}\n    facts_dict = {}\n    if not module:\n        return facts_dict\n\n    distribution = Distribution(module=module)\n    distro_facts = distribution.get_distribution_facts()\n\n    return distro_facts", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\dns.py", "class_name": "DnsFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dns_facts['dns']['nameservers'].append", "dns_facts['dns']['search'].append", "dns_facts['dns']['sortlist'].append", "get_file_content", "get_file_content('/etc/resolv.conf', '').splitlines", "len", "line.split", "line.startswith", "line.strip", "option.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    dns_facts = {}\n\n    # TODO: flatten\n    dns_facts['dns'] = {}\n\n    for line in get_file_content('/etc/resolv.conf', '').splitlines():\n        if line.startswith('#') or line.startswith(';') or line.strip() == '':\n            continue\n        tokens = line.split()\n        if len(tokens) == 0:\n            continue\n        if tokens[0] == 'nameserver':\n            if 'nameservers' not in dns_facts['dns']:\n                dns_facts['dns']['nameservers'] = []\n            for nameserver in tokens[1:]:\n                dns_facts['dns']['nameservers'].append(nameserver)\n        elif tokens[0] == 'domain':\n            if len(tokens) > 1:\n                dns_facts['dns']['domain'] = tokens[1]\n        elif tokens[0] == 'search':\n            dns_facts['dns']['search'] = []\n            for suffix in tokens[1:]:\n                dns_facts['dns']['search'].append(suffix)\n        elif tokens[0] == 'sortlist':\n            dns_facts['dns']['sortlist'] = []\n            for address in tokens[1:]:\n                dns_facts['dns']['sortlist'].append(address)\n        elif tokens[0] == 'options':\n            dns_facts['dns']['options'] = {}\n            if len(tokens) > 1:\n                for option in tokens[1:]:\n                    option_tokens = option.split(':', 1)\n                    if len(option_tokens) == 0:\n                        continue\n                    val = len(option_tokens) == 2 and option_tokens[1] or True\n                    dns_facts['dns']['options'][option_tokens[0]] = val\n\n    return dns_facts", "loc": 39}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\env.py", "class_name": "EnvFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.environ.items"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    env_facts = {}\n    env_facts['env'] = {}\n\n    for k, v in os.environ.items():\n        env_facts['env'][k] = v\n\n    return env_facts", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\fips.py", "class_name": "FipsFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_file_content"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    # NOTE: this is populated even if it is not set\n    fips_facts = {\n        'fips': False\n    }\n    if get_file_content('/proc/sys/crypto/fips_enabled') == '1':\n        fips_facts['fips'] = True\n    return fips_facts", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\loadavg.py", "class_name": "LoadAvgFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.getloadavg"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    facts = {}\n    try:\n        # (0.58, 0.82, 0.98)\n        loadavg = os.getloadavg()\n        facts['loadavg'] = {\n            '1m': loadavg[0],\n            '5m': loadavg[1],\n            '15m': loadavg[2]\n        }\n    except OSError:\n        pass\n\n    return facts", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\local.py", "class_name": "LocalFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["StringIO", "configparser.ConfigParser", "cp.get", "cp.options", "cp.read_file", "cp.sections", "get_file_content", "glob.glob", "json.loads", "module.params.get", "module.run_command", "module.warn", "os.path.basename", "os.path.basename(fn).replace", "os.path.exists", "os.stat", "sorted", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    local_facts = {}\n    local_facts['local'] = {}\n\n    if not module:\n        return local_facts\n\n    fact_path = module.params.get('fact_path', None)\n\n    if not fact_path or not os.path.exists(fact_path):\n        return local_facts\n\n    local = {}\n    # go over .fact files, run executables, read rest, skip bad with warning and note\n    for fn in sorted(glob.glob(fact_path + '/*.fact')):\n        # use filename for key where it will sit under local facts\n        fact_base = os.path.basename(fn).replace('.fact', '')\n        failed = None\n        try:\n            executable_fact = stat.S_IXUSR & os.stat(fn)[stat.ST_MODE]\n        except OSError as e:\n            failed = 'Could not stat fact (%s): %s' % (fn, to_text(e))\n            local[fact_base] = failed\n            module.warn(failed)\n            continue\n        if executable_fact:\n            try:\n                # run it\n                rc, out, err = module.run_command(fn)\n                if rc != 0:\n                    failed = 'Failure executing fact script (%s), rc: %s, err: %s' % (fn, rc, err)\n            except OSError as e:\n                failed = 'Could not execute fact script (%s): %s' % (fn, to_text(e))\n\n            if failed is not None:\n                local[fact_base] = failed\n                module.warn(failed)\n                continue\n        else:\n            # ignores exceptions and returns empty\n            out = get_file_content(fn, default='')\n\n        try:\n            # ensure we have unicode\n            out = to_text(out, errors='surrogate_or_strict')\n        except UnicodeError:\n            fact = 'error loading fact - output of running \"%s\" was not utf-8' % fn\n            local[fact_base] = fact\n            module.warn(fact)\n            continue\n\n        # try to read it as json first\n        try:\n            fact = json.loads(out)\n        except ValueError:\n            # if that fails read it with ConfigParser\n            cp = configparser.ConfigParser()\n            try:\n                cp.read_file(StringIO(out))\n            except configparser.Error:\n                fact = f\"error loading facts as JSON or ini - please check content: {fn}\"\n                module.warn(fact)\n            else:\n                fact = {}\n                for sect in cp.sections():\n                    if sect not in fact:\n                        fact[sect] = {}\n                    for opt in cp.options(sect):\n                        try:\n                            val = cp.get(sect, opt)\n                        except configparser.Error as ex:\n                            fact = f\"error loading facts as ini - please check content: {fn} ({ex})\"\n                            module.warn(fact)\n                            continue\n                        else:\n                            fact[sect][opt] = val\n        except Exception as e:\n            fact = \"Failed to convert (%s) to JSON: %s\" % (fn, to_text(e))\n            module.warn(fact)\n\n        local[fact_base] = fact\n\n    local_facts['local'] = local\n    return local_facts", "loc": 84}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\lsb.py", "class_name": "LSBFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lsb_facts.items", "lsb_facts['release'].split", "module.get_bin_path", "self._lsb_release_bin", "self._lsb_release_file", "v.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    facts_dict = {}\n    lsb_facts = {}\n\n    if not module:\n        return facts_dict\n\n    lsb_path = module.get_bin_path('lsb_release')\n\n    # try the 'lsb_release' script first\n    if lsb_path:\n        lsb_facts = self._lsb_release_bin(lsb_path,\n                                          module=module)\n\n    # no lsb_release, try looking in /etc/lsb-release\n    if not lsb_facts:\n        lsb_facts = self._lsb_release_file('/etc/lsb-release')\n\n    if lsb_facts and 'release' in lsb_facts:\n        lsb_facts['major_release'] = lsb_facts['release'].split('.')[0]\n\n    for k, v in lsb_facts.items():\n        if v:\n            lsb_facts[k] = v.strip(LSBFactCollector.STRIP_QUOTES)\n\n    facts_dict['lsb'] = lsb_facts\n    return facts_dict", "loc": 27}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\pkg_mgr.py", "class_name": "PkgMgrFactCollector", "function_name": "pkg_mgrs", "parameters": ["self", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pkg_mgrs(self, collected_facts):\n    # Filter out the /usr/bin/pkg because on Altlinux it is actually the\n    # perl-Package (not Solaris package manager).\n    # Since the pkg5 takes precedence over apt, this workaround\n    # is required to select the suitable package manager on Altlinux.\n    if collected_facts['ansible_os_family'] == 'Altlinux':\n        return filter(lambda pkg: pkg['path'] != '/usr/bin/pkg', PKG_MGRS)\n    else:\n        return PKG_MGRS", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\platform.py", "class_name": "PlatformFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "arch_bits.replace", "get_file_content", "machine_id.splitlines", "module.get_bin_path", "module.run_command", "out.splitlines", "platform.architecture", "platform.machine", "platform.node", "platform.node().split", "platform.python_version", "platform.release", "platform.system", "platform.uname", "platform.version", "platform_facts['fqdn'].split", "socket.getfqdn", "solaris_i86_re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    platform_facts = {}\n    # platform.system() can be Linux, Darwin, Java, or Windows\n    platform_facts['system'] = platform.system()\n    platform_facts['kernel'] = platform.release()\n    platform_facts['kernel_version'] = platform.version()\n    platform_facts['machine'] = platform.machine()\n\n    platform_facts['python_version'] = platform.python_version()\n\n    platform_facts['fqdn'] = socket.getfqdn()\n    platform_facts['hostname'] = platform.node().split('.')[0]\n    platform_facts['nodename'] = platform.node()\n\n    platform_facts['domain'] = '.'.join(platform_facts['fqdn'].split('.')[1:])\n\n    arch_bits = platform.architecture()[0]\n\n    platform_facts['userspace_bits'] = arch_bits.replace('bit', '')\n    if platform_facts['machine'] == 'x86_64':\n        platform_facts['architecture'] = platform_facts['machine']\n        if platform_facts['userspace_bits'] == '64':\n            platform_facts['userspace_architecture'] = 'x86_64'\n        elif platform_facts['userspace_bits'] == '32':\n            platform_facts['userspace_architecture'] = 'i386'\n    elif solaris_i86_re.search(platform_facts['machine']):\n        platform_facts['architecture'] = 'i386'\n        if platform_facts['userspace_bits'] == '64':\n            platform_facts['userspace_architecture'] = 'x86_64'\n        elif platform_facts['userspace_bits'] == '32':\n            platform_facts['userspace_architecture'] = 'i386'\n    else:\n        platform_facts['architecture'] = platform_facts['machine']\n\n    if platform_facts['system'] == 'AIX':\n        # Attempt to use getconf to figure out architecture\n        # fall back to bootinfo if needed\n        getconf_bin = module.get_bin_path('getconf')\n        if getconf_bin:\n            rc, out, err = module.run_command([getconf_bin, 'MACHINE_ARCHITECTURE'])\n            data = out.splitlines()\n            platform_facts['architecture'] = data[0]\n        else:\n            bootinfo_bin = module.get_bin_path('bootinfo')\n            rc, out, err = module.run_command([bootinfo_bin, '-p'])\n            data = out.splitlines()\n            platform_facts['architecture'] = data[0]\n    elif platform_facts['system'] == 'OpenBSD':\n        platform_facts['architecture'] = platform.uname()[5]\n\n    machine_id = get_file_content(\"/var/lib/dbus/machine-id\") or get_file_content(\"/etc/machine-id\")\n    if machine_id:\n        machine_id = machine_id.splitlines()[0]\n        platform_facts[\"machine_id\"] = machine_id\n\n    return platform_facts", "loc": 56}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\python.py", "class_name": "PythonFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    python_facts = {}\n    python_facts['python'] = {\n        'version': {\n            'major': sys.version_info[0],\n            'minor': sys.version_info[1],\n            'micro': sys.version_info[2],\n            'releaselevel': sys.version_info[3],\n            'serial': sys.version_info[4]\n        },\n        'version_info': list(sys.version_info),\n        'executable': sys.executable,\n        'has_sslcontext': HAS_SSLCONTEXT\n    }\n\n    try:\n        python_facts['python']['type'] = sys.subversion[0]\n    except AttributeError:\n        try:\n            python_facts['python']['type'] = sys.implementation.name\n        except AttributeError:\n            python_facts['python']['type'] = None\n\n    return python_facts", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\selinux.py", "class_name": "SelinuxFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SELINUX_MODE_DICT.get", "selinux.is_selinux_enabled", "selinux.security_getenforce", "selinux.security_policyvers", "selinux.selinux_getenforcemode", "selinux.selinux_getpolicytype"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    facts_dict = {}\n    selinux_facts = {}\n\n    # If selinux library is missing, only set the status and selinux_python_present since\n    # there is no way to tell if SELinux is enabled or disabled on the system\n    # without the library.\n    if not HAVE_SELINUX:\n        selinux_facts['status'] = 'Missing selinux Python library'\n        facts_dict['selinux'] = selinux_facts\n        facts_dict['selinux_python_present'] = False\n        return facts_dict\n\n    # Set a boolean for testing whether the Python library is present\n    facts_dict['selinux_python_present'] = True\n\n    if not selinux.is_selinux_enabled():\n        selinux_facts['status'] = 'disabled'\n    else:\n        selinux_facts['status'] = 'enabled'\n\n        try:\n            selinux_facts['policyvers'] = selinux.security_policyvers()\n        except (AttributeError, OSError):\n            selinux_facts['policyvers'] = 'unknown'\n\n        try:\n            (rc, configmode) = selinux.selinux_getenforcemode()\n            if rc == 0:\n                selinux_facts['config_mode'] = SELINUX_MODE_DICT.get(configmode, 'unknown')\n            else:\n                selinux_facts['config_mode'] = 'unknown'\n        except (AttributeError, OSError):\n            selinux_facts['config_mode'] = 'unknown'\n\n        try:\n            mode = selinux.security_getenforce()\n            selinux_facts['mode'] = SELINUX_MODE_DICT.get(mode, 'unknown')\n        except (AttributeError, OSError):\n            selinux_facts['mode'] = 'unknown'\n\n        try:\n            (rc, policytype) = selinux.selinux_getpolicytype()\n            if rc == 0:\n                selinux_facts['type'] = policytype\n            else:\n                selinux_facts['type'] = 'unknown'\n        except (AttributeError, OSError):\n            selinux_facts['type'] = 'unknown'\n\n    facts_dict['selinux'] = selinux_facts\n    return facts_dict", "loc": 52}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\ssh_pub_keys.py", "class_name": "SshPubKeyFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_file_content", "keydata.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    ssh_pub_key_facts = {}\n    algos = ('dsa', 'rsa', 'ecdsa', 'ed25519')\n\n    # list of directories to check for ssh keys\n    # used in the order listed here, the first one with keys is used\n    keydirs = ['/etc/ssh', '/etc/openssh', '/etc']\n\n    for keydir in keydirs:\n        for algo in algos:\n            factname = 'ssh_host_key_%s_public' % algo\n            if factname in ssh_pub_key_facts:\n                # a previous keydir was already successful, stop looking\n                # for keys\n                return ssh_pub_key_facts\n            key_filename = '%s/ssh_host_%s_key.pub' % (keydir, algo)\n            keydata = get_file_content(key_filename)\n            if keydata is not None:\n                (keytype, key) = keydata.split()[0:2]\n                ssh_pub_key_facts[factname] = key\n                ssh_pub_key_facts[factname + '_keytype'] = keytype\n\n    return ssh_pub_key_facts", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\systemd.py", "class_name": "SystemdFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ServiceMgrFactCollector.is_systemd_managed", "int", "module.get_bin_path", "module.run_command", "stdout.split", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    systemctl_bin = module.get_bin_path(\"systemctl\")\n    systemd_facts = {}\n    if systemctl_bin and ServiceMgrFactCollector.is_systemd_managed(module):\n        rc, stdout, dummy = module.run_command(\n            [systemctl_bin, \"--version\"],\n            check_rc=False,\n        )\n\n        if rc != 0:\n            return systemd_facts\n\n        systemd_facts[\"systemd\"] = {\n            \"features\": str(stdout.split(\"\\n\")[1]),\n            \"version\": int(stdout.split(\" \")[1]),\n        }\n\n    return systemd_facts", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\system\\user.py", "class_name": "UserFactCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getpass.getuser", "os.geteuid", "os.getgid", "os.getuid", "pwd.getpwnam", "pwd.getpwuid"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    user_facts = {}\n\n    user_facts['user_id'] = getpass.getuser()\n\n    try:\n        pwent = pwd.getpwnam(getpass.getuser())\n    except KeyError:\n        pwent = pwd.getpwuid(os.getuid())\n\n    user_facts['user_uid'] = pwent.pw_uid\n    user_facts['user_gid'] = pwent.pw_gid\n    user_facts['user_gecos'] = pwent.pw_gecos\n    user_facts['user_dir'] = pwent.pw_dir\n    user_facts['user_shell'] = pwent.pw_shell\n    user_facts['real_user_id'] = os.getuid()\n    user_facts['effective_user_id'] = os.geteuid()\n    user_facts['real_group_id'] = os.getgid()\n    user_facts['effective_group_id'] = os.getgid()\n\n    return user_facts", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\virtual\\base.py", "class_name": "VirtualCollector", "function_name": "collect", "parameters": ["self", "module", "collected_facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["facts_obj.populate", "self._fact_class"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collect(self, module=None, collected_facts=None):\n    collected_facts = collected_facts or {}\n    if not module:\n        return {}\n\n    # Network munges cached_facts by side effect, so give it a copy\n    facts_obj = self._fact_class(module)\n\n    facts_dict = facts_obj.populate(collected_facts=collected_facts)\n\n    return facts_dict", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\virtual\\openbsd.py", "class_name": "OpenBSDVirtual", "function_name": "get_virtual_facts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dmesg_boot.splitlines", "get_file_content", "guest_tech.update", "host_tech.add", "host_tech.update", "re.match", "self.detect_virt_product", "self.detect_virt_vendor", "set", "virtual_facts.update"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_virtual_facts(self):\n    virtual_facts = {}\n    host_tech = set()\n    guest_tech = set()\n\n    # Set empty values as default\n    virtual_facts['virtualization_type'] = ''\n    virtual_facts['virtualization_role'] = ''\n\n    virtual_product_facts = self.detect_virt_product('hw.product')\n    guest_tech.update(virtual_product_facts['virtualization_tech_guest'])\n    host_tech.update(virtual_product_facts['virtualization_tech_host'])\n    virtual_facts.update(virtual_product_facts)\n\n    virtual_vendor_facts = self.detect_virt_vendor('hw.vendor')\n    guest_tech.update(virtual_vendor_facts['virtualization_tech_guest'])\n    host_tech.update(virtual_vendor_facts['virtualization_tech_host'])\n\n    if virtual_facts['virtualization_type'] == '':\n        virtual_facts.update(virtual_vendor_facts)\n\n    # Check the dmesg if vmm(4) attached, indicating the host is\n    # capable of virtualization.\n    dmesg_boot = get_file_content(OpenBSDVirtual.DMESG_BOOT)\n    for line in dmesg_boot.splitlines():\n        match = re.match('^vmm0 at mainbus0: (SVM/RVI|VMX/EPT)$', line)\n        if match:\n            host_tech.add('vmm')\n            virtual_facts['virtualization_type'] = 'vmm'\n            virtual_facts['virtualization_role'] = 'host'\n\n    virtual_facts['virtualization_tech_guest'] = guest_tech\n    virtual_facts['virtualization_tech_host'] = host_tech\n    return virtual_facts", "loc": 34}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\virtual\\sysctl.py", "class_name": "VirtualSysctlDetectionMixin", "function_name": "detect_virt_product", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["guest_tech.add", "out.rstrip", "re.match", "self.detect_sysctl", "self.module.run_command", "set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def detect_virt_product(self, key):\n    virtual_product_facts = {}\n    host_tech = set()\n    guest_tech = set()\n\n    # We do similar to what we do in linux.py -- We want to allow multiple\n    # virt techs to show up, but maintain compatibility, so we have to track\n    # when we would have stopped, even though now we go through everything.\n    found_virt = False\n\n    self.detect_sysctl()\n    if self.sysctl_path:\n        rc, out, err = self.module.run_command(\"%s -n %s\" % (self.sysctl_path, key))\n        if rc == 0:\n            if re.match('(KVM|kvm|Bochs|SmartDC).*', out):\n                guest_tech.add('kvm')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'kvm'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if re.match('.*VMware.*', out):\n                guest_tech.add('VMware')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'VMware'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if out.rstrip() == 'VirtualBox':\n                guest_tech.add('virtualbox')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'virtualbox'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if re.match('(HVM domU|XenPVH|XenPV|XenPVHVM).*', out):\n                guest_tech.add('xen')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'xen'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if out.rstrip() == 'Hyper-V':\n                guest_tech.add('Hyper-V')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'Hyper-V'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if out.rstrip() == 'Parallels':\n                guest_tech.add('parallels')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'parallels'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if out.rstrip() == 'RHEV Hypervisor':\n                guest_tech.add('RHEV')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'RHEV'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n            if (key == 'security.jail.jailed') and (out.rstrip() == '1'):\n                guest_tech.add('jails')\n                if not found_virt:\n                    virtual_product_facts['virtualization_type'] = 'jails'\n                    virtual_product_facts['virtualization_role'] = 'guest'\n                    found_virt = True\n\n    virtual_product_facts['virtualization_tech_guest'] = guest_tech\n    virtual_product_facts['virtualization_tech_host'] = host_tech\n    return virtual_product_facts", "loc": 66}
{"file": "ansible\\lib\\ansible\\module_utils\\facts\\virtual\\sysctl.py", "class_name": "VirtualSysctlDetectionMixin", "function_name": "detect_virt_vendor", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["guest_tech.add", "out.rstrip", "self.detect_sysctl", "self.module.run_command", "set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def detect_virt_vendor(self, key):\n    virtual_vendor_facts = {}\n    host_tech = set()\n    guest_tech = set()\n    self.detect_sysctl()\n    if self.sysctl_path:\n        rc, out, err = self.module.run_command(\"%s -n %s\" % (self.sysctl_path, key))\n        if rc == 0:\n            if out.rstrip() == 'QEMU':\n                guest_tech.add('kvm')\n                virtual_vendor_facts['virtualization_type'] = 'kvm'\n                virtual_vendor_facts['virtualization_role'] = 'guest'\n            if out.rstrip() == 'OpenBSD':\n                guest_tech.add('vmm')\n                virtual_vendor_facts['virtualization_type'] = 'vmm'\n                virtual_vendor_facts['virtualization_role'] = 'guest'\n\n    virtual_vendor_facts['virtualization_tech_guest'] = guest_tech\n    virtual_vendor_facts['virtualization_tech_host'] = host_tech\n    return virtual_vendor_facts", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\parsing\\convert_bool.py", "class_name": null, "function_name": "boolean", "parameters": ["value", "strict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "TypeError", "isinstance", "repr", "to_text", "to_text(value, errors='surrogate_or_strict').lower", "to_text(value, errors='surrogate_or_strict').lower().strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def boolean(value, strict=True):\n    if isinstance(value, bool):\n        return value\n\n    normalized_value = value\n\n    if isinstance(value, (str, bytes)):\n        normalized_value = to_text(value, errors='surrogate_or_strict').lower().strip()\n\n    if not isinstance(value, c.Hashable):\n        normalized_value = None  # prevent unhashable types from bombing, but keep the rest of the existing fallback/error behavior\n\n    if normalized_value in BOOLEANS_TRUE:\n        return True\n    elif normalized_value in BOOLEANS_FALSE or not strict:\n        return False\n\n    raise TypeError(\"The value '%s' is not a valid boolean. Valid booleans include: %s\" % (to_text(value), ', '.join(repr(i) for i in BOOLEANS)))", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "remove_move", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AttributeError", "delattr"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Remove item from six.moves.", "source_code": "def remove_move(name):\n    \"\"\"Remove item from six.moves.\"\"\"\n    try:\n        delattr(_MovedItems, name)\n    except AttributeError:\n        try:\n            del moves.__dict__[name]\n        except KeyError:\n            raise AttributeError(\"no such move, %r\" % (name,))", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "with_metaclass", "parameters": ["meta"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["meta", "meta.__prepare__", "type.__new__", "types.resolve_bases"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Create a base class with a metaclass.", "source_code": "def with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                # This version introduced PEP 560 that requires a bit\n                # of extra care (we mimic what is done by __build_class__).\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n    return type.__new__(metaclass, 'temporary_class', (), {})", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "add_metaclass", "parameters": ["metaclass"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.__dict__.copy", "hasattr", "isinstance", "metaclass", "orig_vars.get", "orig_vars.pop"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Class decorator for creating a class with a metaclass.", "source_code": "def add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n    return wrapper", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "ensure_binary", "parameters": ["s", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "s.encode", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Coerce **s** to six.binary_type. For Python 2: - `unicode` -> encoded to `str`", "source_code": "def ensure_binary(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce **s** to six.binary_type.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> encoded to `bytes`\n      - `bytes` -> `bytes`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s\n    if isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    raise TypeError(\"not expecting type '%s'\" % type(s))", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "ensure_str", "parameters": ["s", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "s.decode", "s.encode", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Coerce *s* to `str`. For Python 2: - `unicode` -> encoded to `str`", "source_code": "def ensure_str(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to `str`.\n\n    For Python 2:\n      - `unicode` -> encoded to `str`\n      - `str` -> `str`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    # Optimization: Fast return for the common case.\n    if type(s) is str:\n        return s\n    if PY2 and isinstance(s, text_type):\n        return s.encode(encoding, errors)\n    elif PY3 and isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif not isinstance(s, (text_type, binary_type)):\n        raise TypeError(\"not expecting type '%s'\" % type(s))\n    return s", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "ensure_text", "parameters": ["s", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "s.decode", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Coerce *s* to six.text_type. For Python 2: - `unicode` -> `unicode`", "source_code": "def ensure_text(s, encoding='utf-8', errors='strict'):\n    \"\"\"Coerce *s* to six.text_type.\n\n    For Python 2:\n      - `unicode` -> `unicode`\n      - `str` -> `unicode`\n\n    For Python 3:\n      - `str` -> `str`\n      - `bytes` -> decoded to `str`\n    \"\"\"\n    if isinstance(s, binary_type):\n        return s.decode(encoding, errors)\n    elif isinstance(s, text_type):\n        return s\n    else:\n        raise TypeError(\"not expecting type '%s'\" % type(s))", "loc": 17}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "python_2_unicode_compatible", "parameters": ["klass"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self.__unicode__", "self.__unicode__().encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "A class decorator that defines __unicode__ and __str__ methods under Python 2. Under Python 3 it does nothing. To support Python 2 and 3 with a single code base, define a __str__ method", "source_code": "def python_2_unicode_compatible(klass):\n    \"\"\"\n    A class decorator that defines __unicode__ and __str__ methods under Python 2.\n    Under Python 3 it does nothing.\n\n    To support Python 2 and 3 with a single code base, define a __str__ method\n    returning text and apply this decorator to the class.\n    \"\"\"\n    if PY2:\n        if '__str__' not in klass.__dict__:\n            raise ValueError(\"@python_2_unicode_compatible cannot be applied \"\n                             \"to %s because it doesn't define __str__().\" %\n                             klass.__name__)\n        klass.__unicode__ = klass.__str__\n        klass.__str__ = lambda self: self.__unicode__().encode('utf-8')\n    return klass", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": "_SixMetaPathImporter", "function_name": "load_module", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "mod._resolve", "self.__get_module"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_module(self, fullname):\n    try:\n        # in case of a reload\n        return sys.modules[fullname]\n    except KeyError:\n        pass\n    mod = self.__get_module(fullname)\n    if isinstance(mod, MovedModule):\n        mod = mod._resolve()\n    else:\n        mod.__loader__ = self\n    sys.modules[fullname] = mod\n    return mod", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": "_SixMetaPathImporter", "function_name": "is_package", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self.__get_module"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return true, if the named module is a package. We need this method to get correct spec objects with Python 3.4 (see PEP451)", "source_code": "def is_package(self, fullname):\n    \"\"\"\n    Return true, if the named module is a package.\n\n    We need this method to get correct spec objects with\n    Python 3.4 (see PEP451)\n    \"\"\"\n    return hasattr(self.__get_module(fullname), \"__path__\")", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "reraise", "parameters": ["tp", "value", "tb"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["tp", "value.with_traceback"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reraise(tp, value, tb=None):\n    try:\n        if value is None:\n            value = tp()\n        if value.__traceback__ is not tb:\n            raise value.with_traceback(tb)\n        raise value\n    finally:\n        value = None\n        tb = None", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "exec_", "parameters": ["_code_", "_globs_", "_locs_"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["exec", "sys._getframe"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Execute code in a namespace.", "source_code": "def exec_(_code_, _globs_=None, _locs_=None):\n    \"\"\"Execute code in a namespace.\"\"\"\n    if _globs_ is None:\n        frame = sys._getframe(1)\n        _globs_ = frame.f_globals\n        if _locs_ is None:\n            _locs_ = frame.f_locals\n        del frame\n    elif _locs_ is None:\n        _locs_ = _globs_\n    exec(\"\"\"exec _code_ in _globs_, _locs_\"\"\")", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "print_", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "data.encode", "enumerate", "fp.write", "getattr", "isinstance", "kwargs.pop", "str", "unicode", "write"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "The new-style print function for Python 2.4 and 2.5.", "source_code": "def print_(*args, **kwargs):\n    \"\"\"The new-style print function for Python 2.4 and 2.5.\"\"\"\n    fp = kwargs.pop(\"file\", sys.stdout)\n    if fp is None:\n        return\n\n    def write(data):\n        if not isinstance(data, basestring):\n            data = str(data)\n        # If the file has an encoding, encode unicode with it.\n        if (isinstance(fp, file) and\n                isinstance(data, unicode) and\n                fp.encoding is not None):\n            errors = getattr(fp, \"errors\", None)\n            if errors is None:\n                errors = \"strict\"\n            data = data.encode(fp.encoding, errors)\n        fp.write(data)\n    want_unicode = False\n    sep = kwargs.pop(\"sep\", None)\n    if sep is not None:\n        if isinstance(sep, unicode):\n            want_unicode = True\n        elif not isinstance(sep, str):\n            raise TypeError(\"sep must be None or a string\")\n    end = kwargs.pop(\"end\", None)\n    if end is not None:\n        if isinstance(end, unicode):\n            want_unicode = True\n        elif not isinstance(end, str):\n            raise TypeError(\"end must be None or a string\")\n    if kwargs:\n        raise TypeError(\"invalid keyword arguments to print()\")\n    if not want_unicode:\n        for arg in args:\n            if isinstance(arg, unicode):\n                want_unicode = True\n                break\n    if want_unicode:\n        newline = unicode(\"\\n\")\n        space = unicode(\" \")\n    else:\n        newline = \"\\n\"\n        space = \" \"\n    if sep is None:\n        sep = space\n    if end is None:\n        end = newline\n    for i, arg in enumerate(args):\n        if i:\n            write(sep)\n        write(arg)\n    write(end)", "loc": 53}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "print_", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_print", "fp.flush", "kwargs.get", "kwargs.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def print_(*args, **kwargs):\n    fp = kwargs.get(\"file\", sys.stdout)\n    flush = kwargs.pop(\"flush\", False)\n    _print(*args, **kwargs)\n    if flush and fp is not None:\n        fp.flush()", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "wrapper", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.__dict__.copy", "hasattr", "isinstance", "metaclass", "orig_vars.get", "orig_vars.pop"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(cls):\n    orig_vars = cls.__dict__.copy()\n    slots = orig_vars.get('__slots__')\n    if slots is not None:\n        if isinstance(slots, str):\n            slots = [slots]\n        for slots_var in slots:\n            orig_vars.pop(slots_var)\n    orig_vars.pop('__dict__', None)\n    orig_vars.pop('__weakref__', None)\n    if hasattr(cls, '__qualname__'):\n        orig_vars['__qualname__'] = cls.__qualname__\n    return metaclass(cls.__name__, cls.__bases__, orig_vars)", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\six\\__init__.py", "class_name": null, "function_name": "write", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.encode", "fp.write", "getattr", "isinstance", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write(data):\n    if not isinstance(data, basestring):\n        data = str(data)\n    # If the file has an encoding, encode unicode with it.\n    if (isinstance(fp, file) and\n            isinstance(data, unicode) and\n            fp.encoding is not None):\n        errors = getattr(fp, \"errors\", None)\n        if errors is None:\n            errors = \"strict\"\n        data = data.encode(fp.encoding, errors)\n    fp.write(data)", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ambient_context.py", "class_name": "AmbientContextBase", "function_name": "current", "parameters": ["cls", "optional"], "param_types": {"optional": "bool"}, "return_type": "t.Self | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ReferenceError", "cls._contextvar.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Return the currently active context value for the current thread or coroutine.", "source_code": "def current(cls, optional: bool = False) -> t.Self | None:\n    \"\"\"\n    Return the currently active context value for the current thread or coroutine.\n    Raises ReferenceError if a context is not active, unless `optional` is `True`.\n    \"\"\"\n    try:\n        return cls._contextvar.get()\n    except LookupError:\n        if optional:\n            return None\n\n        raise ReferenceError(f\"A required {cls.__name__} context is not active.\") from None", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_dataclass_validation.py", "class_name": null, "function_name": "validate_value", "parameters": ["target_name", "target_ref", "target_type"], "param_types": {"target_name": "str", "target_ref": "str", "target_type": "type"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' or '.join", "', '.join", "ValueError", "_extract_type", "_get_allowed_types", "append_line", "len", "register_type", "repr", "t.get_args", "t.get_origin", "target_ref.startswith", "validate_value"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Generate code to validate the specified value.", "source_code": "def validate_value(target_name: str, target_ref: str, target_type: type) -> None:\n    \"\"\"Generate code to validate the specified value.\"\"\"\n    nonlocal indent\n\n    origin_type = t.get_origin(target_type)\n\n    if origin_type is t.ClassVar:\n        return  # ignore annotations which are not fields, indicated by the t.ClassVar annotation\n\n    allowed_types = _get_allowed_types(target_type)\n\n    # check value\n\n    if origin_type is t.Literal:\n        # DTFIX-FUTURE: support optional literals\n\n        values = t.get_args(target_type)\n\n        append_line(f\"\"\"if {target_ref} not in {values}:\"\"\")\n        append_line(f\"\"\"    raise ValueError(rf\"{target_name} must be one of {values} instead of {{{target_ref}!r}}\")\"\"\")\n\n    allowed_refs = [register_type(allowed_type) for allowed_type in allowed_types]\n    allowed_names = [repr(allowed_type) for allowed_type in allowed_types]\n\n    if allow_subclasses:\n        if len(allowed_refs) == 1:\n            append_line(f\"\"\"if not isinstance({target_ref}, {allowed_refs[0]}):\"\"\")\n        else:\n            append_line(f\"\"\"if not isinstance({target_ref}, ({', '.join(allowed_refs)})):\"\"\")\n    else:\n        if len(allowed_refs) == 1:\n            append_line(f\"\"\"if type({target_ref}) is not {allowed_refs[0]}:\"\"\")\n        else:\n            append_line(f\"\"\"if type({target_ref}) not in ({', '.join(allowed_refs)}):\"\"\")\n\n    append_line(f\"\"\"    raise TypeError(f\"{target_name} must be {' or '.join(allowed_names)} instead of {{type({target_ref})}}\")\"\"\")\n\n    # check elements (for containers)\n\n    if target_ref.startswith('self.'):\n        local_ref = target_ref[5:]\n    else:\n        local_ref = target_ref\n\n    if tuple in allowed_types:\n        tuple_type = _extract_type(target_type, tuple)\n\n        idx_ref = f'{local_ref}_idx'\n        item_ref = f'{local_ref}_item'\n        item_name = f'{target_name}[{{{idx_ref}!r}}]'\n        item_type, _ellipsis = t.get_args(tuple_type)\n\n        if _ellipsis is not ...:\n            raise ValueError(f\"{cls} tuple fields must be a tuple of a single element type\")\n\n        append_line(f\"\"\"if isinstance({target_ref}, {known_types[tuple]}):\"\"\")\n        append_line(f\"\"\"    for {idx_ref}, {item_ref} in enumerate({target_ref}):\"\"\")\n\n        indent += 2\n        validate_value(target_name=item_name, target_ref=item_ref, target_type=item_type)\n        indent -= 2\n\n    if list in allowed_types:\n        list_type = _extract_type(target_type, list)\n\n        idx_ref = f'{local_ref}_idx'\n        item_ref = f'{local_ref}_item'\n        item_name = f'{target_name}[{{{idx_ref}!r}}]'\n        (item_type,) = t.get_args(list_type)\n\n        append_line(f\"\"\"if isinstance({target_ref}, {known_types[list]}):\"\"\")\n        append_line(f\"\"\"    for {idx_ref}, {item_ref} in enumerate({target_ref}):\"\"\")\n\n        indent += 2\n        validate_value(target_name=item_name, target_ref=item_ref, target_type=item_type)\n        indent -= 2\n\n    if dict in allowed_types:\n        dict_type = _extract_type(target_type, dict)\n\n        key_ref, value_ref = f'{local_ref}_key', f'{local_ref}_value'\n        key_type, value_type = t.get_args(dict_type)\n        key_name, value_name = f'{target_name!r} key {{{key_ref}!r}}', f'{target_name}[{{{key_ref}!r}}]'\n\n        append_line(f\"\"\"if isinstance({target_ref}, {known_types[dict]}):\"\"\")\n        append_line(f\"\"\"    for {key_ref}, {value_ref} in {target_ref}.items():\"\"\")\n\n        indent += 2\n        validate_value(target_name=key_name, target_ref=key_ref, target_type=key_type)\n        validate_value(target_name=value_name, target_ref=value_ref, target_type=value_type)\n        indent -= 2", "loc": 91}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_debugging.py", "class_name": null, "function_name": "load_params", "parameters": [], "param_types": {}, "return_type": "tuple[bytes, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["(args_path := pathlib.Path(args)).is_file", "argparse.ArgumentParser", "args.encode", "args_path.read_bytes", "parser.add_argument", "parser.parse_args", "pathlib.Path", "sys.stderr.flush", "sys.stderr.write", "sys.stdin.buffer.read", "sys.stdin.isatty"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Load module arguments and profile when debugging an Ansible module.", "source_code": "def load_params() -> tuple[bytes, str]:\n    \"\"\"Load module arguments and profile when debugging an Ansible module.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Directly invoke an Ansible module for debugging.\")\n    parser.add_argument('args', nargs='?', help='module args JSON (file path or inline string)')\n    parser.add_argument('--profile', default='legacy', help='profile for JSON decoding/encoding of args/response')\n\n    parsed_args = parser.parse_args()\n\n    args: str | None = parsed_args.args\n    profile: str = parsed_args.profile\n\n    if args:\n        if (args_path := pathlib.Path(args)).is_file():\n            buffer = args_path.read_bytes()\n        else:\n            buffer = args.encode(errors='surrogateescape')\n    else:\n        if sys.stdin.isatty():\n            sys.stderr.write('Waiting for Ansible module JSON on STDIN...\\n')\n            sys.stderr.flush()\n\n        buffer = sys.stdin.buffer.read()\n\n    return buffer, profile", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_deprecator.py", "class_name": null, "function_name": "deprecator_from_collection_name", "parameters": ["collection_name"], "param_types": {"collection_name": "str | None"}, "return_type": "_messages.PluginInfo | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_messages.PluginInfo", "_validation.validate_collection_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deprecator_from_collection_name(collection_name: str | None) -> _messages.PluginInfo | None:\n    \"\"\"Returns an instance with the special `collection` type to refer to a non-plugin or ambiguous caller within a collection.\"\"\"\n    # CAUTION: This function is exposed in public API as ansible.module_utils.datatag.deprecator_from_collection_name.\n\n    if not collection_name:\n        return None\n\n    _validation.validate_collection_name(collection_name)\n\n    return _messages.PluginInfo(\n        resolved_name=collection_name,\n        type=None,\n    )", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_deprecator.py", "class_name": null, "function_name": "get_best_deprecator", "parameters": [], "param_types": {}, "return_type": "_messages.PluginInfo", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "deprecator_from_collection_name", "get_caller_plugin_info"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the best-available `PluginInfo` for the caller of this method.", "source_code": "def get_best_deprecator(*, deprecator: _messages.PluginInfo | None = None, collection_name: str | None = None) -> _messages.PluginInfo:\n    \"\"\"Return the best-available `PluginInfo` for the caller of this method.\"\"\"\n    _skip_stackwalk = True\n\n    if deprecator and collection_name:\n        raise ValueError('Specify only one of `deprecator` or `collection_name`.')\n\n    return deprecator or deprecator_from_collection_name(collection_name) or get_caller_plugin_info() or INDETERMINATE_DEPRECATOR", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_deprecator.py", "class_name": null, "function_name": "get_caller_plugin_info", "parameters": [], "param_types": {}, "return_type": "_messages.PluginInfo | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_path_as_plugininfo", "_stack.caller_frame"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Try to get `PluginInfo` for the caller of this method, ignoring marked infrastructure stack frames.", "source_code": "def get_caller_plugin_info() -> _messages.PluginInfo | None:\n    \"\"\"Try to get `PluginInfo` for the caller of this method, ignoring marked infrastructure stack frames.\"\"\"\n    _skip_stackwalk = True\n\n    if frame_info := _stack.caller_frame():\n        return _path_as_plugininfo(frame_info.filename)\n\n    return None  # pragma: nocover", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_event_utils.py", "class_name": null, "function_name": "deduplicate_message_parts", "parameters": ["message_parts"], "param_types": {"message_parts": "list[str]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_text_utils.concat_message", "list", "message_part.endswith", "message_parts.pop", "reversed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Format the given list of messages into a brief message, while deduplicating repeated elements.", "source_code": "def deduplicate_message_parts(message_parts: list[str]) -> str:\n    \"\"\"Format the given list of messages into a brief message, while deduplicating repeated elements.\"\"\"\n    message_parts = list(reversed(message_parts))\n\n    message = message_parts.pop(0)\n\n    for message_part in message_parts:\n        # avoid duplicate messages where the cause was already concatenated to the exception message\n        if message_part.endswith(message):\n            message = message_part\n        else:\n            message = _text_utils.concat_message(message_part, message)\n\n    return message", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_event_utils.py", "class_name": null, "function_name": "format_event_brief_message", "parameters": ["event"], "param_types": {"event": "_messages.Event"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["deduplicate_message_parts", "message_parts.append"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Format an event into a brief message. Help text, contextual information and sub-events will be omitted.", "source_code": "def format_event_brief_message(event: _messages.Event) -> str:\n    \"\"\"\n    Format an event into a brief message.\n    Help text, contextual information and sub-events will be omitted.\n    \"\"\"\n    message_parts: list[str] = []\n\n    while True:\n        message_parts.append(event.msg)\n\n        if not event.chain or not event.chain.follow:\n            break\n\n        event = event.chain.event\n\n    return deduplicate_message_parts(message_parts)", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_event_utils.py", "class_name": null, "function_name": "deprecation_as_dict", "parameters": ["deprecation"], "param_types": {"deprecation": "_messages.DeprecationSummary"}, "return_type": "_t.Dict[str, _t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "deprecation.deprecator.resolved_name.split", "dict", "format_event_brief_message", "result.update"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deprecation_as_dict(deprecation: _messages.DeprecationSummary) -> _t.Dict[str, _t.Any]:\n    \"\"\"Returns a dictionary representation of the deprecation object in the format exposed to playbooks.\"\"\"\n    from ansible.module_utils._internal._deprecator import INDETERMINATE_DEPRECATOR  # circular import from messages\n\n    if deprecation.deprecator and deprecation.deprecator != INDETERMINATE_DEPRECATOR:\n        collection_name = '.'.join(deprecation.deprecator.resolved_name.split('.')[:2])\n    else:\n        collection_name = None\n\n    result = dict(\n        msg=format_event_brief_message(deprecation.event),\n        collection_name=collection_name,\n    )\n\n    if deprecation.date:\n        result.update(date=deprecation.date)\n    else:\n        result.update(version=deprecation.version)\n\n    return result", "loc": 20}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_no_six.py", "class_name": null, "function_name": "with_metaclass", "parameters": ["meta"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["meta", "meta.__prepare__", "type.__new__", "types.resolve_bases"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Create a base class with a metaclass.", "source_code": "def with_metaclass(meta, *bases):\n    \"\"\"Create a base class with a metaclass.\"\"\"\n\n    # This requires a bit of explanation: the basic idea is to make a dummy\n    # metaclass for one level of class instantiation that replaces itself with\n    # the actual metaclass.\n    class metaclass(type):\n\n        def __new__(cls, name, this_bases, d):\n            if sys.version_info[:2] >= (3, 7):\n                # This version introduced PEP 560 that requires a bit\n                # of extra care (we mimic what is done by __build_class__).\n                resolved_bases = types.resolve_bases(bases)\n                if resolved_bases is not bases:\n                    d['__orig_bases__'] = bases\n            else:\n                resolved_bases = bases\n            return meta(name, resolved_bases, d)\n\n        @classmethod\n        def __prepare__(cls, name, this_bases):\n            return meta.__prepare__(name, bases)\n\n    return type.__new__(metaclass, 'temporary_class', (), {})", "loc": 24}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_no_six.py", "class_name": null, "function_name": "add_metaclass", "parameters": ["metaclass"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.__dict__.copy", "hasattr", "isinstance", "metaclass", "orig_vars.get", "orig_vars.pop"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Class decorator for creating a class with a metaclass.", "source_code": "def add_metaclass(metaclass):\n    \"\"\"Class decorator for creating a class with a metaclass.\"\"\"\n\n    def wrapper(cls):\n        orig_vars = cls.__dict__.copy()\n        slots = orig_vars.get('__slots__')\n        if slots is not None:\n            if isinstance(slots, str):\n                slots = [slots]\n            for slots_var in slots:\n                orig_vars.pop(slots_var)\n        orig_vars.pop('__dict__', None)\n        orig_vars.pop('__weakref__', None)\n        if hasattr(cls, '__qualname__'):\n            orig_vars['__qualname__'] = cls.__qualname__\n        return metaclass(cls.__name__, cls.__bases__, orig_vars)\n\n    return wrapper", "loc": 18}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_no_six.py", "class_name": null, "function_name": "deprecate", "parameters": ["importable_name", "module_name"], "param_types": {"importable_name": "str", "module_name": "str"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AttributeError", "_mini_six.get", "warnings.deprecate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Inject import-time deprecation warnings.", "source_code": "def deprecate(importable_name: str, module_name: str, *deprecated_args) -> object:\n    \"\"\"Inject import-time deprecation warnings.\"\"\"\n    if not (importable_name in deprecated_args and (importable := _mini_six.get(importable_name, ...) is not ...)):\n        raise AttributeError(f\"module {module_name!r} has no attribute {importable_name!r}\")\n\n    # TODO Inspect and remove all calls to this function in 2.24\n    warnings.deprecate(\n        msg=f\"Importing {importable_name!r} from {module_name!r} is deprecated.\",\n        version=\"2.24\",\n    )\n\n    return importable", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_no_six.py", "class_name": null, "function_name": "wrapper", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.__dict__.copy", "hasattr", "isinstance", "metaclass", "orig_vars.get", "orig_vars.pop"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(cls):\n    orig_vars = cls.__dict__.copy()\n    slots = orig_vars.get('__slots__')\n    if slots is not None:\n        if isinstance(slots, str):\n            slots = [slots]\n        for slots_var in slots:\n            orig_vars.pop(slots_var)\n    orig_vars.pop('__dict__', None)\n    orig_vars.pop('__weakref__', None)\n    if hasattr(cls, '__qualname__'):\n        orig_vars['__qualname__'] = cls.__qualname__\n    return metaclass(cls.__name__, cls.__bases__, orig_vars)", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_plugin_info.py", "class_name": null, "function_name": "get_plugin_info", "parameters": ["value"], "param_types": {"value": "HasPluginInfo"}, "return_type": "_messages.PluginInfo", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_messages.PluginInfo", "normalize_plugin_type"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Utility method that returns a `PluginInfo` from an object implementing the `HasPluginInfo` protocol.", "source_code": "def get_plugin_info(value: HasPluginInfo) -> _messages.PluginInfo:\n    \"\"\"Utility method that returns a `PluginInfo` from an object implementing the `HasPluginInfo` protocol.\"\"\"\n    return _messages.PluginInfo(\n        resolved_name=value.ansible_name,\n        type=normalize_plugin_type(value.plugin_type),\n    )", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_plugin_info.py", "class_name": null, "function_name": "normalize_plugin_type", "parameters": ["value"], "param_types": {"value": "str"}, "return_type": "_messages.PluginType | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_messages.PluginType", "value.lower"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Normalize value and return it as a PluginType, or None if the value does match any known plugin type.", "source_code": "def normalize_plugin_type(value: str) -> _messages.PluginType | None:\n    \"\"\"Normalize value and return it as a PluginType, or None if the value does match any known plugin type.\"\"\"\n    value = value.lower()\n\n    if value == 'modules':\n        value = 'module'\n\n    try:\n        return _messages.PluginType(value)\n    except ValueError:\n        return None", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_stack.py", "class_name": null, "function_name": "caller_frame", "parameters": [], "param_types": {}, "return_type": "_inspect.FrameInfo | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["iter_stack", "next"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return the caller stack frame, skipping any marked with the `_skip_stackwalk` local.", "source_code": "def caller_frame() -> _inspect.FrameInfo | None:\n    \"\"\"Return the caller stack frame, skipping any marked with the `_skip_stackwalk` local.\"\"\"\n    _skip_stackwalk = True\n\n    return next(iter_stack(), None)", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_stack.py", "class_name": null, "function_name": "iter_stack", "parameters": [], "param_types": {}, "return_type": "_t.Generator[_inspect.FrameInfo]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_inspect.stack"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Iterate over stack frames, skipping any marked with the `_skip_stackwalk` local.", "source_code": "def iter_stack() -> _t.Generator[_inspect.FrameInfo]:\n    \"\"\"Iterate over stack frames, skipping any marked with the `_skip_stackwalk` local.\"\"\"\n    _skip_stackwalk = True\n\n    for frame_info in _inspect.stack():\n        if '_skip_stackwalk' in frame_info.frame.f_locals:\n            continue\n\n        yield frame_info", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_traceback.py", "class_name": null, "function_name": "maybe_capture_traceback", "parameters": ["msg", "event"], "param_types": {"msg": "str", "event": "TracebackEvent"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "_stack.caller_frame", "is_traceback_enabled", "tb_lines.append", "tb_lines.extend", "traceback.format_stack"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Optionally capture a traceback for the current call stack, formatted as a string, if the specified traceback event is enabled. Frames marked with the `_skip_stackwalk` local are omitted.", "source_code": "def maybe_capture_traceback(msg: str, event: TracebackEvent) -> str | None:\n    \"\"\"\n    Optionally capture a traceback for the current call stack, formatted as a string, if the specified traceback event is enabled.\n    Frames marked with the `_skip_stackwalk` local are omitted.\n    \"\"\"\n    _skip_stackwalk = True\n\n    if not is_traceback_enabled(event):\n        return None\n\n    tb_lines = []\n\n    if frame_info := _stack.caller_frame():\n        # DTFIX-FUTURE: rewrite target-side tracebacks to point at controller-side paths?\n        tb_lines.append('Traceback (most recent call last):\\n')\n        tb_lines.extend(traceback.format_stack(frame_info.frame))\n        tb_lines.append(f'Message: {msg}\\n')\n    else:\n        tb_lines.append('(frame not found)\\n')  # pragma: nocover\n\n    return ''.join(tb_lines)", "loc": 21}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_traceback.py", "class_name": null, "function_name": "maybe_extract_traceback", "parameters": ["exception", "event"], "param_types": {"exception": "BaseException", "event": "TracebackEvent"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "is_traceback_enabled", "traceback.format_exception", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Optionally extract a formatted traceback from the given exception, if the specified traceback event is enabled.", "source_code": "def maybe_extract_traceback(exception: BaseException, event: TracebackEvent) -> str | None:\n    \"\"\"Optionally extract a formatted traceback from the given exception, if the specified traceback event is enabled.\"\"\"\n\n    if not is_traceback_enabled(event):\n        return None\n\n    # deprecated: description='use the single-arg version of format_traceback' python_version='3.9'\n    tb_lines = traceback.format_exception(type(exception), exception, exception.__traceback__)\n\n    return ''.join(tb_lines)", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_validation.py", "class_name": null, "function_name": "validate_collection_name", "parameters": ["collection_name", "name"], "param_types": {"collection_name": "object", "name": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "ValueError", "all", "collection_name.split", "isinstance", "keyword.iskeyword", "len", "part.isidentifier", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validate a collection name.", "source_code": "def validate_collection_name(collection_name: object, name: str = 'collection_name') -> None:\n    \"\"\"Validate a collection name.\"\"\"\n    if not isinstance(collection_name, str):\n        raise TypeError(f\"{name} must be {str} instead of {type(collection_name)}\")\n\n    parts = collection_name.split('.')\n\n    if len(parts) != 2 or not all(part.isidentifier() and not keyword.iskeyword(part) for part in parts):\n        raise ValueError(f\"{name} must consist of two non-keyword identifiers separated by '.'\")", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ansiballz\\_loader.py", "class_name": null, "function_name": "run_module", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_handle_exception", "_run_module", "extension_module.run", "extensions.items", "importlib.import_module"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Used internally by the AnsiballZ wrapper to run an Ansible module.", "source_code": "def run_module(\n    *,\n    json_params: bytes,\n    profile: str,\n    module_fqn: str,\n    modlib_path: str,\n    extensions: dict[str, dict[str, object]],\n    init_globals: dict[str, t.Any] | None = None,\n) -> None:  # pragma: nocover\n    \"\"\"Used internally by the AnsiballZ wrapper to run an Ansible module.\"\"\"\n    try:\n        for extension, args in extensions.items():\n            # importing _ansiballz instead of _extensions avoids an unnecessary import when extensions are not in use\n            extension_module = importlib.import_module(f'{_ansiballz.__name__}._extensions.{extension}')\n            extension_module.run(args)\n\n        _run_module(\n            json_params=json_params,\n            profile=profile,\n            module_fqn=module_fqn,\n            modlib_path=modlib_path,\n            init_globals=init_globals,\n        )\n    except Exception as ex:  # not BaseException, since modules are expected to raise SystemExit\n        _handle_exception(ex, profile)", "loc": 25}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ansiballz\\_respawn.py", "class_name": null, "function_name": "create_payload", "parameters": [], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "args.items", "dict", "inspect.getsource"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create and return an AnsiballZ payload for respawning a module.", "source_code": "def create_payload() -> str:\n    \"\"\"Create and return an AnsiballZ payload for respawning a module.\"\"\"\n    main = sys.modules['__main__']\n    code = inspect.getsource(_respawn_wrapper)\n\n    args = dict(\n        module_fqn=main._module_fqn,\n        modlib_path=main._modlib_path,\n        profile=basic._ANSIBLE_PROFILE,\n        json_params=basic._ANSIBLE_ARGS,\n    )\n\n    args_string = '\\n'.join(f'{key}={value!r},' for key, value in args.items())\n\n    wrapper = f\"\"\"{code}\n\nif __name__ == \"__main__\":\n    _respawn_main(\n{args_string}\n)\n\"\"\"\n\n    return wrapper", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ansiballz\\_extensions\\_coverage.py", "class_name": null, "function_name": "run", "parameters": ["args"], "param_types": {"args": "dict[str, t.Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "Options", "RuntimeError", "atexit.register", "cov.save", "cov.start", "cov.stop", "coverage.Coverage", "importlib.util.find_spec", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Bootstrap `coverage` for the current Ansible module invocation.", "source_code": "def run(args: dict[str, t.Any]) -> None:  # pragma: nocover\n    \"\"\"Bootstrap `coverage` for the current Ansible module invocation.\"\"\"\n    options = Options(**args)\n\n    if options.output:\n        # Enable code coverage analysis of the module.\n        # This feature is for internal testing and may change without notice.\n        python_version_string = '.'.join(str(v) for v in sys.version_info[:2])\n        os.environ['COVERAGE_FILE'] = f'{options.output}=python-{python_version_string}=coverage'\n\n        import coverage\n\n        cov = coverage.Coverage(config_file=options.config)\n\n        def atexit_coverage() -> None:\n            cov.stop()\n            cov.save()\n\n        atexit.register(atexit_coverage)\n\n        cov.start()\n    else:\n        # Verify coverage is available without importing it.\n        # This will detect when a module would fail with coverage enabled with minimal overhead.\n        if importlib.util.find_spec('coverage') is None:\n            raise RuntimeError('Could not find the `coverage` Python module.')", "loc": 26}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ansiballz\\_extensions\\_debugpy.py", "class_name": null, "function_name": "run", "parameters": ["args"], "param_types": {"args": "dict[str, t.Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Options", "debugpy.connect", "json.dumps", "options.source_mapping.items", "pathlib.Path", "str"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Enable remote debugging.", "source_code": "def run(args: dict[str, t.Any]) -> None:  # pragma: nocover\n    \"\"\"Enable remote debugging.\"\"\"\n    import debugpy\n\n    options = Options(**args)\n    temp_dir = pathlib.Path(__file__).parent.parent.parent.parent.parent.parent\n    path_mapping = [[key, str(temp_dir / value)] for key, value in options.source_mapping.items()]\n\n    os.environ['PATHS_FROM_ECLIPSE_TO_PYTHON'] = json.dumps(path_mapping)\n\n    debugpy.connect((options.host, options.port), **options.connect)\n\n    pass  # A convenient place to put a breakpoint", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_ansiballz\\_extensions\\_pydevd.py", "class_name": null, "function_name": "run", "parameters": ["args"], "param_types": {"args": "dict[str, t.Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Options", "debugging_module.settrace", "importlib.import_module", "json.dumps", "options.source_mapping.items", "pathlib.Path", "str"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Enable remote debugging.", "source_code": "def run(args: dict[str, t.Any]) -> None:  # pragma: nocover\n    \"\"\"Enable remote debugging.\"\"\"\n\n    options = Options(**args)\n    temp_dir = pathlib.Path(__file__).parent.parent.parent.parent.parent.parent\n    path_mapping = [[key, str(temp_dir / value)] for key, value in options.source_mapping.items()]\n\n    os.environ['PATHS_FROM_ECLIPSE_TO_PYTHON'] = json.dumps(path_mapping)\n\n    debugging_module = importlib.import_module(options.module)\n    debugging_module.settrace(**options.settrace)\n\n    pass  # when suspend is True, execution pauses here -- it's also a convenient place to put a breakpoint", "loc": 13}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "untag", "parameters": ["value"], "param_types": {"value": "_T"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tags", "AnsibleTaggedObject._get_tagged_type", "_AnsibleTagsMapping", "len", "t.cast", "t.cast(AnsibleTaggedObject, value)._native_copy", "tagged_type._instance_factory", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "If tags matching any of `tag_types` are present on `value`, return a copy with those tags removed. If no `tag_types` are specified and the object has tags, return a copy with all tags removed. Otherwise, the original `value` is returned.", "source_code": "def untag(value: _T, *tag_types: t.Type[AnsibleDatatagBase]) -> _T:\n    \"\"\"\n    If tags matching any of `tag_types` are present on `value`, return a copy with those tags removed.\n    If no `tag_types` are specified and the object has tags, return a copy with all tags removed.\n    Otherwise, the original `value` is returned.\n    \"\"\"\n    tag_set = AnsibleTagHelper.tags(value)\n\n    if not tag_set:\n        return value\n\n    if tag_types:\n        tags_mapping = _AnsibleTagsMapping((type(tag), tag) for tag in tag_set if type(tag) not in tag_types)  # pylint: disable=unidiomatic-typecheck\n\n        if len(tags_mapping) == len(tag_set):\n            return value  # if no tags were removed, return the original instance\n    else:\n        tags_mapping = None\n\n    if not tags_mapping:\n        if t.cast(AnsibleTaggedObject, value)._empty_tags_as_native:\n            return t.cast(AnsibleTaggedObject, value)._native_copy()\n\n        tags_mapping = _EMPTY_INTERNAL_TAGS_MAPPING\n\n    tagged_type = AnsibleTaggedObject._get_tagged_type(type(value))\n\n    return t.cast(_T, tagged_type._instance_factory(value, tags_mapping))", "loc": 28}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "tags", "parameters": ["value"], "param_types": {"value": "t.Any"}, "return_type": "t.FrozenSet[AnsibleDatatagBase]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_try_get_internal_tags_mapping", "frozenset", "tags.values"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def tags(value: t.Any) -> t.FrozenSet[AnsibleDatatagBase]:\n    tags = _try_get_internal_tags_mapping(value)\n\n    if tags is _EMPTY_INTERNAL_TAGS_MAPPING:\n        return _empty_frozenset\n\n    return frozenset(tags.values())", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "tag_types", "parameters": ["value"], "param_types": {"value": "t.Any"}, "return_type": "t.FrozenSet[t.Type[AnsibleDatatagBase]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_try_get_internal_tags_mapping", "frozenset"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def tag_types(value: t.Any) -> t.FrozenSet[t.Type[AnsibleDatatagBase]]:\n    tags = _try_get_internal_tags_mapping(value)\n\n    if tags is _EMPTY_INTERNAL_TAGS_MAPPING:\n        return _empty_frozenset\n\n    return frozenset(tags)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "base_type", "parameters": [], "param_types": {}, "return_type": "type", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "issubclass", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the friendly type of the given type or value. If the type is an AnsibleTaggedObject, the native type will be used.", "source_code": "def base_type(type_or_value: t.Any, /) -> type:\n    \"\"\"Return the friendly type of the given type or value. If the type is an AnsibleTaggedObject, the native type will be used.\"\"\"\n    if isinstance(type_or_value, type):\n        the_type = type_or_value\n    else:\n        the_type = type(type_or_value)\n\n    if issubclass(the_type, AnsibleTaggedObject):\n        the_type = type_or_value._native_type\n\n    # DTFIX-FUTURE: provide a knob to optionally report the real type for debugging purposes\n    return the_type", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "as_native_type", "parameters": ["value"], "param_types": {"value": "_T"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "value._native_copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def as_native_type(value: _T) -> _T:\n    \"\"\"\n    Returns an untagged native data type matching the input value, or the original input if the value was not a tagged type.\n    Containers are not recursively processed.\n    \"\"\"\n    if isinstance(value, AnsibleTaggedObject):\n        value = value._native_copy()\n\n    return value", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "tag_copy", "parameters": ["src", "value"], "param_types": {"src": "t.Any", "value": "_T"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag", "AnsibleTagHelper.tags", "AnsibleTagHelper.untag", "tag._get_tag_to_propagate", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a copy of `value`, with tags copied from `src`, overwriting any existing tags of the same types.", "source_code": "def tag_copy(src: t.Any, value: _T, *, value_type: t.Optional[type] = None) -> _T:\n    \"\"\"Return a copy of `value`, with tags copied from `src`, overwriting any existing tags of the same types.\"\"\"\n    src_tags = AnsibleTagHelper.tags(src)\n    value_tags = [(tag, tag._get_tag_to_propagate(src, value, value_type=value_type)) for tag in src_tags]\n    tags = [tag[1] for tag in value_tags if tag[1] is not None]\n    tag_types_to_remove = [type(tag[0]) for tag in value_tags if tag[1] is None]\n\n    if tag_types_to_remove:\n        value = AnsibleTagHelper.untag(value, *tag_types_to_remove)\n\n    return AnsibleTagHelper.tag(value, tags, value_type=value_type)", "loc": 11}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "tag", "parameters": ["value", "tags"], "param_types": {"value": "_T", "tags": "t.Union[AnsibleDatatagBase, t.Iterable[AnsibleDatatagBase]]"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTaggedObject._get_tagged_type", "TypeError", "_AnsibleTagsMapping", "_try_get_internal_tags_mapping", "chain", "enumerate", "existing_internal_tags_mapping.values", "list", "t.cast", "tagged_type._instance_factory", "type", "value_type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a copy of `value`, with `tags` applied, overwriting any existing tags of the same types. If `value` is an ignored type, or `tags` is empty, the original `value` will be returned. If `value` is not taggable, a `NotTaggableError` exception will be raised.", "source_code": "def tag(value: _T, tags: t.Union[AnsibleDatatagBase, t.Iterable[AnsibleDatatagBase]], *, value_type: t.Optional[type] = None) -> _T:\n    \"\"\"\n    Return a copy of `value`, with `tags` applied, overwriting any existing tags of the same types.\n    If `value` is an ignored type, or `tags` is empty, the original `value` will be returned.\n    If `value` is not taggable, a `NotTaggableError` exception will be raised.\n    If `value_type` was given, that type will be returned instead.\n    \"\"\"\n    if value_type is None:\n        value_type_specified = False\n        value_type = type(value)\n    else:\n        value_type_specified = True\n\n    # if no tags to apply, just return what we got\n    # NB: this only works because the untaggable types are singletons (and thus direct type comparison works)\n    if not tags or value_type in _untaggable_types:\n        if value_type_specified:\n            return value_type(value)\n\n        return value\n\n    tag_list: list[AnsibleDatatagBase]\n\n    # noinspection PyProtectedMember\n    if type(tags) in _known_tag_types:\n        tag_list = [tags]  # type: ignore[list-item]\n    else:\n        tag_list = list(tags)  # type: ignore[arg-type]\n\n        for idx, tag in enumerate(tag_list):\n            # noinspection PyProtectedMember\n            if type(tag) not in _known_tag_types:\n                # noinspection PyProtectedMember\n                raise TypeError(f'tags[{idx}] of type {type(tag)} is not one of {_known_tag_types}')\n\n    existing_internal_tags_mapping = _try_get_internal_tags_mapping(value)\n\n    if existing_internal_tags_mapping is not _EMPTY_INTERNAL_TAGS_MAPPING:\n        # include the existing tags first so new tags of the same type will overwrite\n        tag_list = list(chain(existing_internal_tags_mapping.values(), tag_list))\n\n    tags_mapping = _AnsibleTagsMapping((type(tag), tag) for tag in tag_list)\n    tagged_type = AnsibleTaggedObject._get_tagged_type(value_type)\n\n    return t.cast(_T, tagged_type._instance_factory(value, tags_mapping))", "loc": 45}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleTagHelper", "function_name": "try_tag", "parameters": ["value", "tags"], "param_types": {"value": "_T", "tags": "t.Union[AnsibleDatatagBase, t.Iterable[AnsibleDatatagBase]]"}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return a copy of `value`, with `tags` applied, overwriting any existing tags of the same types. If `value` is not taggable or `tags` is empty, the original `value` will be returned.", "source_code": "def try_tag(value: _T, tags: t.Union[AnsibleDatatagBase, t.Iterable[AnsibleDatatagBase]]) -> _T:\n    \"\"\"\n    Return a copy of `value`, with `tags` applied, overwriting any existing tags of the same types.\n    If `value` is not taggable or `tags` is empty, the original `value` will be returned.\n    \"\"\"\n    try:\n        return AnsibleTagHelper.tag(value, tags)\n    except NotTaggableError:\n        return value", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleDatatagBase", "function_name": "first_tagged_on", "parameters": ["cls"], "param_types": {}, "return_type": "t.Any | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.is_tagged_on"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return the first value which is tagged with this type, or None if no match is found.", "source_code": "def first_tagged_on(cls, *values: t.Any) -> t.Any | None:\n    \"\"\"Return the first value which is tagged with this type, or None if no match is found.\"\"\"\n    for value in values:\n        if cls.is_tagged_on(value):\n            return value\n\n    return None", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_datatag\\__init__.py", "class_name": "AnsibleDatatagBase", "function_name": "get_required_tag", "parameters": ["cls", "value"], "param_types": {"value": "t.Any"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "cls.get_tag", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_required_tag(cls, value: t.Any) -> t.Self:\n    if (tag := cls.get_tag(value)) is None:\n        # DTFIX-FUTURE: we really should have a way to use AnsibleError with obj in module_utils when it's controller-side\n        raise ValueError(f'The type {type(value).__name__!r} is not tagged with {cls.__name__!r}.')\n\n    return tag", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_legacy_encoder.py", "class_name": "LegacyTargetJSONEncoder", "function_name": "default", "parameters": ["self", "o"], "param_types": {"o": "object"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "o.decode", "super", "super().default", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def default(self, o: object) -> object:\n    if self._decode_bytes:\n        if type(o) is _profiles._WrappedValue:  # pylint: disable=unidiomatic-typecheck\n            o = o.wrapped\n\n        if isinstance(o, bytes):\n            return o.decode(errors='surrogateescape')  # backward compatibility with `ansible.module_utils.basic.jsonify`\n\n    return super().default(o)", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\__init__.py", "class_name": null, "function_name": "get_module_serialization_profile_name", "parameters": ["name", "controller_to_module"], "param_types": {"name": "str", "controller_to_module": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_module_serialization_profile_name(name: str, controller_to_module: bool) -> str:\n    if controller_to_module:\n        name = f'module_{name}_c2m'\n    else:\n        name = f'module_{name}_m2c'\n\n    return name", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\__init__.py", "class_name": null, "function_name": "get_serialization_module_name", "parameters": ["name"], "param_types": {"name": "str | types.ModuleType"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "ValueError", "importlib.util.find_spec", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_serialization_module_name(name: str | types.ModuleType) -> str:\n    if isinstance(name, str):\n        if '.' in name:\n            return name  # name is already fully qualified\n\n        target_name = f'{__name__}._profiles._{name}'\n    elif isinstance(name, types.ModuleType):\n        return name.__name__\n    else:\n        raise TypeError(f'Name is {type(name)} instead of {str} or {types.ModuleType}.')\n\n    if importlib.util.find_spec(target_name):\n        return target_name\n\n    # the value of is_controller can change after import; always pick it up from the module\n    if _internal.is_controller:\n        controller_name = f'ansible._internal._json._profiles._{name}'\n\n        if importlib.util.find_spec(controller_name):\n            return controller_name\n\n    raise ValueError(f'Unknown profile name {name!r}.')", "loc": 22}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\_fallback_to_str.py", "class_name": "_Profile", "function_name": "handle_key", "parameters": ["cls", "k"], "param_types": {"k": "_t.Any"}, "return_type": "_t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_dumps", "cls.default", "cls.serialize_map.get", "isinstance", "mapped_callable", "type"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_key(cls, k: _t.Any) -> _t.Any:\n    while mapped_callable := cls.serialize_map.get(type(k)):\n        k = mapped_callable(k)\n\n    k = cls.default(k)\n\n    if not isinstance(k, str):\n        k = _dumps(k, cls=Encoder)\n\n    return k", "loc": 10}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\_fallback_to_str.py", "class_name": "_Profile", "function_name": "last_chance", "parameters": ["cls", "o"], "param_types": {"o": "_t.Any"}, "return_type": "_t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["str"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def last_chance(cls, o: _t.Any) -> _t.Any:\n    try:\n        return str(o)\n    except Exception as ex:\n        return str(ex)", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "_JSONSerializationProfile", "function_name": "deserialize_serializable", "parameters": ["cls", "value"], "param_types": {"value": "dict[str, t.Any]"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleSerializable._deserialize", "cls.cannot_deserialize_error"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deserialize_serializable(cls, value: dict[str, t.Any]) -> object:\n    type_key = value[AnsibleSerializable._TYPE_KEY]\n\n    if type_key not in cls._allowed_type_keys:\n        cls.cannot_deserialize_error(type_key)\n\n    return AnsibleSerializable._deserialize(value)", "loc": 7}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "_JSONSerializationProfile", "function_name": "maybe_wrap", "parameters": ["cls", "o"], "param_types": {"o": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_WrappedValue", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def maybe_wrap(cls, o: t.Any) -> t.Any:\n    if type(o) in cls._unwrapped_json_types:\n        return o\n\n    return _WrappedValue(o)", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "_JSONSerializationProfile", "function_name": "handle_key", "parameters": ["cls", "k"], "param_types": {"k": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validation/conversion hook before a dict key is serialized. The default implementation only accepts str-typed keys.", "source_code": "def handle_key(cls, k: t.Any) -> t.Any:\n    \"\"\"Validation/conversion hook before a dict key is serialized. The default implementation only accepts str-typed keys.\"\"\"\n    # NOTE: Since JSON requires string keys, there is no support for preserving tags on dictionary keys during serialization.\n\n    if not isinstance(k, str):  # DTFIX-FUTURE: optimize this to use all known str-derived types in type map / allowed types\n        raise TypeError(f'Key of type {type(k).__name__!r} is not JSON serializable by the {cls.profile_name!r} profile.')\n\n    return k", "loc": 8}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "_JSONSerializationProfile", "function_name": "default", "parameters": ["cls", "o"], "param_types": {"o": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_internal.is_intermediate_iterable", "_internal.is_intermediate_mapping", "cls.handle_key", "cls.last_chance", "cls.maybe_wrap", "isinstance", "o.items"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def default(cls, o: t.Any) -> t.Any:\n    # Preserve the built-in JSON encoder support for subclasses of scalar types.\n\n    if isinstance(o, _json_subclassable_scalar_types):\n        return o\n\n    # Preserve the built-in JSON encoder support for subclasses of dict and list.\n    # Additionally, add universal support for mappings and sequences/sets by converting them to dict and list, respectively.\n\n    if _internal.is_intermediate_mapping(o):\n        return {cls.handle_key(k): cls.maybe_wrap(v) for k, v in o.items()}\n\n    if _internal.is_intermediate_iterable(o):\n        return [cls.maybe_wrap(v) for v in o]\n\n    return cls.last_chance(o)", "loc": 16}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "_JSONSerializationProfile", "function_name": "last_chance", "parameters": ["cls", "o"], "param_types": {"o": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.cannot_serialize_error", "isinstance", "o.trip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def last_chance(cls, o: t.Any) -> t.Any:\n    if isinstance(o, Tripwire):\n        o.trip()\n\n    cls.cannot_serialize_error(o)", "loc": 5}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "AnsibleProfileJSONEncoder", "function_name": "default", "parameters": ["self", "o"], "param_types": {"o": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mapped_callable", "o.items", "self._profile.default", "self._profile.handle_key", "self._profile.maybe_wrap", "self._profile.serialize_map.get", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def default(self, o: t.Any) -> t.Any:\n    o_type = type(o)\n\n    if o_type is _WrappedValue:  # pylint: disable=unidiomatic-typecheck\n        o = o.wrapped\n        o_type = type(o)\n\n    if mapped_callable := self._profile.serialize_map.get(o_type):\n        return self._profile.maybe_wrap(mapped_callable(o))\n\n    # This is our last chance to intercept the values in containers, so they must be wrapped here.\n    # Only containers natively understood by the built-in JSONEncoder are recognized, since any other container types must be present in serialize_map.\n\n    if o_type is dict:  # pylint: disable=unidiomatic-typecheck\n        return {self._profile.handle_key(k): self._profile.maybe_wrap(v) for k, v in o.items()}\n\n    if o_type is list or o_type is tuple:  # pylint: disable=unidiomatic-typecheck\n        return [self._profile.maybe_wrap(v) for v in o]  # JSONEncoder converts tuple to a list, so just make it a list now\n\n    # Any value here is a type not explicitly handled by this encoder.\n    # The profile default handler is responsible for generating an error or converting the value to a supported type.\n\n    return self._profile.default(o)", "loc": 23}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "AnsibleProfileJSONDecoder", "function_name": "raw_decode", "parameters": ["self", "s", "idx"], "param_types": {"s": "str", "idx": "int"}, "return_type": "tuple[t.Any, int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_create_encoding_check_error", "_recursively_check_string_encoding", "_string_encoding_check_enabled", "self._profile.post_deserialize", "super", "super().raw_decode"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def raw_decode(self, s: str, idx: int = 0) -> tuple[t.Any, int]:\n    obj, end = super().raw_decode(s, idx)\n\n    if _string_encoding_check_enabled():\n        try:\n            _recursively_check_string_encoding(obj)\n        except UnicodeEncodeError as ex:\n            raise _create_encoding_check_error() from ex\n\n    obj = self._profile.post_deserialize(self, obj)\n\n    return obj, end", "loc": 12}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_json\\_profiles\\__init__.py", "class_name": "AnsibleProfileJSONDecoder", "function_name": "object_hook", "parameters": ["self", "pairs"], "param_types": {"pairs": "dict[str, object]"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_create_encoding_check_error", "_recursively_check_string_encoding", "_string_encoding_check_enabled", "key.encode", "mapped_callable", "pairs.items", "self._profile.deserialize_map.items"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def object_hook(self, pairs: dict[str, object]) -> object:\n    if _string_encoding_check_enabled():\n        try:\n            for key, value in pairs.items():\n                key.encode()\n                _recursively_check_string_encoding(value)\n        except UnicodeEncodeError as ex:\n            raise _create_encoding_check_error() from ex\n\n    for mapped_key, mapped_callable in self._profile.deserialize_map.items():\n        if mapped_key in pairs:\n            return mapped_callable(pairs)\n\n    return pairs", "loc": 14}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_patches\\_dataclass_annotation_patch.py", "class_name": "DataclassesIsTypePatch", "function_name": "is_patch_needed", "parameters": ["cls"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dataclasses.fields", "len"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_patch_needed(cls) -> bool:\n    @dataclasses.dataclass\n    class CheckClassVar:\n        # this is the broken case requiring patching: ClassVar dot-referenced from a module that is not `typing` is treated as an instance field\n        # DTFIX-FUTURE: file/link CPython bug report, deprecate this patch if/when it's fixed in CPython\n        a_classvar: _ts.ClassVar[int]  # type: ignore[name-defined]\n        a_field: int\n\n    return len(dataclasses.fields(CheckClassVar)) != 1", "loc": 9}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_patches\\_socket_patch.py", "class_name": "GetAddrInfoPatch", "function_name": "is_patch_needed", "parameters": ["cls"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CustomInt", "contextlib.suppress", "socket.getaddrinfo"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_patch_needed(cls) -> bool:\n    with contextlib.suppress(OSError):\n        socket.getaddrinfo('127.0.0.1', _CustomInt(22))\n        return False\n\n    return True", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_patches\\_sys_intern_patch.py", "class_name": "SysInternPatch", "function_name": "is_patch_needed", "parameters": ["cls"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CustomStr", "contextlib.suppress", "sys.intern"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_patch_needed(cls) -> bool:\n    with contextlib.suppress(TypeError):\n        sys.intern(_CustomStr(\"x\"))\n        return False\n\n    return True", "loc": 6}
{"file": "ansible\\lib\\ansible\\module_utils\\_internal\\_patches\\__init__.py", "class_name": "CallablePatch", "function_name": "patch", "parameters": ["cls"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "cls", "cls.get_current_implementation", "cls.is_patch_needed", "cls.is_patched", "setattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Idempotently apply this patch (if needed).", "source_code": "def patch(cls) -> None:\n    \"\"\"Idempotently apply this patch (if needed).\"\"\"\n    if cls.is_patched():\n        return\n\n    cls.unpatched_implementation = cls.get_current_implementation()\n\n    if not cls.is_patch_needed():\n        return\n\n    # __call__ requires an instance (otherwise it'll be __new__)\n    setattr(cls.target_container, cls.target_attribute, cls())\n\n    if not cls.is_patch_needed():\n        return\n\n    setattr(cls.target_container, cls.target_attribute, cls.unpatched_implementation)\n\n    raise RuntimeError(f\"Validation of '{cls.target_container.__name__}.{cls.target_attribute}' failed after patching.\")", "loc": 19}
{"file": "ansible\\lib\\ansible\\parsing\\dataloader.py", "class_name": "DataLoader", "function_name": "load", "parameters": ["self", "data", "file_name", "show_content", "json_only"], "param_types": {"data": "str", "file_name": "str | None", "show_content": "bool", "json_only": "bool"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_error_utils.RedactAnnotatedSourceContext.when", "from_yaml"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Backwards compat for now", "source_code": "def load(\n        self,\n        data: str,\n        file_name: str | None = None,  # DTFIX-FUTURE: consider deprecating this in favor of tagging Origin on data\n        show_content: bool = True,  # DTFIX-FUTURE: consider future deprecation, but would need RedactAnnotatedSourceContext public\n        json_only: bool = False,\n) -> t.Any:\n    \"\"\"Backwards compat for now\"\"\"\n    with _error_utils.RedactAnnotatedSourceContext.when(not show_content):\n        return from_yaml(data=data, file_name=file_name, json_only=json_only)", "loc": 10}
{"file": "ansible\\lib\\ansible\\parsing\\dataloader.py", "class_name": "DataLoader", "function_name": "load_from_file", "parameters": ["self", "file_name", "cache", "unsafe", "json_only", "trusted_as_template"], "param_types": {"file_name": "str", "cache": "str", "unsafe": "bool", "json_only": "bool", "trusted_as_template": "bool"}, "return_type": "t.Any", "param_doc": {"file_name": "The name of the file to load data from.", "cache": "Options for caching: none|all|vaulted", "unsafe": "If True, returns the parsed data as-is without deep copying.", "json_only": "If True, only loads JSON data from the file."}, "return_doc": "The loaded data, optionally deep-copied for safety.", "raises_doc": [], "called_functions": ["SourceWasEncrypted", "SourceWasEncrypted().tag", "SourceWasEncrypted.is_tagged_on", "TrustedAsTemplate", "TrustedAsTemplate().tag", "copy.deepcopy", "display.debug", "self.get_text_file_contents", "self.load", "self.path_dwim"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Loads data from a file, which can contain either JSON or YAML.", "source_code": "def load_from_file(self, file_name: str, cache: str = 'all', unsafe: bool = False, json_only: bool = False, trusted_as_template: bool = False) -> t.Any:\n    \"\"\"\n    Loads data from a file, which can contain either JSON or YAML.\n\n    :param file_name: The name of the file to load data from.\n    :param cache: Options for caching: none|all|vaulted\n    :param unsafe: If True, returns the parsed data as-is without deep copying.\n    :param json_only: If True, only loads JSON data from the file.\n    :return: The loaded data, optionally deep-copied for safety.\n    \"\"\"\n\n    # Resolve the file name\n    file_name = self.path_dwim(file_name)\n\n    # Log the file being loaded\n    display.debug(\"Loading data from %s\" % file_name)\n\n    # Check if the file has been cached and use the cached data if available\n    if cache != 'none' and file_name in self._FILE_CACHE:\n        parsed_data = self._FILE_CACHE[file_name]\n    else:\n        file_data = self.get_text_file_contents(file_name)\n\n        if trusted_as_template:\n            file_data = TrustedAsTemplate().tag(file_data)\n\n        parsed_data = self.load(data=file_data, file_name=file_name, json_only=json_only)\n\n        # only tagging the container, used by include_vars to determine if vars should be shown or not\n        # this is a temporary measure until a proper data senitivity system is in place\n        if SourceWasEncrypted.is_tagged_on(file_data):\n            parsed_data = SourceWasEncrypted().tag(parsed_data)\n\n        # Cache the file contents for next time based on the cache option\n        if cache == 'all':\n            self._FILE_CACHE[file_name] = parsed_data\n        elif cache == 'vaulted' and SourceWasEncrypted.is_tagged_on(file_data):\n            self._FILE_CACHE[file_name] = parsed_data\n\n    # Return the parsed data, optionally deep-copied for safety\n    if unsafe:\n        return parsed_data\n    else:\n        return copy.deepcopy(parsed_data)", "loc": 44}
{"file": "ansible\\lib\\ansible\\parsing\\dataloader.py", "class_name": "DataLoader", "function_name": "get_text_file_contents", "parameters": ["self", "file_name", "encoding"], "param_types": {"file_name": "str", "encoding": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag_copy", "SourceWasEncrypted", "SourceWasEncrypted().tag", "bytes_content.decode", "display.deprecated", "self._get_file_contents"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_text_file_contents(self, file_name: str, encoding: str | None = None) -> str:\n    \"\"\"\n    Returns an `Origin` tagged string with the content of the specified (DWIM-expanded for relative) file path, decrypting if necessary.\n    Callers must only specify `encoding` when the user can configure it, as error messages in that case will imply configurability.\n    If `encoding` is not specified, UTF-8 will be used.\n    \"\"\"\n    bytes_content, source_was_plaintext = self._get_file_contents(file_name)\n\n    if encoding is None:\n        encoding = 'utf-8'\n        help_text = 'This file must be UTF-8 encoded.'\n    else:\n        help_text = 'Ensure the correct encoding was specified.'\n\n    try:\n        str_content = bytes_content.decode(encoding=encoding, errors='strict')\n    except UnicodeDecodeError:\n        str_content = bytes_content.decode(encoding=encoding, errors='surrogateescape')\n\n        display.deprecated(\n            msg=f\"File {file_name!r} could not be decoded as {encoding!r}. Invalid content has been escaped.\",\n            version=\"2.23\",\n            # obj intentionally omitted since there's no value in showing its contents\n            help_text=help_text,\n        )\n\n    if not source_was_plaintext:\n        str_content = SourceWasEncrypted().tag(str_content)\n\n    return AnsibleTagHelper.tag_copy(bytes_content, str_content)", "loc": 30}
{"file": "ansible\\lib\\ansible\\parsing\\dataloader.py", "class_name": "DataLoader", "function_name": "cleanup_tmp_file", "parameters": ["self", "file_path"], "param_types": {"file_path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.unlink", "self._tempfiles.remove"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Removes any temporary files created from a previous call to get_real_file. file_path must be the path returned from a previous call to get_real_file.", "source_code": "def cleanup_tmp_file(self, file_path: str) -> None:\n    \"\"\"\n    Removes any temporary files created from a previous call to\n    get_real_file. file_path must be the path returned from a\n    previous call to get_real_file.\n    \"\"\"\n    if file_path in self._tempfiles:\n        os.unlink(file_path)\n        self._tempfiles.remove(file_path)", "loc": 9}
{"file": "ansible\\lib\\ansible\\parsing\\dataloader.py", "class_name": "DataLoader", "function_name": "cleanup_all_tmp_files", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.warning", "list", "self.cleanup_tmp_file", "to_text"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Removes all temporary files that DataLoader has created NOTE: not thread safe, forks also need special handling see __init__ for details.", "source_code": "def cleanup_all_tmp_files(self) -> None:\n    \"\"\"\n    Removes all temporary files that DataLoader has created\n    NOTE: not thread safe, forks also need special handling see __init__ for details.\n    \"\"\"\n    for f in list(self._tempfiles):\n        try:\n            self.cleanup_tmp_file(f)\n        except Exception as e:\n            display.warning(\"Unable to cleanup temp files: %s\" % to_text(e))", "loc": 10}
{"file": "ansible\\lib\\ansible\\parsing\\mod_args.py", "class_name": "ModuleArgsParser", "function_name": "parse", "parameters": ["self", "skip_action_validation"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"couldn't resolve module/action '{0}'. This often indicates a misspelling, missing collection, or incorrect module path.\".format", "AnsibleParserError", "_get_action_context", "bool", "dict", "k.startswith", "list", "non_task_ds.items", "non_task_ds.keys", "self._normalize_parameters", "self._task_ds.get", "self._task_ds.items"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Given a task in one of the supported forms, parses and returns returns the action, arguments, and delegate_to values for the task, dealing with all sorts of levels of fuzziness.", "source_code": "def parse(self, skip_action_validation=False):\n    \"\"\"\n    Given a task in one of the supported forms, parses and returns\n    returns the action, arguments, and delegate_to values for the\n    task, dealing with all sorts of levels of fuzziness.\n    \"\"\"\n\n    action = None\n    delegate_to = self._task_ds.get('delegate_to', Sentinel)\n    args = dict()\n\n    # This is the standard YAML form for command-type modules. We grab\n    # the args and pass them in as additional arguments, which can/will\n    # be overwritten via dict updates from the other arg sources below\n    additional_args = self._task_ds.get('args', Sentinel)\n\n    # We can have one of action, local_action, or module specified\n    # action\n    if 'action' in self._task_ds:\n        # an old school 'action' statement\n        thing = self._task_ds['action']\n        action, args = self._normalize_parameters(thing, additional_args=additional_args)\n\n    # local_action\n    if 'local_action' in self._task_ds:\n        # local_action is similar but also implies a delegate_to\n        if action is not None:\n            raise AnsibleParserError(\"action and local_action are mutually exclusive\", obj=self._task_ds)\n        thing = self._task_ds.get('local_action', '')\n        delegate_to = 'localhost'\n        action, args = self._normalize_parameters(thing, additional_args=additional_args)\n\n    # module: <stuff> is the more new-style invocation\n\n    # filter out task attributes so we're only querying unrecognized keys as actions/modules\n    non_task_ds = dict((k, v) for k, v in self._task_ds.items() if (k not in self._task_attrs) and (not k.startswith('with_')))\n\n    # walk the filtered input dictionary to see if we recognize a module name\n    for item, value in non_task_ds.items():\n        if item in BUILTIN_TASKS:\n            is_action_candidate = True\n        elif skip_action_validation:\n            is_action_candidate = True\n        else:\n            try:\n                # DTFIX-FUTURE: extract to a helper method, shared with Task.post_validate_args\n                context = _get_action_context(item, self._collection_list)\n            except AnsibleError as e:\n                if e.obj is None:\n                    e.obj = self._task_ds\n                raise e\n\n            is_action_candidate = context.resolved and bool(context.redirect_list)\n            if is_action_candidate:\n                self._resolved_action = context.resolved_fqcn\n\n        if is_action_candidate:\n            # finding more than one module name is a problem\n            if action is not None:\n                raise AnsibleParserError(\"conflicting action statements: %s, %s\" % (action, item), obj=self._task_ds)\n\n            action = item\n            thing = value\n            action, args = self._normalize_parameters(thing, action=action, additional_args=additional_args)\n\n    # if we didn't see any module in the task at all, it's not a task really\n    if action is None:\n        if non_task_ds:  # there was one non-task action, but we couldn't find it\n            bad_action = list(non_task_ds.keys())[0]\n            raise AnsibleParserError(\"couldn't resolve module/action '{0}'. This often indicates a \"\n                                     \"misspelling, missing collection, or incorrect module path.\".format(bad_action),\n                                     obj=self._task_ds)\n        else:\n            raise AnsibleParserError(\"no module/action detected in task.\",\n                                     obj=self._task_ds)\n\n    return action, args, delegate_to", "loc": 77}
{"file": "ansible\\lib\\ansible\\parsing\\plugin_docs.py", "class_name": null, "function_name": "read_docstring_from_yaml_file", "parameters": ["filename", "verbose", "ignore_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "_init_doc_dict", "display.error", "file_data.get", "open", "yaml.load"], "control_structures": ["For", "If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Read docs from 'sidecar' yaml file doc for a plugin", "source_code": "def read_docstring_from_yaml_file(filename, verbose=True, ignore_errors=True):\n    \"\"\" Read docs from 'sidecar' yaml file doc for a plugin \"\"\"\n\n    data = _init_doc_dict()\n    file_data = {}\n\n    try:\n        with open(filename, 'rb') as yamlfile:\n            file_data = yaml.load(yamlfile, Loader=AnsibleLoader)\n    except Exception as ex:\n        msg = f\"Unable to parse yaml file {filename}\"\n        # DTFIX-FUTURE: find a better pattern for this (can we use the new optional error behavior?)\n        if not ignore_errors:\n            raise AnsibleParserError(f'{msg}.') from ex\n        elif verbose:\n            display.error(f'{msg}: {ex}')\n\n    if file_data:\n        for key in string_to_vars:\n            data[string_to_vars[key]] = file_data.get(key, None)\n\n    return data", "loc": 22}
{"file": "ansible\\lib\\ansible\\parsing\\plugin_docs.py", "class_name": null, "function_name": "read_docstring_from_python_file", "parameters": ["filename", "verbose", "ignore_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "_init_doc_dict", "_tags.Origin", "_tags.Origin(path=filename, line_num=child.value.lineno).tag", "ast.literal_eval", "ast.parse", "b_module_data.read", "display.debug", "display.error", "display.warning", "isinstance", "open", "to_text", "yaml.load"], "control_structures": ["For", "If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Use ast to search for assignment of the DOCUMENTATION and EXAMPLES variables in the given file. Parse DOCUMENTATION from YAML and return the YAML doc or None together with EXAMPLES, as plain text.", "source_code": "def read_docstring_from_python_file(filename, verbose=True, ignore_errors=True):\n    \"\"\"\n    Use ast to search for assignment of the DOCUMENTATION and EXAMPLES variables in the given file.\n    Parse DOCUMENTATION from YAML and return the YAML doc or None together with EXAMPLES, as plain text.\n    \"\"\"\n    data = _init_doc_dict()\n\n    try:\n        with open(filename, 'rb') as b_module_data:\n            M = ast.parse(b_module_data.read())\n\n        for child in M.body:\n            if isinstance(child, ast.Assign):\n                for t in child.targets:\n                    try:\n                        theid = t.id\n                    except AttributeError:\n                        # skip errors can happen when trying to use the normal code\n                        display.warning(\"Building documentation, failed to assign id for %s on %s, skipping\" % (t, filename))\n                        continue\n\n                    if theid in string_to_vars:\n                        varkey = string_to_vars[theid]\n                        if isinstance(child.value, ast.Dict):\n                            data[varkey] = ast.literal_eval(child.value)\n                        else:\n                            if theid == 'EXAMPLES':\n                                # examples 'can' be yaml, but even if so, we dont want to parse as such here\n                                # as it can create undesired 'objects' that don't display well as docs.\n                                data[varkey] = to_text(child.value.value)\n                            else:\n                                # string should be yaml if already not a dict\n                                child_value = _tags.Origin(path=filename, line_num=child.value.lineno).tag(child.value.value)\n                                data[varkey] = yaml.load(child_value, Loader=AnsibleLoader)\n\n                        display.debug('Documentation assigned: %s' % varkey)\n\n    except Exception as ex:\n        msg = f\"Unable to parse documentation in python file {filename!r}\"\n        # DTFIX-FUTURE: better pattern to conditionally raise/display\n        if not ignore_errors:\n            raise AnsibleParserError(f'{msg}.') from ex\n        elif verbose:\n            display.error(f'{msg}: {ex}.')\n\n    return data", "loc": 46}
{"file": "ansible\\lib\\ansible\\parsing\\plugin_docs.py", "class_name": null, "function_name": "read_docstring", "parameters": ["filename", "verbose", "ignore_errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "filename.endswith", "read_docstring_from_python_file", "read_docstring_from_yaml_file", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "returns a documentation dictionary from Ansible plugin docstrings", "source_code": "def read_docstring(filename, verbose=True, ignore_errors=True):\n    \"\"\" returns a documentation dictionary from Ansible plugin docstrings \"\"\"\n\n    # NOTE: adjacency of doc file to code file is responsibility of caller\n    if filename.endswith(C.YAML_DOC_EXTENSIONS):\n        docstring = read_docstring_from_yaml_file(filename, verbose=verbose, ignore_errors=ignore_errors)\n    elif filename.endswith(C.PYTHON_DOC_EXTENSIONS):\n        docstring = read_docstring_from_python_file(filename, verbose=verbose, ignore_errors=ignore_errors)\n    elif not ignore_errors:\n        raise AnsibleError(\"Unknown documentation format: %s\" % to_native(filename))\n\n    if not docstring and not ignore_errors:\n        raise AnsibleError(\"Unable to parse documentation for: %s\" % to_native(filename))\n\n    # cause seealso is specially processed from 'doc' later on\n    # TODO: stop any other 'overloaded' implementation in main doc\n    docstring['seealso'] = None\n\n    return docstring", "loc": 19}
{"file": "ansible\\lib\\ansible\\parsing\\plugin_docs.py", "class_name": null, "function_name": "read_docstub", "parameters": ["filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "''.join(doc_stub).strip", "''.join(doc_stub).strip().rstrip", "_tags.Origin", "_tags.Origin(path=str(filename)).tag", "doc_stub.append", "len", "line.lstrip", "line.lstrip().startswith", "line.startswith", "open", "str", "yaml.load"], "control_structures": ["For", "If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Quickly find short_description using string methods instead of node parsing. This does not return a full set of documentation strings and is intended for operations like ansible-doc -l.", "source_code": "def read_docstub(filename):\n    \"\"\"\n    Quickly find short_description using string methods instead of node parsing.\n    This does not return a full set of documentation strings and is intended for\n    operations like ansible-doc -l.\n    \"\"\"\n\n    in_documentation = False\n    capturing = False\n    indent_detection = ''\n    doc_stub = []\n\n    with open(filename, 'r') as t_module_data:\n        for line in t_module_data:\n            if in_documentation:\n                # start capturing the stub until indentation returns\n                if capturing and line.startswith(indent_detection):\n                    doc_stub.append(line)\n\n                elif capturing and not line.startswith(indent_detection):\n                    break\n\n                elif line.lstrip().startswith('short_description:'):\n                    capturing = True\n                    # Detect that the short_description continues on the next line if it's indented more\n                    # than short_description itself.\n                    indent_detection = ' ' * (len(line) - len(line.lstrip()) + 1)\n                    doc_stub.append(line)\n\n            elif line.startswith('DOCUMENTATION') and ('=' in line or ':' in line):\n                in_documentation = True\n\n    short_description = r''.join(doc_stub).strip().rstrip('.')\n    data = yaml.load(_tags.Origin(path=str(filename)).tag(short_description), Loader=AnsibleLoader)\n\n    return data", "loc": 36}
{"file": "ansible\\lib\\ansible\\parsing\\quoting.py", "class_name": null, "function_name": "unquote", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_quoted"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "removes first and last quotes from a string, if the string starts and ends with the same quotes", "source_code": "def unquote(data):\n    \"\"\" removes first and last quotes from a string, if the string starts and ends with the same quotes \"\"\"\n    if is_quoted(data):\n        return data[1:-1]\n    return data", "loc": 5}
{"file": "ansible\\lib\\ansible\\parsing\\splitter.py", "class_name": null, "function_name": "parse_kv", "parameters": ["args", "check_raw"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag", "Origin.get_tag", "TrustedAsTemplate.get_tag", "_decode_escapes", "join_args", "k.strip", "len", "options.items", "origin_tag.tag", "raw_params.append", "split_args", "tags.append", "to_text", "unquote", "v.strip", "x.index", "x.replace"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Convert a string of key/value items to a dict. If any free-form params are found and the check_raw option is set to True, they will be added to a new parameter called '_raw_params'. If check_raw is not enabled,", "source_code": "def parse_kv(args, check_raw=False):\n    \"\"\"\n    Convert a string of key/value items to a dict. If any free-form params\n    are found and the check_raw option is set to True, they will be added\n    to a new parameter called '_raw_params'. If check_raw is not enabled,\n    they will simply be ignored.\n    \"\"\"\n\n    tags = []\n    if origin_tag := Origin.get_tag(args):\n        # NB: adjusting the column number is left as an exercise for the reader\n        tags.append(origin_tag)\n    if trusted_tag := TrustedAsTemplate.get_tag(args):\n        tags.append(trusted_tag)\n\n    args = to_text(args, nonstring='passthru')\n\n    options = {}\n    if args is not None:\n        vargs = split_args(args)\n\n        raw_params = []\n        for orig_x in vargs:\n            x = _decode_escapes(orig_x)\n            if \"=\" in x:\n                pos = 0\n                try:\n                    while True:\n                        pos = x.index('=', pos + 1)\n                        if pos > 0 and x[pos - 1] != '\\\\':\n                            break\n                except ValueError:\n                    # ran out of string, but we must have some escaped equals,\n                    # so replace those and append this to the list of raw params\n                    raw_params.append(x.replace('\\\\=', '='))\n                    continue\n\n                k = x[:pos]\n                v = x[pos + 1:]\n\n                # FIXME: make the retrieval of this list of shell/command options a function, so the list is centralized\n                if check_raw and k not in ('creates', 'removes', 'chdir', 'executable', 'warn', 'stdin', 'stdin_add_newline', 'strip_empty_ends'):\n                    raw_params.append(orig_x)\n                else:\n                    options[k.strip()] = unquote(v.strip())\n            else:\n                raw_params.append(orig_x)\n\n        # recombine the free-form params, if any were found, and assign\n        # them to a special option for use later by the shell/command module\n        if len(raw_params) > 0:\n            options[u'_raw_params'] = join_args(raw_params)\n\n    if tags:\n        options = {AnsibleTagHelper.tag(k, tags): AnsibleTagHelper.tag(v, tags) for k, v in options.items()}\n\n    if origin_tag:\n        options = origin_tag.tag(options)\n\n    return options", "loc": 60}
{"file": "ansible\\lib\\ansible\\parsing\\splitter.py", "class_name": null, "function_name": "join_args", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "result.endswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Join the original cmd based on manipulations by split_args(). This retains the original newlines and whitespaces.", "source_code": "def join_args(s):\n    \"\"\"\n    Join the original cmd based on manipulations by split_args().\n    This retains the original newlines and whitespaces.\n    \"\"\"\n    result = ''\n    for p in s:\n        if len(result) == 0 or result.endswith('\\n'):\n            result += p\n        else:\n            result += ' ' + p\n    return result", "loc": 12}
{"file": "ansible\\lib\\ansible\\parsing\\utils\\addresses.py", "class_name": null, "function_name": "parse_address", "parameters": ["address", "allow_ranges"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleParserError", "int", "m.groups", "patterns[matching].match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Takes a string and returns a (host, port) tuple. If the host is None, then the string could not be parsed as a host identifier with an optional port specification. If the port is None, then no port was specified.", "source_code": "def parse_address(address, allow_ranges=False):\n    \"\"\"\n    Takes a string and returns a (host, port) tuple. If the host is None, then\n    the string could not be parsed as a host identifier with an optional port\n    specification. If the port is None, then no port was specified.\n\n    The host identifier may be a hostname (qualified or not), an IPv4 address,\n    or an IPv6 address. If allow_ranges is True, then any of those may contain\n    [x:y] range specifications, e.g. foo[1:3] or foo[0:5]-bar[x-z].\n\n    The port number is an optional :NN suffix on an IPv4 address or host name,\n    or a mandatory :NN suffix on any square-bracketed expression: IPv6 address,\n    IPv4 address, or host name. (This means the only way to specify a port for\n    an IPv6 address is to enclose it in square brackets.)\n    \"\"\"\n\n    # First, we extract the port number if one is specified.\n\n    port = None\n    for matching in ['bracketed_hostport', 'hostport']:\n        m = patterns[matching].match(address)\n        if m:\n            (address, port) = m.groups()\n            port = int(port)\n            continue\n\n    # What we're left with now must be an IPv4 or IPv6 address, possibly with\n    # numeric ranges, or a hostname with alphanumeric ranges.\n\n    host = None\n    for matching in ['ipv4', 'ipv6', 'hostname']:\n        m = patterns[matching].match(address)\n        if m:\n            host = address\n            continue\n\n    # If it isn't any of the above, we don't understand it.\n    if not host:\n        raise AnsibleError(\"Not a valid network hostname: %s\" % address)\n\n    # If we get to this point, we know that any included ranges are valid.\n    # If the caller is prepared to handle them, all is well.\n    # Otherwise we treat it as a parse failure.\n    if not allow_ranges and '[' in host:\n        raise AnsibleParserError(\"Detected range in host but was asked to ignore ranges\")\n\n    return (host, port)", "loc": 47}
{"file": "ansible\\lib\\ansible\\parsing\\utils\\jsonify.py", "class_name": null, "function_name": "jsonify", "parameters": ["result", "format"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Format JSON output.", "source_code": "def jsonify(result, format=False):\n    \"\"\"Format JSON output.\"\"\"\n\n    if result is None:\n        return \"{}\"\n\n    indent = None\n    if format:\n        indent = 4\n\n    try:\n        return json.dumps(result, sort_keys=True, indent=indent, ensure_ascii=False)\n    except UnicodeDecodeError:\n        return json.dumps(result, sort_keys=True, indent=indent)", "loc": 14}
{"file": "ansible\\lib\\ansible\\parsing\\utils\\yaml.py", "class_name": null, "function_name": "from_yaml", "parameters": ["data", "file_name", "show_content", "vault_secrets", "json_only"], "param_types": {"data": "str", "file_name": "str | None", "show_content": "bool", "vault_secrets": "list[tuple[str, VaultSecret]] | None", "json_only": "bool"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleJSONParserError.handle_exception", "AnsibleYAMLParserError.handle_exception", "Origin.get_or_create_tag", "_error_utils.RedactAnnotatedSourceContext.when", "json.loads", "origin.tag", "yaml.load"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Creates a Python data structure from the given data, which can be either a JSON or YAML string.", "source_code": "def from_yaml(\n    data: str,\n    file_name: str | None = None,\n    show_content: bool = True,\n    vault_secrets: list[tuple[str, VaultSecret]] | None = None,  # deprecated: description='Deprecate vault_secrets, it has no effect.' core_version='2.23'\n    json_only: bool = False,\n) -> t.Any:\n    \"\"\"Creates a Python data structure from the given data, which can be either a JSON or YAML string.\"\"\"\n    # FUTURE: provide Ansible-specific top-level APIs to expose JSON and YAML serialization/deserialization to hide the error handling logic\n    #         once those are in place, defer deprecate this entire function\n\n    origin = Origin.get_or_create_tag(data, file_name)\n\n    data = origin.tag(data)\n\n    with _error_utils.RedactAnnotatedSourceContext.when(not show_content):\n        try:\n            # we first try to load this data as JSON.\n            # Fixes issues with extra vars json strings not being parsed correctly by the yaml parser\n            return json.loads(data, cls=_legacy.Decoder)\n        except Exception as ex:\n            json_ex = ex\n\n        if json_only:\n            AnsibleJSONParserError.handle_exception(json_ex, origin=origin)\n\n        try:\n            return yaml.load(data, Loader=AnsibleLoader)  # type: ignore[arg-type]\n        except Exception as yaml_ex:\n            # DTFIX-FUTURE: how can we indicate in Origin that the data is in-memory only, to support context information -- is that useful?\n            #        we'd need to pass data to handle_exception so it could be used as the content instead of reading from disk\n            AnsibleYAMLParserError.handle_exception(yaml_ex, origin=origin)", "loc": 32}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "is_encrypted", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "True if it is recognized.  Otherwise, False.", "raises_doc": [], "called_functions": ["b_data.startswith", "to_bytes", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Test if this is vault encrypted data blob :arg data: a byte or text string to test whether it is recognized as vault encrypted data", "source_code": "def is_encrypted(data):\n    \"\"\" Test if this is vault encrypted data blob\n\n    :arg data: a byte or text string to test whether it is recognized as vault\n        encrypted data\n    :returns: True if it is recognized.  Otherwise, False.\n    \"\"\"\n    try:\n        # Make sure we have a byte string and that it only contains ascii\n        # bytes.\n        b_data = to_bytes(to_text(data, encoding='ascii', errors='strict', nonstring='strict'), encoding='ascii', errors='strict')\n    except (UnicodeError, TypeError):\n        # The vault format is pure ascii so if we failed to encode to bytes\n        # via ascii we know that this is not vault data.\n        # Similarly, if it's not a string, it's not vault data\n        return False\n\n    if b_data.startswith(b_HEADER):\n        return True\n    return False", "loc": 20}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "is_encrypted_file", "parameters": ["file_obj", "start_pos", "count"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "True if the file looks like a vault file. Otherwise, False.", "raises_doc": [], "called_functions": ["file_obj.read", "file_obj.seek", "file_obj.tell", "is_encrypted", "len"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Test if the contents of a file obj are a vault encrypted data blob. :arg file_obj: A file object that will be read from. :kwarg start_pos: A byte offset in the file to start reading the header from.  Defaults to 0, the beginning of the file. :kwarg count: Read up to this number of bytes from the file to determine if it looks like encrypted vault data. The default is the size of the the vault header, which is what is needed most times. For some IO classes, or files that don't begin with the vault itself, set to -1 to read to the end of file.", "source_code": "def is_encrypted_file(file_obj, start_pos=0, count=len(b_HEADER)):\n    \"\"\"Test if the contents of a file obj are a vault encrypted data blob.\n\n    :arg file_obj: A file object that will be read from.\n    :kwarg start_pos: A byte offset in the file to start reading the header\n        from.  Defaults to 0, the beginning of the file.\n    :kwarg count: Read up to this number of bytes from the file to determine\n        if it looks like encrypted vault data. The default is the size of the\n        the vault header, which is what is needed most times.\n        For some IO classes, or files that don't begin with the vault itself,\n        set to -1 to read to the end of file.\n    :returns: True if the file looks like a vault file. Otherwise, False.\n    \"\"\"\n    # read the header and reset the file stream to where it started\n    current_position = file_obj.tell()\n    try:\n        file_obj.seek(start_pos)\n        return is_encrypted(file_obj.read(count))\n\n    finally:\n        file_obj.seek(current_position)", "loc": 21}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "parse_vaulttext_envelope", "parameters": ["b_vaulttext_envelope", "default_vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A tuple of byte str of the vaulttext suitable to pass to parse_vaultext,", "raises_doc": [], "called_functions": ["AnsibleVaultFormatError", "_parse_vaulttext_envelope"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Parse the vaulttext envelope When data is saved, it has a header prepended and is formatted into 80 character lines.  This method extracts the information from the header and then removes the header and the inserted newlines.  The string returned is suitable for processing by the Cipher classes. :arg b_vaulttext_envelope: byte str containing the data from a save file :arg default_vault_id: The vault_id name to use if the vaulttext does not provide one.", "source_code": "def parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id=None):\n    \"\"\"Parse the vaulttext envelope\n\n    When data is saved, it has a header prepended and is formatted into 80\n    character lines.  This method extracts the information from the header\n    and then removes the header and the inserted newlines.  The string returned\n    is suitable for processing by the Cipher classes.\n\n    :arg b_vaulttext_envelope: byte str containing the data from a save file\n    :arg default_vault_id: The vault_id name to use if the vaulttext does not provide one.\n    :returns: A tuple of byte str of the vaulttext suitable to pass to parse_vaultext,\n        a byte str of the vault format version,\n        the name of the cipher used, and the vault_id.\n    :raises: AnsibleVaultFormatError: if the vaulttext_envelope format is invalid\n    \"\"\"\n    # used by decrypt\n    default_vault_id = default_vault_id or C.DEFAULT_VAULT_IDENTITY\n\n    try:\n        return _parse_vaulttext_envelope(b_vaulttext_envelope, default_vault_id)\n    except Exception as ex:\n        raise AnsibleVaultFormatError(\"Vault envelope format error.\", obj=b_vaulttext_envelope) from ex", "loc": 22}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "format_vaulttext_envelope", "parameters": ["b_ciphertext", "cipher_name", "version", "vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "a byte str that should be dumped into a file.  It's", "raises_doc": [], "called_functions": ["AnsibleError", "b';'.join", "b'\\n'.join", "header_parts.append", "len", "range", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add header and format to 80 columns :arg b_ciphertext: the encrypted and hexlified data as a byte string :arg cipher_name: unicode cipher name (for ex, u'AES256') :arg version: unicode vault version (for ex, '1.2'). Optional ('1.1' is default) :arg vault_id: unicode vault identifier. If provided, the version will be bumped to 1.2.", "source_code": "def format_vaulttext_envelope(b_ciphertext, cipher_name, version=None, vault_id=None):\n    \"\"\" Add header and format to 80 columns\n\n        :arg b_ciphertext: the encrypted and hexlified data as a byte string\n        :arg cipher_name: unicode cipher name (for ex, u'AES256')\n        :arg version: unicode vault version (for ex, '1.2'). Optional ('1.1' is default)\n        :arg vault_id: unicode vault identifier. If provided, the version will be bumped to 1.2.\n        :returns: a byte str that should be dumped into a file.  It's\n            formatted to 80 char columns and has the header prepended\n    \"\"\"\n\n    if not cipher_name:\n        raise AnsibleError(\"the cipher must be set before adding a header\")\n\n    version = version or '1.1'\n\n    # If we specify a vault_id, use format version 1.2. For no vault_id, stick to 1.1\n    if vault_id and vault_id != u'default':\n        version = '1.2'\n\n    b_version = to_bytes(version, 'utf-8', errors='strict')\n    b_vault_id = to_bytes(vault_id, 'utf-8', errors='strict')\n    b_cipher_name = to_bytes(cipher_name, 'utf-8', errors='strict')\n\n    header_parts = [b_HEADER,\n                    b_version,\n                    b_cipher_name]\n\n    if b_version == b'1.2' and b_vault_id:\n        header_parts.append(b_vault_id)\n\n    header = b';'.join(header_parts)\n\n    b_vaulttext = [header]\n    b_vaulttext += [b_ciphertext[i:i + 80] for i in range(0, len(b_ciphertext), 80)]\n    b_vaulttext += [b'']\n    b_vaulttext = b'\\n'.join(b_vaulttext)\n\n    return b_vaulttext", "loc": 39}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "parse_vaulttext", "parameters": ["b_vaulttext"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "A tuple of byte str of the ciphertext suitable for passing to a", "raises_doc": [], "called_functions": ["AnsibleVaultFormatError", "_parse_vaulttext"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Parse the vaulttext :arg b_vaulttext: byte str containing the vaulttext (ciphertext, salt, crypted_hmac)", "source_code": "def parse_vaulttext(b_vaulttext):\n    \"\"\"Parse the vaulttext\n\n    :arg b_vaulttext: byte str containing the vaulttext (ciphertext, salt, crypted_hmac)\n    :returns: A tuple of byte str of the ciphertext suitable for passing to a\n        Cipher class's decrypt() function, a byte str of the salt,\n        and a byte str of the crypted_hmac\n    :raises: AnsibleVaultFormatError: if the vaulttext format is invalid\n    \"\"\"\n    # SPLIT SALT, DIGEST, AND DATA\n    try:\n        return _parse_vaulttext(b_vaulttext)\n    except AnsibleVaultFormatError:\n        raise\n    except Exception as ex:\n        raise AnsibleVaultFormatError(\"Vault vaulttext format error.\", obj=b_vaulttext) from ex", "loc": 16}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "verify_secret_is_not_empty", "parameters": ["secret", "msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleVaultPasswordError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Check the secret against minimal requirements.", "source_code": "def verify_secret_is_not_empty(secret, msg=None):\n    \"\"\"Check the secret against minimal requirements.\n\n    Raises: AnsibleVaultPasswordError if the password does not meet requirements.\n\n    Currently, only requirement is that the password is not None or an empty string.\n    \"\"\"\n    msg = msg or 'Invalid vault password was provided'\n    if not secret:\n        raise AnsibleVaultPasswordError(msg)", "loc": 10}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "match_secrets", "parameters": ["secrets", "target_vault_ids"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Find all VaultSecret objects that are mapped to any of the target_vault_ids in secrets", "source_code": "def match_secrets(secrets, target_vault_ids):\n    \"\"\"Find all VaultSecret objects that are mapped to any of the target_vault_ids in secrets\"\"\"\n    if not secrets:\n        return []\n\n    matches = [(vault_id, secret) for vault_id, secret in secrets if vault_id in target_vault_ids]\n    return matches", "loc": 7}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "match_best_secret", "parameters": ["secrets", "target_vault_ids"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match_secrets"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Find the best secret from secrets that matches target_vault_ids Since secrets should be ordered so the early secrets are 'better' than later ones, this just finds all the matches, then returns the first secret", "source_code": "def match_best_secret(secrets, target_vault_ids):\n    \"\"\"Find the best secret from secrets that matches target_vault_ids\n\n    Since secrets should be ordered so the early secrets are 'better' than later ones, this\n    just finds all the matches, then returns the first secret\"\"\"\n    matches = match_secrets(secrets, target_vault_ids)\n    if matches:\n        return matches[0]\n    # raise exception?\n    return None", "loc": 10}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "match_encrypt_vault_id_secret", "parameters": ["secrets", "encrypt_vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleVaultError", "display.vvvv", "match_best_secret", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match_encrypt_vault_id_secret(secrets, encrypt_vault_id=None):\n    # See if the --encrypt-vault-id matches a vault-id\n    display.vvvv(u'encrypt_vault_id=%s' % to_text(encrypt_vault_id))\n\n    if encrypt_vault_id is None:\n        raise AnsibleError('match_encrypt_vault_id_secret requires a non None encrypt_vault_id')\n\n    encrypt_vault_id_matchers = [encrypt_vault_id]\n    encrypt_secret = match_best_secret(secrets, encrypt_vault_id_matchers)\n\n    # return the best match for --encrypt-vault-id\n    if encrypt_secret:\n        return encrypt_secret\n\n    # If we specified a encrypt_vault_id and we couldn't find it, dont\n    # fallback to using the first/best secret\n    raise AnsibleVaultError('Did not find a match for --encrypt-vault-id=%s in the known vault-ids %s' % (encrypt_vault_id,\n                                                                                                          [_v for _v, _vs in secrets]))", "loc": 18}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": null, "function_name": "match_encrypt_secret", "parameters": ["secrets", "encrypt_vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.vvvv", "match_best_secret", "match_encrypt_vault_id_secret", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Find the best/first/only secret in secrets to use for encrypting", "source_code": "def match_encrypt_secret(secrets, encrypt_vault_id=None):\n    \"\"\"Find the best/first/only secret in secrets to use for encrypting\"\"\"\n\n    display.vvvv(u'encrypt_vault_id=%s' % to_text(encrypt_vault_id))\n    # See if the --encrypt-vault-id matches a vault-id\n    if encrypt_vault_id:\n        return match_encrypt_vault_id_secret(secrets,\n                                             encrypt_vault_id=encrypt_vault_id)\n\n    # Find the best/first secret from secrets since we didn't specify otherwise\n    # ie, consider all the available secrets as matches\n    _vault_id_matchers = [_vault_id for _vault_id, dummy in secrets]\n    best_secret = match_best_secret(secrets, _vault_id_matchers)\n\n    # can be empty list sans any tuple\n    return best_secret", "loc": 16}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "PromptVaultSecret", "function_name": "ask_vault_passwords", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleVaultError", "b_vault_passwords.append", "display.prompt", "self.confirm", "to_bytes", "to_bytes(vault_pass, errors='strict', nonstring='simplerepr').strip", "verify_secret_is_not_empty"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ask_vault_passwords(self):\n    b_vault_passwords = []\n\n    for prompt_format in self.prompt_formats:\n        prompt = prompt_format % {'vault_id': self.vault_id}\n        try:\n            vault_pass = display.prompt(prompt, private=True)\n        except EOFError:\n            raise AnsibleVaultError('EOFError (ctrl-d) on prompt for (%s)' % self.vault_id)\n\n        verify_secret_is_not_empty(vault_pass)\n\n        b_vault_pass = to_bytes(vault_pass, errors='strict', nonstring='simplerepr').strip()\n        b_vault_passwords.append(b_vault_pass)\n\n    # Make sure the passwords match by comparing them all to the first password\n    for b_vault_password in b_vault_passwords:\n        self.confirm(b_vault_passwords[0], b_vault_password)\n\n    if b_vault_passwords:\n        return b_vault_passwords[0]\n\n    return None", "loc": 23}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "PromptVaultSecret", "function_name": "confirm", "parameters": ["self", "b_vault_pass_1", "b_vault_pass_2"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def confirm(self, b_vault_pass_1, b_vault_pass_2):\n    # enforce no newline chars at the end of passwords\n\n    if b_vault_pass_1 != b_vault_pass_2:\n        # FIXME: more specific exception\n        raise AnsibleError(\"Passwords do not match\")", "loc": 6}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "FileVaultSecret", "function_name": "bytes", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._text.encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def bytes(self):\n    if self._bytes:\n        return self._bytes\n    if self._text:\n        return self._text.encode(self.encoding)\n    return None", "loc": 6}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultLib", "function_name": "encrypt", "parameters": ["self", "plaintext", "secret", "vault_id", "salt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "a utf-8 encoded byte str of encrypted data.  The string", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleVaultError", "display.vvvvv", "format_vaulttext_envelope", "is_encrypted", "match_encrypt_secret", "this_cipher.encrypt", "to_bytes", "to_text", "u'{0} cipher could not be found'.format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Vault encrypt a piece of data. :arg plaintext: a text or byte string to encrypt.", "source_code": "def encrypt(self, plaintext, secret=None, vault_id=None, salt=None):\n    \"\"\"Vault encrypt a piece of data.\n\n    :arg plaintext: a text or byte string to encrypt.\n    :returns: a utf-8 encoded byte str of encrypted data.  The string\n        contains a header identifying this as vault encrypted data and\n        formatted to newline terminated lines of 80 characters.  This is\n        suitable for dumping as is to a vault file.\n\n    If the string passed in is a text string, it will be encoded to UTF-8\n    before encryption.\n    \"\"\"\n\n    if secret is None:\n        if self.secrets:\n            dummy, secret = match_encrypt_secret(self.secrets)\n        else:\n            raise AnsibleVaultError(\"A vault password must be specified to encrypt data\")\n\n    b_plaintext = to_bytes(plaintext, errors='surrogate_or_strict')\n\n    if is_encrypted(b_plaintext):\n        raise AnsibleError(\"input is already encrypted\")\n\n    if not self.cipher_name or self.cipher_name not in CIPHER_WRITE_ALLOWLIST:\n        self.cipher_name = u\"AES256\"\n\n    try:\n        this_cipher = CIPHER_MAPPING[self.cipher_name]()\n    except KeyError:\n        raise AnsibleError(u\"{0} cipher could not be found\".format(self.cipher_name))\n\n    # encrypt data\n    if vault_id:\n        display.vvvvv(u'Encrypting with vault_id \"%s\" and vault secret %s' % (to_text(vault_id), to_text(secret)))\n    else:\n        display.vvvvv(u'Encrypting without a vault_id using vault secret %s' % to_text(secret))\n\n    b_ciphertext = this_cipher.encrypt(b_plaintext, secret, salt)\n\n    # format the data for output to the file\n    b_vaulttext = format_vaulttext_envelope(b_ciphertext,\n                                            self.cipher_name,\n                                            vault_id=vault_id)\n    return b_vaulttext", "loc": 45}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultLib", "function_name": "decrypt_and_get_vault_id", "parameters": ["self", "vaulttext"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "a byte string containing the decrypted data and the vault-id vault-secret that was used", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag_copy", "AnsibleVaultError", "Origin.get_tag", "display.vvvv", "display.vvvvv", "is_encrypted", "match_secrets", "parse_vaulttext_envelope", "this_cipher.decrypt", "to_bytes", "to_text", "vault_id_matchers.append", "vault_id_matchers.extend"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Decrypt a piece of vault encrypted data. :arg vaulttext: a string to decrypt.  Since vault encrypted data is an ascii text format this can be either a byte str or unicode string.", "source_code": "def decrypt_and_get_vault_id(self, vaulttext):\n    \"\"\"Decrypt a piece of vault encrypted data.\n\n    :arg vaulttext: a string to decrypt.  Since vault encrypted data is an\n        ascii text format this can be either a byte str or unicode string.\n    :returns: a byte string containing the decrypted data and the vault-id vault-secret that was used\n    \"\"\"\n    origin = Origin.get_tag(vaulttext)\n\n    b_vaulttext = to_bytes(vaulttext, nonstring='error')  # enforce vaulttext is str/bytes, keep type check if removing type conversion\n\n    if self.secrets is None:\n        raise AnsibleVaultError(\"A vault password must be specified to decrypt data.\", obj=vaulttext)\n\n    if not is_encrypted(b_vaulttext):\n        raise AnsibleVaultError(\"Input is not vault encrypted data.\", obj=vaulttext)\n\n    b_vaulttext, dummy, cipher_name, vault_id = parse_vaulttext_envelope(b_vaulttext)\n\n    # create the cipher object, note that the cipher used for decrypt can\n    # be different than the cipher used for encrypt\n    if cipher_name in CIPHER_ALLOWLIST:\n        this_cipher = CIPHER_MAPPING[cipher_name]()\n    else:\n        raise AnsibleVaultError(f\"Cipher {cipher_name!r} could not be found.\", obj=vaulttext)\n\n    if not self.secrets:\n        raise AnsibleVaultError('Attempting to decrypt but no vault secrets found.', obj=vaulttext)\n\n    # WARNING: Currently, the vault id is not required to match the vault id in the vault blob to\n    #          decrypt a vault properly. The vault id in the vault blob is not part of the encrypted\n    #          or signed vault payload. There is no cryptographic checking/verification/validation of the\n    #          vault blobs vault id. It can be tampered with and changed. The vault id is just a nick\n    #          name to use to pick the best secret and provide some ux/ui info.\n\n    # iterate over all the applicable secrets (all of them by default) until one works...\n    # if we specify a vault_id, only the corresponding vault secret is checked and\n    # we check it first.\n\n    vault_id_matchers = []\n\n    if vault_id:\n        display.vvvvv(u'Found a vault_id (%s) in the vaulttext' % to_text(vault_id))\n        vault_id_matchers.append(vault_id)\n        _matches = match_secrets(self.secrets, vault_id_matchers)\n        if _matches:\n            display.vvvvv(u'We have a secret associated with vault id (%s), will try to use to decrypt %s' % (to_text(vault_id), to_text(origin)))\n        else:\n            display.vvvvv(u'Found a vault_id (%s) in the vault text, but we do not have a associated secret (--vault-id)' % to_text(vault_id))\n\n    # Not adding the other secrets to vault_secret_ids enforces a match between the vault_id from the vault_text and\n    # the known vault secrets.\n    if not C.DEFAULT_VAULT_ID_MATCH:\n        # Add all of the known vault_ids as candidates for decrypting a vault.\n        vault_id_matchers.extend([_vault_id for _vault_id, _dummy in self.secrets if _vault_id != vault_id])\n\n    matched_secrets = match_secrets(self.secrets, vault_id_matchers)\n\n    # for vault_secret_id in vault_secret_ids:\n    for vault_secret_id, vault_secret in matched_secrets:\n        display.vvvvv(u'Trying to use vault secret=(%s) id=%s to decrypt %s' % (to_text(vault_secret), to_text(vault_secret_id), to_text(origin)))\n\n        try:\n            # secret = self.secrets[vault_secret_id]\n            display.vvvv(u'Trying secret %s for vault_id=%s' % (to_text(vault_secret), to_text(vault_secret_id)))\n            b_plaintext = this_cipher.decrypt(b_vaulttext, vault_secret)\n            # DTFIX7: possible candidate for propagate_origin\n            b_plaintext = AnsibleTagHelper.tag_copy(vaulttext, b_plaintext)\n            if b_plaintext is not None:\n                vault_id_used = vault_secret_id\n                vault_secret_used = vault_secret\n                file_slug = ''\n                if origin:\n                    file_slug = ' of \"%s\"' % origin\n                display.vvvvv(\n                    u'Decrypt%s successful with secret=%s and vault_id=%s' % (to_text(file_slug), to_text(vault_secret), to_text(vault_secret_id))\n                )\n                break\n        except AnsibleVaultFormatError:\n            raise\n        except AnsibleError as e:\n            display.vvvv(u'Tried to use the vault secret (%s) to decrypt (%s) but it failed. Error: %s' %\n                         (to_text(vault_secret_id), to_text(origin), e))\n            continue\n    else:\n        raise AnsibleVaultError(\"Decryption failed (no vault secrets were found that could decrypt).\", obj=vaulttext)\n\n    return b_plaintext, vault_id_used, vault_secret_used", "loc": 88}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultEditor", "function_name": "encrypt_file", "parameters": ["self", "filename", "secret", "vault_id", "output_file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._real_path", "self.read_data", "self.vault.encrypt", "self.write_data"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encrypt_file(self, filename, secret, vault_id=None, output_file=None):\n\n    # A file to be encrypted into a vaultfile could be any encoding\n    # so treat the contents as a byte string.\n\n    # follow the symlink\n    filename = self._real_path(filename)\n\n    b_plaintext = self.read_data(filename)\n    b_ciphertext = self.vault.encrypt(b_plaintext, secret, vault_id=vault_id)\n    self.write_data(b_ciphertext, output_file or filename)", "loc": 11}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultEditor", "function_name": "decrypt_file", "parameters": ["self", "filename", "output_file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "self._real_path", "self.read_data", "self.vault.decrypt", "self.write_data", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decrypt_file(self, filename, output_file=None):\n\n    # follow the symlink\n    filename = self._real_path(filename)\n\n    ciphertext = self.read_data(filename)\n\n    try:\n        plaintext = self.vault.decrypt(ciphertext)\n    except AnsibleError as e:\n        raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n    self.write_data(plaintext, output_file or filename, shred=False)", "loc": 12}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultEditor", "function_name": "edit_file", "parameters": ["self", "filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "parse_vaulttext_envelope", "self._edit_file_helper", "self._real_path", "self.read_data", "self.vault.decrypt_and_get_vault_id", "to_native", "to_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def edit_file(self, filename):\n    vault_id_used = None\n    vault_secret_used = None\n    # follow the symlink\n    filename = self._real_path(filename)\n\n    b_vaulttext = self.read_data(filename)\n\n    # vault or yaml files are always utf8\n    vaulttext = to_text(b_vaulttext)\n\n    try:\n        # vaulttext gets converted back to bytes, but alas\n        # TODO: return the vault_id that worked?\n        plaintext, vault_id_used, vault_secret_used = self.vault.decrypt_and_get_vault_id(vaulttext)\n    except AnsibleError as e:\n        raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n\n    # Figure out the vault id from the file, to select the right secret to re-encrypt it\n    # (duplicates parts of decrypt, but alas...)\n    dummy, dummy, cipher_name, vault_id = parse_vaulttext_envelope(b_vaulttext)\n\n    # vault id here may not be the vault id actually used for decrypting\n    # as when the edited file has no vault-id but is decrypted by non-default id in secrets\n    # (vault_id=default, while a different vault-id decrypted)\n\n    # we want to get rid of files encrypted with the AES cipher\n    force_save = (cipher_name not in CIPHER_WRITE_ALLOWLIST)\n\n    # Keep the same vault-id (and version) as in the header\n    self._edit_file_helper(filename, vault_secret_used, existing_data=plaintext, force_save=force_save, vault_id=vault_id)", "loc": 31}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultEditor", "function_name": "plaintext", "parameters": ["self", "filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleVaultError", "self.read_data", "self.vault.decrypt", "to_native", "to_text"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def plaintext(self, filename):\n\n    b_vaulttext = self.read_data(filename)\n    vaulttext = to_text(b_vaulttext)\n\n    try:\n        plaintext = self.vault.decrypt(vaulttext)\n        return plaintext\n    except AnsibleError as e:\n        raise AnsibleVaultError(\"%s for %s\" % (to_native(e), to_native(filename)))", "loc": 10}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultEditor", "function_name": "rekey_file", "parameters": ["self", "filename", "new_vault_secret", "new_vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "VaultLib", "display.vvvvv", "new_vault.encrypt", "os.chmod", "os.chown", "os.stat", "self._real_path", "self.read_data", "self.vault.decrypt_and_get_vault_id", "self.write_data", "to_native", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def rekey_file(self, filename, new_vault_secret, new_vault_id=None):\n\n    # follow the symlink\n    filename = self._real_path(filename)\n\n    prev = os.stat(filename)\n    b_vaulttext = self.read_data(filename)\n    vaulttext = to_text(b_vaulttext)\n\n    display.vvvvv(u'Rekeying file \"%s\" to with new vault-id \"%s\" and vault secret %s' %\n                  (to_text(filename), to_text(new_vault_id), to_text(new_vault_secret)))\n    try:\n        plaintext, vault_id_used, _dummy = self.vault.decrypt_and_get_vault_id(vaulttext)\n    except AnsibleError as e:\n        raise AnsibleError(\"%s for %s\" % (to_native(e), to_native(filename)))\n\n    # This is more or less an assert, see #18247\n    if new_vault_secret is None:\n        raise AnsibleError('The value for the new_password to rekey %s with is not valid' % filename)\n\n    # FIXME: VaultContext...?  could rekey to a different vault_id in the same VaultSecrets\n\n    # Need a new VaultLib because the new vault data can be a different\n    # vault lib format or cipher (for ex, when we migrate 1.0 style vault data to\n    # 1.1 style data we change the version and the cipher). This is where a VaultContext might help\n\n    # the new vault will only be used for encrypting, so it doesn't need the vault secrets\n    # (we will pass one in directly to encrypt)\n    new_vault = VaultLib(secrets={})\n    b_new_vaulttext = new_vault.encrypt(plaintext, new_vault_secret, vault_id=new_vault_id)\n\n    self.write_data(b_new_vaulttext, filename)\n\n    # preserve permissions\n    os.chmod(filename, prev.st_mode)\n    os.chown(filename, prev.st_uid, prev.st_gid)\n\n    display.vvvvv(u'Rekeyed file \"%s\" (decrypted with vault id \"%s\") was encrypted with new vault-id \"%s\" and vault secret %s' %\n                  (to_text(filename), to_text(vault_id_used), to_text(new_vault_id), to_text(new_vault_secret)))", "loc": 39}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultAES256", "function_name": "encrypt", "parameters": ["cls", "b_plaintext", "secret", "salt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleVaultError", "b'\\n'.join", "cls._encrypt_cryptography", "cls._gen_key_initctr", "cls._get_salt", "hexlify", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encrypt(cls, b_plaintext, secret, salt=None):\n\n    if secret is None:\n        raise AnsibleVaultError('The secret passed to encrypt() was None')\n\n    if salt is None:\n        b_salt = cls._get_salt()\n    elif not salt:\n        raise AnsibleVaultError('Empty or invalid salt passed to encrypt()')\n    else:\n        b_salt = to_bytes(salt)\n\n    b_password = secret.bytes\n    b_key1, b_key2, b_iv = cls._gen_key_initctr(b_password, b_salt)\n\n    if HAS_CRYPTOGRAPHY:\n        b_hmac, b_ciphertext = cls._encrypt_cryptography(b_plaintext, b_key1, b_key2, b_iv)\n    else:\n        raise AnsibleError(NEED_CRYPTO_LIBRARY + '(Detected in encrypt)')\n\n    b_vaulttext = b'\\n'.join([hexlify(b_salt), b_hmac, b_ciphertext])\n    # Unnecessary but getting rid of it is a backwards incompatible vault\n    # format change\n    b_vaulttext = hexlify(b_vaulttext)\n    return b_vaulttext", "loc": 25}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultAES256", "function_name": "decrypt", "parameters": ["cls", "b_vaulttext", "secret"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "cls._decrypt_cryptography", "cls._gen_key_initctr", "parse_vaulttext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decrypt(cls, b_vaulttext, secret):\n\n    b_ciphertext, b_salt, b_crypted_hmac = parse_vaulttext(b_vaulttext)\n\n    # TODO: would be nice if a VaultSecret could be passed directly to _decrypt_*\n    #       (move _gen_key_initctr() to a AES256 VaultSecret or VaultContext impl?)\n    # though, likely needs to be python cryptography specific impl that basically\n    # creates a Cipher() with b_key1, a Mode.CTR() with b_iv, and a HMAC() with sign key b_key2\n    b_password = secret.bytes\n\n    b_key1, b_key2, b_iv = cls._gen_key_initctr(b_password, b_salt)\n\n    if HAS_CRYPTOGRAPHY:\n        b_plaintext = cls._decrypt_cryptography(b_ciphertext, b_crypted_hmac, b_key1, b_key2, b_iv)\n    else:\n        raise AnsibleError(NEED_CRYPTO_LIBRARY + '(Detected in decrypt)')\n\n    return b_plaintext", "loc": 18}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultSecretsContext", "function_name": "initialize", "parameters": ["cls", "value"], "param_types": {"value": "t.Self"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Initialize VaultSecretsContext with the specified instance and secrets (since it's not a lazy or per-thread context). This method will fail if called more than once.", "source_code": "def initialize(cls, value: t.Self) -> None:\n    \"\"\"\n    Initialize VaultSecretsContext with the specified instance and secrets (since it's not a lazy or per-thread context).\n    This method will fail if called more than once.\n    \"\"\"\n    if cls._current:\n        raise RuntimeError(f\"The {cls.__name__} context is already initialized.\")\n\n    cls._current = value", "loc": 9}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultSecretsContext", "function_name": "current", "parameters": ["cls", "optional"], "param_types": {"optional": "bool"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ReferenceError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Access vault secrets, if initialized, ala `AmbientContextBase.current()`.", "source_code": "def current(cls, optional: bool = False) -> t.Self:\n    \"\"\"Access vault secrets, if initialized, ala `AmbientContextBase.current()`.\"\"\"\n    if not cls._current and not optional:\n        raise ReferenceError(f\"A required {cls.__name__} context is not active.\")\n\n    return cls._current", "loc": 6}
{"file": "ansible\\lib\\ansible\\parsing\\vault\\__init__.py", "class_name": "VaultHelper", "function_name": "get_ciphertext", "parameters": ["value"], "param_types": {"value": "t.Any"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag", "AnsibleTagHelper.tags", "AnsibleTagHelper.untag", "VaultedValue.get_tag", "VaultedValue.untag", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "If the given value is an `EncryptedString`, `VaultExceptionMarker` or tagged with `VaultedValue`, return the ciphertext, otherwise return `None`. Tags on the value other than `VaultedValue` will be included on the ciphertext if `with_tags` is `True`, otherwise it will be tagless.", "source_code": "def get_ciphertext(value: t.Any, *, with_tags: bool) -> str | None:\n    \"\"\"\n    If the given value is an `EncryptedString`, `VaultExceptionMarker` or tagged with `VaultedValue`, return the ciphertext, otherwise return `None`.\n    Tags on the value other than `VaultedValue` will be included on the ciphertext if `with_tags` is `True`, otherwise it will be tagless.\n    \"\"\"\n    value_type = type(value)\n    ciphertext: str | None\n    tags = AnsibleTagHelper.tags(value)\n\n    if value_type is _jinja_common.VaultExceptionMarker:\n        ciphertext = value._marker_undecryptable_ciphertext\n        tags = AnsibleTagHelper.tags(ciphertext)  # ciphertext has tags but value does not\n    elif value_type is EncryptedString:\n        ciphertext = value._ciphertext\n    elif value_type in _jinja_common.Marker._concrete_subclasses:  # avoid wasteful raise/except of Marker when calling get_tag below\n        ciphertext = None\n    elif vaulted_value := VaultedValue.get_tag(value):\n        ciphertext = vaulted_value.ciphertext\n    else:\n        ciphertext = None\n\n    if ciphertext:\n        if with_tags:\n            ciphertext = VaultedValue.untag(AnsibleTagHelper.tag(ciphertext, tags))\n        else:\n            ciphertext = AnsibleTagHelper.untag(ciphertext)\n\n    return ciphertext", "loc": 28}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "dump_me", "parameters": ["self", "depth"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep.dump_me", "display.debug", "hasattr", "id", "self._parent.dump_me", "self._parent.get_dep_chain", "self._play.dump_me"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "this is never called from production code, it is here to be used when debugging as a 'complex print'", "source_code": "def dump_me(self, depth=0):\n    \"\"\" this is never called from production code, it is here to be used when debugging as a 'complex print' \"\"\"\n    if depth == 0:\n        display.debug(\"DUMPING OBJECT ------------------------------------------------------\")\n    display.debug(\"%s- %s (%s, id=%s)\" % (\" \" * depth, self.__class__.__name__, self, id(self)))\n    if hasattr(self, '_parent') and self._parent:\n        self._parent.dump_me(depth + 2)\n        dep_chain = self._parent.get_dep_chain()\n        if dep_chain:\n            for dep in dep_chain:\n                dep.dump_me(depth + 2)\n    if hasattr(self, '_play') and self._play:\n        self._play.dump_me(depth + 2)", "loc": 13}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "load_data", "parameters": ["self", "ds", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "DataLoader", "Origin.get_tag", "getattr", "method", "operator.itemgetter", "self._validate_attributes", "self.fattributes.items", "self.preprocess_data", "self.validate", "setattr", "sorted"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "walk the input datastructure and assign any values", "source_code": "def load_data(self, ds, variable_manager=None, loader=None):\n    \"\"\" walk the input datastructure and assign any values \"\"\"\n\n    if ds is None:\n        raise AnsibleAssertionError('ds (%s) should not be None but it is.' % ds)\n\n    # cache the datastructure internally\n    setattr(self, '_ds', ds)\n\n    # the variable manager class is used to manage and merge variables\n    # down to a single dictionary for reference in templating, etc.\n    self._variable_manager = variable_manager\n    self._origin = Origin.get_tag(ds)\n\n    # the data loader class is used to parse data from strings and files\n    if loader is not None:\n        self._loader = loader\n    else:\n        self._loader = DataLoader()\n\n    # call the preprocess_data() function to massage the data into\n    # something we can more easily parse, and then call the validation\n    # function on it to ensure there are no incorrect key values\n    ds = self.preprocess_data(ds)\n    self._validate_attributes(ds)\n\n    # Walk all attributes in the class. We sort them based on their priority\n    # so that certain fields can be loaded before others, if they are dependent.\n    for name, attr in sorted(self.fattributes.items(), key=operator.itemgetter(1)):\n        # copy the value over unless a _load_field method is defined\n        if name in ds:\n            method = getattr(self, '_load_%s' % name, None)\n            if method:\n                setattr(self, name, method(name, ds[name]))\n            else:\n                setattr(self, name, ds[name])\n\n    # run early, non-critical validation\n    self.validate()\n\n    # return the constructed object\n    return self", "loc": 42}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "get_ds", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ds(self):\n    try:\n        return getattr(self, '_ds')\n    except AttributeError:\n        return None", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "validate", "parameters": ["self", "all_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "getattr", "isinstance", "method", "self.fattributes.items", "self.get_ds", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "validation that is done at parse time, not load time", "source_code": "def validate(self, all_vars=None):\n    \"\"\" validation that is done at parse time, not load time \"\"\"\n    if not self._validated:\n        # walk all fields in the object\n        for (name, attribute) in self.fattributes.items():\n            # run validator only if present\n            method = getattr(self, '_validate_%s' % name, None)\n            if method:\n                method(attribute, name, getattr(self, name))\n            else:\n                # and make sure the attribute is of the type it should be\n                value = getattr(self, f'_{name}', Sentinel)\n                if value is not None:\n                    if attribute.isa == 'string' and isinstance(value, (list, dict)):\n                        raise AnsibleParserError(\n                            \"The field '%s' is supposed to be a string type,\"\n                            \" however the incoming data structure is a %s\" % (name, type(value)), obj=self.get_ds()\n                        )\n\n    self._validated = True", "loc": 20}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "play", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def play(self):\n    if hasattr(self, '_play'):\n        play = self._play\n    elif hasattr(self, '_parent') and hasattr(self._parent, '_play'):\n        play = self._parent._play\n    else:\n        play = self\n\n    if play.__class__.__name__ != 'Play':\n        # Should never happen, but handle gracefully by returning None, just in case\n        return None\n\n    return play", "loc": 13}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "squash", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "setattr"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Evaluates all attributes and sets them to the evaluated version, so that all future accesses of attributes do not need to evaluate parent attributes.", "source_code": "def squash(self):\n    \"\"\"\n    Evaluates all attributes and sets them to the evaluated version,\n    so that all future accesses of attributes do not need to evaluate\n    parent attributes.\n    \"\"\"\n    if not self._squashed:\n        for name in self.fattributes:\n            setattr(self, name, getattr(self, name))\n        self._squashed = True", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "copy", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "getattr", "hasattr", "self.__class__", "setattr", "shallowcopy"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Create a copy of this object and return it.", "source_code": "def copy(self):\n    \"\"\"\n    Create a copy of this object and return it.\n    \"\"\"\n\n    try:\n        new_me = self.__class__()\n    except RecursionError as ex:\n        raise AnsibleError(\"Exceeded maximum object depth. This may have been caused by excessive role recursion.\") from ex\n\n    for name in self.fattributes:\n        setattr(new_me, name, shallowcopy(getattr(self, f'_{name}', Sentinel)))\n\n    new_me._loader = self._loader\n    new_me._variable_manager = self._variable_manager\n    new_me._origin = self._origin\n    new_me._validated = self._validated\n    new_me._finalized = self._finalized\n    new_me._uuid = self._uuid\n\n    # if the ds value was set on the object, copy it to the new copy too\n    if hasattr(self, '_ds'):\n        new_me._ds = self._ds\n\n    return new_me", "loc": 25}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "get_validated_value", "parameters": ["self", "name", "attribute", "value", "templar"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "self._get_validated_value"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_validated_value(self, name, attribute, value, templar):\n    try:\n        return self._get_validated_value(name, attribute, value, templar)\n    except (TypeError, ValueError):\n        raise AnsibleError(f\"The value {value!r} could not be converted to {attribute.isa!r}.\", obj=value)", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "set_to_context", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._get_parent_attribute", "setattr"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "set to parent inherited value or Sentinel as appropriate", "source_code": "def set_to_context(self, name: str) -> t.Any:\n    \"\"\" set to parent inherited value or Sentinel as appropriate\"\"\"\n\n    attribute = self.fattributes[name]\n    if isinstance(attribute, NonInheritableFieldAttribute):\n        # setting to sentinel will trigger 'default/default()' on getter\n        value = Sentinel\n    else:\n        try:\n            value = self._get_parent_attribute(name, omit=True)\n        except AttributeError:\n            # mostly playcontext as only tasks/handlers/blocks really resolve parent\n            value = Sentinel\n\n    setattr(self, name, value)\n    return value", "loc": 16}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "post_validate", "parameters": ["self", "templar"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.post_validate_attribute", "setattr"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "we can't tell that everything is of the right type until we have all the variables.  Run basic types (from isa) as well as any _post_validate_<foo> functions.", "source_code": "def post_validate(self, templar):\n    \"\"\"\n    we can't tell that everything is of the right type until we have\n    all the variables.  Run basic types (from isa) as well as\n    any _post_validate_<foo> functions.\n    \"\"\"\n\n    for name in self.fattributes:\n        value = self.post_validate_attribute(name, templar=templar)\n\n        if value is not Sentinel:\n            # and assign the massaged value back to the attribute field\n            setattr(self, name, value)\n\n    self._finalized = True", "loc": 15}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "post_validate_attribute", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFieldAttributeError", "display.warning", "getattr", "isinstance", "method", "self.get_ds", "self.get_validated_value", "self.set_to_context", "templar.is_template", "templar.template"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_validate_attribute(self, name: str, *, templar: TemplateEngine):\n    attribute: FieldAttribute = self.fattributes[name]\n\n    # DTFIX-FUTURE: this can probably be used in many getattr cases below, but the value may be out-of-date in some cases\n    original_value = getattr(self, name)  # we save this original (likely Origin-tagged) value to pass as `obj` for errors\n\n    if attribute.static:\n        value = getattr(self, name)\n\n        # we don't template 'vars' but allow template as values for later use\n        if name not in ('vars',) and templar.is_template(value):\n            display.warning('\"%s\" is not templatable, but we found: %s, '\n                            'it will not be templated and will be used \"as is\".' % (name, value))\n        return Sentinel\n\n    if getattr(self, name) is None:\n        if not attribute.required:\n            return Sentinel\n\n        raise AnsibleFieldAttributeError(f'The field {name!r} is required but was not set.', obj=self.get_ds())\n\n    from .role_include import IncludeRole\n\n    if not attribute.always_post_validate and isinstance(self, IncludeRole) and self.statically_loaded:  # import_role\n        # normal field attributes should not go through post validation on import_role/import_tasks\n        # only import_role is checked here because import_tasks never reaches this point\n        return Sentinel\n\n    # Skip post validation unless always_post_validate is True, or the object requires post validation.\n    if not attribute.always_post_validate and not self._post_validate_object:\n        # Intermediate objects like Play() won't have their fields validated by\n        # default, as their values are often inherited by other objects and validated\n        # later, so we don't want them to fail out early\n        return Sentinel\n\n    try:\n        # Run the post-validator if present. These methods are responsible for\n        # using the given templar to template the values, if required.\n        method = getattr(self, '_post_validate_%s' % name, None)\n\n        if method:\n            value = method(attribute, getattr(self, name), templar)\n        elif attribute.isa == 'class':\n            value = getattr(self, name)\n        else:\n            try:\n                # if the attribute contains a variable, template it now\n                value = templar.template(getattr(self, name))\n            except AnsibleValueOmittedError:\n                # If this evaluated to the omit value, set the value back to inherited by context\n                # or default specified in the FieldAttribute and move on\n                value = self.set_to_context(name)\n\n                if value is Sentinel:\n                    return value\n\n        # and make sure the attribute is of the type it should be\n        if value is not None:\n            value = self.get_validated_value(name, attribute, value, templar)\n\n        # returning the value results in assigning the massaged value back to the attribute field\n        return value\n    except Exception as ex:\n        if name == 'args':\n            raise  # no useful information to contribute, raise the original exception\n\n        raise AnsibleFieldAttributeError(f'Error processing keyword {name!r}.', obj=original_value) from ex", "loc": 67}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "dump_attrs", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["attr.dump_attrs", "getattr", "self.fattributes.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Dumps all attributes to a dictionary", "source_code": "def dump_attrs(self):\n    \"\"\"\n    Dumps all attributes to a dictionary\n    \"\"\"\n    attrs = {}\n    for (name, attribute) in self.fattributes.items():\n        attr = getattr(self, name)\n        if attribute.isa == 'class':\n            attrs[name] = attr.dump_attrs()\n        else:\n            attrs[name] = attr\n    return attrs", "loc": 12}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "FieldAttributeBase", "function_name": "from_attrs", "parameters": ["self", "attrs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["attribute.class_type", "attrs.items", "isinstance", "obj.from_attrs", "setattr"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Loads attributes from a dictionary", "source_code": "def from_attrs(self, attrs):\n    \"\"\"\n    Loads attributes from a dictionary\n    \"\"\"\n    for (attr, value) in attrs.items():\n        if attr in self.fattributes:\n            attribute = self.fattributes[attr]\n            if attribute.isa == 'class' and isinstance(value, dict):\n                obj = attribute.class_type()\n                obj.from_attrs(value)\n                setattr(self, attr, obj)\n            else:\n                setattr(self, attr, value)\n        else:\n            setattr(self, attr, value)  # overridden dump_attrs in derived types may dump attributes which are not field attributes", "loc": 15}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "Base", "function_name": "update_result_no_log", "parameters": ["self", "templar", "result"], "param_types": {"templar": "TemplateEngine", "result": "dict[str, t.Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.error_as_warning", "display.warning", "isinstance", "result.get", "result.update", "self.post_validate_attribute", "type"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set the post-validated no_log value for the result, falling back to a default on validation/templating failure with a warning.", "source_code": "def update_result_no_log(self, templar: TemplateEngine, result: dict[str, t.Any]) -> None:\n    \"\"\"Set the post-validated no_log value for the result, falling back to a default on validation/templating failure with a warning.\"\"\"\n\n    if self.finalized:\n        no_log = self.no_log\n    else:\n        try:\n            no_log = self.post_validate_attribute('no_log', templar=templar)\n        except Exception as ex:\n            display.error_as_warning('Invalid no_log value for task, output will be masked.', exception=ex)\n            no_log = True\n\n    result_no_log = result.get('_ansible_no_log', False)\n\n    if not isinstance(result_no_log, bool):\n        display.warning(f'Invalid _ansible_no_log value of type {type(result_no_log).__name__!r} in task result, output will be masked.')\n        no_log = True\n\n    no_log = no_log or result_no_log\n\n    result.update(_ansible_no_log=no_log)", "loc": 21}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "Base", "function_name": "get_path", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "return the absolute path of the playbook object and its line number", "source_code": "def get_path(self) -> str:\n    \"\"\" return the absolute path of the playbook object and its line number \"\"\"\n    origin = self._origin\n\n    if not origin:\n        try:\n            origin = self._parent._play._origin\n        except AttributeError:\n            pass\n\n    if origin and origin.path:\n        path = f\"{origin.path}:{origin.line_num or 1}\"\n    else:\n        path = \"\"\n\n    return path", "loc": 16}
{"file": "ansible\\lib\\ansible\\playbook\\base.py", "class_name": "Base", "function_name": "get_dep_chain", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self._parent.get_dep_chain"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dep_chain(self):\n\n    if hasattr(self, '_parent') and self._parent:\n        return self._parent.get_dep_chain()\n    else:\n        return None", "loc": 6}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "get_vars", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent.get_vars", "self.vars.copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Blocks do not store variables directly, however they may be a member of a role or task include which does, so return those if present.", "source_code": "def get_vars(self):\n    \"\"\"\n    Blocks do not store variables directly, however they may be a member\n    of a role or task include which does, so return those if present.\n    \"\"\"\n\n    all_vars = {}\n\n    if self._parent:\n        all_vars |= self._parent.get_vars()\n\n    all_vars |= self.vars.copy()\n\n    return all_vars", "loc": 14}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "is_block", "parameters": ["ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_block(ds):\n    is_block = False\n    if isinstance(ds, dict):\n        for attr in ('block', 'rescue', 'always'):\n            if attr in ds:\n                is_block = True\n                break\n    return is_block", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Block.is_block", "dict", "isinstance", "super", "super(Block, self).preprocess_data"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "If a simple task is given, an implicit block for that single task is created, which goes in the main portion of the block", "source_code": "def preprocess_data(self, ds):\n    \"\"\"\n    If a simple task is given, an implicit block for that single task\n    is created, which goes in the main portion of the block\n    \"\"\"\n\n    if not Block.is_block(ds):\n        if isinstance(ds, list):\n            return super(Block, self).preprocess_data(dict(block=ds))\n        else:\n            return super(Block, self).preprocess_data(dict(block=[ds]))\n\n    return super(Block, self).preprocess_data(ds)", "loc": 13}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "get_dep_chain", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent.get_dep_chain"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dep_chain(self):\n    if self._dep_chain is None:\n        if self._parent:\n            return self._parent.get_dep_chain()\n        else:\n            return None\n    else:\n        return self._dep_chain[:]", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "copy", "parameters": ["self", "exclude_parent", "exclude_tasks"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_dupe_task_list", "new_me.validate", "new_task_list.append", "self._parent.copy", "super", "super(Block, self).copy", "task._parent.copy", "task.copy"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(self, exclude_parent=False, exclude_tasks=False):\n    def _dupe_task_list(task_list, new_block):\n        new_task_list = []\n        for task in task_list:\n            new_task = task.copy(exclude_parent=True, exclude_tasks=exclude_tasks)\n            if task._parent:\n                new_task._parent = task._parent.copy(exclude_tasks=True)\n                if task._parent == new_block:\n                    # If task._parent is the same as new_block, just replace it\n                    new_task._parent = new_block\n                else:\n                    # task may not be a direct child of new_block, search for the correct place to insert new_block\n                    cur_obj = new_task._parent\n                    while cur_obj._parent and cur_obj._parent != new_block:\n                        cur_obj = cur_obj._parent\n\n                    cur_obj._parent = new_block\n            else:\n                new_task._parent = new_block\n            new_task_list.append(new_task)\n        return new_task_list\n\n    new_me = super(Block, self).copy()\n    new_me._play = self._play\n    new_me._use_handlers = self._use_handlers\n\n    if self._dep_chain is not None:\n        new_me._dep_chain = self._dep_chain[:]\n\n    new_me._parent = None\n    if self._parent and not exclude_parent:\n        new_me._parent = self._parent.copy(exclude_tasks=True)\n\n    if not exclude_tasks:\n        new_me.block = _dupe_task_list(self.block or [], new_me)\n        new_me.rescue = _dupe_task_list(self.rescue or [], new_me)\n        new_me.always = _dupe_task_list(self.always or [], new_me)\n\n    new_me._role = None\n    if self._role:\n        new_me._role = self._role\n\n    new_me.validate()\n    return new_me", "loc": 44}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "set_loader", "parameters": ["self", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep.set_loader", "self._parent.set_loader", "self._role.set_loader", "self.get_dep_chain"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_loader(self, loader):\n    self._loader = loader\n    if self._parent:\n        self._parent.set_loader(loader)\n    elif self._role:\n        self._role.set_loader(loader)\n\n    dep_chain = self.get_dep_chain()\n    if dep_chain:\n        for dep in dep_chain:\n            dep.set_loader(loader)", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "filter_tagged_tasks", "parameters": ["self", "all_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["block.copy", "evaluate_and_append_task", "evaluate_block", "filtered_block.has_tasks", "isinstance", "task.evaluate_tags", "tmp_list.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Creates a new block, with task lists filtered based on the tags.", "source_code": "def filter_tagged_tasks(self, all_vars):\n    \"\"\"\n    Creates a new block, with task lists filtered based on the tags.\n    \"\"\"\n\n    def evaluate_and_append_task(target):\n        tmp_list = []\n        for task in target:\n            if isinstance(task, Block):\n                filtered_block = evaluate_block(task)\n                if filtered_block.has_tasks():\n                    tmp_list.append(filtered_block)\n            elif task.evaluate_tags(self._play.only_tags, self._play.skip_tags, all_vars=all_vars):\n                tmp_list.append(task)\n        return tmp_list\n\n    def evaluate_block(block):\n        new_block = block.copy(exclude_parent=True, exclude_tasks=True)\n        new_block._parent = block._parent\n        new_block.block = evaluate_and_append_task(block.block)\n        new_block.rescue = evaluate_and_append_task(block.rescue)\n        new_block.always = evaluate_and_append_task(block.always)\n        return new_block\n\n    return evaluate_block(self)", "loc": 25}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "get_tasks", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["evaluate_and_append_task", "evaluate_block", "isinstance", "rv.extend", "tmp_list.append", "tmp_list.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_tasks(self):\n    def evaluate_and_append_task(target):\n        tmp_list = []\n        for task in target:\n            if isinstance(task, Block):\n                tmp_list.extend(evaluate_block(task))\n            else:\n                tmp_list.append(task)\n        return tmp_list\n\n    def evaluate_block(block):\n        rv = evaluate_and_append_task(block.block)\n        rv.extend(evaluate_and_append_task(block.rescue))\n        rv.extend(evaluate_and_append_task(block.always))\n        return rv\n\n    return evaluate_block(self)", "loc": 17}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "get_include_params", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self._parent.get_include_params"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_include_params(self):\n    if self._parent:\n        return self._parent.get_include_params()\n    else:\n        return dict()", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "all_parents_static", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._parent.all_parents_static"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Determine if all of the parents of this block were statically loaded or not. Since Task/TaskInclude objects may be in the chain, they simply call their parents all_parents_static() method. Only Block objects in", "source_code": "def all_parents_static(self):\n    \"\"\"\n    Determine if all of the parents of this block were statically loaded\n    or not. Since Task/TaskInclude objects may be in the chain, they simply\n    call their parents all_parents_static() method. Only Block objects in\n    the chain check the statically_loaded value of the parent.\n    \"\"\"\n    from ansible.playbook.task_include import TaskInclude\n    if self._parent:\n        if isinstance(self._parent, TaskInclude) and not self._parent.statically_loaded:\n            return False\n        return self._parent.all_parents_static()\n\n    return True", "loc": 14}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": "Block", "function_name": "get_first_parent_include", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._parent.get_first_parent_include"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_first_parent_include(self):\n    from ansible.playbook.task_include import TaskInclude\n    if self._parent:\n        if isinstance(self._parent, TaskInclude):\n            return self._parent\n        return self._parent.get_first_parent_include()\n    return None", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": null, "function_name": "evaluate_and_append_task", "parameters": ["target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["evaluate_block", "filtered_block.has_tasks", "isinstance", "task.evaluate_tags", "tmp_list.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def evaluate_and_append_task(target):\n    tmp_list = []\n    for task in target:\n        if isinstance(task, Block):\n            filtered_block = evaluate_block(task)\n            if filtered_block.has_tasks():\n                tmp_list.append(filtered_block)\n        elif task.evaluate_tags(self._play.only_tags, self._play.skip_tags, all_vars=all_vars):\n            tmp_list.append(task)\n    return tmp_list", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": null, "function_name": "evaluate_block", "parameters": ["block"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["block.copy", "evaluate_and_append_task"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def evaluate_block(block):\n    new_block = block.copy(exclude_parent=True, exclude_tasks=True)\n    new_block._parent = block._parent\n    new_block.block = evaluate_and_append_task(block.block)\n    new_block.rescue = evaluate_and_append_task(block.rescue)\n    new_block.always = evaluate_and_append_task(block.always)\n    return new_block", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": null, "function_name": "evaluate_and_append_task", "parameters": ["target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["evaluate_block", "isinstance", "tmp_list.append", "tmp_list.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def evaluate_and_append_task(target):\n    tmp_list = []\n    for task in target:\n        if isinstance(task, Block):\n            tmp_list.extend(evaluate_block(task))\n        else:\n            tmp_list.append(task)\n    return tmp_list", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\block.py", "class_name": null, "function_name": "evaluate_block", "parameters": ["block"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["evaluate_and_append_task", "rv.extend"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def evaluate_block(block):\n    rv = evaluate_and_append_task(block.block)\n    rv.extend(evaluate_and_append_task(block.rescue))\n    rv.extend(evaluate_and_append_task(block.always))\n    return rv", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\handler.py", "class_name": "Handler", "function_name": "notify_host", "parameters": ["self", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.is_host_notified", "self.notified_hosts.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def notify_host(self, host):\n    if not self.is_host_notified(host):\n        self.notified_hosts.append(host)\n        return True\n    return False", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\handler.py", "class_name": "Handler", "function_name": "remove_host", "parameters": ["self", "host"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "self.notified_hosts.remove"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_host(self, host):\n    try:\n        self.notified_hosts.remove(host)\n    except ValueError:\n        raise AnsibleAssertionError(\n            f\"Attempting to remove a notification on handler '{self}' for host '{host}' but it has not been notified.\"\n        )", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\handler_task_include.py", "class_name": "HandlerTaskInclude", "function_name": "load", "parameters": ["data", "block", "role", "task_include", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HandlerTaskInclude", "t.check_options", "t.load_data"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n    t = HandlerTaskInclude(block=block, role=role, task_include=task_include)\n    handler = t.check_options(\n        t.load_data(data, variable_manager=variable_manager, loader=loader),\n        data\n    )\n\n    return handler", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\helpers.py", "class_name": null, "function_name": "load_list_of_blocks", "parameters": ["ds", "play", "parent_block", "role", "task_include", "use_handlers", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "Block.is_block", "Block.load", "block_list.append", "implicit_blocks.append", "isinstance", "iter", "len", "next", "range", "type"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Given a list of mixed task/block data (parsed from YAML), return a list of Block() objects, where implicit blocks are created for each bare Task.", "source_code": "def load_list_of_blocks(ds, play, parent_block=None, role=None, task_include=None, use_handlers=False, variable_manager=None, loader=None):\n    \"\"\"\n    Given a list of mixed task/block data (parsed from YAML),\n    return a list of Block() objects, where implicit blocks\n    are created for each bare Task.\n    \"\"\"\n\n    # we import here to prevent a circular dependency with imports\n    from ansible.playbook.block import Block\n\n    if not isinstance(ds, (list, type(None))):\n        raise AnsibleAssertionError('%s should be a list or None but is %s' % (ds, type(ds)))\n\n    block_list = []\n    if ds:\n        count = iter(range(len(ds)))\n        for i in count:\n            block_ds = ds[i]\n            # Implicit blocks are created by bare tasks listed in a play without\n            # an explicit block statement. If we have two implicit blocks in a row,\n            # squash them down to a single block to save processing time later.\n            implicit_blocks = []\n            while block_ds is not None and not Block.is_block(block_ds):\n                implicit_blocks.append(block_ds)\n                i += 1\n                # Advance the iterator, so we don't repeat\n                next(count, None)\n                try:\n                    block_ds = ds[i]\n                except IndexError:\n                    block_ds = None\n\n            # Loop both implicit blocks and block_ds as block_ds is the next in the list\n            for b in (implicit_blocks, block_ds):\n                if b:\n                    block_list.append(\n                        Block.load(\n                            b,\n                            play=play,\n                            parent_block=parent_block,\n                            role=role,\n                            task_include=task_include,\n                            use_handlers=use_handlers,\n                            variable_manager=variable_manager,\n                            loader=loader,\n                        )\n                    )\n\n    return block_list", "loc": 49}
{"file": "ansible\\lib\\ansible\\playbook\\helpers.py", "class_name": null, "function_name": "load_list_of_roles", "parameters": ["ds", "play", "current_role_path", "variable_manager", "loader", "collection_search_list"], "param_types": {}, "return_type": null, "param_doc": {"ds": "list of roles to load", "play": "calling Play object", "current_role_path": "path of the owning role, if any", "variable_manager": "varmgr to use for templating", "loader": "loader to use for DS parsing/services", "collection_search_list": "list of collections to search for unqualified role names"}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "RoleInclude.load", "isinstance", "roles.append", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Loads and returns a list of RoleInclude objects from the ds list of role definitions", "source_code": "def load_list_of_roles(ds, play, current_role_path=None, variable_manager=None, loader=None, collection_search_list=None):\n    \"\"\"\n    Loads and returns a list of RoleInclude objects from the ds list of role definitions\n    :param ds: list of roles to load\n    :param play: calling Play object\n    :param current_role_path: path of the owning role, if any\n    :param variable_manager: varmgr to use for templating\n    :param loader: loader to use for DS parsing/services\n    :param collection_search_list: list of collections to search for unqualified role names\n    :return:\n    \"\"\"\n    # we import here to prevent a circular dependency with imports\n    from ansible.playbook.role.include import RoleInclude\n\n    if not isinstance(ds, list):\n        raise AnsibleAssertionError('ds (%s) should be a list but was a %s' % (ds, type(ds)))\n\n    roles = []\n    for role_def in ds:\n        i = RoleInclude.load(role_def, play=play, current_role_path=current_role_path, variable_manager=variable_manager,\n                             loader=loader, collection_list=collection_search_list)\n        roles.append(i)\n\n    return roles", "loc": 24}
{"file": "ansible\\lib\\ansible\\playbook\\included_file.py", "class_name": "IncludedFile", "function_name": "add_host", "parameters": ["self", "host"], "param_types": {"host": "Host"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self._hosts.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_host(self, host: Host) -> None:\n    if host not in self._hosts:\n        self._hosts.append(host)\n        return\n\n    raise ValueError()", "loc": 6}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "get_name", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "is_sequence"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return the name of the Play", "source_code": "def get_name(self):\n    \"\"\" return the name of the Play \"\"\"\n    if self.name:\n        return self.name\n\n    if is_sequence(self.hosts):\n        self.name = ','.join(self.hosts)\n    else:\n        self.name = self.hosts or ''\n\n    return self.name", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "load", "parameters": ["data", "variable_manager", "loader", "vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Play", "p.load_data", "vars.copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, variable_manager=None, loader=None, vars=None):\n    p = Play()\n    if vars:\n        p.vars = vars.copy()\n    return p.load_data(data, variable_manager=variable_manager, loader=loader)", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "AnsibleParserError", "isinstance", "super", "super(Play, self).preprocess_data", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Adjusts play datastructure to cleanup old/legacy items", "source_code": "def preprocess_data(self, ds):\n    \"\"\"\n    Adjusts play datastructure to cleanup old/legacy items\n    \"\"\"\n\n    if not isinstance(ds, dict):\n        raise AnsibleAssertionError('while preprocessing data (%s), ds should be a dict but was a %s' % (ds, type(ds)))\n\n    # The use of 'user' in the Play datastructure was deprecated to\n    # line up with the same change for Tasks, due to the fact that\n    # 'user' conflicted with the user module.\n    if 'user' in ds:\n        # this should never happen, but error out with a helpful message\n        # to the user if it does...\n        if 'remote_user' in ds:\n            raise AnsibleParserError(\"both 'user' and 'remote_user' are set for this play. \"\n                                     \"The use of 'user' is deprecated, and should be removed\", obj=ds)\n\n        ds['remote_user'] = ds['user']\n        del ds['user']\n\n    return super(Play, self).preprocess_data(ds)", "loc": 22}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "compile_roles_handlers", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["block_list.extend", "len", "r.get_handler_blocks"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Handles the role handler compilation step, returning a flat list of Handlers This is done for all roles in the Play.", "source_code": "def compile_roles_handlers(self):\n    \"\"\"\n    Handles the role handler compilation step, returning a flat list of Handlers\n    This is done for all roles in the Play.\n    \"\"\"\n\n    block_list = []\n\n    if len(self.roles) > 0:\n        for r in self.roles:\n            if r.from_include:\n                continue\n            block_list.extend(r.get_handler_blocks(play=self))\n\n    return block_list", "loc": 15}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "compile", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Block", "Task", "block_list.append", "block_list.extend", "noop_task.copy", "noop_task.set_loader", "self._compile_roles", "t.set_loader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Compiles and returns the task list for this play, compiled from the roles (which are themselves compiled recursively) and/or the list of tasks specified in the play.", "source_code": "def compile(self):\n    \"\"\"\n    Compiles and returns the task list for this play, compiled from the\n    roles (which are themselves compiled recursively) and/or the list of\n    tasks specified in the play.\n    \"\"\"\n    # create a block containing a single flush handlers meta\n    # task, so we can be sure to run handlers at certain points\n    # of the playbook execution\n    flush_block = Block(play=self)\n\n    t = Task(block=flush_block)\n    t.action = 'meta'\n    t._resolved_action = 'ansible.builtin.meta'\n    t.args['_raw_params'] = 'flush_handlers'\n    t.implicit = True\n    t.set_loader(self._loader)\n    t.tags = ['always']\n\n    flush_block.block = [t]\n\n    # NOTE keep flush_handlers tasks even if a section has no regular tasks,\n    #      there may be notified handlers from the previous section\n    #      (typically when a handler notifies a handler defined before)\n    block_list = []\n    if self.force_handlers:\n        noop_task = Task()\n        noop_task.action = 'meta'\n        noop_task.args['_raw_params'] = 'noop'\n        noop_task.implicit = True\n        noop_task.set_loader(self._loader)\n\n        b = Block(play=self)\n        if self.pre_tasks:\n            b.block = self.pre_tasks\n        else:\n            nt = noop_task.copy(exclude_parent=True)\n            nt._parent = b\n            b.block = [nt]\n        b.always = [flush_block]\n        block_list.append(b)\n\n        tasks = self._compile_roles() + self.tasks\n        b = Block(play=self)\n        if tasks:\n            b.block = tasks\n        else:\n            nt = noop_task.copy(exclude_parent=True)\n            nt._parent = b\n            b.block = [nt]\n        b.always = [flush_block]\n        block_list.append(b)\n\n        b = Block(play=self)\n        if self.post_tasks:\n            b.block = self.post_tasks\n        else:\n            nt = noop_task.copy(exclude_parent=True)\n            nt._parent = b\n            b.block = [nt]\n        b.always = [flush_block]\n        block_list.append(b)\n\n        return block_list\n\n    block_list.extend(self.pre_tasks)\n    block_list.append(flush_block)\n    block_list.extend(self._compile_roles())\n    block_list.extend(self.tasks)\n    block_list.append(flush_block)\n    block_list.extend(self.post_tasks)\n    block_list.append(flush_block)\n\n    return block_list", "loc": 74}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "get_vars_files", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_vars_files(self):\n    if self.vars_files is None:\n        return []\n    elif not isinstance(self.vars_files, list):\n        return [self.vars_files]\n    return self.vars_files", "loc": 6}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "get_tasks", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "tasklist.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_tasks(self):\n    tasklist = []\n    for task in self.pre_tasks + self.tasks + self.post_tasks:\n        if isinstance(task, Block):\n            tasklist.append(task.block + task.rescue + task.always)\n        else:\n            tasklist.append(task)\n    return tasklist", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "copy", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.role_cache.copy", "super", "super(Play, self).copy"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(self):\n    new_me = super(Play, self).copy()\n    new_me.role_cache = self.role_cache.copy()\n    new_me._included_conditional = self._included_conditional\n    new_me._included_path = self._included_path\n    new_me._action_groups = self._action_groups\n    new_me._group_actions = self._group_actions\n    return new_me", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\play.py", "class_name": "Play", "function_name": "argument_spec", "parameters": ["self"], "param_types": {}, "return_type": "dict", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._loader.load_from_file"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Retrieve the argument spec if one is configured.", "source_code": "def argument_spec(self) -> dict:\n    \"\"\"Retrieve the argument spec if one is configured.\"\"\"\n    if not self.validate_argspec:\n        return {}\n\n    return self._loader.load_from_file(self._metadata_path)['argument_specs'][self.validate_argspec]['options']", "loc": 6}
{"file": "ansible\\lib\\ansible\\playbook\\playbook_include.py", "class_name": "PlaybookInclude", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleError", "ds.pop", "iter", "len", "next", "sorted"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def preprocess_data(self, ds):\n    keys = {action for action in C._ACTION_IMPORT_PLAYBOOK if action in ds}\n\n    if len(keys) != 1:\n        raise AnsibleError(f'Found conflicting import_playbook actions: {\", \".join(sorted(keys))}')\n\n    key = next(iter(keys))\n\n    ds['import_playbook'] = ds.pop(key)\n\n    return ds", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\play_context.py", "class_name": "PlayContext", "function_name": "set_attributes_from_plugin", "parameters": ["self", "plugin"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.config.get_configuration_definitions", "options[option].get", "plugin.get_option", "setattr"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_attributes_from_plugin(self, plugin):\n    # generic derived from connection plugin, temporary for backwards compat, in the end we should not set play_context properties\n\n    # get options for plugins\n    options = C.config.get_configuration_definitions(plugin.plugin_type, plugin._load_name)\n    for option in options:\n        if option:\n            flag = options[option].get('name')\n            if flag:\n                setattr(self, flag, plugin.get_option(flag))", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\play_context.py", "class_name": "PlayContext", "function_name": "set_attributes_from_cli", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["context.CLIARGS.get", "int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Configures this connection information instance with data from options specified by the user on the command line. These have a lower precedence than those set on the play or host.", "source_code": "def set_attributes_from_cli(self):\n    \"\"\"\n    Configures this connection information instance with data from\n    options specified by the user on the command line. These have a\n    lower precedence than those set on the play or host.\n    \"\"\"\n    if context.CLIARGS.get('timeout', False):\n        self.timeout = int(context.CLIARGS['timeout'])\n\n    # From the command line.  These should probably be used directly by plugins instead\n    # For now, they are likely to be moved to FieldAttribute defaults\n    self.private_key_file = context.CLIARGS.get('private_key_file')  # Else default\n    self._internal_verbosity = context.CLIARGS.get('verbosity')  # Else default\n\n    # Not every cli that uses PlayContext has these command line args so have a default\n    self.start_at_task = context.CLIARGS.get('start_at_task', None)  # Else default", "loc": 16}
{"file": "ansible\\lib\\ansible\\playbook\\play_context.py", "class_name": "PlayContext", "function_name": "update_vars", "parameters": ["self", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.MAGIC_VARIABLE_MAPPING.items", "getattr"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Adds 'magic' variables relating to connections to the variable dictionary provided. In case users need to access from the play, this is a legacy from runner.", "source_code": "def update_vars(self, variables):\n    \"\"\"\n    Adds 'magic' variables relating to connections to the variable dictionary provided.\n    In case users need to access from the play, this is a legacy from runner.\n    \"\"\"\n\n    for prop, var_list in C.MAGIC_VARIABLE_MAPPING.items():\n        try:\n            if 'become' in prop:\n                continue\n\n            var_val = getattr(self, prop)\n            for var_opt in var_list:\n                if var_opt not in variables and var_val is not None:\n                    variables[var_opt] = var_val\n        except AttributeError:\n            continue", "loc": 17}
{"file": "ansible\\lib\\ansible\\playbook\\role_include.py", "class_name": "IncludeRole", "function_name": "get_block_list", "parameters": ["self", "play", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Role.load", "RoleInclude.load", "TemplateEngine", "actual_role.compile", "actual_role.get_dep_chain", "actual_role.get_handler_blocks", "myplay.roles.append", "self.build_parent_block", "templar.template", "variable_manager.get_vars"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_block_list(self, play=None, variable_manager=None, loader=None):\n\n    # only need play passed in when dynamic\n    if play is None:\n        myplay = self._parent._play\n    else:\n        myplay = play\n\n    ri = RoleInclude.load(self._role_name, play=myplay, variable_manager=variable_manager, loader=loader, collection_list=self.collections)\n    ri.vars |= self.vars\n\n    if variable_manager is not None:\n        available_variables = variable_manager.get_vars(play=myplay, task=self)\n    else:\n        available_variables = {}\n    templar = TemplateEngine(loader=loader, variables=available_variables)\n    from_files = templar.template(self._from_files)\n\n    # build role\n    actual_role = Role.load(ri, myplay, parent_role=self._parent_role, from_files=from_files,\n                            from_include=True, validate=self.rolespec_validate, public=self.public, static=self.statically_loaded)\n    actual_role._metadata.allow_duplicates = self.allow_duplicates\n\n    # add role to play\n    myplay.roles.append(actual_role)\n\n    # save this for later use\n    self._role_path = actual_role._role_path\n\n    # compile role with parent roles as dependencies to ensure they inherit\n    # variables\n    dep_chain = actual_role.get_dep_chain()\n\n    p_block = self.build_parent_block()\n\n    # collections value is not inherited; override with the value we calculated during role setup\n    p_block.collections = actual_role.collections\n\n    blocks = actual_role.compile(play=myplay, dep_chain=dep_chain)\n    for b in blocks:\n        b._parent = p_block\n        # HACK: parent inheritance doesn't seem to have a way to handle this intermediate override until squashed/finalized\n        b.collections = actual_role.collections\n\n    # updated available handlers in play\n    handlers = actual_role.get_handler_blocks(play=myplay, dep_chain=dep_chain)\n    for h in handlers:\n        h._parent = p_block\n    myplay.handlers = myplay.handlers + handlers\n    return blocks, handlers", "loc": 50}
{"file": "ansible\\lib\\ansible\\playbook\\role_include.py", "class_name": "IncludeRole", "function_name": "load", "parameters": ["data", "block", "role", "task_include", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "AnsibleParserError", "IncludeRole", "IncludeRole(block, role, task_include=task_include).load_data", "frozenset", "ir.args.get", "ir.args.keys", "isinstance", "key.removesuffix", "list", "my_arg_names.difference", "my_arg_names.intersection", "setattr", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n\n    ir = IncludeRole(block, role, task_include=task_include).load_data(data, variable_manager=variable_manager, loader=loader)\n\n    # Validate options\n    my_arg_names = frozenset(ir.args.keys())\n\n    # name is needed, or use role as alias\n    ir._role_name = ir.args.get('name', ir.args.get('role'))\n    if ir._role_name is None:\n        raise AnsibleParserError(\"'name' is a required field for %s.\" % ir.action, obj=data)\n\n    # validate bad args, otherwise we silently ignore\n    bad_opts = my_arg_names.difference(IncludeRole.VALID_ARGS)\n    if bad_opts:\n        raise AnsibleParserError('Invalid options for %s: %s' % (ir.action, ','.join(list(bad_opts))), obj=data)\n\n    # build options for role include/import tasks\n    for key in my_arg_names.intersection(IncludeRole.FROM_ARGS):\n        from_key = key.removesuffix('_from')\n        args_value = ir.args.get(key)\n        if not isinstance(args_value, str):\n            raise AnsibleParserError('Expected a string for %s but got %s instead' % (key, type(args_value)))\n        ir._from_files[from_key] = args_value\n\n    # apply is only valid for includes, not imports as they inherit directly\n    apply_attrs = ir.args.get('apply', {})\n    if apply_attrs and ir.action not in C._ACTION_INCLUDE_ROLE:\n        raise AnsibleParserError('Invalid options for %s: apply' % ir.action, obj=data)\n    elif not isinstance(apply_attrs, dict):\n        raise AnsibleParserError('Expected a dict for apply but got %s instead' % type(apply_attrs), obj=data)\n\n    # manual list as otherwise the options would set other task parameters we don't want.\n    for option in my_arg_names.intersection(IncludeRole.OTHER_ARGS):\n        setattr(ir, option, ir.args.get(option))\n\n    return ir", "loc": 37}
{"file": "ansible\\lib\\ansible\\playbook\\role_include.py", "class_name": "IncludeRole", "function_name": "copy", "parameters": ["self", "exclude_parent", "exclude_tasks"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._from_files.copy", "super", "super(IncludeRole, self).copy"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(self, exclude_parent=False, exclude_tasks=False):\n\n    new_me = super(IncludeRole, self).copy(exclude_parent=exclude_parent, exclude_tasks=exclude_tasks)\n    new_me.statically_loaded = self.statically_loaded\n    new_me._from_files = self._from_files.copy()\n    new_me._parent_role = self._parent_role\n    new_me._role_name = self._role_name\n    new_me._role_path = self._role_path\n\n    return new_me", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\role_include.py", "class_name": "IncludeRole", "function_name": "get_include_params", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent_role.get_name", "self._parent_role.get_role_params", "super", "super(IncludeRole, self).get_include_params", "v.setdefault", "v.setdefault('ansible_parent_role_names', []).insert", "v.setdefault('ansible_parent_role_paths', []).insert"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_include_params(self):\n    v = super(IncludeRole, self).get_include_params()\n    if self._parent_role:\n        v |= self._parent_role.get_role_params()\n        v.setdefault('ansible_parent_role_names', []).insert(0, self._parent_role.get_name())\n        v.setdefault('ansible_parent_role_paths', []).insert(0, self._parent_role._role_path)\n    return v", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\taggable.py", "class_name": "Taggable", "function_name": "evaluate_tags", "parameters": ["self", "only_tags", "skip_tags", "all_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateEngine", "_flatten_tags", "getattr", "self._get_all_taggable_objects", "set", "tags.isdisjoint", "templar.template"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Check if the current item should be executed depending on the specified tags. NOTE this method is assumed to be called only on Task objects.", "source_code": "def evaluate_tags(self, only_tags, skip_tags, all_vars):\n    \"\"\"Check if the current item should be executed depending on the specified tags.\n\n    NOTE this method is assumed to be called only on Task objects.\n    \"\"\"\n    if self.tags:\n        templar = TemplateEngine(loader=self._loader, variables=all_vars)\n        for obj in self._get_all_taggable_objects():\n            if (_tags := getattr(obj, \"_tags\", Sentinel)) is not Sentinel:\n                obj._tags = _flatten_tags(templar.template(_tags))\n        tags = set(self.tags)\n    else:\n        # this makes isdisjoint work for untagged\n        tags = self.untagged\n\n    should_run = True  # default, tasks to run\n\n    if only_tags:\n        if 'always' in tags:\n            should_run = True\n        elif ('all' in only_tags and 'never' not in tags):\n            should_run = True\n        elif not tags.isdisjoint(only_tags):\n            should_run = True\n        elif 'tagged' in only_tags and tags != self.untagged and 'never' not in tags:\n            should_run = True\n        else:\n            should_run = False\n\n    if should_run and skip_tags:\n\n        # Check for tags that we need to skip\n        if 'all' in skip_tags:\n            if 'always' not in tags or 'always' in skip_tags:\n                should_run = False\n        elif not tags.isdisjoint(skip_tags):\n            should_run = False\n        elif 'tagged' in skip_tags and tags != self.untagged:\n            should_run = False\n\n    return should_run", "loc": 41}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "resolved_action", "parameters": ["self"], "param_types": {}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.warning", "is_possibly_template", "self._resolve_action"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "The templated and resolved FQCN of the task action or None. If the action is a template, callback plugins can only use this value in certain methods. - v2_runner_on_ok and v2_runner_on_failed if there's no task loop", "source_code": "def resolved_action(self) -> str | None:\n    \"\"\"The templated and resolved FQCN of the task action or None.\n\n    If the action is a template, callback plugins can only use this value in certain methods.\n    - v2_runner_on_ok and v2_runner_on_failed if there's no task loop\n    - v2_runner_item_on_ok and v2_runner_item_on_failed if there is a task loop\n    \"\"\"\n    # Consider deprecating this because it's difficult to use?\n    # Moving it to the task result would improve the no-loop limitation on v2_runner_on_ok\n    # but then wouldn't be accessible to v2_playbook_on_task_start, *_on_skipped, etc.\n    if self._resolved_action is not None:\n        return self._resolved_action\n    if not is_possibly_template(self.action):\n        try:\n            return self._resolve_action(self.action)\n        except AnsibleParserError:\n            display.warning(self._resolved_action_warning, obj=self.action)\n    else:\n        display.warning(self._resolved_action_warning, obj=self.action)\n    return None", "loc": 20}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "get_name", "parameters": ["self", "include_role_fqcn"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._role.get_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return the name of the task", "source_code": "def get_name(self, include_role_fqcn=True):\n    \"\"\" return the name of the task \"\"\"\n\n    if self._role:\n        role_name = self._role.get_name(include_role_fqcn=include_role_fqcn)\n\n    if self._role and self.name:\n        return \"%s : %s\" % (role_name, self.name)\n    elif self.name:\n        return self.name\n    else:\n        if self._role:\n            return \"%s : %s\" % (role_name, self.action)\n        else:\n            return \"%s\" % (self.action,)", "loc": 15}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "AnsibleParserError", "AnsibleTagHelper.tag_copy", "ModuleArgsParser", "args_parser.parse", "collections_list.append", "collections_list.insert", "dict", "display.warning", "ds.get", "ds.items", "isinstance", "k.removeprefix", "k.startswith", "self._load_vars", "self._preprocess_with_loop", "self.fattributes.get", "self.get_validated_value", "super", "super(Task, self).preprocess_data", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "tasks are especially complex arguments so need pre-processing. keep it short.", "source_code": "def preprocess_data(self, ds):\n    \"\"\"\n    tasks are especially complex arguments so need pre-processing.\n    keep it short.\n    \"\"\"\n\n    if not isinstance(ds, dict):\n        raise AnsibleAssertionError('ds (%s) should be a dict but was a %s' % (ds, type(ds)))\n\n    # the new, cleaned datastructure, which will have legacy items reduced to a standard structure suitable for the\n    # attributes of the task class; copy any tagged data to preserve things like origin\n    new_ds = AnsibleTagHelper.tag_copy(ds, {})\n\n    # since this affects the task action parsing, we have to resolve in preprocess instead of in typical validator\n    default_collection = AnsibleCollectionConfig.default_collection\n\n    collections_list = ds.get('collections')\n    if collections_list is None:\n        # use the parent value if our ds doesn't define it\n        collections_list = self.collections\n    else:\n        # Validate this untemplated field early on to guarantee we are dealing with a list.\n        # This is also done in CollectionSearch._load_collections() but this runs before that call.\n        collections_list = self.get_validated_value('collections', self.fattributes.get('collections'), collections_list, None)\n\n    if default_collection and not self._role:  # FIXME: and not a collections role\n        if collections_list:\n            if default_collection not in collections_list:\n                collections_list.insert(0, default_collection)\n        else:\n            collections_list = [default_collection]\n\n    if collections_list and 'ansible.builtin' not in collections_list and 'ansible.legacy' not in collections_list:\n        collections_list.append('ansible.legacy')\n\n    if collections_list:\n        ds['collections'] = collections_list\n\n    # use the args parsing class to determine the action, args,\n    # and the delegate_to value from the various possible forms\n    # supported as legacy\n    args_parser = ModuleArgsParser(task_ds=ds, collection_list=collections_list)\n    try:\n        (action, args, delegate_to) = args_parser.parse()\n    except AnsibleParserError as ex:\n        # if the raises exception was created with obj=ds args, then it includes the detail\n        # so we dont need to add it so we can just re raise.\n        if ex.obj:\n            raise\n        # But if it wasn't, we can add the yaml object now to get more detail\n        raise AnsibleParserError(\"Error parsing task arguments.\", obj=ds) from ex\n\n    if args_parser._resolved_action is not None:\n        self._resolved_action = args_parser._resolved_action\n\n    new_ds['action'] = action\n    new_ds['args'] = args\n    new_ds['delegate_to'] = delegate_to\n\n    # we handle any 'vars' specified in the ds here, as we may\n    # be adding things to them below (special handling for includes).\n    # When that deprecated feature is removed, this can be too.\n    if 'vars' in ds:\n        # _load_vars is defined in Base, and is used to load a dictionary\n        # or list of dictionaries in a standard way\n        new_ds['vars'] = self._load_vars(None, ds.get('vars'))\n    else:\n        new_ds['vars'] = dict()\n\n    for (k, v) in ds.items():\n        if k in ('action', 'local_action', 'args', 'delegate_to') or k == action or k == 'shell':\n            # we don't want to re-assign these values, which were determined by the ModuleArgsParser() above\n            continue\n        elif k.startswith('with_') and k.removeprefix(\"with_\") in lookup_loader:\n            # transform into loop property\n            self._preprocess_with_loop(ds, new_ds, k, v)\n        elif C.INVALID_TASK_ATTRIBUTE_FAILED or k in self.fattributes:\n            new_ds[k] = v\n        else:\n            display.warning(\"Ignoring invalid attribute: %s\" % k)\n\n    return super(Task, self).preprocess_data(new_ds)", "loc": 82}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "post_validate", "parameters": ["self", "templar"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent.post_validate", "super", "super(Task, self).post_validate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Override of base class post_validate, to also do final validation on the block and task include (if any) to which this task belongs.", "source_code": "def post_validate(self, templar):\n    \"\"\"\n    Override of base class post_validate, to also do final validation on\n    the block and task include (if any) to which this task belongs.\n    \"\"\"\n\n    if self._parent:\n        self._parent.post_validate(templar)\n\n    super(Task, self).post_validate(templar)", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "get_vars", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self._parent.get_vars"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_vars(self):\n    all_vars = dict()\n    if self._parent:\n        all_vars |= self._parent.get_vars()\n\n    all_vars |= self.vars\n\n    if 'tags' in all_vars:\n        del all_vars['tags']\n    if 'when' in all_vars:\n        del all_vars['when']\n\n    return all_vars", "loc": 13}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "get_include_params", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self._parent.get_include_params"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_include_params(self):\n    all_vars = dict()\n    if self._parent:\n        all_vars |= self._parent.get_include_params()\n    if self.action in C._ACTION_ALL_INCLUDES:\n        all_vars |= self.vars\n    return all_vars", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "copy", "parameters": ["self", "exclude_parent", "exclude_tasks"], "param_types": {"exclude_parent": "bool", "exclude_tasks": "bool"}, "return_type": "Task", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent.copy", "super", "super(Task, self).copy"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(self, exclude_parent: bool = False, exclude_tasks: bool = False) -> Task:\n    new_me = super(Task, self).copy()\n\n    new_me._parent = None\n    if self._parent and not exclude_parent:\n        new_me._parent = self._parent.copy(exclude_tasks=exclude_tasks)\n\n    new_me._role = None\n    if self._role:\n        new_me._role = self._role\n\n    new_me.implicit = self.implicit\n    new_me._resolved_action = self._resolved_action\n    new_me._uuid = self._uuid\n\n    return new_me", "loc": 16}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "set_loader", "parameters": ["self", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parent.set_loader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the loader on this object and recursively on parent, child objects. This is used primarily after the Task has been serialized/deserialized, which does not preserve the loader.", "source_code": "def set_loader(self, loader):\n    \"\"\"\n    Sets the loader on this object and recursively on parent, child objects.\n    This is used primarily after the Task has been serialized/deserialized, which\n    does not preserve the loader.\n    \"\"\"\n\n    self._loader = loader\n\n    if self._parent:\n        self._parent.set_loader(loader)", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "get_first_parent_include", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._parent.get_first_parent_include"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_first_parent_include(self):\n    from ansible.playbook.task_include import TaskInclude\n    if self._parent:\n        if isinstance(self._parent, TaskInclude):\n            return self._parent\n        return self._parent.get_first_parent_include()\n    return None", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "get_play", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance"], "control_structures": ["While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_play(self):\n    parent = self._parent\n    while not isinstance(parent, Block):\n        parent = parent._parent\n    return parent._play", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "dump_attrs", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["attrs.update", "super", "super().dump_attrs"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Override to smuggle important non-FieldAttribute values back to the controller.", "source_code": "def dump_attrs(self):\n    \"\"\"Override to smuggle important non-FieldAttribute values back to the controller.\"\"\"\n    attrs = super().dump_attrs()\n    attrs.update(_resolved_action=self._resolved_action)\n    return attrs", "loc": 5}
{"file": "ansible\\lib\\ansible\\playbook\\task.py", "class_name": "Task", "function_name": "from_attrs", "parameters": ["self", "attrs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().from_attrs"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_attrs(self, attrs):\n    super().from_attrs(attrs)\n\n    # from_attrs is only used to create a finalized task\n    # from attrs from the Worker/TaskExecutor\n    # Those attrs are finalized and squashed in the TE\n    # and controller side use needs to reflect that\n    self._finalized = True\n    self._squashed = True", "loc": 9}
{"file": "ansible\\lib\\ansible\\playbook\\task_include.py", "class_name": "TaskInclude", "function_name": "load", "parameters": ["data", "block", "role", "task_include", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TaskInclude", "ti.check_options", "ti.load_data"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, block=None, role=None, task_include=None, variable_manager=None, loader=None):\n    ti = TaskInclude(block=block, role=role, task_include=task_include)\n    task = ti.check_options(\n        ti.load_data(data, variable_manager=variable_manager, loader=loader),\n        data\n    )\n\n    return task", "loc": 8}
{"file": "ansible\\lib\\ansible\\playbook\\task_include.py", "class_name": "TaskInclude", "function_name": "check_options", "parameters": ["self", "task", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "AnsibleParserError", "frozenset", "isinstance", "list", "my_arg_names.difference", "task.args.get", "task.args.keys", "task.args.pop", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Method for options validation to use in 'load_data' for TaskInclude and HandlerTaskInclude since they share the same validations. It is not named 'validate_options' on purpose to prevent confusion with '_validate_*\" methods. Note that the task passed might be changed", "source_code": "def check_options(self, task, data):\n    \"\"\"\n    Method for options validation to use in 'load_data' for TaskInclude and HandlerTaskInclude\n    since they share the same validations. It is not named 'validate_options' on purpose\n    to prevent confusion with '_validate_*\" methods. Note that the task passed might be changed\n    as a side-effect of this method.\n    \"\"\"\n    my_arg_names = frozenset(task.args.keys())\n\n    # validate bad args, otherwise we silently ignore\n    bad_opts = my_arg_names.difference(self.VALID_ARGS)\n    if bad_opts and task.action in C._ACTION_ALL_PROPER_INCLUDE_IMPORT_TASKS:\n        raise AnsibleParserError('Invalid options for %s: %s' % (task.action, ','.join(list(bad_opts))), obj=data)\n\n    if not task.args.get('_raw_params'):\n        task.args['_raw_params'] = task.args.pop('file', None)\n        if not task.args['_raw_params']:\n            raise AnsibleParserError('No file specified for %s' % task.action, obj=data)\n\n    apply_attrs = task.args.get('apply', {})\n    if apply_attrs and task.action not in C._ACTION_INCLUDE_TASKS:\n        raise AnsibleParserError('Invalid options for %s: apply' % task.action, obj=data)\n    elif not isinstance(apply_attrs, dict):\n        raise AnsibleParserError('Expected a dict for apply but got %s instead' % type(apply_attrs), obj=data)\n\n    return task", "loc": 26}
{"file": "ansible\\lib\\ansible\\playbook\\task_include.py", "class_name": "TaskInclude", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "display.warning", "ds.keys", "set", "set(ds.keys()).difference", "super", "super(TaskInclude, self).preprocess_data"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def preprocess_data(self, ds):\n    ds = super(TaskInclude, self).preprocess_data(ds)\n\n    diff = set(ds.keys()).difference(self.VALID_INCLUDE_KEYWORDS)\n    for k in diff:\n        # This check doesn't handle ``include`` as we have no idea at this point if it is static or not\n        if ds[k] is not Sentinel and ds['action'] in C._ACTION_ALL_INCLUDE_ROLE_TASKS:\n            if C.INVALID_TASK_ATTRIBUTE_FAILED:\n                raise AnsibleParserError(\"'%s' is not a valid attribute for a %s\" % (k, self.__class__.__name__), obj=ds)\n            else:\n                display.warning(\"Ignoring invalid attribute: %s\" % k)\n\n    return ds", "loc": 13}
{"file": "ansible\\lib\\ansible\\playbook\\task_include.py", "class_name": "TaskInclude", "function_name": "build_parent_block", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Block.load", "self.args.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "This method is used to create the parent block for the included tasks when ``apply`` is specified", "source_code": "def build_parent_block(self):\n    \"\"\"\n    This method is used to create the parent block for the included tasks\n    when ``apply`` is specified\n    \"\"\"\n    apply_attrs = self.args.pop('apply', {})\n    if apply_attrs:\n        apply_attrs['block'] = []\n        p_block = Block.load(\n            apply_attrs,\n            play=self._parent._play,\n            task_include=self,\n            role=self._role,\n            variable_manager=self._variable_manager,\n            loader=self._loader,\n        )\n    else:\n        p_block = self\n\n    return p_block", "loc": 20}
{"file": "ansible\\lib\\ansible\\playbook\\role\\definition.py", "class_name": "RoleDefinition", "function_name": "preprocess_data", "parameters": ["self", "ds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "AnsibleTagHelper.tag_copy", "isinstance", "self._load_role_name", "self._load_role_path", "self._split_role_params", "super", "super(RoleDefinition, self).preprocess_data"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def preprocess_data(self, ds):\n    # role names that are simply numbers can be parsed by PyYAML\n    # as integers even when quoted, so turn it into a string type\n    if isinstance(ds, int):\n        ds = \"%s\" % ds\n\n    if not isinstance(ds, dict) and not isinstance(ds, str):\n        raise AnsibleAssertionError()\n\n    if isinstance(ds, dict):\n        ds = super(RoleDefinition, self).preprocess_data(ds)\n\n    # save the original ds for use later\n    self._ds = ds\n\n    # the new, cleaned datastructure, which will have legacy items reduced to a standard structure suitable for the\n    # attributes of the task class; copy any tagged data to preserve things like origin\n    new_ds = AnsibleTagHelper.tag_copy(ds, {})\n\n    # first we pull the role name out of the data structure,\n    # and then use that to determine the role path (which may\n    # result in a new role name, if it was a file path)\n    role_name = self._load_role_name(ds)\n    (role_name, role_path) = self._load_role_path(role_name)\n\n    # next, we split the role params out from the valid role\n    # attributes and update the new datastructure with that\n    # result and the role name\n    if isinstance(ds, dict):\n        (new_role_def, role_params) = self._split_role_params(ds)\n        new_ds |= new_role_def\n        self._role_params = role_params\n\n    # set the role name in the new ds\n    new_ds['role'] = role_name\n\n    # we store the role path internally\n    self._role_path = role_path\n\n    # and return the cleaned-up data structure\n    return new_ds", "loc": 41}
{"file": "ansible\\lib\\ansible\\playbook\\role\\include.py", "class_name": "RoleInclude", "function_name": "load", "parameters": ["data", "play", "current_role_path", "parent_role", "variable_manager", "loader", "collection_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleParserError", "RoleInclude", "isinstance", "ri.load_data"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, play, current_role_path=None, parent_role=None, variable_manager=None, loader=None, collection_list=None):\n\n    if not (isinstance(data, str) or isinstance(data, dict)):\n        raise AnsibleParserError(\"Invalid role definition.\", obj=data)\n\n    if isinstance(data, str) and ',' in data:\n        raise AnsibleError(\"Invalid old style role requirement: %s\" % data)\n\n    ri = RoleInclude(play=play, role_basedir=current_role_path, variable_manager=variable_manager, loader=loader, collection_list=collection_list)\n    return ri.load_data(data, variable_manager=variable_manager, loader=loader)", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\role\\metadata.py", "class_name": "RoleMetadata", "function_name": "load", "parameters": ["data", "owner", "variable_manager", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "RoleMetadata", "RoleMetadata(owner=owner).load_data", "isinstance", "owner.get_name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(data, owner, variable_manager=None, loader=None):\n    \"\"\"\n    Returns a new RoleMetadata object based on the datastructure passed in.\n    \"\"\"\n\n    if not isinstance(data, dict):\n        raise AnsibleParserError(\"the 'meta/main.yml' for role %s is not a dictionary\" % owner.get_name())\n\n    m = RoleMetadata(owner=owner).load_data(data, variable_manager=variable_manager, loader=loader)\n    return m", "loc": 10}
{"file": "ansible\\lib\\ansible\\playbook\\role\\requirement.py", "class_name": "RoleRequirement", "function_name": "repo_url_to_role_name", "parameters": ["repo_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["repo_url.split", "trailing_path.endswith", "trailing_path.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def repo_url_to_role_name(repo_url):\n    # gets the role name out of a repo like\n    # http://git.example.com/repos/repo.git\" => \"repo\"\n\n    if '://' not in repo_url and '@' not in repo_url:\n        return repo_url\n    trailing_path = repo_url.split('/')[-1]\n    if trailing_path.endswith('.git'):\n        trailing_path = trailing_path[:-4]\n    if trailing_path.endswith('.tar.gz'):\n        trailing_path = trailing_path[:-7]\n    if ',' in trailing_path:\n        trailing_path = trailing_path.split(',')[0]\n    return trailing_path", "loc": 14}
{"file": "ansible\\lib\\ansible\\playbook\\role\\requirement.py", "class_name": "RoleRequirement", "function_name": "role_yaml_parse", "parameters": ["role"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "RoleRequirement.repo_url_to_role_name", "dict", "isinstance", "list", "role.copy", "role.count", "role.keys", "role.pop", "role.strip", "role.strip().split", "role['src'].endswith", "role['src'].partition", "src.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def role_yaml_parse(role):\n\n    if isinstance(role, str):\n        name = None\n        scm = None\n        src = None\n        version = None\n        if ',' in role:\n            if role.count(',') == 1:\n                (src, version) = role.strip().split(',', 1)\n            elif role.count(',') == 2:\n                (src, version, name) = role.strip().split(',', 2)\n            else:\n                raise AnsibleError(\"Invalid role line (%s). Proper format is 'role_name[,version[,name]]'\" % role)\n        else:\n            src = role\n\n        if name is None:\n            name = RoleRequirement.repo_url_to_role_name(src)\n        if '+' in src:\n            (scm, src) = src.split('+', 1)\n\n        return dict(name=name, src=src, scm=scm, version=version)\n\n    if 'role' in role:\n        name = role['role']\n        if ',' in name:\n            raise AnsibleError(\"Invalid old style role requirement: %s\" % name)\n        else:\n            del role['role']\n            role['name'] = name\n    else:\n        role = role.copy()\n\n        if 'src' in role:\n            # New style: { src: 'galaxy.role,version,name', other_vars: \"here\" }\n            if 'github.com' in role[\"src\"] and 'http' in role[\"src\"] and '+' not in role[\"src\"] and not role[\"src\"].endswith('.tar.gz'):\n                role[\"src\"] = \"git+\" + role[\"src\"]\n\n            if '+' in role[\"src\"]:\n                role[\"scm\"], dummy, role[\"src\"] = role[\"src\"].partition('+')\n\n            if 'name' not in role:\n                role[\"name\"] = RoleRequirement.repo_url_to_role_name(role[\"src\"])\n\n        if 'version' not in role:\n            role['version'] = ''\n\n        if 'scm' not in role:\n            role['scm'] = None\n\n    for key in list(role.keys()):\n        if key not in VALID_SPEC_KEYS:\n            role.pop(key)\n\n    return role", "loc": 56}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": null, "function_name": "hash_params", "parameters": ["params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_display.deprecated", "frozenset", "hash_params", "isinstance", "new_params.add", "params.items", "set"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "DEPRECATED Construct a data structure of parameters that is hashable. This requires changing any mutable data structures into immutable ones.", "source_code": "def hash_params(params):\n    \"\"\"\n    DEPRECATED\n    Construct a data structure of parameters that is hashable.\n\n    This requires changing any mutable data structures into immutable ones.\n    We chose a frozenset because role parameters have to be unique.\n\n    .. warning::  this does not handle unhashable scalars.  Two things\n        mitigate that limitation:\n\n        1) There shouldn't be any unhashable scalars specified in the yaml\n        2) Our only choice would be to return an error anyway.\n    \"\"\"\n\n    _display.deprecated(\n        msg=\"The hash_params function is deprecated as its consumers have moved to internal alternatives\",\n        version='2.24',\n        help_text='Contact the plugin author to update their code',\n    )\n    # Any container is unhashable if it contains unhashable items (for\n    # instance, tuple() is a Hashable subclass but if it contains a dict, it\n    # cannot be hashed)\n    if isinstance(params, Container) and not isinstance(params, (str, bytes)):\n        if isinstance(params, Mapping):\n            try:\n                # Optimistically hope the contents are all hashable\n                new_params = frozenset(params.items())\n            except TypeError:\n                new_params = set()\n                for k, v in params.items():\n                    # Hash each entry individually\n                    new_params.add((k, hash_params(v)))\n                new_params = frozenset(new_params)\n\n        elif isinstance(params, (Set, Sequence)):\n            try:\n                # Optimistically hope the contents are all hashable\n                new_params = frozenset(params)\n            except TypeError:\n                new_params = set()\n                for v in params:\n                    # Hash each entry individually\n                    new_params.add(hash_params(v))\n                new_params = frozenset(new_params)\n        else:\n            # This is just a guess.\n            new_params = frozenset(params)\n        return new_params\n\n    # Note: We do not handle unhashable scalars but our only choice would be\n    # to raise an error there anyway.\n    return frozenset((params,))", "loc": 53}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "load", "parameters": ["role_include", "play", "parent_role", "from_files", "from_include", "validate", "public", "static"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Role", "play.role_cache[role_path].append", "r._load_role_data", "r.get_role_path"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(role_include, play, parent_role=None, from_files=None, from_include=False, validate=True, public=None, static=True):\n    if from_files is None:\n        from_files = {}\n    try:\n        # TODO: need to fix cycle detection in role load (maybe use an empty dict\n        #  for the in-flight in role cache as a sentinel that we're already trying to load\n        #  that role?)\n        # see https://github.com/ansible/ansible/issues/61527\n        r = Role(play=play, from_files=from_files, from_include=from_include, validate=validate, public=public, static=static)\n        r._load_role_data(role_include, parent_role=parent_role)\n\n        role_path = r.get_role_path()\n        if role_path not in play.role_cache:\n            play.role_cache[role_path] = []\n\n        # Using the role path as a cache key is done to improve performance when a large number of roles\n        # are in use in the play\n        if r not in play.role_cache[role_path]:\n            play.role_cache[role_path].append(r)\n\n        return r\n\n    except RecursionError as ex:\n        raise AnsibleError(\"A recursion loop was detected with the roles specified. Make sure child roles do not have dependencies on parent roles\",\n                           obj=role_include._ds) from ex", "loc": 25}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "add_parent", "parameters": ["self", "parent_role"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "isinstance", "self._parents.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "adds a role to the list of this roles parents", "source_code": "def add_parent(self, parent_role):\n    \"\"\" adds a role to the list of this roles parents \"\"\"\n    if not isinstance(parent_role, Role):\n        raise AnsibleAssertionError()\n\n    if parent_role not in self._parents:\n        self._parents.append(parent_role)", "loc": 7}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_dep_chain", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep_chain.append", "dep_chain.extend", "parent.get_dep_chain"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_dep_chain(self):\n    dep_chain = []\n    for parent in self._parents:\n        dep_chain.extend(parent.get_dep_chain())\n        dep_chain.append(parent)\n    return dep_chain", "loc": 6}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_default_vars", "parameters": ["self", "dep_chain"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "dep.get_default_vars", "dict", "self.get_all_dependencies"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_default_vars(self, dep_chain=None):\n    dep_chain = [] if dep_chain is None else dep_chain\n\n    default_vars = dict()\n    for dep in self.get_all_dependencies():\n        default_vars = combine_vars(default_vars, dep.get_default_vars())\n    if dep_chain:\n        for parent in dep_chain:\n            default_vars = combine_vars(default_vars, parent._default_vars)\n    default_vars = combine_vars(default_vars, self._default_vars)\n    return default_vars", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_inherited_vars", "parameters": ["self", "dep_chain", "only_exports"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "dict"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_inherited_vars(self, dep_chain=None, only_exports=False):\n    dep_chain = [] if dep_chain is None else dep_chain\n\n    inherited_vars = dict()\n\n    if dep_chain:\n        for parent in dep_chain:\n            if not only_exports:\n                inherited_vars = combine_vars(inherited_vars, parent.vars)\n            inherited_vars = combine_vars(inherited_vars, parent._role_vars)\n    return inherited_vars", "loc": 11}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_role_params", "parameters": ["self", "dep_chain"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_role_params(self, dep_chain=None):\n    dep_chain = [] if dep_chain is None else dep_chain\n\n    params = {}\n    if dep_chain:\n        for parent in dep_chain:\n            params = combine_vars(params, parent._role_params)\n    params = combine_vars(params, self._role_params)\n    return params", "loc": 9}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_vars", "parameters": ["self", "dep_chain", "include_params", "only_exports"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "dep.get_vars", "seen.append", "self.get_all_dependencies", "self.get_inherited_vars", "self.get_role_params"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_vars(self, dep_chain=None, include_params=True, only_exports=False):\n    dep_chain = [] if dep_chain is None else dep_chain\n\n    all_vars = {}\n\n    # get role_vars: from parent objects\n    # TODO: is this right precedence for inherited role_vars?\n    all_vars = self.get_inherited_vars(dep_chain, only_exports=only_exports)\n\n    # get exported variables from meta/dependencies\n    seen = []\n    for dep in self.get_all_dependencies():\n        # Avoid rerunning dupe deps since they can have vars from previous invocations and they accumulate in deps\n        # TODO: re-examine dep loading to see if we are somehow improperly adding the same dep too many times\n        if dep not in seen:\n            # only take 'exportable' vars from deps\n            all_vars = combine_vars(all_vars, dep.get_vars(include_params=False, only_exports=True))\n            seen.append(dep)\n\n    # role_vars come from vars/ in a role\n    all_vars = combine_vars(all_vars, self._role_vars)\n\n    if not only_exports:\n        # include_params are 'inline variables' in role invocation. - {role: x, varname: value}\n        if include_params:\n            # TODO: add deprecation notice\n            all_vars = combine_vars(all_vars, self.get_role_params(dep_chain=dep_chain))\n\n        # these come from vars: keyword in role invocation. - {role: x, vars: {varname: value}}\n        all_vars = combine_vars(all_vars, self.vars)\n\n    return all_vars", "loc": 32}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_all_dependencies", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep.get_all_dependencies", "self._all_dependencies.append", "self.get_direct_dependencies"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_dependencies(self):\n    \"\"\"\n    Returns a list of all deps, built recursively from all child dependencies,\n    in the proper order in which they should be executed or evaluated.\n    \"\"\"\n    if self._all_dependencies is None:\n\n        self._all_dependencies = []\n        for dep in self.get_direct_dependencies():\n            for child_dep in dep.get_all_dependencies():\n                self._all_dependencies.append(child_dep)\n            self._all_dependencies.append(dep)\n\n    return self._all_dependencies", "loc": 14}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "get_handler_blocks", "parameters": ["self", "play", "dep_chain"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["block_list.append", "block_list.extend", "dep.get_handler_blocks", "self.get_direct_dependencies", "task_block.copy"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_handler_blocks(self, play, dep_chain=None):\n    # Do not recreate this list each time ``get_handler_blocks`` is called.\n    # Cache the results so that we don't potentially overwrite with copied duplicates\n    #\n    # ``get_handler_blocks`` may be called when handling ``import_role`` during parsing\n    # as well as with ``Play.compile_roles_handlers`` from ``TaskExecutor``\n    if self._compiled_handler_blocks:\n        return self._compiled_handler_blocks\n\n    self._compiled_handler_blocks = block_list = []\n\n    # update the dependency chain here\n    if dep_chain is None:\n        dep_chain = []\n    new_dep_chain = dep_chain + [self]\n\n    for dep in self.get_direct_dependencies():\n        dep_blocks = dep.get_handler_blocks(play=play, dep_chain=new_dep_chain)\n        block_list.extend(dep_blocks)\n\n    for task_block in self._handler_blocks:\n        new_task_block = task_block.copy()\n        new_task_block._dep_chain = new_dep_chain\n        new_task_block._play = play\n        block_list.append(new_task_block)\n\n    return block_list", "loc": 27}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "compile", "parameters": ["self", "play", "dep_chain"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Block", "Task", "block_list.append", "block_list.extend", "dep.compile", "self.get_direct_dependencies", "task_block.copy"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compile(self, play, dep_chain=None):\n    \"\"\"\n    Returns the task list for this role, which is created by first\n    recursively compiling the tasks for all direct dependencies, and\n    then adding on the tasks for this role.\n\n    The role compile() also remembers and saves the dependency chain\n    with each task, so tasks know by which route they were found, and\n    can correctly take their parent's tags/conditionals into account.\n    \"\"\"\n    from ansible.playbook.block import Block\n    from ansible.playbook.task import Task\n\n    block_list = []\n\n    # update the dependency chain here\n    if dep_chain is None:\n        dep_chain = []\n    new_dep_chain = dep_chain + [self]\n\n    deps = self.get_direct_dependencies()\n    for dep in deps:\n        dep_blocks = dep.compile(play=play, dep_chain=new_dep_chain)\n        block_list.extend(dep_blocks)\n\n    for task_block in self._task_blocks:\n        new_task_block = task_block.copy()\n        new_task_block._dep_chain = new_dep_chain\n        new_task_block._play = play\n        block_list.append(new_task_block)\n\n    eor_block = Block(play=play)\n    eor_block._loader = self._loader\n    eor_block._role = self\n    eor_block._variable_manager = self._variable_manager\n    eor_block.run_once = False\n\n    eor_task = Task(block=eor_block)\n    eor_task._role = self\n    eor_task.action = 'meta'\n    eor_task.args = {'_raw_params': 'role_complete'}\n    eor_task.implicit = True\n    eor_task.tags = ['always']\n    eor_task.when = True\n\n    eor_block.block = [eor_task]\n    block_list.append(eor_block)\n\n    return block_list", "loc": 49}
{"file": "ansible\\lib\\ansible\\playbook\\role\\__init__.py", "class_name": "Role", "function_name": "set_loader", "parameters": ["self", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dep.set_loader", "parent.set_loader", "self.get_direct_dependencies"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_loader(self, loader):\n    self._loader = loader\n    for parent in self._parents:\n        parent.set_loader(loader)\n    for dep in self.get_direct_dependencies():\n        dep.set_loader(loader)", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\list.py", "class_name": null, "function_name": "get_composite_name", "parameters": ["collection", "name", "path", "depth"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "composite.append", "composite.extend", "name.split", "name.startswith", "path.split", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_composite_name(collection, name, path, depth):\n    resolved_collection = collection\n    if '.' not in name:\n        resource_name = name\n    else:\n        if collection == 'ansible.legacy' and name.startswith('ansible.builtin.'):\n            resolved_collection = 'ansible.builtin'\n        resource_name = '.'.join(name.split(f\"{resolved_collection}.\")[1:])\n\n    # create FQCN\n    composite = [resolved_collection]\n    if depth:\n        composite.extend(path.split(os.path.sep)[depth * -1:])\n    composite.append(to_native(resource_name))\n    return '.'.join(composite)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\list.py", "class_name": null, "function_name": "list_collection_plugins", "parameters": ["ptype", "collections", "search_paths"], "param_types": {"ptype": "str", "collections": "dict[str, bytes]", "search_paths": "list[str] | None"}, "return_type": "dict[str, tuple[bytes, object | None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_list_collection_plugins_with_info", "_list_collection_plugins_with_info(ptype, collections).items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_collection_plugins(ptype: str, collections: dict[str, bytes], search_paths: list[str] | None = None) -> dict[str, tuple[bytes, object | None]]:\n    # Kept for backwards compatibility.\n    return {\n        name: (info.path, info.plugin_obj)\n        for name, info in _list_collection_plugins_with_info(ptype, collections).items()\n    }", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\list.py", "class_name": null, "function_name": "list_plugins", "parameters": ["ptype", "collections", "search_paths"], "param_types": {"ptype": "str", "collections": "list[str] | None", "search_paths": "list[str] | None"}, "return_type": "dict[str, tuple[bytes, object | None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_list_plugins_with_info", "_list_plugins_with_info(ptype, collections, search_paths).items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_plugins(ptype: str, collections: list[str] | None = None, search_paths: list[str] | None = None) -> dict[str, tuple[bytes, object | None]]:\n    # Kept for backwards compatibility.\n    return {\n        name: (info.path, info.plugin_obj)\n        for name, info in _list_plugins_with_info(ptype, collections, search_paths).items()\n    }", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": null, "function_name": "get_plugin_loader_namespace", "parameters": [], "param_types": {}, "return_type": "types.SimpleNamespace", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_all_plugin_loaders", "setattr", "types.SimpleNamespace"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_loader_namespace() -> types.SimpleNamespace:\n    ns = types.SimpleNamespace()\n    for name, obj in get_all_plugin_loaders():\n        setattr(ns, name, obj)\n    return ns", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": null, "function_name": "add_dirs_to_loader", "parameters": ["which_loader", "paths"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "loader.add_directory"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_dirs_to_loader(which_loader, paths):\n\n    loader = getattr(sys.modules[__name__], '%s_loader' % which_loader)\n    for path in paths:\n        loader.add_directory(path, with_subdir=True)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": null, "function_name": "get_fqcr_and_name", "parameters": ["resource", "collection"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["resource.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_fqcr_and_name(resource, collection='ansible.builtin'):\n    if '.' not in resource:\n        name = resource\n        fqcr = collection + '.' + resource\n    else:\n        name = resource.split('.')[-1]\n        fqcr = resource\n\n    return fqcr, name", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": null, "function_name": "init_plugin_loader", "parameters": ["prefix_collections_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_configure_collection_loader", "_load_plugin_filter"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Initialize the plugin filters and the collection loaders This method must be called to configure and insert the collection python loaders into ``sys.meta_path`` and ``sys.path_hooks``.", "source_code": "def init_plugin_loader(prefix_collections_path=None):\n    \"\"\"Initialize the plugin filters and the collection loaders\n\n    This method must be called to configure and insert the collection python loaders\n    into ``sys.meta_path`` and ``sys.path_hooks``.\n\n    This method is only called in ``CLI.run`` after CLI args have been parsed, so that\n    instantiation of the collection finder can utilize parsed CLI args, and to not cause\n    side effects.\n    \"\"\"\n    _load_plugin_filter()\n    _configure_collection_loader(prefix_collections_path)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoadContext", "function_name": "resolved_fqcn", "parameters": ["self"], "param_types": {}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleCollectionRef.is_valid_fqcr", "final_plugin.split", "final_plugin.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolved_fqcn(self) -> str | None:\n    if not self.resolved:\n        return None\n\n    if not self._resolved_fqcn:\n        final_plugin = self.redirect_list[-1]\n        if AnsibleCollectionRef.is_valid_fqcr(final_plugin) and final_plugin.startswith('ansible.legacy.'):\n            final_plugin = final_plugin.split('ansible.legacy.')[-1]\n        if self.plugin_resolved_collection and not AnsibleCollectionRef.is_valid_fqcr(final_plugin):\n            final_plugin = self.plugin_resolved_collection + '.' + final_plugin\n        self._resolved_fqcn = final_plugin\n\n    return self._resolved_fqcn", "loc": 13}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoadContext", "function_name": "record_deprecation", "parameters": ["self", "name", "deprecation", "collection_name"], "param_types": {"name": "str", "deprecation": "dict[str, t.Any] | None", "collection_name": "str"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0} has been deprecated.{1}{2}'.format", "deprecation.get", "deprecator_from_collection_name", "display.deprecated", "self.deprecation_warnings.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def record_deprecation(self, name: str, deprecation: dict[str, t.Any] | None, collection_name: str) -> t.Self:\n    if not deprecation:\n        return self\n\n    # The `or ''` instead of using `.get(..., '')` makes sure that even if the user explicitly\n    # sets `warning_text` to `~` (None) or `false`, we still get an empty string.\n    warning_text = deprecation.get('warning_text', None) or ''\n    removal_date = deprecation.get('removal_date', None)\n    removal_version = deprecation.get('removal_version', None)\n    # If both removal_date and removal_version are specified, use removal_date\n    if removal_date is not None:\n        removal_version = None\n    warning_text = '{0} has been deprecated.{1}{2}'.format(name, ' ' if warning_text else '', warning_text)\n\n    display.deprecated(  # pylint: disable=ansible-deprecated-date-not-permitted,ansible-deprecated-unnecessary-collection-name\n        msg=warning_text,\n        date=removal_date,\n        version=removal_version,\n        deprecator=deprecator_from_collection_name(collection_name),\n    )\n\n    self.deprecated = True\n    if removal_date:\n        self.removal_date = removal_date\n    if removal_version:\n        self.removal_version = removal_version\n    self.deprecation_warnings.append(warning_text)\n    return self", "loc": 28}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoadContext", "function_name": "resolve_legacy_jinja_plugin", "parameters": ["self", "name", "known_plugin"], "param_types": {"name": "str", "known_plugin": "AnsibleJinja2Plugin"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["known_plugin.ansible_name.startswith", "self._make_legacy_python_module_name"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Record a resolved legacy Jinja plugin.", "source_code": "def resolve_legacy_jinja_plugin(self, name: str, known_plugin: AnsibleJinja2Plugin) -> t.Self:\n    \"\"\"Record a resolved legacy Jinja plugin.\"\"\"\n    internal = known_plugin.ansible_name.startswith('ansible.builtin.')\n\n    self.plugin_resolved_name = name\n    self.plugin_resolved_path = known_plugin._original_path\n    self.plugin_resolved_collection = 'ansible.builtin' if internal else ''\n    self._resolved_fqcn = known_plugin.ansible_name\n    self._python_module_name = self._make_legacy_python_module_name()\n    self.resolved = True\n\n    return self", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoader", "function_name": "find_plugin", "parameters": ["self", "name", "mod_type", "ignore_deprecated", "check_aliases", "collection_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.find_plugin_with_context"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Find a plugin named name", "source_code": "def find_plugin(self, name, mod_type='', ignore_deprecated=False, check_aliases=False, collection_list=None):\n    \"\"\" Find a plugin named name \"\"\"\n    result = self.find_plugin_with_context(name, mod_type, ignore_deprecated, check_aliases, collection_list)\n    if result.resolved and result.plugin_resolved_path:\n        return result.plugin_resolved_path\n\n    return None", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoader", "function_name": "find_plugin_with_context", "parameters": ["self", "name", "mod_type", "ignore_deprecated", "check_aliases", "collection_list"], "param_types": {"name": "str", "mod_type": "str", "ignore_deprecated": "bool", "check_aliases": "bool", "collection_list": "list[str] | None"}, "return_type": "PluginLoadContext", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'plugin redirect loop resolving {0} (path: {1})'.format", "AnsiblePluginCircularRedirect", "PluginLoadContext", "display.error_as_warning", "self._resolve_plugin_step"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Find a plugin named name, returning contextual info about the load, recursively resolving redirection", "source_code": "def find_plugin_with_context(\n    self,\n    name: str,\n    mod_type: str = '',\n    ignore_deprecated: bool = False,\n    check_aliases: bool = False,\n    collection_list: list[str] | None = None,\n) -> PluginLoadContext:\n    \"\"\" Find a plugin named name, returning contextual info about the load, recursively resolving redirection \"\"\"\n    plugin_load_context = PluginLoadContext(self.type, self.package)\n    plugin_load_context.original_name = name\n    while True:\n        result = self._resolve_plugin_step(name, mod_type, ignore_deprecated, check_aliases, collection_list, plugin_load_context=plugin_load_context)\n        if result.pending_redirect:\n            if result.pending_redirect in result.redirect_list:\n                raise AnsiblePluginCircularRedirect('plugin redirect loop resolving {0} (path: {1})'.format(result.original_name, result.redirect_list))\n            name = result.pending_redirect\n            result.pending_redirect = None\n            plugin_load_context = result\n        else:\n            break\n\n    for ex in plugin_load_context.raw_error_list:\n        display.error_as_warning(f\"Error loading plugin {name!r}.\", ex)\n\n    # FIXME: store structured deprecation data in PluginLoadContext and use display.deprecate\n    # if plugin_load_context.deprecated and C.config.get_config_value('DEPRECATION_WARNINGS'):\n    #     for dw in plugin_load_context.deprecation_warnings:\n    #         # TODO: need to smuggle these to the controller if we're in a worker context\n    #         display.warning('[DEPRECATION WARNING] ' + dw)\n\n    return plugin_load_context", "loc": 32}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoader", "function_name": "has_plugin", "parameters": ["self", "name", "collection_list"], "param_types": {"name": "str", "collection_list": "list[str] | None"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'has_plugin error: {0}'.format", "display.debug", "isinstance", "self.find_plugin", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Checks if a plugin named name exists", "source_code": "def has_plugin(self, name: str, collection_list: list[str] | None = None) -> bool:\n    \"\"\" Checks if a plugin named name exists \"\"\"\n\n    try:\n        return self.find_plugin(name, collection_list=collection_list) is not None\n    except Exception as ex:\n        if isinstance(ex, AnsibleError):\n            raise\n        # log and continue, likely an innocuous type/package loading failure in collections import\n        display.debug('has_plugin error: {0}'.format(to_text(ex)))\n\n        return False", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoader", "function_name": "get", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "self.get_with_context"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self, name, *args, **kwargs):\n    ctx = self.get_with_context(name, *args, **kwargs)\n    is_core_plugin = ctx.plugin_load_context.plugin_resolved_collection == 'ansible.builtin'\n    if self.class_name == 'StrategyModule' and not is_core_plugin:\n        display.deprecated(  # pylint: disable=ansible-deprecated-no-version\n            msg='Use of strategy plugins not included in ansible.builtin are deprecated and do not carry '\n                'any backwards compatibility guarantees. No alternative for third party strategy plugins '\n                'is currently planned.',\n        )\n\n    return ctx.object", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "PluginLoader", "function_name": "get_with_context", "parameters": ["self", "name"], "param_types": {}, "return_type": "get_with_context_result", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PluginLoadContext", "ValueError", "__import__", "display.v", "display.warning", "get_with_context_result", "getattr", "issubclass", "kwargs.pop", "obj.__init__", "object.__new__", "self._display_plugin_load", "self._load_config_defs", "self._load_module_source", "self._plugin_instance_cache or {}.get", "self._update_object", "self.find_plugin_with_context", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "instantiates a plugin of the given name using arguments", "source_code": "def get_with_context(self, name, *args, **kwargs) -> get_with_context_result:\n    \"\"\" instantiates a plugin of the given name using arguments \"\"\"\n\n    if not name:\n        raise ValueError('A non-empty plugin name is required.')\n\n    found_in_cache = True\n    class_only = kwargs.pop('class_only', False)\n    collection_list = kwargs.pop('collection_list', None)\n    if name in self.aliases:\n        name = self.aliases[name]\n\n    if (cached_result := (self._plugin_instance_cache or {}).get(name)) and cached_result[1].resolved:\n        # Resolving the FQCN is slow, even if we've passed in the resolved FQCN.\n        # Short-circuit here if we've previously resolved this name.\n        # This will need to be restricted if non-vars plugins start using the cache, since\n        # some non-fqcn plugin need to be resolved again with the collections list.\n        return get_with_context_result(*cached_result)\n\n    plugin_load_context = self.find_plugin_with_context(name, collection_list=collection_list)\n    if not plugin_load_context.resolved or not plugin_load_context.plugin_resolved_path:\n        # FIXME: this is probably an error (eg removed plugin)\n        return get_with_context_result(None, plugin_load_context)\n\n    fq_name = plugin_load_context.resolved_fqcn\n    resolved_type_name = plugin_load_context.plugin_resolved_name\n    path = plugin_load_context.plugin_resolved_path\n    if (cached_result := (self._plugin_instance_cache or {}).get(fq_name)) and cached_result[1].resolved:\n        # This is unused by vars plugins, but it's here in case the instance cache expands to other plugin types.\n        # We get here if we've seen this plugin before, but it wasn't called with the resolved FQCN.\n        return get_with_context_result(*cached_result)\n    redirected_names = plugin_load_context.redirect_list or []\n\n    if path not in self._module_cache:\n        self._module_cache[path] = self._load_module_source(python_module_name=plugin_load_context._python_module_name, path=path)\n        found_in_cache = False\n\n    self._load_config_defs(resolved_type_name, self._module_cache[path], path)\n\n    obj = getattr(self._module_cache[path], self.class_name)\n\n    if self.base_class:\n        # The import path is hardcoded and should be the right place,\n        # so we are not expecting an ImportError.\n        module = __import__(self.package, fromlist=[self.base_class])\n        # Check whether this obj has the required base class.\n        try:\n            plugin_class = getattr(module, self.base_class)\n        except AttributeError:\n            return get_with_context_result(None, plugin_load_context)\n        if not issubclass(obj, plugin_class):\n            display.warning(f\"Ignoring {self.type} plugin {resolved_type_name!r} due to missing base class {self.base_class!r}.\")\n            return get_with_context_result(None, plugin_load_context)\n\n    # FIXME: update this to use the load context\n    self._display_plugin_load(self.class_name, resolved_type_name, self._searched_paths, path, found_in_cache=found_in_cache, class_only=class_only)\n\n    if not class_only:\n        try:\n            # A plugin may need to use its _load_name in __init__ (for example, to set\n            # or get options from config), so update the object before using the constructor\n            instance = object.__new__(obj)\n            self._update_object(obj=instance, name=resolved_type_name, path=path, redirected_names=redirected_names, resolved=fq_name)\n            obj.__init__(instance, *args, **kwargs)  # pylint: disable=unnecessary-dunder-call\n            obj = instance\n        except TypeError as e:\n            if \"abstract\" in e.args[0]:\n                # Abstract Base Class or incomplete plugin, don't load\n                display.v('Returning not found on \"%s\" as it has unimplemented abstract methods; %s' % (resolved_type_name, to_native(e)))\n                return get_with_context_result(None, plugin_load_context)\n            raise\n\n    self._update_object(obj=obj, name=resolved_type_name, path=path, redirected_names=redirected_names, resolved=fq_name)\n    if self._plugin_instance_cache is not None and getattr(obj, 'is_stateless', False):\n        self._plugin_instance_cache[fq_name] = (obj, plugin_load_context)\n    elif self._plugin_instance_cache is not None:\n        # The cache doubles as the load order, so record the FQCN even if the plugin hasn't set is_stateless = True\n        self._plugin_instance_cache[fq_name] = (None, PluginLoadContext(self.type, self.package))\n    return get_with_context_result(obj, plugin_load_context)", "loc": 79}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "_CacheLoader", "function_name": "get", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "BaseCacheModule", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_cache.PluginInterposer", "super", "super().get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self, name: str, *args, **kwargs) -> BaseCacheModule:\n    plugin = super().get(name, *args, **kwargs)\n\n    if not plugin:\n        raise AnsibleError(f'Unable to load the cache plugin {name!r}.')\n\n    if plugin._persistent:\n        return _cache.PluginInterposer(plugin)\n\n    return plugin", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "Jinja2Loader", "function_name": "get_contained_plugins", "parameters": ["self", "collection", "plugin_path", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "KeyError", "display.warning", "getattr", "method_map", "method_map().items", "obj", "plugins.append", "self._load_module_source", "self._plugin_wrapper_type", "self._update_object", "to_native", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_contained_plugins(self, collection, plugin_path, name):\n\n    plugins = []\n\n    full_name = '.'.join(['ansible_collections', collection, 'plugins', self.type, name])\n    try:\n        # use 'parent' loader class to find files, but cannot return this as it can contain multiple plugins per file\n        if plugin_path not in self._module_cache:\n            self._module_cache[plugin_path] = self._load_module_source(python_module_name=full_name, path=plugin_path)\n        module = self._module_cache[plugin_path]\n        obj = getattr(module, self.class_name)\n    except Exception as e:\n        raise KeyError('Failed to load %s for %s: %s' % (plugin_path, collection, to_native(e)))\n\n    plugin_impl = obj()\n    if plugin_impl is None:\n        raise KeyError('Could not find %s.%s' % (collection, name))\n\n    try:\n        method_map = getattr(plugin_impl, self.method_map_name)\n        plugin_map = method_map().items()\n    except Exception as e:\n        display.warning(\"Ignoring %s plugins in '%s' as it seems to be invalid: %r\" % (self.type, to_text(plugin_path), e))\n        return plugins\n\n    for func_name, func in plugin_map:\n        fq_name = '.'.join((collection, func_name))\n        full = '.'.join((full_name, func_name))\n        plugin = self._plugin_wrapper_type(func)\n        if plugin in plugins:\n            continue\n        self._update_object(obj=plugin, name=full, path=plugin_path, resolved=fq_name)\n        plugins.append(plugin)\n\n    return plugins", "loc": 35}
{"file": "ansible\\lib\\ansible\\plugins\\loader.py", "class_name": "Jinja2Loader", "function_name": "all", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "isinstance", "kwargs.pop", "self._cached_non_collection_wrappers.values", "self._ensure_non_collection_wrappers"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def all(self, *args, **kwargs):\n    kwargs.pop('_dedupe', None)\n    path_only = kwargs.pop('path_only', False)\n    class_only = kwargs.pop('class_only', False)  # basically ignored for test/filters since they are functions\n\n    # Having both path_only and class_only is a coding bug\n    if path_only and class_only:\n        raise AnsibleError('Do not set both path_only and class_only when calling PluginLoader.all()')\n\n    self._ensure_non_collection_wrappers(*args, **kwargs)\n\n    plugins = [plugin for plugin in self._cached_non_collection_wrappers.values() if not isinstance(plugin, _DeferredPluginLoadFailure)]\n\n    if path_only:\n        yield from (w._original_path for w in plugins)\n    else:\n        yield from (w for w in plugins)", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": null, "function_name": "get_plugin_class", "parameters": ["obj"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "obj.__class__.__name__.lower", "obj.__class__.__name__.lower().replace", "obj.lower", "obj.lower().replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_class(obj):\n    if isinstance(obj, str):\n        return obj.lower().replace('module', '')\n    else:\n        return obj.__class__.__name__.lower().replace('module', '')", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": "AnsiblePlugin", "function_name": "matches_name", "parameters": ["self", "possible_names"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "name.removeprefix", "name.startswith", "possible_fqcns.add", "possible_fqcns.intersection", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def matches_name(self, possible_names):\n    possible_fqcns = set()\n    for name in possible_names:\n        if '.' not in name:\n            possible_fqcns.add(f\"ansible.builtin.{name}\")\n        elif name.startswith(\"ansible.legacy.\"):\n            possible_fqcns.add(name.removeprefix(\"ansible.legacy.\"))\n        possible_fqcns.add(name)\n    return bool(possible_fqcns.intersection(set(self.ansible_aliases)))", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": "AnsiblePlugin", "function_name": "get_option_and_origin", "parameters": ["self", "option", "hostvars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.config.get_config_value_and_origin", "KeyError"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_option_and_origin(self, option, hostvars=None):\n    if option not in self._options:\n        try:\n            # some plugins don't use set_option(s) and cannot use direct settings, so this populates the local copy for them\n            self._options[option], self._origins[option] = C.config.get_config_value_and_origin(option, plugin_type=self.plugin_type,\n                                                                                                plugin_name=self._load_name, variables=hostvars)\n        except AnsibleError as e:\n            # callers expect key error on missing\n            raise KeyError() from e\n\n    return self._options[option], self._origins[option]", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": "AnsiblePlugin", "function_name": "get_option", "parameters": ["self", "option", "hostvars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_option_and_origin"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_option(self, option, hostvars=None):\n    if option not in self._options:\n        # let it populate _options\n        self.get_option_and_origin(option, hostvars=hostvars)\n    return self._options[option]", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": "AnsiblePlugin", "function_name": "get_options", "parameters": ["self", "hostvars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_option", "self.option_definitions.keys"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_options(self, hostvars=None):\n    options = {}\n    for option in self.option_definitions.keys():\n        options[option] = self.get_option(option, hostvars=hostvars)\n    return options", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\__init__.py", "class_name": "AnsiblePlugin", "function_name": "set_options", "parameters": ["self", "task_keys", "var_options", "direct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["C.config.get_plugin_options_and_origins", "_display._report_config_warnings"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the _options attribute with the configuration/keyword information for this plugin :arg task_keys: Dict with playbook keywords that affect this option :arg var_options: Dict with either 'connection variables'", "source_code": "def set_options(self, task_keys=None, var_options=None, direct=None):\n    \"\"\"\n    Sets the _options attribute with the configuration/keyword information for this plugin\n\n    :arg task_keys: Dict with playbook keywords that affect this option\n    :arg var_options: Dict with either 'connection variables'\n    :arg direct: Dict with 'direct assignment'\n    \"\"\"\n    self._options, self._origins = C.config.get_plugin_options_and_origins(self.plugin_type, self._load_name, keys=task_keys,\n                                                                           variables=var_options, direct=direct)\n\n    # allow extras/wildcards from vars that are not directly consumed in configuration\n    # this is needed to support things like winrm that can have extended protocol options we don't directly handle\n    if self.allow_extras and var_options and '_extras' in var_options:\n        # these are largely unvalidated passthroughs, either plugin or underlying API will validate\n        # TODO: deprecate and remove, most plugins that needed this don't use this facility anymore\n        self._options['_extras'] = var_options['_extras']\n    _display._report_config_warnings(self.__plugin_info)", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\action\\add_host.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "args.get", "args.keys", "args.pop", "combine_vars", "dict", "display.vv", "frozenset", "group_name.strip", "groups.split", "isinstance", "new_groups.append", "parse_address", "super", "super(ActionModule, self).run", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    args = self._task.args\n    raw = args.pop('_raw_params', {})\n    if isinstance(raw, Mapping):\n        # TODO: create 'conflict' detection in base class to deal with repeats and aliases and warn user\n        args = combine_vars(raw, args)\n    else:\n        raise AnsibleActionFail('Invalid raw parameters passed, requires a dictionary/mapping got a  %s' % type(raw))\n\n    # Parse out any hostname:port patterns\n    new_name = args.get('name', args.get('hostname', args.get('host', None)))\n    if new_name is None:\n        raise AnsibleActionFail('name, host or hostname needs to be provided')\n\n    display.vv(\"creating host via 'add_host': hostname=%s\" % new_name)\n\n    try:\n        name, port = parse_address(new_name, allow_ranges=False)\n    except Exception:\n        # not a parsable hostname, but might still be usable\n        name = new_name\n        port = None\n\n    if port:\n        args['ansible_ssh_port'] = port\n\n    groups = args.get('groupname', args.get('groups', args.get('group', '')))\n    # add it to the group if that was specified\n    new_groups = []\n    if groups:\n        if isinstance(groups, list):\n            group_list = groups\n        elif isinstance(groups, str):\n            group_list = groups.split(\",\")\n        else:\n            raise AnsibleActionFail(\"Groups must be specified as a list.\", obj=groups)\n\n        for group_name in group_list:\n            if group_name not in new_groups:\n                new_groups.append(group_name.strip())\n\n    # Add any variables to the new_host\n    host_vars = dict()\n    special_args = frozenset(('name', 'hostname', 'groupname', 'groups'))\n    for k in args.keys():\n        if k not in special_args:\n            host_vars[k] = args[k]\n\n    result['changed'] = False\n    result['add_host'] = dict(host_name=name, groups=new_groups, host_vars=host_vars)\n    return result", "loc": 55}
{"file": "ansible\\lib\\ansible\\plugins\\action\\assert.py", "class_name": null, "function_name": "str_or_list_of_str", "parameters": ["value"], "param_types": {"value": "t.Any"}, "return_type": "str | list[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "any", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def str_or_list_of_str(value: t.Any) -> str | list[str]:\n    if isinstance(value, str):\n        return value\n\n    if not isinstance(value, list) or any(not isinstance(item, str) for item in value):\n        raise TypeError(\"a string or list of strings is required\")\n\n    return value", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\action\\assert.py", "class_name": "ActionModule", "function_name": "finalize_task_arg", "parameters": ["cls", "name", "value", "templar", "context"], "param_types": {"name": "str", "value": "t.Any", "templar": "TemplateEngine", "context": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_jinja_bits.is_possibly_all_template", "isinstance", "super", "super().finalize_task_arg", "templar.resolve_to_container"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finalize_task_arg(cls, name: str, value: t.Any, templar: TemplateEngine, context: t.Any) -> t.Any:\n    if name != 'that':\n        # `that` is the only key requiring special handling; delegate to base handling otherwise\n        return super().finalize_task_arg(name, value, templar, context)\n\n    if not isinstance(value, str):\n        # if `that` is not a string, we don't need to attempt to resolve it as a template before validation (which will also listify it)\n        return value\n\n    # if `that` is entirely a string template, we only want to resolve to the container and avoid templating the container contents\n    if _jinja_bits.is_possibly_all_template(value):\n        try:\n            templated_that = templar.resolve_to_container(value)\n        except AnsibleTemplateError:\n            pass\n        else:\n            if isinstance(templated_that, list):  # only use `templated_that` if it is a list\n                return templated_that\n\n    return value", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\action\\assert.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self._templar.evaluate_conditional", "self.validate_argument_spec", "super", "super(ActionModule, self).run"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    validation_result, new_module_args = self.validate_argument_spec(\n        argument_spec=dict(\n            fail_msg=dict(type=str_or_list_of_str, aliases=['msg'], default='Assertion failed'),\n            success_msg=dict(type=str_or_list_of_str, default='All assertions passed'),\n            quiet=dict(type='bool', default=False),\n            # explicitly not validating types `elements` here to let type rules for conditionals apply\n            that=dict(type=_check_type_list_strict, required=True),\n        ),\n    )\n\n    fail_msg = new_module_args['fail_msg']\n    success_msg = new_module_args['success_msg']\n    quiet = new_module_args['quiet']\n    that_list = new_module_args['that']\n\n    if not quiet:\n        result['_ansible_verbose_always'] = True\n\n    for that in that_list:\n        test_result = self._templar.evaluate_conditional(conditional=that)\n        if not test_result:\n            result['failed'] = True\n            result['evaluated_to'] = test_result\n            result['assertion'] = that\n\n            result['msg'] = fail_msg\n\n            return result\n\n    result['changed'] = False\n    result['msg'] = success_msg\n    return result", "loc": 39}
{"file": "ansible\\lib\\ansible\\plugins\\action\\async_status.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "merge_hash", "self._connection._shell.join_path", "self._execute_module", "self._get_async_dir", "self.validate_argument_spec", "super", "super(ActionModule, self).run"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n\n    results = super(ActionModule, self).run(tmp, task_vars)\n\n    validation_result, new_module_args = self.validate_argument_spec(\n        argument_spec={\n            'jid': {'type': 'str', 'required': True},\n            'mode': {'type': 'str', 'choices': ['status', 'cleanup'], 'default': 'status'},\n        },\n    )\n\n    # initialize response\n    results['started'] = results['finished'] = False\n    results['stdout'] = results['stderr'] = ''\n    results['stdout_lines'] = results['stderr_lines'] = []\n\n    jid = new_module_args[\"jid\"]\n    mode = new_module_args[\"mode\"]\n\n    results['ansible_job_id'] = jid\n    async_dir = self._get_async_dir()\n    log_path = self._connection._shell.join_path(async_dir, jid)\n\n    if mode == 'cleanup':\n        results['erased'] = log_path\n    else:\n        results['results_file'] = log_path\n        results['started'] = True\n\n    new_module_args['_async_dir'] = async_dir\n    results = merge_hash(results, self._execute_module(module_name='ansible.legacy.async_status', task_vars=task_vars, module_args=new_module_args))\n\n    # Backwards compat shim for when started/finished were ints,\n    # mostly to work with ansible.windows.async_status\n    for convert in ('started', 'finished'):\n        results[convert] = bool(results[convert])\n\n    return results", "loc": 38}
{"file": "ansible\\lib\\ansible\\plugins\\action\\command.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["merge_hash", "self._execute_module", "self._remove_tmp_path", "super", "super(ActionModule, self).run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    self._supports_async = True\n    results = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    wrap_async = self._task.async_val and not self._connection.has_native_async\n    # explicitly call `ansible.legacy.command` for backcompat to allow library/ override of `command` while not allowing\n    # collections search for an unqualified `command` module\n    results = merge_hash(results, self._execute_module(module_name='ansible.legacy.command', task_vars=task_vars, wrap_async=wrap_async))\n\n    if not wrap_async:\n        # remove a temporary path we created\n        self._remove_tmp_path(self._connection._shell.tmpdir)\n\n    return results", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\action\\debug.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "ReplacingMarkerBehavior", "RoutingMarkerBehavior", "dict", "display.warning", "replacing_behavior.emit_warnings", "repr", "self._templar._engine.extend", "self._templar._engine.extend(marker_behavior=var_behavior).evaluate_expression", "self.validate_argument_spec", "super", "super(ActionModule, self).run"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    validation_result, new_module_args = self.validate_argument_spec(\n        argument_spec=dict(\n            msg=dict(type='raw', default='Hello world!'),\n            var=dict(type=_check_type_str_no_conversion),\n            verbosity=dict(type='int', default=0),\n        ),\n        mutually_exclusive=(\n            ('msg', 'var'),\n        ),\n    )\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    # get task verbosity\n    verbosity = new_module_args['verbosity']\n\n    replacing_behavior = ReplacingMarkerBehavior()\n\n    var_behavior = RoutingMarkerBehavior({\n        UndefinedMarker: replacing_behavior,\n        TruncationMarker: replacing_behavior,\n    })\n\n    if verbosity <= self._display.verbosity:\n        if raw_var_arg := new_module_args['var']:\n            # If var name is same as result, try to template it\n            try:\n                results = self._templar._engine.extend(marker_behavior=var_behavior).evaluate_expression(raw_var_arg)\n            except AnsibleValueOmittedError as ex:\n                results = repr(Omit)\n                display.warning(\"The result of the `var` expression could not be omitted; a placeholder was used instead.\", obj=ex.obj)\n            except Exception as ex:\n                raise AnsibleError('Error while resolving `var` expression.', obj=raw_var_arg) from ex\n\n            result[raw_var_arg] = results\n        else:\n            result['msg'] = new_module_args['msg']\n\n        # force flag to make debug output module always verbose\n        result['_ansible_verbose_always'] = True\n\n        # propagate any warnings in the task result unless we're skipping the task\n        replacing_behavior.emit_warnings()\n\n    else:\n        result['skipped_reason'] = \"Verbosity threshold not met.\"\n        result['skipped'] = True\n\n    result['failed'] = False\n\n    return result", "loc": 53}
{"file": "ansible\\lib\\ansible\\plugins\\action\\dnf.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "dict", "display.debug", "display.vvvv", "facts.get", "facts.get('ansible_facts', {}).get", "result.update", "self._execute_module", "self._shared_loader_obj.module_loader.has_plugin", "self._task.args.copy", "self._task.args.get", "self._templar.resolve_variable_expression", "super", "super(ActionModule, self).run"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    self._supports_check_mode = True\n    self._supports_async = True\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    # Carry-over concept from the package action plugin\n    if 'use' in self._task.args and 'use_backend' in self._task.args:\n        raise AnsibleActionFail(\"parameters are mutually exclusive: ('use', 'use_backend')\")\n\n    module = self._task.args.get('use', self._task.args.get('use_backend', 'auto'))\n\n    if module in {'yum', 'auto'}:\n        try:\n            # if we delegate, we should use delegated host's facts\n            expr = \"hostvars[delegate_to].ansible_facts.pkg_mgr\" if self._task.delegate_to else \"ansible_facts.pkg_mgr\"\n            module = self._templar.resolve_variable_expression(expr, local_variables=dict(delegate_to=self._task.delegate_to))\n        except Exception:\n            pass  # could not get it from template!\n\n    if module not in VALID_BACKENDS:\n        facts = self._execute_module(\n            module_name=\"ansible.legacy.setup\", module_args=dict(filter=\"ansible_pkg_mgr\", gather_subset=\"!all\"),\n            task_vars=task_vars)\n\n        if facts.get(\"failed\", False):\n            raise AnsibleActionFail(\n                f\"Failed to fetch ansible_pkg_mgr to determine the dnf action backend: {facts.get('msg')}\",\n                result=facts,\n            )\n\n        display.debug(\"Facts %s\" % facts)\n        module = facts.get(\"ansible_facts\", {}).get(\"ansible_pkg_mgr\", \"auto\")\n        if (not self._task.delegate_to or self._task.delegate_facts) and module != 'auto':\n            result['ansible_facts'] = {'pkg_mgr': module}\n\n    if module not in VALID_BACKENDS:\n        result.update(\n            {\n                'failed': True,\n                'msg': (\"Could not detect which major revision of dnf is in use, which is required to determine module backend.\",\n                        \"You should manually specify use_backend to tell the module whether to use the dnf4 or dnf5 backend})\"),\n            }\n        )\n\n    else:\n        if module in {\"yum4\", \"dnf4\"}:\n            module = \"dnf\"\n\n        # eliminate collisions with collections search while still allowing local override\n        module = 'ansible.legacy.' + module\n\n        if not self._shared_loader_obj.module_loader.has_plugin(module):\n            result.update({'failed': True, 'msg': \"Could not find a dnf module backend for %s.\" % module})\n        else:\n            new_module_args = self._task.args.copy()\n            if 'use_backend' in new_module_args:\n                del new_module_args['use_backend']\n            if 'use' in new_module_args:\n                del new_module_args['use']\n\n            display.vvvv(\"Running %s as the backend for the dnf action plugin\" % module)\n            result.update(self._execute_module(\n                module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val))\n\n    return result", "loc": 67}
{"file": "ansible\\lib\\ansible\\plugins\\action\\fail.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "self._task.args.get", "super", "super(ActionModule, self).run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    msg = 'Failed as requested from task'\n    if self._task.args and 'msg' in self._task.args:\n        msg = self._task.args.get('msg')\n\n    result['failed'] = True\n    result['msg'] = msg\n    return result", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\action\\group_by.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "group_name.replace", "isinstance", "name.replace", "self._task.args.get", "super", "super(ActionModule, self).run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    if 'key' not in self._task.args:\n        result['failed'] = True\n        result['msg'] = \"the 'key' param is required when using group_by\"\n        return result\n\n    group_name = self._task.args.get('key')\n    parent_groups = self._task.args.get('parents', ['all'])\n    if isinstance(parent_groups, str):\n        parent_groups = [parent_groups]\n\n    result['changed'] = False\n    result['add_group'] = group_name.replace(' ', '-')\n    result['parent_groups'] = [name.replace(' ', '-') for name in parent_groups]\n    return result", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\action\\include_vars.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "combine_vars", "dict", "path.exists", "path.isdir", "results.update", "self._find_needle", "self._load_files", "self._load_files_in_dir", "self._set_args", "self._set_dir_defaults", "self._set_root_dir", "self._traverse_dir_depth", "super", "super(ActionModule, self).run", "task_vars.items", "to_native"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Load yml files recursively from a directory.", "source_code": "def run(self, tmp=None, task_vars=None):\n    \"\"\" Load yml files recursively from a directory.\n    \"\"\"\n    del tmp  # tmp no longer has any effect\n\n    if task_vars is None:\n        task_vars = dict()\n\n    self.show_content = True\n    self.included_files = []\n\n    # Validate arguments\n    dirs = 0\n    files = 0\n    for arg in self._task.args:\n        if arg in self.VALID_DIR_ARGUMENTS:\n            dirs += 1\n        elif arg in self.VALID_FILE_ARGUMENTS:\n            files += 1\n        elif arg in self.VALID_ALL:\n            pass\n        else:\n            raise AnsibleError(f'{arg} is not a valid option in include_vars', obj=arg)\n\n    if dirs and files:\n        raise AnsibleError(\"You are mixing file only and dir only arguments, these are incompatible\", obj=self._task.args)\n\n    # set internal vars from args\n    self._set_args()\n\n    results = dict()\n    failed = False\n    if self.source_dir:\n        self._set_dir_defaults()\n        self._set_root_dir()\n        if not path.exists(self.source_dir):\n            failed = True\n            err_msg = f\"{self.source_dir} directory does not exist\"\n        elif not path.isdir(self.source_dir):\n            failed = True\n            err_msg = f\"{self.source_dir} is not a directory\"\n        else:\n            for root_dir, filenames in self._traverse_dir_depth():\n                failed, err_msg, updated_results = self._load_files_in_dir(root_dir, filenames)\n                if failed:\n                    break\n                results.update(updated_results)\n    else:\n        try:\n            self.source_file = self._find_needle('vars', self.source_file)\n            failed, err_msg, updated_results = (\n                self._load_files(self.source_file)\n            )\n            if not failed:\n                results.update(updated_results)\n\n        except AnsibleError as e:\n            failed = True\n            err_msg = to_native(e)\n\n    if self.return_results_as_name:\n        scope = dict()\n        scope[self.return_results_as_name] = results\n        results = scope\n\n    result = super(ActionModule, self).run(task_vars=task_vars)\n\n    if failed:\n        result['failed'] = failed\n        result['message'] = err_msg\n    elif self.hash_behaviour is not None and self.hash_behaviour != C.DEFAULT_HASH_BEHAVIOUR:\n        merge_hashes = self.hash_behaviour == 'merge'\n        existing_variables = {k: v for k, v in task_vars.items() if k in results}\n        results = combine_vars(existing_variables, results, merge=merge_hashes)\n\n    result['ansible_included_var_files'] = self.included_files\n    result['ansible_facts'] = results\n    result['_ansible_no_log'] = not self.show_content\n\n    return result", "loc": 80}
{"file": "ansible\\lib\\ansible\\plugins\\action\\normal.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["merge_hash", "self._execute_module", "self._remove_tmp_path", "super", "super(ActionModule, self).run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n\n    # individual modules might disagree but as the generic the action plugin, pass at this point.\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    wrap_async = self._task.async_val and not self._connection.has_native_async\n\n    # do work!\n    result = merge_hash(result, self._execute_module(task_vars=task_vars, wrap_async=wrap_async))\n\n    # hack to keep --verbose from showing all the setup module result\n    # moved from setup module as now we filter out all _ansible_ from result\n    if self._task.action in C._ACTION_SETUP:\n        result['_ansible_verbose_override'] = True\n\n    if not wrap_async:\n        # remove a temporary path we created\n        self._remove_tmp_path(self._connection._shell.tmpdir)\n\n    return result", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\action\\package.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "_apply_action_arg_defaults", "combine_vars", "dict", "display.vvvv", "facts.get", "hosts_vars.get", "hosts_vars.get('ansible_facts', {}).get", "self._execute_module", "self._shared_loader_obj.module_loader.find_plugin_with_context", "self._shared_loader_obj.module_loader.has_plugin", "self._task.args.copy", "self._task.args.get", "super", "super(ActionModule, self).run", "task_vars.get", "tvars.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "handler for package operations", "source_code": "def run(self, tmp=None, task_vars=None):\n    \"\"\" handler for package operations \"\"\"\n\n    self._supports_check_mode = True\n    self._supports_async = True\n\n    super(ActionModule, self).run(tmp, task_vars)\n\n    module = self._task.args.get('use', 'auto')\n\n    try:\n        if module == 'auto':\n\n            if self._task.delegate_to:\n                hosts_vars = task_vars['hostvars'][self._task.delegate_to]\n                tvars = combine_vars(self._task.vars, task_vars.get('delegated_vars', {}))\n            else:\n                hosts_vars = task_vars\n                tvars = task_vars\n\n            # use config\n            module = tvars.get('ansible_package_use', None)\n\n            if not module:\n                # no use, no config, get from facts\n                if hosts_vars.get('ansible_facts', {}).get('pkg_mgr', False):\n                    facts = hosts_vars\n                    pmgr = 'pkg_mgr'\n                else:\n                    # we had no facts, so generate them\n                    # very expensive step, we actually run fact gathering because we don't have facts for this host.\n                    facts = self._execute_module(\n                        module_name='ansible.legacy.setup',\n                        module_args=dict(filter='ansible_pkg_mgr', gather_subset='!all'),\n                        task_vars=task_vars,\n                    )\n                    if facts.get(\"failed\", False):\n                        raise AnsibleActionFail(\n                            f\"Failed to fetch ansible_pkg_mgr to determine the package action backend: {facts.get('msg')}\",\n                            result=facts,\n                        )\n                    pmgr = 'ansible_pkg_mgr'\n\n                try:\n                    # actually get from facts\n                    module = facts['ansible_facts'][pmgr]\n                except KeyError:\n                    raise AnsibleActionFail('Could not detect a package manager. Try using the \"use\" option.')\n\n        if module and module != 'auto':\n            if not self._shared_loader_obj.module_loader.has_plugin(module):\n                raise AnsibleActionFail('Could not find a matching action for the \"%s\" package manager.' % module)\n            else:\n                # run the 'package' module\n                new_module_args = self._task.args.copy()\n                if 'use' in new_module_args:\n                    del new_module_args['use']\n\n                # get defaults for specific module\n                context = self._shared_loader_obj.module_loader.find_plugin_with_context(module, collection_list=self._task.collections)\n                new_module_args = _apply_action_arg_defaults(context.resolved_fqcn, self._task, new_module_args, self._templar)\n\n                if module in self.BUILTIN_PKG_MGR_MODULES:\n                    # prefix with ansible.legacy to eliminate external collisions while still allowing library/ override\n                    module = 'ansible.legacy.' + module\n\n                display.vvvv(\"Running %s\" % module)\n                return self._execute_module(module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val)\n        else:\n            raise AnsibleActionFail('Could not detect which package manager to use. Try gathering facts or setting the \"use\" option.')\n    finally:\n        pass  # avoid de-dent all on refactor", "loc": 72}
{"file": "ansible\\lib\\ansible\\plugins\\action\\raw.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "dict", "result.update", "self._display.warning", "self._low_level_execute_command", "self._task.args.get", "super", "super(ActionModule, self).run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    if self._task.environment and any(self._task.environment):\n        self._display.warning('raw module does not support the environment keyword')\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    if self._task.check_mode:\n        # in --check mode, always skip this module execution\n        result['skipped'] = True\n        return result\n\n    executable = self._task.args.get('executable', False)\n    result.update(self._low_level_execute_command(self._task.args.get('_raw_params'), executable=executable))\n\n    result['changed'] = True\n\n    if 'rc' in result and result['rc'] != 0:\n        result['failed'] = True\n        result['msg'] = 'non-zero return code'\n\n    return result", "loc": 25}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "get_shutdown_command_args", "parameters": ["self", "distribution"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "args.format", "check_type_str", "reboot_command.split", "self._get_value_from_facts", "self._task.args.get", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_shutdown_command_args(self, distribution):\n    reboot_command = self._task.args.get('reboot_command')\n    if reboot_command is not None:\n        try:\n            reboot_command = check_type_str(reboot_command, allow_conversion=False)\n        except TypeError as e:\n            raise AnsibleError(\"Invalid value given for 'reboot_command': %s.\" % to_native(e))\n\n        # No args were provided\n        try:\n            return reboot_command.split(' ', 1)[1]\n        except IndexError:\n            return ''\n    else:\n        args = self._get_value_from_facts('SHUTDOWN_COMMAND_ARGS', distribution, 'DEFAULT_SHUTDOWN_COMMAND_ARGS')\n\n        # Convert seconds to minutes. If less than 60, set it to 0.\n        delay_min = self.pre_reboot_delay // 60\n        reboot_message = self._task.args.get('msg', self.DEFAULT_REBOOT_MESSAGE)\n        return args.format(delay_sec=self.pre_reboot_delay, delay_min=delay_min, message=reboot_message)", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "get_distribution", "parameters": ["self", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Failed to determine system distribution. {0}, {1}'.format", "'Failed to get distribution information. Missing \"{0}\" in output.'.format", "'{action}: distribution: {dist}'.format", "'{action}: running setup module to get distribution'.format", "AnsibleError", "display.debug", "module_output.get", "module_output['ansible_facts']['ansible_distribution'].lower", "module_output['ansible_facts']['ansible_distribution_version'].split", "module_output['ansible_facts']['ansible_os_family'].lower", "self._execute_module", "to_native", "to_native(module_output['module_stderr']).strip", "to_native(module_output['module_stdout']).strip", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_distribution(self, task_vars):\n    # FIXME: only execute the module if we don't already have the facts we need\n    distribution = {}\n    display.debug('{action}: running setup module to get distribution'.format(action=self._task.action))\n    module_output = self._execute_module(\n        task_vars=task_vars,\n        module_name='ansible.legacy.setup',\n        module_args={'gather_subset': 'min'})\n    try:\n        if module_output.get('failed', False):\n            raise AnsibleError('Failed to determine system distribution. {0}, {1}'.format(\n                to_native(module_output['module_stdout']).strip(),\n                to_native(module_output['module_stderr']).strip()))\n        distribution['name'] = module_output['ansible_facts']['ansible_distribution'].lower()\n        distribution['version'] = to_text(module_output['ansible_facts']['ansible_distribution_version'].split('.')[0])\n        distribution['family'] = to_text(module_output['ansible_facts']['ansible_os_family'].lower())\n        display.debug(\"{action}: distribution: {dist}\".format(action=self._task.action, dist=distribution))\n        return distribution\n    except KeyError as ke:\n        raise AnsibleError('Failed to get distribution information. Missing \"{0}\" in output.'.format(ke.args[0]))", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "get_shutdown_command", "parameters": ["self", "task_vars", "distribution"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Unable to find command \"{0}\" in search paths: {1}'.format", "'{action}: running find module looking in {paths} to get path for \"{command}\"'.format", "AnsibleError", "check_type_list", "check_type_str", "display.debug", "err_msg.format", "reboot_command.split", "self._execute_module", "self._get_value_from_facts", "self._task.args.get", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_shutdown_command(self, task_vars, distribution):\n    reboot_command = self._task.args.get('reboot_command')\n    if reboot_command is not None:\n        try:\n            reboot_command = check_type_str(reboot_command, allow_conversion=False)\n        except TypeError as e:\n            raise AnsibleError(\"Invalid value given for 'reboot_command': %s.\" % to_native(e))\n        shutdown_bin = reboot_command.split(' ', 1)[0]\n    else:\n        shutdown_bin = self._get_value_from_facts('SHUTDOWN_COMMANDS', distribution, 'DEFAULT_SHUTDOWN_COMMAND')\n\n    if shutdown_bin[0] == '/':\n        return shutdown_bin\n    else:\n        default_search_paths = ['/sbin', '/bin', '/usr/sbin', '/usr/bin', '/usr/local/sbin']\n        search_paths = self._task.args.get('search_paths', default_search_paths)\n\n        try:\n            # Convert bare strings to a list\n            search_paths = check_type_list(search_paths)\n        except TypeError:\n            err_msg = \"'search_paths' must be a string or flat list of strings, got {0}\"\n            raise AnsibleError(err_msg.format(search_paths))\n\n        display.debug('{action}: running find module looking in {paths} to get path for \"{command}\"'.format(\n            action=self._task.action,\n            command=shutdown_bin,\n            paths=search_paths))\n\n        find_result = self._execute_module(\n            task_vars=task_vars,\n            # prevent collection search by calling with ansible.legacy (still allows library/ override of find)\n            module_name='ansible.legacy.find',\n            module_args={\n                'paths': search_paths,\n                'patterns': [shutdown_bin],\n                'file_type': 'any'\n            }\n        )\n\n        full_path = [x['path'] for x in find_result['files']]\n        if not full_path:\n            raise AnsibleError('Unable to find command \"{0}\" in search paths: {1}'.format(shutdown_bin, search_paths))\n        return full_path[0]", "loc": 44}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "deprecated_args", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Since Ansible {version}, {arg} is no longer a valid option for {action}'.format", "display.warning", "self.DEPRECATED_ARGS.items", "self._task.args.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deprecated_args(self):\n    for arg, version in self.DEPRECATED_ARGS.items():\n        if self._task.args.get(arg) is not None:\n            display.warning(\"Since Ansible {version}, {arg} is no longer a valid option for {action}\".format(\n                version=version,\n                arg=arg,\n                action=self._task.action))", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "get_system_boot_time", "parameters": ["self", "distribution"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"{action}: getting boot time with command: '{command}'\".format", "'{action}: failed to get host boot time info, rc: {rc}, stdout: {out}, stderr: {err}'.format", "'{action}: last boot time: {boot}'.format", "AnsibleError", "check_type_str", "command_result['stdout'].strip", "display.debug", "self._get_value_from_facts", "self._low_level_execute_command", "self._task.args.get", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_system_boot_time(self, distribution):\n    boot_time_command = self._get_value_from_facts('BOOT_TIME_COMMANDS', distribution, 'DEFAULT_BOOT_TIME_COMMAND')\n    if self._task.args.get('boot_time_command'):\n        boot_time_command = self._task.args.get('boot_time_command')\n\n        try:\n            check_type_str(boot_time_command, allow_conversion=False)\n        except TypeError as e:\n            raise AnsibleError(\"Invalid value given for 'boot_time_command': %s.\" % to_native(e))\n\n    display.debug(\"{action}: getting boot time with command: '{command}'\".format(action=self._task.action, command=boot_time_command))\n    command_result = self._low_level_execute_command(boot_time_command, sudoable=self.DEFAULT_SUDOABLE)\n\n    if command_result['rc'] != 0:\n        stdout = command_result['stdout']\n        stderr = command_result['stderr']\n        raise AnsibleError(\"{action}: failed to get host boot time info, rc: {rc}, stdout: {out}, stderr: {err}\".format(\n                           action=self._task.action,\n                           rc=command_result['rc'],\n                           out=to_native(stdout),\n                           err=to_native(stderr)))\n    display.debug(\"{action}: last boot time: {boot}\".format(action=self._task.action, boot=command_result['stdout'].strip()))\n    return command_result['stdout'].strip()", "loc": 23}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "check_boot_time", "parameters": ["self", "distribution", "previous_boot_time"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{action}: attempting to get system boot time'.format", "'{action}: setting connect_timeout to {value}'.format", "ValueError", "display.debug", "display.vvv", "display.warning", "len", "self._connection.reset", "self._connection.set_option", "self._task.args.get", "self.get_system_boot_time"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_boot_time(self, distribution, previous_boot_time):\n    display.vvv(\"{action}: attempting to get system boot time\".format(action=self._task.action))\n    connect_timeout = self._task.args.get('connect_timeout', self._task.args.get('connect_timeout_sec', self.DEFAULT_CONNECT_TIMEOUT))\n\n    # override connection timeout from defaults to the custom value\n    if connect_timeout:\n        try:\n            display.debug(\"{action}: setting connect_timeout to {value}\".format(action=self._task.action, value=connect_timeout))\n            self._connection.set_option(\"connection_timeout\", connect_timeout)\n        except AnsibleError:\n            try:\n                self._connection.set_option(\"timeout\", connect_timeout)\n            except (AnsibleError, AttributeError):\n                display.warning(\"Connection plugin does not allow the connection timeout to be overridden\")\n\n        self._connection.reset()\n\n    # try and get boot time\n    try:\n        current_boot_time = self.get_system_boot_time(distribution)\n    except Exception as e:\n        raise e\n\n    # FreeBSD returns an empty string immediately before reboot so adding a length\n    # check to prevent prematurely assuming system has rebooted\n    if len(current_boot_time) == 0 or current_boot_time == previous_boot_time:\n        raise ValueError(\"boot time has not changed\")", "loc": 27}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "run_test_command", "parameters": ["self", "distribution"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"{action}: attempting post-reboot test command '{command}'\".format", "'Test command failed: {err} {out}'.format", "'{action}: attempting post-reboot test command'.format", "'{action}: system successfully rebooted'.format", "RuntimeError", "display.debug", "display.vvv", "self._connection.reset", "self._get_value_from_facts", "self._low_level_execute_command", "self._task.args.get", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_test_command(self, distribution, **kwargs):\n    test_command = self._task.args.get('test_command', self._get_value_from_facts('TEST_COMMANDS', distribution, 'DEFAULT_TEST_COMMAND'))\n    display.vvv(\"{action}: attempting post-reboot test command\".format(action=self._task.action))\n    display.debug(\"{action}: attempting post-reboot test command '{command}'\".format(action=self._task.action, command=test_command))\n    try:\n        command_result = self._low_level_execute_command(test_command, sudoable=self.DEFAULT_SUDOABLE)\n    except Exception:\n        # may need to reset the connection in case another reboot occurred\n        # which has invalidated our connection\n        try:\n            self._connection.reset()\n        except AttributeError:\n            pass\n        raise\n\n    if command_result['rc'] != 0:\n        msg = 'Test command failed: {err} {out}'.format(\n            err=to_native(command_result['stderr']),\n            out=to_native(command_result['stdout']))\n        raise RuntimeError(msg)\n\n    display.vvv(\"{action}: system successfully rebooted\".format(action=self._task.action))", "loc": 22}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "do_until_success_or_timeout", "parameters": ["self", "action", "reboot_timeout", "action_desc", "distribution", "action_kwargs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Timed out waiting for {desc} (timeout={timeout})'.format", "'{action}: {desc} success'.format", "TimedOutException", "action", "datetime.now", "display.debug", "display.vvv", "isinstance", "secrets.randbelow", "self._connection.reset", "time.sleep", "timedelta", "to_text", "to_text(e).splitlines"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_until_success_or_timeout(self, action, reboot_timeout, action_desc, distribution, action_kwargs=None):\n    max_end_time = datetime.now(timezone.utc) + timedelta(seconds=reboot_timeout)\n    if action_kwargs is None:\n        action_kwargs = {}\n\n    fail_count = 0\n    max_fail_sleep = 12\n    last_error_msg = ''\n\n    while datetime.now(timezone.utc) < max_end_time:\n        try:\n            action(distribution=distribution, **action_kwargs)\n            if action_desc:\n                display.debug('{action}: {desc} success'.format(action=self._task.action, desc=action_desc))\n            return\n        except Exception as e:\n            if isinstance(e, AnsibleConnectionFailure):\n                try:\n                    self._connection.reset()\n                except AnsibleConnectionFailure:\n                    pass\n            # Use exponential backoff with a max timeout, plus a little bit of randomness\n            random_int = secrets.randbelow(1000) / 1000\n            fail_sleep = 2 ** fail_count + random_int\n            if fail_sleep > max_fail_sleep:\n\n                fail_sleep = max_fail_sleep + random_int\n            if action_desc:\n                try:\n                    error = to_text(e).splitlines()[-1]\n                except IndexError as e:\n                    error = to_text(e)\n                last_error_msg = f\"{self._task.action}: {action_desc} fail '{error}'\"\n                msg = f\"{last_error_msg}, retrying in {fail_sleep:.4f} seconds...\"\n\n                display.debug(msg)\n                display.vvv(msg)\n            fail_count += 1\n            time.sleep(fail_sleep)\n\n    if last_error_msg:\n        msg = f\"Last error message before the timeout exception - {last_error_msg}\"\n        display.debug(msg)\n        display.vvv(msg)\n    raise TimedOutException('Timed out waiting for {desc} (timeout={timeout})'.format(desc=action_desc, timeout=reboot_timeout))", "loc": 45}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "perform_reboot", "parameters": ["self", "task_vars", "distribution"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Reboot command failed. Error was: '{stdout}, {stderr}'\".format", "\"{action}: rebooting server with command '{command}'\".format", "'{0} {1}'.format", "'{action}: AnsibleConnectionFailure caught and handled: {error}'.format", "'{action}: rebooting server...'.format", "datetime.now", "display.debug", "display.vvv", "reboot_result['stderr'].strip", "reboot_result['stdout'].strip", "self._low_level_execute_command", "self.get_shutdown_command", "self.get_shutdown_command_args", "to_native", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def perform_reboot(self, task_vars, distribution):\n    result = {}\n    reboot_result = {}\n    shutdown_command = self.get_shutdown_command(task_vars, distribution)\n    shutdown_command_args = self.get_shutdown_command_args(distribution)\n    reboot_command = '{0} {1}'.format(shutdown_command, shutdown_command_args)\n\n    try:\n        display.vvv(\"{action}: rebooting server...\".format(action=self._task.action))\n        display.debug(\"{action}: rebooting server with command '{command}'\".format(action=self._task.action, command=reboot_command))\n        reboot_result = self._low_level_execute_command(reboot_command, sudoable=self.DEFAULT_SUDOABLE)\n    except AnsibleConnectionFailure as e:\n        # If the connection is closed too quickly due to the system being shutdown, carry on\n        display.debug('{action}: AnsibleConnectionFailure caught and handled: {error}'.format(action=self._task.action, error=to_text(e)))\n        reboot_result['rc'] = 0\n\n    result['start'] = datetime.now(timezone.utc)\n\n    if reboot_result['rc'] != 0:\n        result['failed'] = True\n        result['rebooted'] = False\n        result['msg'] = \"Reboot command failed. Error was: '{stdout}, {stderr}'\".format(\n            stdout=to_native(reboot_result['stdout'].strip()),\n            stderr=to_native(reboot_result['stderr'].strip()))\n        return result\n\n    result['failed'] = False\n    return result", "loc": 28}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "validate_reboot", "parameters": ["self", "distribution", "original_connection_timeout", "action_kwargs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{action}: failed to reset connection_timeout back to default: {error}'.format", "'{action}: setting connect_timeout/timeout back to original value of {value}'.format", "'{action}: validating reboot'.format", "display.debug", "display.vvv", "int", "self._connection.get_option", "self._connection.reset", "self._connection.set_option", "self._task.args.get", "self.do_until_success_or_timeout", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def validate_reboot(self, distribution, original_connection_timeout=None, action_kwargs=None):\n    display.vvv('{action}: validating reboot'.format(action=self._task.action))\n    result = {}\n\n    try:\n        # keep on checking system boot_time with short connection responses\n        reboot_timeout = int(self._task.args.get('reboot_timeout', self._task.args.get('reboot_timeout_sec', self.DEFAULT_REBOOT_TIMEOUT)))\n\n        self.do_until_success_or_timeout(\n            action=self.check_boot_time,\n            action_desc=\"last boot time check\",\n            reboot_timeout=reboot_timeout,\n            distribution=distribution,\n            action_kwargs=action_kwargs)\n\n        # Get the connect_timeout set on the connection to compare to the original\n        try:\n            connect_timeout = self._connection.get_option('connection_timeout')\n        except KeyError:\n            try:\n                connect_timeout = self._connection.get_option('timeout')\n            except KeyError:\n                pass\n        else:\n            if original_connection_timeout != connect_timeout:\n                try:\n                    display.debug(\"{action}: setting connect_timeout/timeout back to original value of {value}\".format(action=self._task.action,\n                                                                                                                       value=original_connection_timeout))\n                    try:\n                        self._connection.set_option(\"connection_timeout\", original_connection_timeout)\n                    except AnsibleError:\n                        try:\n                            self._connection.set_option(\"timeout\", original_connection_timeout)\n                        except AnsibleError:\n                            raise\n                    # reset the connection to clear the custom connection timeout\n                    self._connection.reset()\n                except (AnsibleError, AttributeError) as e:\n                    display.debug(\"{action}: failed to reset connection_timeout back to default: {error}\".format(action=self._task.action,\n                                                                                                                 error=to_text(e)))\n\n        # finally run test command to ensure everything is working\n        # FUTURE: add a stability check (system must remain up for N seconds) to deal with self-multi-reboot updates\n        self.do_until_success_or_timeout(\n            action=self.run_test_command,\n            action_desc=\"post-reboot test command\",\n            reboot_timeout=reboot_timeout,\n            distribution=distribution,\n            action_kwargs=action_kwargs)\n\n        result['rebooted'] = True\n        result['changed'] = True\n\n    except TimedOutException as toex:\n        result['failed'] = True\n        result['rebooted'] = True\n        result['msg'] = to_text(toex)\n        return result\n\n    return result", "loc": 60}
{"file": "ansible\\lib\\ansible\\plugins\\action\\reboot.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Running {0} with local connection would reboot the control node.'.format", "'{action}: connect_timeout connection option has not been set'.format", "'{action}: saving original connect_timeout of {timeout}'.format", "'{action}: waiting an additional {delay} seconds'.format", "datetime.now", "display.debug", "display.vvv", "result.get", "self._connection.get_option", "self.deprecated_args", "self.get_distribution", "self.get_system_boot_time", "self.perform_reboot", "self.validate_reboot", "super", "super(ActionModule, self).run", "time.sleep", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    self._supports_check_mode = True\n\n    # If running with local connection, fail so we don't reboot ourselves\n    if self._connection.transport == 'local':\n        msg = 'Running {0} with local connection would reboot the control node.'.format(self._task.action)\n        return {'changed': False, 'elapsed': 0, 'rebooted': False, 'failed': True, 'msg': msg}\n\n    if self._task.check_mode:\n        return {'changed': True, 'elapsed': 0, 'rebooted': True}\n\n    if task_vars is None:\n        task_vars = {}\n\n    self.deprecated_args()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n\n    if result.get('skipped', False) or result.get('failed', False):\n        return result\n\n    distribution = self.get_distribution(task_vars)\n\n    # Get current boot time\n    try:\n        previous_boot_time = self.get_system_boot_time(distribution)\n    except Exception as e:\n        result['failed'] = True\n        result['reboot'] = False\n        result['msg'] = to_text(e)\n        return result\n\n    # Get the original connection_timeout option var so it can be reset after\n    original_connection_timeout = None\n\n    display.debug(\"{action}: saving original connect_timeout of {timeout}\".format(action=self._task.action, timeout=original_connection_timeout))\n    try:\n        original_connection_timeout = self._connection.get_option('connection_timeout')\n    except KeyError:\n        try:\n            original_connection_timeout = self._connection.get_option('timeout')\n        except KeyError:\n            display.debug(\"{action}: connect_timeout connection option has not been set\".format(action=self._task.action))\n\n    # Initiate reboot\n    reboot_result = self.perform_reboot(task_vars, distribution)\n\n    if reboot_result['failed']:\n        result = reboot_result\n        elapsed = datetime.now(timezone.utc) - reboot_result['start']\n        result['elapsed'] = elapsed.seconds\n        return result\n\n    if self.post_reboot_delay != 0:\n        display.debug(\"{action}: waiting an additional {delay} seconds\".format(action=self._task.action, delay=self.post_reboot_delay))\n        display.vvv(\"{action}: waiting an additional {delay} seconds\".format(action=self._task.action, delay=self.post_reboot_delay))\n        time.sleep(self.post_reboot_delay)\n\n    # Make sure reboot was successful\n    result = self.validate_reboot(distribution, original_connection_timeout, action_kwargs={'previous_boot_time': previous_boot_time})\n\n    elapsed = datetime.now(timezone.utc) - reboot_result['start']\n    result['elapsed'] = elapsed.seconds\n\n    return result", "loc": 65}
{"file": "ansible\\lib\\ansible\\plugins\\action\\service.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "_apply_action_arg_defaults", "dict", "facts.get", "facts.get('ansible_facts', {}).get", "self._display.debug", "self._display.vvvv", "self._display.warning", "self._execute_module", "self._remove_tmp_path", "self._shared_loader_obj.module_loader.find_plugin_with_context", "self._shared_loader_obj.module_loader.has_plugin", "self._task.args.copy", "self._task.args.get", "self._task.args.get('use', 'auto').lower", "self._templar.resolve_variable_expression", "super", "super(ActionModule, self).run"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "handler for package operations", "source_code": "def run(self, tmp=None, task_vars=None):\n    \"\"\" handler for package operations \"\"\"\n\n    self._supports_check_mode = True\n    self._supports_async = True\n\n    super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    module = self._task.args.get('use', 'auto').lower()\n\n    if module == 'auto':\n        try:\n            # if we delegate, we should use delegated host's facts\n            expr = \"hostvars[delegate_to].ansible_facts.service_mgr\" if self._task.delegate_to else \"ansible_facts.service_mgr\"\n            module = self._templar.resolve_variable_expression(expr, local_variables=dict(delegate_to=self._task.delegate_to))\n        except Exception:\n            pass  # could not get it from template!\n\n    try:\n        if module == 'auto':\n            facts = self._execute_module(\n                module_name='ansible.legacy.setup',\n                module_args=dict(gather_subset='!all', filter='ansible_service_mgr'), task_vars=task_vars)\n            self._display.debug(\"Facts %s\" % facts)\n            module = facts.get('ansible_facts', {}).get('ansible_service_mgr', 'auto')\n\n        if not module or module == 'auto' or not self._shared_loader_obj.module_loader.has_plugin(module):\n            module = 'ansible.legacy.service'\n\n        if module != 'auto':\n            # run the 'service' module\n            new_module_args = self._task.args.copy()\n            if 'use' in new_module_args:\n                del new_module_args['use']\n\n            if module in self.UNUSED_PARAMS:\n                for unused in self.UNUSED_PARAMS[module]:\n                    if unused in new_module_args:\n                        del new_module_args[unused]\n                        self._display.warning('Ignoring \"%s\" as it is not used in \"%s\"' % (unused, module))\n\n            # get defaults for specific module\n            context = self._shared_loader_obj.module_loader.find_plugin_with_context(module, collection_list=self._task.collections)\n            new_module_args = _apply_action_arg_defaults(context.resolved_fqcn, self._task, new_module_args, self._templar)\n\n            # collection prefix known internal modules to avoid collisions from collections search, while still allowing library/ overrides\n            if module in self.BUILTIN_SVC_MGR_MODULES:\n                module = 'ansible.legacy.' + module\n\n            self._display.vvvv(\"Running %s\" % module)\n            return self._execute_module(module_name=module, module_args=new_module_args, task_vars=task_vars, wrap_async=self._task.async_val)\n        else:\n            raise AnsibleActionFail('Could not detect which service manager to use. Try gathering facts or setting the \"use\" option.')\n\n    finally:\n        if not self._task.async_val:\n            self._remove_tmp_path(self._connection._shell.tmpdir)", "loc": 58}
{"file": "ansible\\lib\\ansible\\plugins\\action\\set_fact.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "boolean", "dict", "self._task.args.items", "self._task.args.pop", "self._templar.template", "super", "super(ActionModule, self).run", "validate_variable_name"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    facts = {}\n    cacheable = boolean(self._task.args.pop('cacheable', False))\n\n    if self._task.args:\n        for (k, v) in self._task.args.items():\n            k = self._templar.template(k)  # a rare case where key templating is allowed; backward-compatibility for dynamic storage\n\n            validate_variable_name(k)\n\n            facts[k] = v\n    else:\n        raise AnsibleActionFail('No key/value pairs provided, at least one is required for this action to succeed')\n\n    if facts:\n        # just as _facts actions, we don't set changed=true as we are not modifying the actual host\n        result['ansible_facts'] = facts\n        result['_ansible_facts_cacheable'] = cacheable\n    else:\n        # this should not happen, but JIC we get here\n        raise AnsibleActionFail('Unable to create any variables with provided arguments')\n\n    return result", "loc": 29}
{"file": "ansible\\lib\\ansible\\plugins\\action\\set_stats.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["boolean", "data.items", "dict", "isinstance", "self._task.args.get", "self._templar.template", "super", "super(ActionModule, self).run", "validate_variable_name"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    stats = {'data': {}, 'per_host': False, 'aggregate': True}\n\n    if self._task.args:\n        data = self._task.args.get('data', {})\n\n        if not isinstance(data, dict):\n            data = self._templar.template(data)\n\n        if not isinstance(data, dict):\n            result['failed'] = True\n            result['msg'] = \"The 'data' option needs to be a dictionary/hash\"\n            return result\n\n        # set boolean options, defaults are set above in stats init\n        for opt in ['per_host', 'aggregate']:\n            val = self._task.args.get(opt, None)\n            if val is not None:\n                if not isinstance(val, bool):\n                    stats[opt] = boolean(self._templar.template(val), strict=False)\n                else:\n                    stats[opt] = val\n\n        for (k, v) in data.items():\n            k = self._templar.template(k)\n\n            validate_variable_name(k)\n\n            stats['data'][k] = self._templar.template(v)\n\n    result['changed'] = False\n    result['ansible_stats'] = stats\n\n    return result", "loc": 40}
{"file": "ansible\\lib\\ansible\\plugins\\action\\shell.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "command_action.run", "self._shared_loader_obj.action_loader.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    del tmp  # tmp no longer has any effect\n\n    # Shell module is implemented via command with a special arg\n    self._task.args['_uses_shell'] = True\n\n    # Shell shares the same module code as command. Fail if command\n    # specific options are set.\n    if \"expand_argument_vars\" in self._task.args:\n        raise AnsibleActionFail(f\"Unsupported parameters for ({self._task.action}) module: expand_argument_vars\")\n\n    command_action = self._shared_loader_obj.action_loader.get('ansible.legacy.command',\n                                                               task=self._task,\n                                                               connection=self._connection,\n                                                               play_context=self._play_context,\n                                                               loader=self._loader,\n                                                               templar=self._templar,\n                                                               shared_loader_obj=self._shared_loader_obj)\n    result = command_action.run(task_vars=task_vars)\n\n    return result", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\action\\validate_argument_spec.py", "class_name": "ActionModule", "function_name": "get_args_from_task_vars", "parameters": ["self", "argument_spec", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {"argument_spec": "A dict of the argument spec.", "task_vars": "A dict of task variables."}, "return_doc": "A dict of values that can be validated against the arg spec.", "raises_doc": [], "called_functions": ["argument_spec.items", "self._templar.template"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Get any arguments that may come from `task_vars`. Expand templated variables so we can validate the actual values.", "source_code": "def get_args_from_task_vars(self, argument_spec, task_vars):\n    \"\"\"\n    Get any arguments that may come from `task_vars`.\n\n    Expand templated variables so we can validate the actual values.\n\n    :param argument_spec: A dict of the argument spec.\n    :param task_vars: A dict of task variables.\n\n    :returns: A dict of values that can be validated against the arg spec.\n    \"\"\"\n    args = {}\n\n    for argument_name, argument_attrs in argument_spec.items():\n        if argument_name in task_vars:\n            args[argument_name] = task_vars[argument_name]\n    args = self._templar.template(args)\n    return args", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\action\\validate_argument_spec.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {"tmp": "Deprecated. Do not use.", "task_vars": "A dict of task variables."}, "return_doc": "An action result dict, including a 'argument_errors' key with a", "raises_doc": [], "called_functions": ["'\\n'.join", "AnsibleError", "ArgumentSpecValidator", "combine_vars", "dict", "isinstance", "self._task.args.get", "self.get_args_from_task_vars", "super", "super(ActionModule, self).run", "type", "validator.validate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validate an argument specification against a provided set of data. The `validate_argument_spec` module expects to receive the arguments: - argument_spec: A dict whose keys are the valid argument names, and whose values are dicts of the argument attributes (type, etc). - provided_arguments: A dict whose keys are the argument names, and whose values are the argument value.", "source_code": "def run(self, tmp=None, task_vars=None):\n    \"\"\"\n    Validate an argument specification against a provided set of data.\n\n    The `validate_argument_spec` module expects to receive the arguments:\n        - argument_spec: A dict whose keys are the valid argument names, and\n              whose values are dicts of the argument attributes (type, etc).\n        - provided_arguments: A dict whose keys are the argument names, and\n              whose values are the argument value.\n\n    :param tmp: Deprecated. Do not use.\n    :param task_vars: A dict of task variables.\n    :return: An action result dict, including a 'argument_errors' key with a\n        list of validation errors found.\n    \"\"\"\n    if task_vars is None:\n        task_vars = dict()\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    # This action can be called from anywhere, so pass in some info about what it is\n    # validating args for so the error results make some sense\n    result['validate_args_context'] = self._task.args.get('validate_args_context', {})\n\n    if 'argument_spec' not in self._task.args:\n        raise AnsibleError('\"argument_spec\" arg is required in args: %s' % self._task.args)\n\n    # Get the task var called argument_spec. This will contain the arg spec\n    # data dict (for the proper entry point for a role).\n    argument_spec_data = self._task.args.get('argument_spec')\n\n    # the values that were passed in and will be checked against argument_spec\n    provided_arguments = self._task.args.get('provided_arguments', {})\n\n    if not isinstance(argument_spec_data, dict):\n        raise AnsibleError('Incorrect type for argument_spec, expected dict and got %s' % type(argument_spec_data))\n\n    if not isinstance(provided_arguments, dict):\n        raise AnsibleError('Incorrect type for provided_arguments, expected dict and got %s' % type(provided_arguments))\n\n    args_from_vars = self.get_args_from_task_vars(argument_spec_data, task_vars)\n    validator = ArgumentSpecValidator(argument_spec_data)\n    validation_result = validator.validate(combine_vars(args_from_vars, provided_arguments), validate_role_argument_spec=True)\n\n    if validation_result.error_messages:\n        result['failed'] = True\n        result['msg'] = 'Validation of arguments failed:\\n%s' % '\\n'.join(validation_result.error_messages)\n        result['argument_spec_data'] = argument_spec_data\n        result['argument_errors'] = validation_result.error_messages\n        return result\n\n    result['changed'] = False\n    result['msg'] = 'The arg spec validation passed'\n\n    return result", "loc": 56}
{"file": "ansible\\lib\\ansible\\plugins\\action\\wait_for_connection.py", "class_name": "ActionModule", "function_name": "do_until_success_or_timeout", "parameters": ["self", "what", "timeout", "connect_timeout", "what_desc", "sleep"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TimedOutException", "datetime.now", "display.debug", "time.sleep", "timedelta", "what"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_until_success_or_timeout(self, what, timeout, connect_timeout, what_desc, sleep=1):\n    max_end_time = datetime.now(timezone.utc) + timedelta(seconds=timeout)\n\n    e = None\n    while datetime.now(timezone.utc) < max_end_time:\n        try:\n            what(connect_timeout)\n            if what_desc:\n                display.debug(\"wait_for_connection: %s success\" % what_desc)\n            return\n        except Exception as e:\n            error = e  # PY3 compatibility to store exception for use outside of this block\n            if what_desc:\n                display.debug(\"wait_for_connection: %s fail (expected), retrying in %d seconds...\" % (what_desc, sleep))\n            time.sleep(sleep)\n\n    raise TimedOutException(\"timed out waiting for %s: %s\" % (what_desc, error))", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\action\\wait_for_connection.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "datetime.now", "dict", "display.vvv", "hasattr", "int", "self._connection.reset", "self._execute_module", "self._remove_tmp_path", "self._task.args.get", "self.do_until_success_or_timeout", "super", "super(ActionModule, self).run", "task_vars['ansible_facts'].pop", "time.sleep", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    if task_vars is None:\n        task_vars = dict()\n\n    connect_timeout = int(self._task.args.get('connect_timeout', self.DEFAULT_CONNECT_TIMEOUT))\n    delay = int(self._task.args.get('delay', self.DEFAULT_DELAY))\n    sleep = int(self._task.args.get('sleep', self.DEFAULT_SLEEP))\n    timeout = int(self._task.args.get('timeout', self.DEFAULT_TIMEOUT))\n\n    if self._task.check_mode:\n        display.vvv(\"wait_for_connection: skipping for check_mode\")\n        return dict(skipped=True)\n\n    result = super(ActionModule, self).run(tmp, task_vars)\n    del tmp  # tmp no longer has any effect\n\n    def ping_module_test(connect_timeout):\n        \"\"\" Test ping module, if available \"\"\"\n        display.vvv(\"wait_for_connection: attempting ping module test\")\n        # re-run interpreter discovery if we ran it in the first iteration\n        if self._discovered_interpreter_key:\n            task_vars['ansible_facts'].pop(self._discovered_interpreter_key, None)\n        # call connection reset between runs if it's there\n        try:\n            self._connection.reset()\n        except AttributeError:\n            pass\n\n        ping_result = self._execute_module(module_name='ansible.legacy.ping', module_args=dict(), task_vars=task_vars)\n\n        # Test module output\n        if ping_result['ping'] != 'pong':\n            raise Exception('ping test failed')\n\n    start = datetime.now()\n\n    if delay:\n        time.sleep(delay)\n\n    try:\n        # If the connection has a transport_test method, use it first\n        if hasattr(self._connection, 'transport_test'):\n            self.do_until_success_or_timeout(self._connection.transport_test, timeout, connect_timeout, what_desc=\"connection port up\", sleep=sleep)\n\n        # Use the ping module test to determine end-to-end connectivity\n        self.do_until_success_or_timeout(ping_module_test, timeout, connect_timeout, what_desc=\"ping module test\", sleep=sleep)\n\n    except TimedOutException as e:\n        result['failed'] = True\n        result['msg'] = to_text(e)\n\n    elapsed = datetime.now() - start\n    result['elapsed'] = elapsed.seconds\n\n    # remove a temporary path we created\n    self._remove_tmp_path(self._connection._shell.tmpdir)\n\n    return result", "loc": 58}
{"file": "ansible\\lib\\ansible\\plugins\\action\\wait_for_connection.py", "class_name": null, "function_name": "ping_module_test", "parameters": ["connect_timeout"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "dict", "display.vvv", "self._connection.reset", "self._execute_module", "task_vars['ansible_facts'].pop"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Test ping module, if available", "source_code": "def ping_module_test(connect_timeout):\n    \"\"\" Test ping module, if available \"\"\"\n    display.vvv(\"wait_for_connection: attempting ping module test\")\n    # re-run interpreter discovery if we ran it in the first iteration\n    if self._discovered_interpreter_key:\n        task_vars['ansible_facts'].pop(self._discovered_interpreter_key, None)\n    # call connection reset between runs if it's there\n    try:\n        self._connection.reset()\n    except AttributeError:\n        pass\n\n    ping_result = self._execute_module(module_name='ansible.legacy.ping', module_args=dict(), task_vars=task_vars)\n\n    # Test module output\n    if ping_result['ping'] != 'pong':\n        raise Exception('ping test failed')", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\action\\__init__.py", "class_name": "ActionBase", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {"tmp": "str | None", "task_vars": "dict[str, t.Any] | None"}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "dictionary of results from the module", "raises_doc": [], "called_functions": ["','.join", "AnsibleActionFail", "AnsibleActionSkip", "display.warning", "frozenset", "list", "self._early_needs_tmp_path", "self._make_tmp_path", "self._task.args.keys", "task_opts.difference"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Action Plugins should implement this method to perform their tasks.  Everything else in this base class is a helper method for the action plugin to do that. :kwarg tmp: Deprecated parameter.  This is no longer used.  An action plugin that calls another one and wants to use the same remote tmp for both should set self._connection._shell.tmpdir rather than this parameter. :kwarg task_vars: The variables (host vars, group vars, config vars, etc) associated with this task.", "source_code": "def run(self, tmp: str | None = None, task_vars: dict[str, t.Any] | None = None) -> dict[str, t.Any]:\n    \"\"\" Action Plugins should implement this method to perform their\n    tasks.  Everything else in this base class is a helper method for the\n    action plugin to do that.\n\n    :kwarg tmp: Deprecated parameter.  This is no longer used.  An action plugin that calls\n        another one and wants to use the same remote tmp for both should set\n        self._connection._shell.tmpdir rather than this parameter.\n    :kwarg task_vars: The variables (host vars, group vars, config vars,\n        etc) associated with this task.\n    :returns: dictionary of results from the module\n\n    Implementers of action modules may find the following variables especially useful:\n\n    * Module parameters.  These are stored in self._task.args\n    \"\"\"\n    # does not default to {'changed': False, 'failed': False}, as it used to break async\n    result: dict[str, t.Any] = {}\n\n    if tmp is not None:\n        display.warning('ActionModule.run() no longer honors the tmp parameter. Action'\n                        ' plugins should set self._connection._shell.tmpdir to share'\n                        ' the tmpdir.')\n    del tmp\n\n    if self._task.async_val and not self._supports_async:\n        raise AnsibleActionFail('This action (%s) does not support async.' % self._task.action)\n    elif self._task.check_mode and not self._supports_check_mode:\n        raise AnsibleActionSkip('This action (%s) does not support check mode.' % self._task.action)\n\n    # Error if invalid argument is passed\n    if self._VALID_ARGS:\n        task_opts = frozenset(self._task.args.keys())\n        bad_opts = task_opts.difference(self._VALID_ARGS)\n        if bad_opts:\n            raise AnsibleActionFail('Invalid options for %s: %s' % (self._task.action, ','.join(list(bad_opts))))\n\n    if self._connection._shell.tmpdir is None and self._early_needs_tmp_path():\n        self._make_tmp_path()\n\n    return result", "loc": 41}
{"file": "ansible\\lib\\ansible\\plugins\\action\\__init__.py", "class_name": "ActionBase", "function_name": "validate_argument_spec", "parameters": ["self", "argument_spec", "mutually_exclusive", "required_together", "required_one_of", "required_if", "required_by"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleActionFail", "ArgumentSpecValidator", "isinstance", "new_module_args.update", "self._task.args.copy", "validator.validate"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Validate an argument spec against the task args This will return a tuple of (ValidationResult, dict) where the dict is the validated, coerced, and normalized task args.", "source_code": "def validate_argument_spec(self, argument_spec=None,\n                           mutually_exclusive=None,\n                           required_together=None,\n                           required_one_of=None,\n                           required_if=None,\n                           required_by=None,\n                           ):\n    \"\"\"Validate an argument spec against the task args\n\n    This will return a tuple of (ValidationResult, dict) where the dict\n    is the validated, coerced, and normalized task args.\n\n    Be cautious when directly passing ``new_module_args`` directly to a\n    module invocation, as it will contain the defaults, and not only\n    the args supplied from the task. If you do this, the module\n    should not define ``mutually_exclusive`` or similar.\n\n    This code is roughly copied from the ``validate_argument_spec``\n    action plugin for use by other action plugins.\n    \"\"\"\n\n    new_module_args = self._task.args.copy()\n\n    validator = ArgumentSpecValidator(\n        argument_spec,\n        mutually_exclusive=mutually_exclusive,\n        required_together=required_together,\n        required_one_of=required_one_of,\n        required_if=required_if,\n        required_by=required_by,\n    )\n    validation_result = validator.validate(new_module_args)\n\n    new_module_args.update(validation_result.validated_parameters)\n\n    try:\n        error = validation_result.errors[0]\n    except IndexError:\n        error = None\n\n    # Fail for validation errors, even in check mode\n    if error:\n        msg = validation_result.errors.msg\n        if isinstance(error, UnsupportedError):\n            msg = f\"Unsupported parameters for ({self._load_name}) module: {msg}\"\n\n        raise AnsibleActionFail(msg, obj=self._task.args)\n\n    return validation_result, new_module_args", "loc": 49}
{"file": "ansible\\lib\\ansible\\plugins\\action\\__init__.py", "class_name": "ActionBase", "function_name": "cleanup", "parameters": ["self", "force"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._remove_tmp_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Method to perform a clean up at the end of an action plugin execution By default this is designed to clean up the shell tmpdir, and is toggled based on whether async is in use", "source_code": "def cleanup(self, force=False):\n    \"\"\"Method to perform a clean up at the end of an action plugin execution\n\n    By default this is designed to clean up the shell tmpdir, and is toggled based on whether\n    async is in use\n\n    Action plugins may override this if they deem necessary, but should still call this method\n    via super\n    \"\"\"\n    if force or not self._task.async_val:\n        self._remove_tmp_path(self._connection._shell.tmpdir)", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\action\\__init__.py", "class_name": "ActionBase", "function_name": "get_plugin_option", "parameters": ["self", "plugin", "option", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["plugin.get_option"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Helper to get an option from a plugin without having to use the try/except dance everywhere to set a default", "source_code": "def get_plugin_option(self, plugin, option, default=None):\n    \"\"\"Helper to get an option from a plugin without having to use\n    the try/except dance everywhere to set a default\n    \"\"\"\n    try:\n        return plugin.get_option(option)\n    except (AttributeError, KeyError):\n        return default", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\become\\su.py", "class_name": "BecomeModule", "function_name": "check_password_prompt", "parameters": ["self", "b_output"], "param_types": {"b_output": "bytes"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'|'.join", "bool", "match.group", "re.escape", "re.search", "self.get_option", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "checks if the expected password prompt exists in b_output", "source_code": "def check_password_prompt(self, b_output: bytes) -> bool:\n    \"\"\" checks if the expected password prompt exists in b_output \"\"\"\n    prompts = self.get_option('prompt_l10n') or self.SU_PROMPT_LOCALIZATIONS\n    password_prompt_strings = \"|\".join(re.escape(p) for p in prompts)\n    # Colon or unicode fullwidth colon\n    prompt_pattern = rf\"(?:{password_prompt_strings})\\s*[:：]\"\n    match = re.search(prompt_pattern, to_text(b_output), flags=re.IGNORECASE)\n\n    if match:\n        self.prompt = match.group(0)  # preserve the actual matched string so we can scrub the output\n\n    return bool(match)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\become\\su.py", "class_name": "BecomeModule", "function_name": "build_become_command", "parameters": ["self", "cmd", "shell"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._build_success_command", "self.get_option", "shlex.quote", "super", "super(BecomeModule, self).build_become_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_become_command(self, cmd, shell):\n    super(BecomeModule, self).build_become_command(cmd, shell)\n\n    # Prompt handling for ``su`` is more complicated, this\n    # is used to satisfy the connection plugin\n    self.prompt = True\n\n    if not cmd:\n        return cmd\n\n    exe = self.get_option('become_exe') or self.name\n    flags = self.get_option('become_flags') or ''\n    user = self.get_option('become_user') or ''\n    success_cmd = self._build_success_command(cmd, shell)\n\n    return \"%s %s %s -c %s\" % (exe, flags, user, shlex.quote(success_cmd))", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\become\\sudo.py", "class_name": "BecomeModule", "function_name": "build_become_command", "parameters": ["self", "cmd", "shell"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "AnsibleError", "flag.startswith", "re.sub", "reflag.append", "self._build_success_command", "self.get_option", "shlex.join", "shlex.quote", "shlex.split", "super", "super(BecomeModule, self).build_become_command"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_become_command(self, cmd, shell):\n    super(BecomeModule, self).build_become_command(cmd, shell)\n\n    if not cmd:\n        return cmd\n\n    becomecmd = self.get_option('become_exe') or self.name\n\n    flags = self.get_option('become_flags') or ''\n    prompt = ''\n    if self.get_option('become_pass'):\n        self.prompt = '[sudo via ansible, key=%s] password:' % self._id\n        if flags:  # this could be simplified, but kept as is for now for backwards string matching\n            reflag = []\n            for flag in shlex.split(flags):\n                if flag in ('-n', '--non-interactive'):\n                    continue\n                elif not flag.startswith('--'):\n                    # handle -XnxxX flags only\n                    flag = re.sub(r'^(-\\w*)n(\\w*.*)', r'\\1\\2', flag)\n                reflag.append(flag)\n            flags = shlex.join(reflag)\n\n        prompt = '-p \"%s\"' % (self.prompt)\n\n    user = self.get_option('become_user') or ''\n    if user:\n        user = '-u %s' % (user)\n\n    if chdir := self.get_option('sudo_chdir'):\n        try:\n            becomecmd = f'{shell.CD} {shlex.quote(chdir)} {shell._SHELL_AND} {becomecmd}'\n        except AttributeError as ex:\n            raise AnsibleError(f'The {shell._load_name!r} shell plugin does not support sudo chdir. It is missing the {ex.name!r} attribute.')\n\n    return ' '.join([becomecmd, flags, prompt, user, self._build_success_command(cmd, shell)])", "loc": 36}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "get_option", "parameters": ["self", "option", "hostvars", "playcontext"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "super", "super(BecomeBase, self).get_option"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Overrides the base get_option to provide a fallback to playcontext vars in case a 3rd party plugin did not implement the base become options required in Ansible.", "source_code": "def get_option(self, option, hostvars=None, playcontext=None):\n    \"\"\" Overrides the base get_option to provide a fallback to playcontext vars in case a 3rd party plugin did not\n    implement the base become options required in Ansible. \"\"\"\n    # TODO: add deprecation warning for ValueError in devel that removes the playcontext fallback\n    try:\n        return super(BecomeBase, self).get_option(option, hostvars=hostvars)\n    except KeyError:\n        pc_fallback = ['become_user', 'become_pass', 'become_flags', 'become_exe']\n        if option not in pc_fallback:\n            raise\n\n        return getattr(playcontext, option, None)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "expect_prompt", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "self.get_option"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "This function assists connection plugins in determining if they need to wait for a prompt. Both a prompt and a password are required.", "source_code": "def expect_prompt(self) -> bool:\n    \"\"\"This function assists connection plugins in determining if they need to wait for\n    a prompt. Both a prompt and a password are required.\n    \"\"\"\n    return bool(self.prompt and self.get_option('become_pass'))", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "strip_become_prompt", "parameters": ["self", "data"], "param_types": {"data": "bytes"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._strip_through_prefix", "self.expect_prompt"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Strips the first found configured become prompt from `data`, trailing whitespace and anything that precedes the prompt, then returns the result. If no prompt is expected, or the prompt is not `str` or `bytes`, `data` will be returned as-is.", "source_code": "def strip_become_prompt(self, data: bytes) -> bytes:\n    \"\"\"\n    Strips the first found configured become prompt from `data`, trailing whitespace and anything that precedes the prompt, then returns the result.\n    If no prompt is expected, or the prompt is not `str` or `bytes`, `data` will be returned as-is.\n    \"\"\"\n    if not self.prompt or not isinstance(self.prompt, (str, bytes)) or not self.expect_prompt():\n        return data\n\n    return self._strip_through_prefix(self.prompt, data)", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "check_password_prompt", "parameters": ["self", "b_output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "b_output.splitlines", "l.strip", "l.strip().startswith", "to_bytes", "to_bytes(self.prompt).strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "checks if the expected password prompt exists in b_output", "source_code": "def check_password_prompt(self, b_output):\n    \"\"\" checks if the expected password prompt exists in b_output \"\"\"\n    if self.prompt:\n        b_prompt = to_bytes(self.prompt).strip()\n        return any(l.strip().startswith(b_prompt) for l in b_output.splitlines())\n    return False", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "check_incorrect_password", "parameters": ["self", "b_output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._check_password_error"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_incorrect_password(self, b_output):\n    for errstring in self.fail:\n        if self._check_password_error(b_output, errstring):\n            return True\n    return False", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\become\\__init__.py", "class_name": "BecomeBase", "function_name": "check_missing_password", "parameters": ["self", "b_output"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._check_password_error"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_missing_password(self, b_output):\n    for errstring in self.missing:\n        if self._check_password_error(b_output, errstring):\n            return True\n    return False", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "BaseFileCacheModule", "function_name": "get", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.warning", "self._cache.get", "self._get_cache_file_name", "self._load", "self.delete", "self.has_expired", "to_bytes"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "This checks the in memory cache first as the fact was not expired at 'gather time' and it would be problematic if the key did expire after some long running tasks and user gets 'undefined' error in the same play", "source_code": "def get(self, key):\n    \"\"\" This checks the in memory cache first as the fact was not expired at 'gather time'\n    and it would be problematic if the key did expire after some long running tasks and\n    user gets 'undefined' error in the same play \"\"\"\n\n    if key not in self._cache:\n\n        if self.has_expired(key) or key == \"\":\n            raise KeyError\n\n        cachefile = self._get_cache_file_name(key)\n        try:\n            value = self._load(cachefile)\n            self._cache[key] = value\n        except ValueError as e:\n            display.warning(\"error in '%s' cache plugin while trying to read %s : %s. \"\n                            \"Most likely a corrupt file, so erasing and failing.\" % (self.plugin_name, cachefile, to_bytes(e)))\n            self.delete(key)\n            raise AnsibleError(\"The cache file %s was corrupt, or did not otherwise contain valid data. \"\n                               \"It has been removed, so you can re-run your command now.\" % cachefile)\n        except FileNotFoundError:\n            raise KeyError\n        except Exception as ex:\n            raise AnsibleError(f\"Error while accessing the cache file {cachefile!r}.\") from ex\n\n    return self._cache.get(key)", "loc": 26}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "BaseFileCacheModule", "function_name": "has_expired", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.error_as_warning", "os.stat", "self._get_cache_file_name", "time.time"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def has_expired(self, key):\n\n    if self._timeout == 0:\n        return False\n\n    cachefile = self._get_cache_file_name(key)\n    try:\n        st = os.stat(cachefile)\n    except FileNotFoundError:\n        return False\n    except OSError as ex:\n        display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to stat {cachefile!r}.\", exception=ex)\n\n        return False\n\n    if time.time() - st.st_mtime <= self._timeout:\n        return False\n\n    if key in self._cache:\n        del self._cache[key]\n    return True", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "BaseFileCacheModule", "function_name": "keys", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["k.startswith", "keys.append", "len", "os.listdir", "self.get_option", "self.has_expired"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def keys(self):\n    # When using a prefix we must remove it from the key name before\n    # checking the expiry and returning it to the caller. Keys that do not\n    # share the same prefix cannot be fetched from the cache.\n    prefix = self.get_option('_prefix')\n    prefix_length = len(prefix)\n    keys = []\n    for k in os.listdir(self._cache_dir):\n        if k.startswith('.') or not k.startswith(prefix):\n            continue\n\n        k = k[prefix_length:]\n        if not self.has_expired(k):\n            keys.append(k)\n\n    return keys", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "BaseFileCacheModule", "function_name": "contains", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.error_as_warning", "os.stat", "self._get_cache_file_name", "self.has_expired"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def contains(self, key):\n    cachefile = self._get_cache_file_name(key)\n\n    if key in self._cache:\n        return True\n\n    if self.has_expired(key):\n        return False\n    try:\n        os.stat(cachefile)\n        return True\n    except FileNotFoundError:\n        return False\n    except OSError as ex:\n        display.error_as_warning(f\"Error in {self.plugin_name!r} cache plugin while trying to stat {cachefile!r}.\", exception=ex)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "CachePluginAdjudicator", "function_name": "set_cache", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["copy.deepcopy", "self._cache.keys", "self._plugin.set"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_cache(self):\n    for top_level_cache_key in self._cache.keys():\n        self._plugin.set(top_level_cache_key, self._cache[top_level_cache_key])\n\n    self._retrieved = copy.deepcopy(self._cache)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "CachePluginAdjudicator", "function_name": "get", "parameters": ["self", "key", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._cache.get", "self._do_load_key", "self._plugin.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self, key, default=None):\n    if self._do_load_key(key):\n        try:\n            self._cache[key] = self._plugin.get(key)\n        except KeyError:\n            pass\n        else:\n            self._retrieved[key] = self._cache[key]\n\n    return self._cache.get(key, default)", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\cache\\__init__.py", "class_name": "CachePluginAdjudicator", "function_name": "pop", "parameters": ["self", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._cache.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pop(self, key, *args):\n    if args:\n        return self._cache.pop(key, args[0])\n\n    return self._cache.pop(key)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_failed", "parameters": ["self", "result", "ignore_errors"], "param_types": {"result": "CallbackTaskResult", "ignore_errors": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._clean_results", "self._display.display", "self._dump_results", "self._handle_warnings_and_exception", "self._print_task_banner", "self._print_task_path", "self._process_items", "self.get_option", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_failed(self, result: CallbackTaskResult, ignore_errors: bool = False) -> None:\n    host_label = self.host_label(result)\n\n    if self._last_task_banner != result.task._uuid:\n        self._print_task_banner(result.task)\n\n    self._handle_warnings_and_exception(result)\n\n    # FIXME: this method should not exist, delegate \"suggested keys to display\" to the plugin or something... As-is, the placement of this\n    #          call obliterates `results`, which causes a task summary to be printed on loop failures, which we don't do anywhere else.\n    self._clean_results(result.result, result.task.action)\n\n    if result.task.loop and 'results' in result.result:\n        self._process_items(result)\n    else:\n        if self._display.verbosity < 2 and self.get_option('show_task_path_on_failure'):\n            self._print_task_path(result.task)\n        msg = \"fatal: [%s]: FAILED! => %s\" % (host_label, self._dump_results(result.result))\n        self._display.display(msg, color=C.COLOR_ERROR, stderr=self.get_option('display_failed_stderr'))\n\n    if ignore_errors:\n        self._display.display(\"...ignoring\", color=C.COLOR_SKIP)", "loc": 22}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "result.result.get", "self._clean_results", "self._display.display", "self._dump_results", "self._handle_warnings_and_exception", "self._print_task_banner", "self._process_items", "self._run_is_verbose", "self.get_option", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_ok(self, result: CallbackTaskResult) -> None:\n    host_label = self.host_label(result)\n\n    if isinstance(result.task, TaskInclude):\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n        return\n    elif result.result.get('changed', False):\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        msg = \"changed: [%s]\" % (host_label,)\n        color = C.COLOR_CHANGED\n    else:\n        if not self.get_option('display_ok_hosts'):\n            return\n\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        msg = \"ok: [%s]\" % (host_label,)\n        color = C.COLOR_OK\n\n    self._handle_warnings_and_exception(result)\n\n    if result.task.loop and 'results' in result.result:\n        self._process_items(result)\n    else:\n        self._clean_results(result.result, result.task.action)\n\n        if self._run_is_verbose(result):\n            msg += \" => %s\" % (self._dump_results(result.result),)\n        self._display.display(msg, color=color)", "loc": 33}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_skipped", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "self._clean_results", "self._display.display", "self._dump_results", "self._handle_warnings_and_exception", "self._print_task_banner", "self._process_items", "self._run_is_verbose", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_skipped(self, result: CallbackTaskResult) -> None:\n    if self.get_option('display_skipped_hosts'):\n\n        self._clean_results(result.result, result.task.action)\n\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        self._handle_warnings_and_exception(result)\n\n        if result.task.loop is not None and 'results' in result.result:\n            self._process_items(result)\n\n        msg = \"skipping: [%s]\" % result.host.get_name()\n        if self._run_is_verbose(result):\n            msg += \" => %s\" % self._dump_results(result.result)\n        self._display.display(msg, color=C.COLOR_SKIP)", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_unreachable", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._display.display", "self._dump_results", "self._handle_warnings_and_exception", "self._print_task_banner", "self.get_option", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_unreachable(self, result: CallbackTaskResult) -> None:\n    if self._last_task_banner != result.task._uuid:\n        self._print_task_banner(result.task)\n\n    self._handle_warnings_and_exception(result)\n\n    host_label = self.host_label(result)\n    msg = \"fatal: [%s]: UNREACHABLE! => %s\" % (host_label, self._dump_results(result.result))\n    self._display.display(msg, color=C.COLOR_UNREACHABLE, stderr=self.get_option('display_failed_stderr'))\n\n    if result.task.ignore_unreachable:\n        self._display.display(\"...ignoring\", color=C.COLOR_SKIP)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_playbook_on_play_start", "parameters": ["self", "play"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["play.get_name", "play.get_name().strip", "self._display.banner", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_playbook_on_play_start(self, play):\n    name = play.get_name().strip()\n    if play.check_mode and self.get_option('check_mode_markers'):\n        checkmsg = \" [CHECK MODE]\"\n    else:\n        checkmsg = \"\"\n    if not name:\n        msg = u\"PLAY%s\" % checkmsg\n    else:\n        msg = u\"PLAY [%s]%s\" % (name, checkmsg)\n\n    self._play = play\n\n    self._display.banner(msg)", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_on_file_diff", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["res.get", "result.result.get", "self._display.display", "self._get_diff", "self._print_task_banner"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_on_file_diff(self, result: CallbackTaskResult) -> None:\n    if result.task.loop and 'results' in result.result:\n        for res in result.result['results']:\n            if 'diff' in res and res['diff'] and res.get('changed', False):\n                diff = self._get_diff(res['diff'])\n                if diff:\n                    if self._last_task_banner != result.task._uuid:\n                        self._print_task_banner(result.task)\n                    self._display.display(diff)\n    elif 'diff' in result.result and result.result['diff'] and result.result.get('changed', False):\n        diff = self._get_diff(result.result['diff'])\n        if diff:\n            if self._last_task_banner != result.task._uuid:\n                self._print_task_banner(result.task)\n            self._display.display(diff)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_item_on_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "result.result.get", "self._clean_results", "self._display.display", "self._dump_results", "self._get_item_label", "self._handle_warnings_and_exception", "self._print_task_banner", "self._run_is_verbose", "self.get_option", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_item_on_ok(self, result: CallbackTaskResult) -> None:\n    host_label = self.host_label(result)\n    if isinstance(result.task, TaskInclude):\n        return\n    elif result.result.get('changed', False):\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        msg = 'changed'\n        color = C.COLOR_CHANGED\n    else:\n        if not self.get_option('display_ok_hosts'):\n            return\n\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        msg = 'ok'\n        color = C.COLOR_OK\n\n    self._handle_warnings_and_exception(result)\n\n    msg = \"%s: [%s] => (item=%s)\" % (msg, host_label, self._get_item_label(result.result))\n    self._clean_results(result.result, result.task.action)\n    if self._run_is_verbose(result):\n        msg += \" => %s\" % self._dump_results(result.result)\n    self._display.display(msg, color=color)", "loc": 27}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_item_on_failed", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._clean_results", "self._display.display", "self._dump_results", "self._get_item_label", "self._handle_warnings_and_exception", "self._print_task_banner", "self.get_option", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_item_on_failed(self, result: CallbackTaskResult) -> None:\n    if self._last_task_banner != result.task._uuid:\n        self._print_task_banner(result.task)\n\n    self._handle_warnings_and_exception(result)\n\n    host_label = self.host_label(result)\n\n    msg = \"failed: [%s]\" % (host_label,)\n    self._clean_results(result.result, result.task.action)\n    self._display.display(\n        msg + \" (item=%s) => %s\" % (self._get_item_label(result.result), self._dump_results(result.result)),\n        color=C.COLOR_ERROR,\n        stderr=self.get_option('display_failed_stderr')\n    )", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_item_on_skipped", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "self._clean_results", "self._display.display", "self._dump_results", "self._get_item_label", "self._handle_warnings_and_exception", "self._print_task_banner", "self._run_is_verbose", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_item_on_skipped(self, result: CallbackTaskResult) -> None:\n    if self.get_option('display_skipped_hosts'):\n        if self._last_task_banner != result.task._uuid:\n            self._print_task_banner(result.task)\n\n        self._handle_warnings_and_exception(result)\n\n        self._clean_results(result.result, result.task.action)\n        msg = \"skipping: [%s] => (item=%s) \" % (result.host.get_name(), self._get_item_label(result.result))\n        if self._run_is_verbose(result):\n            msg += \" => %s\" % self._dump_results(result.result)\n        self._display.display(msg, color=C.COLOR_SKIP)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_playbook_on_include", "parameters": ["self", "included_file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "self._display.display", "self._get_item_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_playbook_on_include(self, included_file):\n    msg = 'included: %s for %s' % (included_file._filename, \", \".join([h.name for h in included_file._hosts]))\n    label = self._get_item_label(included_file._vars)\n    if label:\n        msg += \" => (item=%s)\" % label\n    self._display.display(msg, color=C.COLOR_INCLUDED)", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_playbook_on_stats", "parameters": ["self", "stats"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["colorize", "hostcolor", "self._display.banner", "self._display.display", "self._dump_results", "self._dump_results(stats.custom['_run'], indent=1).replace", "self._dump_results(stats.custom[k], indent=1).replace", "self.get_option", "sorted", "stats.custom.keys", "stats.processed.keys", "stats.summarize"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_playbook_on_stats(self, stats):\n    self._display.banner(\"PLAY RECAP\")\n\n    hosts = sorted(stats.processed.keys())\n    for h in hosts:\n        t = stats.summarize(h)\n\n        self._display.display(\n            u\"%s : %s %s %s %s %s %s %s\" % (\n                hostcolor(h, t),\n                colorize(u'ok', t['ok'], C.COLOR_OK),\n                colorize(u'changed', t['changed'], C.COLOR_CHANGED),\n                colorize(u'unreachable', t['unreachable'], C.COLOR_UNREACHABLE),\n                colorize(u'failed', t['failures'], C.COLOR_ERROR),\n                colorize(u'skipped', t['skipped'], C.COLOR_SKIP),\n                colorize(u'rescued', t['rescued'], C.COLOR_OK),\n                colorize(u'ignored', t['ignored'], C.COLOR_WARN),\n            ),\n            screen_only=True\n        )\n\n        self._display.display(\n            u\"%s : %s %s %s %s %s %s %s\" % (\n                hostcolor(h, t, False),\n                colorize(u'ok', t['ok'], None),\n                colorize(u'changed', t['changed'], None),\n                colorize(u'unreachable', t['unreachable'], None),\n                colorize(u'failed', t['failures'], None),\n                colorize(u'skipped', t['skipped'], None),\n                colorize(u'rescued', t['rescued'], None),\n                colorize(u'ignored', t['ignored'], None),\n            ),\n            log_only=True\n        )\n\n    self._display.display(\"\", screen_only=True)\n\n    # print custom stats if required\n    if stats.custom and self.get_option('show_custom_stats'):\n        self._display.banner(\"CUSTOM STATS: \")\n        # per host\n        # TODO: come up with 'pretty format'\n        for k in sorted(stats.custom.keys()):\n            if k == '_run':\n                continue\n            self._display.display('\\t%s: %s' % (k, self._dump_results(stats.custom[k], indent=1).replace('\\n', '')))\n\n        # print per run custom stats\n        if '_run' in stats.custom:\n            self._display.display(\"\", screen_only=True)\n            self._display.display('\\tRUN: %s' % self._dump_results(stats.custom['_run'], indent=1).replace('\\n', ''))\n        self._display.display(\"\", screen_only=True)\n\n    if context.CLIARGS['check'] and self.get_option('check_mode_markers'):\n        self._display.banner(\"DRY RUN\")", "loc": 55}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_playbook_on_start", "parameters": ["self", "playbook"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "basename", "context.CLIARGS.get", "self._display.banner", "self._display.display", "self.get_option"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_playbook_on_start(self, playbook):\n    if self._display.verbosity > 1:\n        from os.path import basename\n        self._display.banner(\"PLAYBOOK: %s\" % basename(playbook._file_name))\n\n    # show CLI arguments\n    if self._display.verbosity > 3:\n        if context.CLIARGS.get('args'):\n            self._display.display('Positional arguments: %s' % ' '.join(context.CLIARGS['args']),\n                                  color=C.COLOR_VERBOSE, screen_only=True)\n\n        for argument in (a for a in context.CLIARGS if a != 'args'):\n            val = context.CLIARGS[argument]\n            if val:\n                self._display.display('%s: %s' % (argument, val), color=C.COLOR_VERBOSE, screen_only=True)\n\n    if context.CLIARGS['check'] and self.get_option('check_mode_markers'):\n        self._display.banner(\"DRY RUN\")", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_retry", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._display.display", "self._dump_results", "self._run_is_verbose", "self.host_label"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_retry(self, result: CallbackTaskResult) -> None:\n    task_name = result.task_name or result.task\n    host_label = self.host_label(result)\n    msg = \"FAILED - RETRYING: [%s]: %s (%d retries left).\" % (host_label, task_name, result.result['retries'] - result.result['attempts'])\n    if self._run_is_verbose(result, verbosity=2):\n        msg += \"Result was: %s\" % self._dump_results(result.result)\n    self._display.display(msg, color=C.COLOR_DEBUG)", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_async_poll", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "self._display.display"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_async_poll(self, result: CallbackTaskResult) -> None:\n    host = result.host.get_name()\n    jid = result.result.get('ansible_job_id')\n    started = result.result.get('started')\n    finished = result.result.get('finished')\n    self._display.display(\n        'ASYNC POLL on %s: jid=%s started=%s finished=%s' % (host, jid, started, finished),\n        color=C.COLOR_DEBUG\n    )", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\default.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_async_failed", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "result.result['async_result'].get", "self._display.display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_async_failed(self, result: CallbackTaskResult) -> None:\n    host = result.host.get_name()\n\n    # Attempt to get the async job ID. If the job does not finish before the\n    # async timeout value, the ID may be within the unparsed 'async_result' dict.\n    jid = result.result.get('ansible_job_id')\n    if not jid and 'async_result' in result.result:\n        jid = result.result['async_result'].get('ansible_job_id')\n    self._display.display(\"ASYNC FAILED on %s: jid=%s\" % (host, jid), color=C.COLOR_DEBUG)", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\junit.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_failed", "parameters": ["self", "result", "ignore_errors"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._finish_task"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_failed(self, result: CallbackTaskResult, ignore_errors=False) -> None:\n    if ignore_errors and self._fail_on_ignore != 'true':\n        self._finish_task('ok', result)\n    else:\n        self._finish_task('failed', result)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\junit.py", "class_name": "TaskData", "function_name": "add_host", "parameters": ["self", "host"], "param_types": {"host": "HostData"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_host(self, host: HostData) -> None:\n    if host.uuid in self.host_data:\n        if host.status == 'included':\n            # concatenate task include output from multiple items\n            host.result = f'{self.host_data[host.uuid].result}\\n{host.result}'\n        else:\n            raise Exception('%s: %s: %s: duplicate host callback: %s' % (self.path, self.play, self.name, host.name))\n\n    self.host_data[host.uuid] = host", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\minimal.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_failed", "parameters": ["self", "result", "ignore_errors"], "param_types": {"result": "CallbackTaskResult", "ignore_errors": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "self._command_generic_msg", "self._display.display", "self._dump_results", "self._handle_warnings_and_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_failed(self, result: CallbackTaskResult, ignore_errors: bool = False) -> None:\n    self._handle_warnings_and_exception(result)\n\n    if result.task.action in C.MODULE_NO_JSON and 'module_stderr' not in result.result:\n        self._display.display(self._command_generic_msg(result.host.get_name(), result.result, \"FAILED\"), color=C.COLOR_ERROR)\n    else:\n        self._display.display(\"%s | FAILED! => %s\" % (result.host.get_name(), self._dump_results(result.result, indent=4)), color=C.COLOR_ERROR)", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\minimal.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "self._clean_results", "self._command_generic_msg", "self._display.display", "self._dump_results", "self._handle_warnings_and_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_ok(self, result: CallbackTaskResult) -> None:\n    self._handle_warnings_and_exception(result)\n\n    self._clean_results(result.result, result.task.action)\n\n    if result.result.get('changed', False):\n        color = C.COLOR_CHANGED\n        state = 'CHANGED'\n    else:\n        color = C.COLOR_OK\n        state = 'SUCCESS'\n\n    if result.task.action in C.MODULE_NO_JSON and 'ansible_job_id' not in result.result:\n        self._display.display(self._command_generic_msg(result.host.get_name(), result.result, state), color=color)\n    else:\n        self._display.display(\"%s | %s => %s\" % (result.host.get_name(), state, self._dump_results(result.result, indent=4)), color=color)", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\oneline.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_failed", "parameters": ["self", "result", "ignore_errors"], "param_types": {"result": "CallbackTaskResult", "ignore_errors": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Templar", "Templar().template", "error_text.replace", "error_text.strip", "error_text.strip().split", "result.host.get_name", "self._command_generic_msg", "self._display.display", "self._dump_results", "self._dump_results(result.result, indent=0).replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_failed(self, result: CallbackTaskResult, ignore_errors: bool = False) -> None:\n    if 'exception' in result.result:\n        error_text = Templar().template(result.result['exception'])  # transform to a string\n        if self._display.verbosity < 3:\n            # extract just the actual error message from the exception text\n            error = error_text.strip().split('\\n')[-1]\n            msg = \"An exception occurred during task execution. To see the full traceback, use -vvv. The error was: %s\" % error\n        else:\n            msg = \"An exception occurred during task execution. The full traceback is:\\n\" + error_text.replace('\\n', '')\n\n        if result.task.action in C.MODULE_NO_JSON and 'module_stderr' not in result.result:\n            self._display.display(self._command_generic_msg(result.host.get_name(), result.result, 'FAILED'), color=C.COLOR_ERROR)\n        else:\n            self._display.display(msg, color=C.COLOR_ERROR)\n\n    self._display.display(\"%s | FAILED! => %s\" % (result.host.get_name(), self._dump_results(result.result, indent=0).replace('\\n', '')),\n                          color=C.COLOR_ERROR)", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\oneline.py", "class_name": "CallbackModule", "function_name": "v2_runner_on_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "self._command_generic_msg", "self._display.display", "self._dump_results", "self._dump_results(result.result, indent=0).replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_ok(self, result: CallbackTaskResult) -> None:\n\n    if result.result.get('changed', False):\n        color = C.COLOR_CHANGED\n        state = 'CHANGED'\n    else:\n        color = C.COLOR_OK\n        state = 'SUCCESS'\n\n    if result.task.action in C.MODULE_NO_JSON and 'ansible_job_id' not in result.result:\n        self._display.display(self._command_generic_msg(result.host.get_name(), result.result, state), color=color)\n    else:\n        self._display.display(\"%s | %s => %s\" % (result.host.get_name(), state, self._dump_results(result.result, indent=0).replace('\\n', '')),\n                              color=color)", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\tree.py", "class_name": "CallbackModule", "function_name": "set_options", "parameters": ["self", "task_keys", "var_options", "direct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_option", "super", "super(CallbackModule, self).set_options", "unfrackpath"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "override to set self.tree", "source_code": "def set_options(self, task_keys=None, var_options=None, direct=None):\n    \"\"\" override to set self.tree \"\"\"\n\n    super(CallbackModule, self).set_options(task_keys=task_keys, var_options=var_options, direct=direct)\n\n    if TREE_DIR:\n        # TREE_DIR comes from the CLI option --tree, only available for adhoc\n        self.tree = unfrackpath(TREE_DIR)\n    else:\n        self.tree = self.get_option('directory')", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "host_label", "parameters": ["result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "result.result.get('_ansible_delegated_vars', {}).get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return label for the hostname (& delegated hostname) of a task result.", "source_code": "def host_label(result: CallbackTaskResult) -> str:\n    \"\"\"Return label for the hostname (& delegated hostname) of a task result.\"\"\"\n    label = result.host.get_name()\n    if result.task.delegate_to and result.task.delegate_to != result.host.get_name():\n        # show delegated host\n        label += \" -> %s\" % result.task.delegate_to\n        # in case we have 'extra resolution'\n        ahost = result.result.get('_ansible_delegated_vars', {}).get('ansible_host', result.task.delegate_to)\n        if result.task.delegate_to != ahost:\n            label += \"(%s)\" % ahost\n    return label", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_failed", "parameters": ["self", "result", "ignore_errors"], "param_types": {"result": "CallbackTaskResult", "ignore_errors": "bool"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its results.", "ignore_errors": "Whether Ansible should continue             running tasks on the host where the task failed."}, "return_doc": "None", "raises_doc": [], "called_functions": ["result.host.get_name", "self.runner_on_failed"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Process results of a failed task. Note: The value of 'ignore_errors' tells Ansible whether to continue running tasks on the host where this task failed. But the 'ignore_errors' directive only works when the task can run and returns a value of 'failed'. It does not make Ansible ignore undefined variable errors, connection failures, execution issues (for example, missing packages), or syntax errors.", "source_code": "def v2_runner_on_failed(self, result: CallbackTaskResult, ignore_errors: bool = False) -> None:\n    \"\"\"Process results of a failed task.\n\n    Note: The value of 'ignore_errors' tells Ansible whether to\n    continue running tasks on the host where this task failed.\n    But the 'ignore_errors' directive only works when the task can\n    run and returns a value of 'failed'. It does not make Ansible\n    ignore undefined variable errors, connection failures, execution\n    issues (for example, missing packages), or syntax errors.\n\n    :param result: The parameters of the task and its results.\n    :type result: CallbackTaskResult\n    :param ignore_errors: Whether Ansible should continue \\\n        running tasks on the host where the task failed.\n    :type ignore_errors: bool\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    host = result.host.get_name()\n    self.runner_on_failed(host, result.result, ignore_errors)", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its results."}, "return_doc": "None", "raises_doc": [], "called_functions": ["result.host.get_name", "self.runner_on_ok"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Process results of a successful task.", "source_code": "def v2_runner_on_ok(self, result: CallbackTaskResult) -> None:\n    \"\"\"Process results of a successful task.\n\n    :param result: The parameters of the task and its results.\n    :type result: CallbackTaskResult\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    host = result.host.get_name()\n    self.runner_on_ok(host, result.result)", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_skipped", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its results."}, "return_doc": "None", "raises_doc": [], "called_functions": ["getattr", "result.host.get_name", "self._get_item_label", "self.runner_on_skipped"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Process results of a skipped task.", "source_code": "def v2_runner_on_skipped(self, result: CallbackTaskResult) -> None:\n    \"\"\"Process results of a skipped task.\n\n    :param result: The parameters of the task and its results.\n    :type result: CallbackTaskResult\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    if C.DISPLAY_SKIPPED_HOSTS:\n        host = result.host.get_name()\n        self.runner_on_skipped(host, self._get_item_label(getattr(result.result, 'results', {})))", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_unreachable", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its results."}, "return_doc": "None", "raises_doc": [], "called_functions": ["result.host.get_name", "self.runner_on_unreachable"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Process results of a task if a target node is unreachable.", "source_code": "def v2_runner_on_unreachable(self, result: CallbackTaskResult) -> None:\n    \"\"\"Process results of a task if a target node is unreachable.\n\n    :param result: The parameters of the task and its results.\n    :type result: CallbackTaskResult\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    host = result.host.get_name()\n    self.runner_on_unreachable(host, result.result)", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_async_poll", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its status."}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "self.runner_on_async_poll"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Get details about an unfinished task running in async mode. Note: The value of the `poll` keyword in the task determines the interval at which polling occurs and this method is run.", "source_code": "def v2_runner_on_async_poll(self, result: CallbackTaskResult) -> None:\n    \"\"\"Get details about an unfinished task running in async mode.\n\n    Note: The value of the `poll` keyword in the task determines\n    the interval at which polling occurs and this method is run.\n\n    :param result: The parameters of the task and its status.\n    :type result: CallbackTaskResult\n\n    :rtype: None\n    :rtype: None\n    \"\"\"\n    host = result.host.get_name()\n    jid = result.result.get('ansible_job_id')\n    # FIXME, get real clock\n    clock = 0\n    self.runner_on_async_poll(host, result.result, jid, clock)", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_async_ok", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {"result": "The parameters of the task and its results."}, "return_doc": "None", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "self.runner_on_async_ok"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Process results of a successful task that ran in async mode.", "source_code": "def v2_runner_on_async_ok(self, result: CallbackTaskResult) -> None:\n    \"\"\"Process results of a successful task that ran in async mode.\n\n    :param result: The parameters of the task and its results.\n    :type result: CallbackTaskResult\n\n    :return: None\n    :rtype: None\n    \"\"\"\n    host = result.host.get_name()\n    jid = result.result.get('ansible_job_id')\n    self.runner_on_async_ok(host, result.result, jid)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\callback\\__init__.py", "class_name": "CallbackBase", "function_name": "v2_runner_on_async_failed", "parameters": ["self", "result"], "param_types": {"result": "CallbackTaskResult"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.host.get_name", "result.result.get", "result.result['async_result'].get", "self.runner_on_async_failed"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def v2_runner_on_async_failed(self, result: CallbackTaskResult) -> None:\n    host = result.host.get_name()\n    # Attempt to get the async job ID. If the job does not finish before the\n    # async timeout value, the ID may be within the unparsed 'async_result' dict.\n    jid = result.result.get('ansible_job_id')\n    if not jid and 'async_result' in result.result:\n        jid = result.result['async_result'].get('ansible_job_id')\n    self.runner_on_async_failed(host, result.result, jid)", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\cliconf\\__init__.py", "class_name": null, "function_name": "enable_mode", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "func", "self._connection.get_prompt", "to_text", "to_text(prompt, errors='surrogate_or_strict').strip", "to_text(prompt, errors='surrogate_or_strict').strip().endswith", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def enable_mode(func):\n    @wraps(func)\n    def wrapped(self, *args, **kwargs):\n        prompt = self._connection.get_prompt()\n        if not to_text(prompt, errors='surrogate_or_strict').strip().endswith('#'):\n            raise AnsibleError('operation requires privilege escalation')\n        return func(self, *args, **kwargs)\n    return wrapped", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\cliconf\\__init__.py", "class_name": null, "function_name": "wrapped", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "func", "self._connection.get_prompt", "to_text", "to_text(prompt, errors='surrogate_or_strict').strip", "to_text(prompt, errors='surrogate_or_strict').strip().endswith", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(self, *args, **kwargs):\n    prompt = self._connection.get_prompt()\n    if not to_text(prompt, errors='surrogate_or_strict').strip().endswith('#'):\n        raise AnsibleError('operation requires privilege escalation')\n    return func(self, *args, **kwargs)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\cliconf\\__init__.py", "class_name": "CliconfBase", "function_name": "send_command", "parameters": ["self", "command", "prompt", "answer", "sendonly", "newline", "prompt_retry_check", "check_all"], "param_types": {}, "return_type": null, "param_doc": {"command": "The command to send over the connection to the device", "prompt": "A single regex pattern or a sequence of patterns to evaluate the expected prompt from the command", "answer": "The answer to respond with if the prompt is matched.", "sendonly": "Bool value that will send the command but not wait for a result.", "newline": "Bool value that will append the newline character to the command", "prompt_retry_check": "Bool value for trying to detect more prompts", "check_all": "Bool value to indicate if all the values in prompt sequence should be matched or any one of"}, "return_doc": "The output from the device after executing the command", "raises_doc": [], "called_functions": ["isinstance", "self._connection.send", "self.history.append", "to_bytes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Executes a command over the device connection This method will execute a command over the device connection and return the results to the caller.  This method will also perform logging of any commands based on the `nolog` argument.", "source_code": "def send_command(self, command=None, prompt=None, answer=None, sendonly=False, newline=True, prompt_retry_check=False, check_all=False):\n    \"\"\"Executes a command over the device connection\n\n    This method will execute a command over the device connection and\n    return the results to the caller.  This method will also perform\n    logging of any commands based on the `nolog` argument.\n\n    :param command: The command to send over the connection to the device\n    :param prompt: A single regex pattern or a sequence of patterns to evaluate the expected prompt from the command\n    :param answer: The answer to respond with if the prompt is matched.\n    :param sendonly: Bool value that will send the command but not wait for a result.\n    :param newline: Bool value that will append the newline character to the command\n    :param prompt_retry_check: Bool value for trying to detect more prompts\n    :param check_all: Bool value to indicate if all the values in prompt sequence should be matched or any one of\n                      given prompt.\n    :returns: The output from the device after executing the command\n    \"\"\"\n    kwargs = {\n        'command': to_bytes(command),\n        'sendonly': sendonly,\n        'newline': newline,\n        'prompt_retry_check': prompt_retry_check,\n        'check_all': check_all\n    }\n\n    if prompt is not None:\n        if isinstance(prompt, list):\n            kwargs['prompt'] = [to_bytes(p) for p in prompt]\n        else:\n            kwargs['prompt'] = to_bytes(prompt)\n    if answer is not None:\n        if isinstance(answer, list):\n            kwargs['answer'] = [to_bytes(p) for p in answer]\n        else:\n            kwargs['answer'] = to_bytes(answer)\n\n    resp = self._connection.send(**kwargs)\n\n    if not self.response_logging:\n        self.history.append(('*****', '*****'))\n    else:\n        self.history.append((kwargs['command'], resp))\n\n    return resp", "loc": 44}
{"file": "ansible\\lib\\ansible\\plugins\\cliconf\\__init__.py", "class_name": "CliconfBase", "function_name": "get_capabilities", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "capability as dict", "raises_doc": [], "called_functions": ["self.get_base_rpc", "self.get_device_info"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_capabilities(self):\n    \"\"\"Returns the basic capabilities of the network device\n    This method will provide some basic facts about the device and\n    what capabilities it has to modify the configuration.  The minimum\n    return from this method takes the following format.\n    eg:\n        {\n\n            'rpc': [list of supported rpcs],\n            'network_api': <str>,            # the name of the transport\n            'device_info': {\n                'network_os': <str>,\n                'network_os_version': <str>,\n                'network_os_model': <str>,\n                'network_os_hostname': <str>,\n                'network_os_image': <str>,\n                'network_os_platform': <str>,\n            },\n            'device_operations': {\n                'supports_diff_replace': <bool>,       # identify if config should be merged or replaced is supported\n                'supports_commit': <bool>,             # identify if commit is supported by device or not\n                'supports_rollback': <bool>,           # identify if rollback is supported or not\n                'supports_defaults': <bool>,           # identify if fetching running config with default is supported\n                'supports_commit_comment': <bool>,     # identify if adding comment to commit is supported of not\n                'supports_onbox_diff': <bool>,          # identify if on box diff capability is supported or not\n                'supports_generate_diff': <bool>,       # identify if diff capability is supported within plugin\n                'supports_multiline_delimiter': <bool>, # identify if multiline delimiter is supported within config\n                'supports_diff_match': <bool>,          # identify if match is supported\n                'supports_diff_ignore_lines': <bool>,   # identify if ignore line in diff is supported\n                'supports_config_replace': <bool>,     # identify if running config replace with candidate config is supported\n                'supports_admin': <bool>,              # identify if admin configure mode is supported or not\n                'supports_commit_label': <bool>,       # identify if commit label is supported or not\n            }\n            'format': [list of supported configuration format],\n            'diff_match': [list of supported match values],\n            'diff_replace': [list of supported replace values],\n            'output': [list of supported command output format]\n        }\n    :return: capability as dict\n    \"\"\"\n    result = {}\n    result['rpc'] = self.get_base_rpc()\n    result['device_info'] = self.get_device_info()\n    result['network_api'] = 'cliconf'\n    return result", "loc": 45}
{"file": "ansible\\lib\\ansible\\plugins\\cliconf\\__init__.py", "class_name": "CliconfBase", "function_name": "check_edit_config_capability", "parameters": ["self", "operations", "candidate", "commit", "replace", "comment"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "operations.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_edit_config_capability(self, operations, candidate=None, commit=True, replace=None, comment=None):\n\n    if not candidate and not replace:\n        raise ValueError(\"must provide a candidate or replace to load configuration\")\n\n    if commit not in (True, False):\n        raise ValueError(\"'commit' must be a bool, got %s\" % commit)\n\n    if replace and not operations['supports_replace']:\n        raise ValueError(\"configuration replace is not supported\")\n\n    if comment and not operations.get('supports_commit_comment', False):\n        raise ValueError(\"commit comment is not supported\")\n\n    if replace and not operations.get('supports_replace', False):\n        raise ValueError(\"configuration replace is not supported\")", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\local.py", "class_name": "Connection", "function_name": "fetch_file", "parameters": ["self", "in_path", "out_path"], "param_types": {"in_path": "str", "out_path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.vvv", "self.put_file", "super", "super(Connection, self).fetch_file", "u'FETCH {0} TO {1}'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "fetch a file from local to local -- for compatibility", "source_code": "def fetch_file(self, in_path: str, out_path: str) -> None:\n    \"\"\" fetch a file from local to local -- for compatibility \"\"\"\n\n    super(Connection, self).fetch_file(in_path, out_path)\n\n    display.vvv(u\"FETCH {0} TO {1}\".format(in_path, out_path), host=self._play_context.remote_addr)\n    self.put_file(in_path, out_path)", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\local.py", "class_name": null, "function_name": "become_error_msg", "parameters": ["reason"], "param_types": {"reason": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bytes", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def become_error_msg(reason: str) -> str:\n    error_message = f'{reason} waiting for become success'\n\n    if expect_password_prompt and not sent_password:\n        error_message += ' or become password prompt'\n\n    error_message += '.'\n\n    if become_stdout:\n        error_message += f'\\n>>> Standard Output\\n{to_text(bytes(become_stdout))}'\n\n    if become_stderr:\n        error_message += f'\\n>>> Standard Error\\n{to_text(bytes(become_stderr))}'\n\n    return error_message", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\paramiko_ssh.py", "class_name": "MyAddPolicy", "function_name": "missing_host_key", "parameters": ["self", "client", "hostname", "key"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "all", "client._host_keys.add", "display.prompt_until", "hexlify", "key.get_fingerprint", "key.get_name", "self.connection.get_option", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def missing_host_key(self, client, hostname, key) -> None:\n\n    if all((self.connection.get_option('host_key_checking'), not self.connection.get_option('host_key_auto_add'))):\n\n        fingerprint = hexlify(key.get_fingerprint())\n        ktype = key.get_name()\n\n        if self.connection.get_option('use_persistent_connections') or self.connection.force_persistence:\n            # don't print the prompt string since the user cannot respond\n            # to the question anyway\n            raise AnsibleError(AUTHENTICITY_MSG[1:92] % (hostname, ktype, fingerprint))\n\n        inp = to_text(\n            display.prompt_until(AUTHENTICITY_MSG % (hostname, ktype, fingerprint), private=False),\n            errors='surrogate_or_strict'\n        )\n\n        if inp not in ['yes', 'y', '']:\n            raise AnsibleError(\"host connection rejected by user\")\n\n    key._added_by_ansible_this_time = True\n\n    # existing implementation below:\n    client._host_keys.add(hostname, key.get_name(), key)", "loc": 24}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\paramiko_ssh.py", "class_name": "Connection", "function_name": "fetch_file", "parameters": ["self", "in_path", "out_path"], "param_types": {"in_path": "str", "out_path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.vvv", "self._connect_sftp", "self.get_option", "self.sftp.get", "super", "super(Connection, self).fetch_file", "to_bytes", "to_native"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "save a remote file to the specified path", "source_code": "def fetch_file(self, in_path: str, out_path: str) -> None:\n    \"\"\" save a remote file to the specified path \"\"\"\n\n    super(Connection, self).fetch_file(in_path, out_path)\n\n    display.vvv(\"FETCH %s TO %s\" % (in_path, out_path), host=self.get_option('remote_addr'))\n\n    try:\n        self.sftp = self._connect_sftp()\n    except Exception as e:\n        raise AnsibleError(\"failed to open a SFTP connection (%s)\" % to_native(e))\n\n    try:\n        self.sftp.get(to_bytes(in_path, errors='surrogate_or_strict'), to_bytes(out_path, errors='surrogate_or_strict'))\n    except OSError as ex:\n        raise AnsibleError(f\"Failed to transfer file from {in_path!r}.\") from ex", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\paramiko_ssh.py", "class_name": "Connection", "function_name": "reset", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._connect", "self.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reset(self) -> None:\n    if not self._connected:\n        return\n    self.close()\n    self._connect()", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\psrp.py", "class_name": "Connection", "function_name": "reset", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.debug", "display.vvvvv", "self._connect", "self.close", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reset(self) -> None:\n    if not self._connected:\n        self.runspace = None\n        return\n\n    # Try out best to ensure the runspace is closed to free up server side resources\n    try:\n        self.close()\n    except Exception as e:\n        # There's a good chance the connection was already closed so just log the error and move on\n        display.debug(\"PSRP reset - failed to closed runspace: %s\" % to_text(e))\n\n    display.vvvvv(\"PSRP: Reset Connection\", host=self._psrp_host)\n    self.runspace = None\n    self._connect()", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\psrp.py", "class_name": "Connection", "function_name": "exec_command", "parameters": ["self", "cmd", "in_data", "sudoable"], "param_types": {"cmd": "str", "in_data": "bytes | None", "sudoable": "bool"}, "return_type": "tuple[int, bytes, bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "AnsibleConnectionFailure", "AnsibleError", "base64.b64decode", "cmd.split", "cmd.startswith", "display.vvv", "isinstance", "len", "pwsh_in_data.splitlines", "pwsh_in_data.startswith", "self._exec_psrp_script", "self._shell.build_module_command", "shlex.split", "super", "super(Connection, self).exec_command", "to_native", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_command(self, cmd: str, in_data: bytes | None = None, sudoable: bool = True) -> tuple[int, bytes, bytes]:\n    super(Connection, self).exec_command(cmd, in_data=in_data,\n                                         sudoable=sudoable)\n\n    pwsh_in_data: bytes | str | None = None\n    script_args: list[str] | None = None\n\n    common_args_prefix = \" \".join(_common_args)\n    if cmd.startswith(f\"{common_args_prefix} -EncodedCommand\"):\n        # This is a PowerShell script encoded by the shell plugin, we will\n        # decode the script and execute it in the runspace instead of\n        # starting a new interpreter to save on time\n        b_command = base64.b64decode(cmd.split(\" \")[-1])\n        script = to_text(b_command, 'utf-16-le')\n        pwsh_in_data = to_text(in_data, errors=\"surrogate_or_strict\", nonstring=\"passthru\")\n\n        if pwsh_in_data and isinstance(pwsh_in_data, str) and pwsh_in_data.startswith(\"#!\"):\n            # ANSIBALLZ wrapper, we need to get the interpreter and execute\n            # that as the script - note this won't work as basic.py relies\n            # on packages not available on Windows, once fixed we can enable\n            # this path\n            interpreter = to_native(pwsh_in_data.splitlines()[0][2:])\n            # script = \"$input | &'%s' -\" % interpreter\n            raise AnsibleError(\"cannot run the interpreter '%s' on the psrp \"\n                               \"connection plugin\" % interpreter)\n\n        # call build_module_command to get the bootstrap wrapper text\n        bootstrap_wrapper = self._shell.build_module_command('', '', '')\n        if bootstrap_wrapper == cmd:\n            # Do not display to the user each invocation of the bootstrap wrapper\n            display.vvv(\"PSRP: EXEC (via pipeline wrapper)\")\n        else:\n            display.vvv(\"PSRP: EXEC %s\" % script, host=self._psrp_host)\n\n    elif cmd.startswith(f\"{common_args_prefix} -File \"):  # trailing space is on purpose\n        # Used when executing a script file, we will execute it in the runspace process\n        # instead on a new subprocess\n        script = 'param([string]$Path, [Parameter(ValueFromRemainingArguments)][string[]]$ScriptArgs) & $Path @ScriptArgs'\n\n        # Using shlex isn't perfect but it's good enough.\n        cmd = cmd[len(common_args_prefix) + 7:]\n        script_args = shlex.split(cmd)\n        display.vvv(f\"PSRP: EXEC {cmd}\")\n\n    else:\n        # In other cases we want to execute the cmd as the script. We add on the 'exit $LASTEXITCODE' to ensure the\n        # rc is propagated back to the connection plugin.\n        script = to_text(u\"%s\\nexit $LASTEXITCODE\" % cmd)\n        pwsh_in_data = in_data\n        display.vvv(u\"PSRP: EXEC %s\" % script, host=self._psrp_host)\n\n    try:\n        rc, stdout, stderr = self._exec_psrp_script(\n            script=script,\n            input_data=pwsh_in_data.splitlines() if pwsh_in_data else None,\n            arguments=script_args,\n        )\n    except ReadTimeout as e:\n        raise AnsibleConnectionFailure(\n            \"HTTP read timeout during PSRP script execution\"\n        ) from e\n    return rc, stdout, stderr", "loc": 62}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\psrp.py", "class_name": "Connection", "function_name": "put_file", "parameters": ["self", "in_path", "out_path"], "param_types": {"in_path": "str", "out_path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleFileNotFound", "_bootstrap_powershell_script", "base64.b64encode", "display.vvv", "display.vvvvv", "in_data.decode", "in_data.decode().splitlines", "int", "iter", "json.loads", "len", "open", "os.path.exists", "put_output.get", "read_gen", "self._exec_psrp_script", "sha1", "sha1_hash.hexdigest", "sha1_hash.update", "src_fd.read", "super", "super(Connection, self).put_file", "to_bytes", "to_native", "to_text"], "control_structures": ["For", "If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def put_file(self, in_path: str, out_path: str) -> None:\n    super(Connection, self).put_file(in_path, out_path)\n\n    display.vvv(\"PUT %s TO %s\" % (in_path, out_path), host=self._psrp_host)\n\n    script, in_data = _bootstrap_powershell_script('psrp_put_file.ps1', {\n        'Path': out_path,\n    }, has_input=True)\n\n    # Get the buffer size of each fragment to send, subtract 82 for the fragment, message, and other header info\n    # fields that PSRP adds. Adjust to size of the base64 encoded bytes length.\n    buffer_size = int((self.runspace.connection.max_payload_size - 82) / 4 * 3)\n\n    sha1_hash = sha1()\n\n    b_in_path = to_bytes(in_path, errors='surrogate_or_strict')\n    if not os.path.exists(b_in_path):\n        raise AnsibleFileNotFound('file or module does not exist: \"%s\"' % to_native(in_path))\n\n    def read_gen():\n        yield from in_data.decode().splitlines()\n\n        offset = 0\n\n        with open(b_in_path, 'rb') as src_fd:\n            for b_data in iter((lambda: src_fd.read(buffer_size)), b\"\"):\n                data_len = len(b_data)\n                offset += data_len\n                sha1_hash.update(b_data)\n\n                # PSRP technically supports sending raw bytes but that method requires a larger CLIXML message.\n                # Sending base64 is still more efficient here.\n                display.vvvvv(\"PSRP PUT %s to %s (offset=%d, size=%d\" % (in_path, out_path, offset, data_len),\n                              host=self._psrp_host)\n                b64_data = base64.b64encode(b_data)\n                yield [to_text(b64_data)]\n\n            if offset == 0:  # empty file\n                yield [\"\"]\n\n    rc, stdout, stderr = self._exec_psrp_script(script, read_gen())\n\n    if rc != 0:\n        raise AnsibleError(to_native(stderr))\n\n    put_output = json.loads(to_text(stdout))\n    local_sha1 = sha1_hash.hexdigest()\n    remote_sha1 = put_output.get(\"sha1\")\n\n    if not remote_sha1:\n        raise AnsibleError(\"Remote sha1 was not returned, stdout: '%s', stderr: '%s'\"\n                           % (to_native(stdout), to_native(stderr)))\n\n    if not remote_sha1 == local_sha1:\n        raise AnsibleError(\"Remote sha1 hash %s does not match local hash %s\"\n                           % (to_native(remote_sha1), to_native(local_sha1)))", "loc": 56}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\psrp.py", "class_name": "Connection", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.vvvvv", "self.runspace.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    if self.runspace and self.runspace.state == RunspacePoolState.OPENED:\n        display.vvvvv(\"PSRP CLOSE RUNSPACE: %s\" % (self.runspace.id),\n                      host=self._psrp_host)\n        self.runspace.close()\n    self.runspace = None\n    self._connected = False\n    self._last_pipeline = None", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\winrm.py", "class_name": "Connection", "function_name": "reset", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._connect"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reset(self) -> None:\n    if not self._connected:\n        return\n    self.protocol = None\n    self.shell_id = None\n    self._connect()", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\winrm.py", "class_name": "Connection", "function_name": "exec_command", "parameters": ["self", "cmd", "in_data", "sudoable"], "param_types": {"cmd": "str", "in_data": "bytes | None", "sudoable": "bool"}, "return_type": "tuple[int, bytes, bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.split", "cmd.startswith", "display.vvv", "self._shell._encode_script", "self._winrm_exec", "self._wrapper_payload_stream", "super", "super(Connection, self).exec_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_command(self, cmd: str, in_data: bytes | None = None, sudoable: bool = True) -> tuple[int, bytes, bytes]:\n    super(Connection, self).exec_command(cmd, in_data=in_data, sudoable=sudoable)\n\n    encoded_prefix = self._shell._encode_script('', as_list=False, strict_mode=False, preserve_rc=False)\n    if cmd.startswith(encoded_prefix) or cmd.startswith(\"type \"):\n        # Avoid double encoding the script, the first means we are already\n        # running the standard PowerShell command, the latter is used for\n        # the no pipeline case where it uses type to pipe the script into\n        # powershell which is known to work without re-encoding as pwsh.\n        cmd_parts = cmd.split(\" \")\n    else:\n        cmd_parts = self._shell._encode_script(cmd, as_list=True, strict_mode=False, preserve_rc=False)\n\n    # TODO: display something meaningful here\n    display.vvv(\"EXEC (via pipeline wrapper)\")\n\n    stdin_iterator = None\n\n    if in_data:\n        stdin_iterator = self._wrapper_payload_stream(in_data)\n\n    return self._winrm_exec(cmd_parts[0], cmd_parts[1:], from_exec=True, stdin_iterator=stdin_iterator)", "loc": 22}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\winrm.py", "class_name": "Connection", "function_name": "put_file", "parameters": ["self", "in_path", "out_path"], "param_types": {"in_path": "str", "out_path": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Remote sha1 hash {0} does not match local hash {1}'.format", "AnsibleError", "AnsibleFileNotFound", "_bootstrap_powershell_script", "display.vvv", "json.loads", "os.path.exists", "put_output.get", "secure_hash", "self._put_file_stdin_iterator", "self._shell._encode_script", "self._winrm_exec", "super", "super(Connection, self).put_file", "to_bytes", "to_native", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def put_file(self, in_path: str, out_path: str) -> None:\n    super(Connection, self).put_file(in_path, out_path)\n    display.vvv('PUT \"%s\" TO \"%s\"' % (in_path, out_path), host=self._winrm_host)\n    if not os.path.exists(to_bytes(in_path, errors='surrogate_or_strict')):\n        raise AnsibleFileNotFound('file or module does not exist: \"%s\"' % to_native(in_path))\n\n    copy_script, copy_script_stdin = _bootstrap_powershell_script('winrm_put_file.ps1', {\n        'Path': out_path,\n    }, has_input=True)\n    cmd_parts = self._shell._encode_script(copy_script, as_list=True, strict_mode=False, preserve_rc=False)\n\n    status_code, b_stdout, b_stderr = self._winrm_exec(\n        cmd_parts[0],\n        cmd_parts[1:],\n        stdin_iterator=self._put_file_stdin_iterator(copy_script_stdin, in_path, out_path),\n    )\n    stdout = to_text(b_stdout)\n    stderr = to_text(b_stderr)\n\n    if status_code != 0:\n        raise AnsibleError(stderr)\n\n    try:\n        put_output = json.loads(stdout)\n    except ValueError:\n        # stdout does not contain a valid response\n        raise AnsibleError('winrm put_file failed; \\nstdout: %s\\nstderr %s' % (stdout, stderr))\n\n    remote_sha1 = put_output.get(\"sha1\")\n    if not remote_sha1:\n        raise AnsibleError(\"Remote sha1 was not returned\")\n\n    local_sha1 = secure_hash(in_path)\n\n    if not remote_sha1 == local_sha1:\n        raise AnsibleError(\"Remote sha1 hash {0} does not match local hash {1}\".format(to_native(remote_sha1), to_native(local_sha1)))", "loc": 36}
{"file": "ansible\\lib\\ansible\\plugins\\connection\\winrm.py", "class_name": "Connection", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.vvvvv", "self.protocol.close_shell"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    if self.protocol and self.shell_id:\n        display.vvvvv('WINRM CLOSE SHELL: %s' % self.shell_id, host=self._winrm_host)\n        self.protocol.close_shell(self.shell_id)\n    self.shell_id = None\n    self.protocol = None\n    self._connected = False", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "from_json", "parameters": ["a", "profile"], "param_types": {"profile": "str | None"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_decoder", "json.loads"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Deserialize JSON with an optional decoder profile.", "source_code": "def from_json(a, profile: str | None = None, **kwargs) -> t.Any:\n    \"\"\"Deserialize JSON with an optional decoder profile.\"\"\"\n    cls = get_decoder(profile or \"tagless\")\n\n    return json.loads(a, cls=cls, **kwargs)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "to_json", "parameters": ["a", "profile", "vault_to_text", "preprocess_unsafe"], "param_types": {"profile": "str | None", "vault_to_text": "t.Any", "preprocess_unsafe": "t.Any"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "get_encoder", "json.dumps"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "Serialize as JSON with an optional encoder profile.", "source_code": "def to_json(a, profile: str | None = None, vault_to_text: t.Any = ..., preprocess_unsafe: t.Any = ..., **kwargs) -> str:\n    \"\"\"Serialize as JSON with an optional encoder profile.\"\"\"\n\n    if profile and vault_to_text is not ...:\n        raise ValueError(\"Only one of `vault_to_text` or `profile` can be specified.\")\n\n    if profile and preprocess_unsafe is not ...:\n        raise ValueError(\"Only one of `preprocess_unsafe` or `profile` can be specified.\")\n\n    # deprecated: description='deprecate vault_to_text' core_version='2.23'\n    # deprecated: description='deprecate preprocess_unsafe' core_version='2.23'\n\n    cls = get_encoder(profile or \"tagless\")\n\n    return json.dumps(a, cls=cls, **kwargs)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "to_nice_json", "parameters": ["a", "indent", "sort_keys"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.pop", "to_json"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Make verbose, human-readable JSON.", "source_code": "def to_nice_json(a, indent=4, sort_keys=True, **kwargs):\n    \"\"\"Make verbose, human-readable JSON.\"\"\"\n    # TODO separators can be potentially exposed to the user as well\n    kwargs.pop('separators', None)\n    return to_json(a, indent=indent, sort_keys=sort_keys, separators=(',', ': '), **kwargs)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "to_bool", "parameters": ["value"], "param_types": {"value": "object"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "isinstance", "native_type_name", "str", "str(value).lower", "value.lower"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Convert well-known input values to a boolean value.", "source_code": "def to_bool(value: object) -> bool:\n    \"\"\"Convert well-known input values to a boolean value.\"\"\"\n    value_to_check: object\n\n    if isinstance(value, str):\n        value_to_check = value.lower()  # accept mixed case variants\n    elif isinstance(value, int):  # bool is also an int\n        value_to_check = str(value).lower()  # accept int (0, 1) and bool (True, False) -- not just string versions\n    else:\n        value_to_check = value\n\n    try:\n        if value_to_check in _valid_bool_true:\n            return True\n\n        if value_to_check in _valid_bool_false:\n            return False\n\n        # if we're still here, the value is unsupported- always fire a deprecation warning\n        result = value_to_check == 1  # backwards compatibility with the old code which checked: value in ('yes', 'on', '1', 'true', 1)\n    except TypeError:\n        result = False\n\n    # NB: update the doc string to reflect reality once this fallback is removed\n    display.deprecated(\n        msg=f'The `bool` filter coerced invalid value {value!r} ({native_type_name(value)}) to {result!r}.',\n        version='2.23',\n    )\n\n    return result", "loc": 30}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "strftime", "parameters": ["string_format", "second", "utc"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "float", "time.strftime", "timefn"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "return a date string using string. See https://docs.python.org/3/library/time.html#time.strftime for format", "source_code": "def strftime(string_format, second=None, utc=False):\n    \"\"\" return a date string using string. See https://docs.python.org/3/library/time.html#time.strftime for format \"\"\"\n    if utc:\n        timefn = time.gmtime\n    else:\n        timefn = time.localtime\n    if second is not None:\n        try:\n            second = float(second)\n        except Exception:\n            raise AnsibleFilterError('Invalid value for epoch value (%s)' % second)\n    return time.strftime(string_format, timefn(second))", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "quote", "parameters": ["a"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["shlex.quote", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "return its argument quoted for shell usage", "source_code": "def quote(a):\n    \"\"\" return its argument quoted for shell usage \"\"\"\n    if a is None:\n        a = u''\n    return shlex.quote(to_text(a))", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "regex_replace", "parameters": ["value", "pattern", "replacement", "ignorecase", "multiline", "count", "mandatory_count"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "_re.subn", "re.compile", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Perform a `re.sub` returning a string", "source_code": "def regex_replace(value='', pattern='', replacement='', ignorecase=False, multiline=False, count=0, mandatory_count=0):\n    \"\"\" Perform a `re.sub` returning a string \"\"\"\n\n    value = to_text(value, errors='surrogate_or_strict', nonstring='simplerepr')\n\n    flags = 0\n    if ignorecase:\n        flags |= re.I\n    if multiline:\n        flags |= re.M\n    _re = re.compile(pattern, flags=flags)\n    (output, subs) = _re.subn(replacement, value, count=count)\n    if mandatory_count and mandatory_count != subs:\n        raise AnsibleFilterError(\"'%s' should match %d times, but matches %d times in '%s'\"\n                                 % (pattern, mandatory_count, count, value))\n    return output", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "regex_findall", "parameters": ["value", "regex", "multiline", "ignorecase"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Perform re.findall and return the list of matches", "source_code": "def regex_findall(value, regex, multiline=False, ignorecase=False):\n    \"\"\" Perform re.findall and return the list of matches \"\"\"\n\n    value = to_text(value, errors='surrogate_or_strict', nonstring='simplerepr')\n\n    flags = 0\n    if ignorecase:\n        flags |= re.I\n    if multiline:\n        flags |= re.M\n    return re.findall(regex, value, flags)", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "regex_search", "parameters": ["value", "regex"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "arg.startswith", "groups.append", "int", "items.append", "kwargs.get", "list", "match.group", "re.match", "re.match('\\\\\\\\(\\\\d+)', arg).group", "re.match('\\\\\\\\g<(\\\\S+)>', arg).group", "re.search", "to_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Perform re.search and return the list of matches or a backref", "source_code": "def regex_search(value, regex, *args, **kwargs):\n    \"\"\" Perform re.search and return the list of matches or a backref \"\"\"\n\n    value = to_text(value, errors='surrogate_or_strict', nonstring='simplerepr')\n\n    groups = list()\n    for arg in args:\n        if arg.startswith('\\\\g'):\n            match = re.match(r'\\\\g<(\\S+)>', arg).group(1)\n            groups.append(match)\n        elif arg.startswith('\\\\'):\n            match = int(re.match(r'\\\\(\\d+)', arg).group(1))\n            groups.append(match)\n        else:\n            raise AnsibleFilterError('Unknown argument')\n\n    flags = 0\n    if kwargs.get('ignorecase'):\n        flags |= re.I\n    if kwargs.get('multiline'):\n        flags |= re.M\n\n    match = re.search(regex, value, flags)\n    if match:\n        if not groups:\n            return match.group()\n        else:\n            items = list()\n            for item in groups:\n                items.append(match.group(item))\n            return items", "loc": 31}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "ternary", "parameters": ["value", "true_val", "false_val", "none_val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "value ? true_val : false_val", "source_code": "def ternary(value, true_val, false_val, none_val=None):\n    \"\"\"  value ? true_val : false_val \"\"\"\n    if value is None and none_val is not None:\n        return none_val\n    elif bool(value):\n        return true_val\n    else:\n        return false_val", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "regex_escape", "parameters": ["string", "re_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "re.escape", "regex_replace", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Escape all regular expressions special characters from STRING.", "source_code": "def regex_escape(string, re_type='python'):\n    \"\"\"Escape all regular expressions special characters from STRING.\"\"\"\n    string = to_text(string, errors='surrogate_or_strict', nonstring='simplerepr')\n    if re_type == 'python':\n        return re.escape(string)\n    elif re_type == 'posix_basic':\n        # list of BRE special chars:\n        # https://en.wikibooks.org/wiki/Regular_Expressions/POSIX_Basic_Regular_Expressions\n        return regex_replace(string, r'([].[^$*\\\\])', r'\\\\\\1')\n    # TODO: implement posix_extended\n    # It's similar to, but different from python regex, which is similar to,\n    # but different from PCRE.  It's possible that re.escape would work here.\n    # https://remram44.github.io/regex-cheatsheet/regex.html#programs\n    elif re_type == 'posix_extended':\n        raise AnsibleFilterError('Regex type (%s) not yet implemented' % re_type)\n    else:\n        raise AnsibleFilterError('Invalid regex type (%s)' % re_type)", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "from_yaml", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "isinstance", "native_type_name", "yaml.load"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def from_yaml(data):\n    if data is None:\n        return None\n\n    if isinstance(data, str):\n        return yaml.load(data, Loader=_yaml_loader.AnsibleInstrumentedLoader)  # type: ignore[arg-type]\n\n    display.deprecated(f\"The from_yaml filter ignored non-string input of type {native_type_name(data)!r}.\", version='2.23', obj=data)\n    return data", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "from_yaml_all", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "isinstance", "native_type_name", "yaml.load_all"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def from_yaml_all(data):\n    if data is None:\n        return []  # backward compatibility; ensure consistent result between classic/native Jinja for None/empty string input\n\n    if isinstance(data, str):\n        return yaml.load_all(data, Loader=_yaml_loader.AnsibleInstrumentedLoader)  # type: ignore[arg-type]\n\n    display.deprecated(f\"The from_yaml_all filter ignored non-string input of type {native_type_name(data)!r}.\", version='2.23', obj=data)\n    return data", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "rand", "parameters": ["environment", "end", "start", "step", "seed"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "Random", "SystemRandom", "hasattr", "isinstance", "r.choice", "r.randrange"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def rand(environment, end, start=None, step=None, seed=None):\n    if seed is None:\n        r = SystemRandom()\n    else:\n        r = Random(seed)\n    if isinstance(end, int):\n        if not start:\n            start = 0\n        if not step:\n            step = 1\n        return r.randrange(start, end, step)\n    elif hasattr(end, '__iter__'):\n        if start or step:\n            raise AnsibleFilterError('start and step can only be used with integer values')\n        return r.choice(end)\n    else:\n        raise AnsibleFilterError('random can only be used on sequences and integers')", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "randomize_list", "parameters": ["mylist", "seed"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Random", "list", "r.shuffle", "shuffle"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def randomize_list(mylist, seed=None):\n    try:\n        mylist = list(mylist)\n        if seed:\n            r = Random(seed)\n            r.shuffle(mylist)\n        else:\n            shuffle(mylist)\n    except Exception:\n        pass\n    return mylist", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "get_hash", "parameters": ["data", "hashtype"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "h.hexdigest", "h.update", "hashlib.new", "to_bytes"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_hash(data, hashtype='sha1'):\n    try:\n        h = hashlib.new(hashtype)\n    except Exception as e:\n        # hash is not supported?\n        raise AnsibleFilterError(e)\n\n    h.update(to_bytes(data, errors='surrogate_or_strict'))\n    return h.hexdigest()", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "get_encrypted_password", "parameters": ["password", "hashtype", "salt", "salt_size", "rounds", "ident"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleFilterError", "do_encrypt", "passlib_mapping.get", "passlib_mapping.values"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_encrypted_password(password, hashtype='sha512', salt=None, salt_size=None, rounds=None, ident=None):\n    passlib_mapping = {\n        'md5': 'md5_crypt',\n        'blowfish': 'bcrypt',\n        'sha256': 'sha256_crypt',\n        'sha512': 'sha512_crypt',\n    }\n\n    hashtype = passlib_mapping.get(hashtype, hashtype)\n\n    if PASSLIB_AVAILABLE and hashtype not in passlib_mapping and hashtype not in passlib_mapping.values():\n        raise AnsibleFilterError(f\"{hashtype} is not in the list of supported passlib algorithms: {', '.join(passlib_mapping)}\")\n\n    return do_encrypt(password, hashtype, salt=salt, salt_size=salt_size, rounds=rounds, ident=ident)", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "to_uuid", "parameters": ["string", "namespace"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "isinstance", "to_native", "to_text", "uuid.UUID", "uuid.uuid5"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_uuid(string, namespace=UUID_NAMESPACE_ANSIBLE):\n    uuid_namespace = namespace\n    if not isinstance(uuid_namespace, uuid.UUID):\n        try:\n            uuid_namespace = uuid.UUID(namespace)\n        except (AttributeError, ValueError) as e:\n            raise AnsibleFilterError(\"Invalid value '%s' for 'namespace': %s\" % (to_native(namespace), to_native(e)))\n    # uuid.uuid5() requires bytes on Python 2 and bytes or text or Python 3\n    return to_text(uuid.uuid5(uuid_namespace, to_native(string, errors='surrogate_or_strict')))", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "mandatory", "parameters": ["a", "msg"], "param_types": {"a": "object", "msg": "str | None"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "isinstance", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Make a variable mandatory.", "source_code": "def mandatory(a: object, msg: str | None = None) -> object:\n    \"\"\"Make a variable mandatory.\"\"\"\n    # DTFIX-FUTURE: deprecate this filter; there are much better ways via undef, etc...\n    #                also remember to remove unit test checking for _undefined_name\n    if isinstance(a, UndefinedMarker):\n        if msg is not None:\n            raise AnsibleFilterError(to_text(msg))\n\n        if a._undefined_name is not None:\n            name = f'{to_text(a._undefined_name)!r} '\n        else:\n            name = ''\n\n        raise AnsibleFilterError(f\"Mandatory variable {name}not defined.\")\n\n    return a", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "combine", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "flatten", "kwargs.pop", "len", "merge_hash", "next", "reversed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def combine(*terms, **kwargs):\n    recursive = kwargs.pop('recursive', False)\n    list_merge = kwargs.pop('list_merge', 'replace')\n    if kwargs:\n        raise AnsibleFilterError(\"'recursive' and 'list_merge' are the only valid keyword arguments\")\n\n    # allow the user to do `[dict1, dict2, ...] | combine`\n    dictionaries = flatten(terms, levels=1)\n\n    if not dictionaries:\n        return {}\n\n    if len(dictionaries) == 1:\n        return dictionaries[0]\n\n    # merge all the dicts so that the dict at the end of the array have precedence\n    # over the dict at the beginning.\n    # we merge the dicts from the highest to the lowest priority because there is\n    # a huge probability that the lowest priority dict will be the biggest in size\n    # (as the low prio dict will hold the \"default\" values and the others will be \"patches\")\n    # and merge_hash create a copy of it's first argument.\n    # so high/right -> low/left is more efficient than low/left -> high/right\n    high_to_low_prio_dict_iterator = reversed(dictionaries)\n    result = next(high_to_low_prio_dict_iterator)\n    for dictionary in high_to_low_prio_dict_iterator:\n        result = merge_hash(dictionary, result, recursive, list_merge)\n\n    return result", "loc": 28}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "comment", "parameters": ["text", "style"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'%s%s' % (p['decoration'], text.replace(p['newline'], '%s%s' % (p['newline'], p['decoration']))).replace", "', '.join", "AnsibleTemplatePluginError", "int", "p.update", "p['decoration'].rstrip", "p['newline'].join", "prepostfix.rstrip", "range", "str", "text.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def comment(text, style='plain', **kw):\n    # Predefined comment types\n    comment_styles = {\n        'plain': {\n            'decoration': '# '\n        },\n        'erlang': {\n            'decoration': '% '\n        },\n        'c': {\n            'decoration': '// '\n        },\n        'cblock': {\n            'beginning': '/*',\n            'decoration': ' * ',\n            'end': ' */'\n        },\n        'xml': {\n            'beginning': '<!--',\n            'decoration': ' - ',\n            'end': '-->'\n        }\n    }\n\n    if style not in comment_styles:\n        raise AnsibleTemplatePluginError(\n            message=f\"Invalid style {style!r}.\",\n            help_text=f\"Available styles: {', '.join(comment_styles)}\",\n            obj=style,\n        )\n\n    # Pointer to the right comment type\n    style_params = comment_styles[style]\n\n    if 'decoration' in kw:\n        prepostfix = kw['decoration']\n    else:\n        prepostfix = style_params['decoration']\n\n    # Default params\n    p = {\n        'newline': '\\n',\n        'beginning': '',\n        'prefix': (prepostfix).rstrip(),\n        'prefix_count': 1,\n        'decoration': '',\n        'postfix': (prepostfix).rstrip(),\n        'postfix_count': 1,\n        'end': ''\n    }\n\n    # Update default params\n    p.update(style_params)\n    p.update(kw)\n\n    # Compose substrings for the final string\n    str_beginning = ''\n    if p['beginning']:\n        str_beginning = \"%s%s\" % (p['beginning'], p['newline'])\n    str_prefix = ''\n    if p['prefix']:\n        if p['prefix'] != p['newline']:\n            str_prefix = str(\n                \"%s%s\" % (p['prefix'], p['newline'])) * int(p['prefix_count'])\n        else:\n            str_prefix = str(\n                \"%s\" % (p['newline'])) * int(p['prefix_count'])\n    str_text = (\"%s%s\" % (\n        p['decoration'],\n        # Prepend each line of the text with the decorator\n        text.replace(\n            p['newline'], \"%s%s\" % (p['newline'], p['decoration'])))).replace(\n                # Remove trailing spaces when only decorator is on the line\n                \"%s%s\" % (p['decoration'], p['newline']),\n                \"%s%s\" % (p['decoration'].rstrip(), p['newline']))\n    str_postfix = p['newline'].join(\n        [''] + [p['postfix'] for x in range(p['postfix_count'])])\n    str_end = ''\n    if p['end']:\n        str_end = \"%s%s\" % (p['newline'], p['end'])\n\n    # Return the final string\n    return \"%s%s%s%s%s\" % (\n        str_beginning,\n        str_prefix,\n        str_text,\n        str_postfix,\n        str_end)", "loc": 88}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "extract", "parameters": ["environment", "item", "container", "morekeys"], "param_types": {"environment": "Environment"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["environment.getitem", "isinstance"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract(environment: Environment, item, container, morekeys=None):\n    if morekeys is None:\n        keys = [item]\n    elif isinstance(morekeys, list):\n        keys = [item] + morekeys\n    else:\n        keys = [item, morekeys]\n\n    value = container\n\n    for key in keys:\n        try:\n            value = environment.getitem(value, key)\n        except MarkerError as ex:\n            value = ex.source\n\n    return value", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "b64encode", "parameters": ["string", "encoding", "urlsafe"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "to_bytes", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def b64encode(string, encoding='utf-8', urlsafe=False):\n    func = base64.b64encode\n    if urlsafe:\n        func = base64.urlsafe_b64encode\n    return to_text(func(to_bytes(string, encoding=encoding, errors='surrogate_or_strict')))", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "b64decode", "parameters": ["string", "encoding", "urlsafe"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "to_bytes", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def b64decode(string, encoding='utf-8', urlsafe=False):\n    func = base64.b64decode\n    if urlsafe:\n        func = base64.urlsafe_b64decode\n    return to_text(func(to_bytes(string, errors='surrogate_or_strict')), encoding=encoding)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "flatten", "parameters": ["mylist", "levels", "skip_nulls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["flatten", "int", "is_sequence", "ret.append", "ret.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def flatten(mylist, levels=None, skip_nulls=True):\n\n    ret = []\n    for element in mylist:\n        if skip_nulls and element in (None, 'None', 'null'):\n            # ignore null items\n            continue\n        elif is_sequence(element):\n            if levels is None:\n                ret.extend(flatten(element, skip_nulls=skip_nulls))\n            elif levels >= 1:\n                # decrement as we go down the stack\n                ret.extend(flatten(element, levels=(int(levels) - 1), skip_nulls=skip_nulls))\n            else:\n                ret.append(element)\n        else:\n            ret.append(element)\n\n    return ret", "loc": 19}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "subelements", "parameters": ["obj", "subelements", "skip_missing"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleFilterError", "AnsibleTypeError", "isinstance", "list", "obj.values", "results.append", "subelements.split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Accepts a dict or list of dicts, and a dotted accessor and produces a product of the element and the results of the dotted accessor >>> obj = [{\"name\": \"alice\", \"groups\": [\"wheel\"], \"authorized\": [\"/tmp/alice/onekey.pub\"]}]", "source_code": "def subelements(obj, subelements, skip_missing=False):\n    \"\"\"Accepts a dict or list of dicts, and a dotted accessor and produces a product\n    of the element and the results of the dotted accessor\n\n    >>> obj = [{\"name\": \"alice\", \"groups\": [\"wheel\"], \"authorized\": [\"/tmp/alice/onekey.pub\"]}]\n    >>> subelements(obj, 'groups')\n    [({'name': 'alice', 'groups': ['wheel'], 'authorized': ['/tmp/alice/onekey.pub']}, 'wheel')]\n\n    \"\"\"\n    if isinstance(obj, dict):\n        element_list = list(obj.values())\n    elif isinstance(obj, list):\n        element_list = obj[:]\n    else:\n        raise AnsibleFilterError('obj must be a list of dicts or a nested dict')\n\n    if isinstance(subelements, list):\n        subelement_list = subelements[:]\n    elif isinstance(subelements, str):\n        subelement_list = subelements.split('.')\n    else:\n        raise AnsibleTypeError('subelements must be a list or a string')\n\n    results = []\n\n    for element in element_list:\n        values = element\n        for subelement in subelement_list:\n            try:\n                values = values[subelement]\n            except KeyError:\n                if skip_missing:\n                    values = []\n                    break\n                raise AnsibleFilterError(\"could not find %r key in iterated item %r\" % (subelement, values))\n            except TypeError as ex:\n                raise AnsibleTypeError(\"the key %s should point to a dictionary, got '%s'\" % (subelement, values)) from ex\n        if not isinstance(values, list):\n            raise AnsibleTypeError(\"the key %r should point to a list, got %r\" % (subelement, values))\n\n        for value in values:\n            results.append((element, value))\n\n    return results", "loc": 44}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "dict_to_list_of_dict_key_value_elements", "parameters": ["mydict", "key_name", "value_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTypeError", "isinstance", "ret.append", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "takes a dictionary and transforms it into a list of dictionaries, with each having a 'key' and 'value' keys that correspond to the keys and values of the original", "source_code": "def dict_to_list_of_dict_key_value_elements(mydict, key_name='key', value_name='value'):\n    \"\"\" takes a dictionary and transforms it into a list of dictionaries,\n        with each having a 'key' and 'value' keys that correspond to the keys and values of the original \"\"\"\n\n    if not isinstance(mydict, Mapping):\n        raise AnsibleTypeError(\"dict2items requires a dictionary, got %s instead.\" % type(mydict))\n\n    ret = []\n    for key in mydict:\n        ret.append({key_name: key, value_name: mydict[key]})\n    return ret", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "list_of_dict_key_value_elements_to_dict", "parameters": ["mylist", "key_name", "value_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTypeError", "dict", "is_sequence", "type"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "takes a list of dicts with each having a 'key' and 'value' keys, and transforms the list into a dictionary, effectively as the reverse of dict2items", "source_code": "def list_of_dict_key_value_elements_to_dict(mylist, key_name='key', value_name='value'):\n    \"\"\" takes a list of dicts with each having a 'key' and 'value' keys, and transforms the list into a dictionary,\n        effectively as the reverse of dict2items \"\"\"\n\n    if not is_sequence(mylist):\n        raise AnsibleTypeError(\"items2dict requires a list, got %s instead.\" % type(mylist))\n\n    try:\n        return dict((item[key_name], item[value_name]) for item in mylist)\n    except KeyError:\n        raise AnsibleTypeError(\n            \"items2dict requires each dictionary in the list to contain the keys '%s' and '%s', got %s instead.\"\n            % (key_name, value_name, mylist)\n        )\n    except TypeError:\n        raise AnsibleTypeError(\"items2dict requires a list of dictionaries, got %s instead.\" % mylist)", "loc": 16}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\core.py", "class_name": null, "function_name": "ansible_default", "parameters": ["value", "default_value", "boolean"], "param_types": {"value": "t.Any", "default_value": "t.Any", "boolean": "bool"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "validate_arg_type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Updated `default` filter that only coalesces classic undefined objects; other Undefined-derived types (eg, ErrorMarker) pass through.", "source_code": "def ansible_default(\n    value: t.Any,\n    default_value: t.Any = '',\n    boolean: bool = False,\n) -> t.Any:\n    \"\"\"Updated `default` filter that only coalesces classic undefined objects; other Undefined-derived types (eg, ErrorMarker) pass through.\"\"\"\n    validate_arg_type('boolean', boolean, bool)\n\n    if isinstance(value, UndefinedMarker):\n        return default_value\n\n    if boolean and not value:\n        return default_value\n\n    return value", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\encryption.py", "class_name": null, "function_name": "do_vault", "parameters": ["data", "secret", "salt", "vault_id", "wrap_object"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "TypeError", "VaultLib", "VaultSecret", "VaultedValue", "VaultedValue(ciphertext=str(vault)).tag", "isinstance", "str", "to_bytes", "to_native", "type", "vl.encrypt"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_vault(data, secret, salt=None, vault_id='filter_default', wrap_object=False):\n    if not isinstance(secret, (str, bytes)):\n        raise TypeError(f\"Secret passed is required to be a string, instead we got {type(secret)}.\")\n\n    if not isinstance(data, (str, bytes)):\n        raise TypeError(f\"Can only vault strings, instead we got {type(data)}.\")\n\n    vs = VaultSecret(to_bytes(secret))\n    vl = VaultLib()\n    try:\n        vault = vl.encrypt(to_bytes(data), vs, vault_id, salt)\n    except Exception as ex:\n        raise AnsibleError(\"Unable to encrypt.\") from ex\n\n    if wrap_object:\n        vault = VaultedValue(ciphertext=str(vault)).tag(secret)\n    else:\n        vault = to_native(vault)\n\n    return vault", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\encryption.py", "class_name": null, "function_name": "do_unvault", "parameters": ["vault", "secret", "vault_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "TypeError", "VaultHelper.get_ciphertext", "VaultLib", "VaultSecret", "_template.get_first_marker_arg", "is_encrypted", "isinstance", "to_bytes", "to_native", "type", "vault._disarm", "vl.decrypt"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_unvault(vault, secret, vault_id='filter_default'):\n    if isinstance(vault, VaultExceptionMarker):\n        vault = vault._disarm()\n\n    if (first_marker := _template.get_first_marker_arg((vault, secret, vault_id), {})) is not None:\n        return first_marker\n\n    if not isinstance(secret, (str, bytes)):\n        raise TypeError(f\"Secret passed is required to be as string, instead we got {type(secret)}.\")\n\n    if not isinstance(vault, (str, bytes)):\n        raise TypeError(f\"Vault should be in the form of a string, instead we got {type(vault)}.\")\n\n    vs = VaultSecret(to_bytes(secret))\n    vl = VaultLib([(vault_id, vs)])\n\n    if ciphertext := VaultHelper.get_ciphertext(vault, with_tags=True):\n        vault = ciphertext\n\n    if is_encrypted(vault):\n        try:\n            data = vl.decrypt(vault)\n        except Exception as ex:\n            raise AnsibleError(\"Unable to decrypt.\") from ex\n    else:\n        data = vault\n\n    return to_native(data)", "loc": 28}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "unique", "parameters": ["environment", "a", "case_sensitive", "attribute"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_do_fail", "bool", "c.append", "display.error_as_warning", "do_unique", "list"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unique(environment, a, case_sensitive=None, attribute=None):\n\n    def _do_fail(ex):\n        if case_sensitive is False or attribute:\n            raise AnsibleError(\n                \"Jinja2's unique filter failed and we cannot fall back to Ansible's version as it does not support the parameters supplied.\"\n            ) from ex\n\n    error = e = None\n    try:\n        if HAS_UNIQUE:\n            c = list(do_unique(environment, a, case_sensitive=bool(case_sensitive), attribute=attribute))\n    except TypeError as e:\n        error = e\n        _do_fail(e)\n    except Exception as e:\n        error = e\n        _do_fail(e)\n        display.error_as_warning('Falling back to Ansible unique filter as Jinja2 one failed.', e)\n\n    if not HAS_UNIQUE or error:\n\n        # handle Jinja2 specific attributes when using Ansible's version\n        if case_sensitive is False or attribute:\n            raise AnsibleError(\"Ansible's unique filter does not support case_sensitive=False nor attribute parameters, \"\n                               \"you need a newer version of Jinja2 that provides their version of the filter.\")\n\n        c = []\n        for x in a:\n            if x not in c:\n                c.append(x)\n\n    return c", "loc": 33}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "intersect", "parameters": ["environment", "a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "set", "unique"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def intersect(environment, a, b):\n    try:\n        c = list(set(a) & set(b))\n    except TypeError:\n        c = unique(environment, [x for x in a if x in b], True)\n    return c", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "difference", "parameters": ["environment", "a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "set", "unique"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def difference(environment, a, b):\n    try:\n        c = list(set(a) - set(b))\n    except TypeError:\n        c = unique(environment, [x for x in a if x not in b], True)\n    return c", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "symmetric_difference", "parameters": ["environment", "a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["intersect", "list", "set", "union"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def symmetric_difference(environment, a, b):\n    try:\n        c = list(set(a) ^ set(b))\n    except TypeError:\n        isect = intersect(environment, a, b)\n        c = [x for x in union(environment, a, b) if x not in isect]\n    return c", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "union", "parameters": ["environment", "a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "set", "unique"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def union(environment, a, b):\n    try:\n        c = list(set(a) | set(b))\n    except TypeError:\n        c = unique(environment, a + b, True)\n    return c", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "logarithm", "parameters": ["x", "base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "math.log", "math.log10"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def logarithm(x, base=math.e):\n    try:\n        if base == 10:\n            return math.log10(x)\n        else:\n            return math.log(x, base)\n    except TypeError as ex:\n        raise AnsibleError('log() can only be used on numbers') from ex", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "power", "parameters": ["x", "y"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "math.pow"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def power(x, y):\n    try:\n        return math.pow(x, y)\n    except TypeError as ex:\n        raise AnsibleError('pow() can only be used on numbers') from ex", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "inversepower", "parameters": ["x", "base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "float", "math.pow", "math.sqrt"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inversepower(x, base=2):\n    try:\n        if base == 2:\n            return math.sqrt(x)\n        else:\n            return math.pow(x, 1.0 / float(base))\n    except (ValueError, TypeError) as ex:\n        raise AnsibleError('root() can only be used on numbers') from ex", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "human_readable", "parameters": ["size", "isbits", "unit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "formatters.bytes_to_human"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return a human-readable string", "source_code": "def human_readable(size, isbits=False, unit=None):\n    \"\"\" Return a human-readable string \"\"\"\n    try:\n        return formatters.bytes_to_human(size, isbits, unit)\n    except TypeError as ex:\n        raise AnsibleError(\"human_readable() failed on bad input\") from ex\n    except Exception as ex:\n        raise AnsibleError(\"human_readable() can't interpret the input\") from ex", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "human_to_bytes", "parameters": ["size", "default_unit", "isbits"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "formatters.human_to_bytes"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return bytes count from a human-readable string", "source_code": "def human_to_bytes(size, default_unit=None, isbits=False):\n    \"\"\" Return bytes count from a human-readable string \"\"\"\n    try:\n        return formatters.human_to_bytes(size, default_unit, isbits)\n    except TypeError as ex:\n        raise AnsibleError(\"human_to_bytes() failed on bad input\") from ex\n    except Exception as ex:\n        raise AnsibleError(\"human_to_bytes() can't interpret the input\") from ex", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\mathstuff.py", "class_name": null, "function_name": "rekey_on_member", "parameters": ["data", "key", "duplicates"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "data.values", "isinstance", "new_obj.get"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Rekey a dict of dicts on another member May also create a dict from a list of dicts. duplicates can be one of ``error`` or ``overwrite`` to specify whether to error out if the key", "source_code": "def rekey_on_member(data, key, duplicates='error'):\n    \"\"\"\n    Rekey a dict of dicts on another member\n\n    May also create a dict from a list of dicts.\n\n    duplicates can be one of ``error`` or ``overwrite`` to specify whether to error out if the key\n    value would be duplicated or to overwrite previous entries if that's the case.\n    \"\"\"\n    if duplicates not in ('error', 'overwrite'):\n        raise AnsibleError(f\"duplicates parameter to rekey_on_member has unknown value {duplicates!r}\")\n\n    new_obj = {}\n\n    if isinstance(data, Mapping):\n        iterate_over = data.values()\n    elif isinstance(data, Iterable) and not isinstance(data, (str, bytes)):\n        iterate_over = data\n    else:\n        raise AnsibleError(\"Type is not a valid list, set, or dict\")\n\n    for item in iterate_over:\n        if not isinstance(item, Mapping):\n            raise AnsibleError(\"List item is not a valid dict\")\n\n        try:\n            key_elem = item[key]\n        except KeyError:\n            raise AnsibleError(f\"Key {key!r} was not found.\", obj=item) from None\n\n        # Note: if new_obj[key_elem] exists it will always be a non-empty dict (it will at\n        # minimum contain {key: key_elem}\n        if new_obj.get(key_elem, None):\n            if duplicates == 'error':\n                raise AnsibleError(f\"Key {key_elem!r} is not unique, cannot convert to dict.\")\n            elif duplicates == 'overwrite':\n                new_obj[key_elem] = item\n        else:\n            new_obj[key_elem] = item\n\n    return new_obj", "loc": 41}
{"file": "ansible\\lib\\ansible\\plugins\\filter\\urlsplit.py", "class_name": null, "function_name": "split_url", "parameters": ["value", "query", "alias"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "helpers.object_to_dict", "urlsplit"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def split_url(value, query='', alias='urlsplit'):\n\n    results = helpers.object_to_dict(urlsplit(value), exclude=['count', 'index', 'geturl', 'encode'])\n\n    # If a query is supplied, make sure it's valid then return the results.\n    # If no option is supplied, return the entire dictionary.\n    if query:\n        if query not in results:\n            raise ValueError(alias + ': unknown URL component: %s' % query)\n        return results[query]\n    else:\n        return results", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\httpapi\\__init__.py", "class_name": "HttpApiBase", "function_name": "update_auth", "parameters": ["self", "response", "response_text"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["response.info", "response.info().get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return per-request auth token. The response should be a dictionary that can be plugged into the headers of a request. The default implementation uses cookie data.", "source_code": "def update_auth(self, response, response_text):\n    \"\"\"Return per-request auth token.\n\n    The response should be a dictionary that can be plugged into the\n    headers of a request. The default implementation uses cookie data.\n    If no authentication data is found, return None\n    \"\"\"\n    cookie = response.info().get('Set-Cookie')\n    if cookie:\n        return {'Cookie': cookie}\n\n    return None", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\httpapi\\__init__.py", "class_name": "HttpApiBase", "function_name": "handle_httperror", "parameters": ["self", "exc"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.connection.get_option", "self.login"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Overridable method for dealing with HTTP codes. This method will attempt to handle known cases of HTTP status codes. If your API uses status codes to convey information in a regular way, you can override this method to handle it appropriately.", "source_code": "def handle_httperror(self, exc):\n    \"\"\"Overridable method for dealing with HTTP codes.\n\n    This method will attempt to handle known cases of HTTP status codes.\n    If your API uses status codes to convey information in a regular way,\n    you can override this method to handle it appropriately.\n\n    :returns:\n        * True if the code has been handled in a way that the request\n        may be resent without changes.\n        * False if the error cannot be handled or recovered from by the\n        plugin. This will result in the HTTPError being raised as an\n        exception for the caller to deal with as appropriate (most likely\n        by failing).\n        * Any other value returned is taken as a valid response from the\n        server without making another request. In many cases, this can just\n        be the original exception.\n        \"\"\"\n    if exc.code == 401:\n        if self.connection._auth:\n            # Stored auth appears to be invalid, clear and retry\n            self.connection._auth = None\n            self.login(self.connection.get_option('remote_user'), self.connection.get_option('password'))\n            return True\n        else:\n            # Unauthorized and there's no token. Return an error\n            return False\n\n    return exc", "loc": 29}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\advanced_host_list.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "host_list", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "h.strip", "host_list.split", "self._expand_hostpattern", "self.display.vvv", "self.inventory.add_host", "super", "super(InventoryModule, self).parse", "to_native", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, host_list, cache=True):\n    \"\"\" parses the inventory file \"\"\"\n\n    super(InventoryModule, self).parse(inventory, loader, host_list)\n\n    try:\n        for h in host_list.split(','):\n            h = h.strip()\n            if h:\n                try:\n                    (hostnames, port) = self._expand_hostpattern(h)\n                except AnsibleError as e:\n                    self.display.vvv(\"Unable to parse address from hostname, leaving unchanged: %s\" % to_text(e))\n                    hostnames = [h]\n                    port = None\n\n                for host in hostnames:\n                    if host not in self.inventory.hosts:\n                        self.inventory.add_host(host, group='ungrouped', port=port)\n    except Exception as e:\n        raise AnsibleParserError(\"Invalid data from string, could not parse: %s\" % to_native(e))", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\auto.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Using inventory plugin '{0}' to process inventory source '{1}'\".format", "\"inventory config '{0}' specifies unknown plugin '{1}'\".format", "\"inventory source '{0}' could not be verified by inventory plugin '{1}'\".format", "\"no root 'plugin' key found, '{0}' is not a valid YAML inventory plugin config file\".format", "AnsibleParserError", "config_data.get", "inventory_loader.get", "loader.load_from_file", "plugin.parse", "plugin.update_cache_if_changed", "plugin.verify_file", "self.display.v"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(self, inventory, loader, path, cache=True):\n    config_data = loader.load_from_file(path, cache='none')\n\n    try:\n        plugin_name = config_data.get('plugin', None)\n    except AttributeError:\n        plugin_name = None\n\n    if not plugin_name:\n        raise AnsibleParserError(\"no root 'plugin' key found, '{0}' is not a valid YAML inventory plugin config file\".format(path))\n\n    plugin = inventory_loader.get(plugin_name)\n\n    if not plugin:\n        raise AnsibleParserError(\"inventory config '{0}' specifies unknown plugin '{1}'\".format(path, plugin_name))\n\n    if not plugin.verify_file(path):\n        raise AnsibleParserError(\"inventory source '{0}' could not be verified by inventory plugin '{1}'\".format(path, plugin_name))\n\n    self.display.v(\"Using inventory plugin '{0}' to process inventory source '{1}'\".format(plugin._load_name, path))\n\n    # unfortunate magic to swap the real plugin type we're proxying here into the inventory data API wrapper, so the wrapper can make the right compat\n    # decisions based on the metadata the real plugin provides instead of our metadata\n    inventory._target_plugin = plugin\n\n    plugin.parse(inventory, loader, path, cache=cache)\n    try:\n        plugin.update_cache_if_changed()\n    except AttributeError:\n        pass", "loc": 30}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\constructed.py", "class_name": "InventoryModule", "function_name": "host_groupvars", "parameters": ["self", "host", "loader", "sources"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "get_group_vars", "get_vars_from_inventory_sources", "host.get_groups", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "requires host object", "source_code": "def host_groupvars(self, host, loader, sources):\n    \"\"\" requires host object \"\"\"\n    gvars = get_group_vars(host.get_groups())\n\n    if self.get_option('use_vars_plugins'):\n        gvars = combine_vars(gvars, get_vars_from_inventory_sources(loader, sources, host.get_groups(), 'all'))\n\n    return gvars", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\constructed.py", "class_name": "InventoryModule", "function_name": "host_vars", "parameters": ["self", "host", "loader", "sources"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "get_vars_from_inventory_sources", "host.get_vars", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "requires host object", "source_code": "def host_vars(self, host, loader, sources):\n    \"\"\" requires host object \"\"\"\n    hvars = host.get_vars()\n\n    if self.get_option('use_vars_plugins'):\n        hvars = combine_vars(hvars, get_vars_from_inventory_sources(loader, sources, [host], 'all'))\n\n    return hvars", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\constructed.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "cache.contains", "cache.get", "cache_loader.get", "combine_vars", "self._add_host_to_composed_groups", "self._add_host_to_keyed_groups", "self._read_config_data", "self._set_composite_vars", "self.get_all_host_vars", "self.get_option", "super", "super(InventoryModule, self).parse"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, path, cache=False):\n    \"\"\" parses the inventory file \"\"\"\n\n    super(InventoryModule, self).parse(inventory, loader, path, cache=cache)\n\n    self._read_config_data(path)\n\n    sources = []\n    try:\n        sources = inventory.processed_sources\n    except AttributeError:\n        if self.get_option('use_vars_plugins'):\n            raise\n\n    strict = self.get_option('strict')\n\n    cache = cache_loader.get(C.CACHE_PLUGIN)\n\n    try:\n        # Go over hosts (less var copies)\n        for host in inventory.hosts:\n\n            # get available variables to templar\n            hostvars = self.get_all_host_vars(inventory.hosts[host], loader, sources)\n            if cache.contains(host):  # adds facts if cache is active\n                hostvars = combine_vars(hostvars, cache.get(host))\n\n            # create composite vars\n            self._set_composite_vars(self.get_option('compose'), hostvars, host, strict=strict)\n\n            # refetch host vars in case new ones have been created above\n            hostvars = self.get_all_host_vars(inventory.hosts[host], loader, sources)\n            if cache.contains(host):  # adds facts if cache is active\n                hostvars = combine_vars(hostvars, cache.get(host))\n\n            # constructed groups based on conditionals\n            self._add_host_to_composed_groups(self.get_option('groups'), hostvars, host, strict=strict, fetch_hostvars=False)\n\n            # constructed groups based variable values\n            self._add_host_to_keyed_groups(self.get_option('keyed_groups'), hostvars, host, strict=strict, fetch_hostvars=False)\n\n    except Exception as ex:\n        raise AnsibleParserError(f\"Failed to parse {path!r}.\") from ex", "loc": 43}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\generator.py", "class_name": "InventoryModule", "function_name": "template", "parameters": ["self", "pattern", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self.templar.copy_with_new_env", "self.templar.copy_with_new_env(available_variables=variables).template"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def template(self, pattern, variables):\n    # Allow pass-through of data structures for templating later (if applicable).\n    # This limitation was part of the original plugin implementation and was updated to maintain feature parity with the new templating API.\n    if not isinstance(pattern, str):\n        return pattern\n\n    return self.templar.copy_with_new_env(available_variables=variables).template(pattern)", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\generator.py", "class_name": "InventoryModule", "function_name": "add_parents", "parameters": ["self", "inventory", "child", "parents", "template_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "group.set_variable", "inventory.add_child", "inventory.add_group", "parent.get", "parent.get('vars', {}).items", "self.add_parents", "self.template"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_parents(self, inventory, child, parents, template_vars):\n    for parent in parents:\n        groupname = self.template(parent.get('name'), template_vars)\n        if not groupname:\n            raise AnsibleParserError(f\"Element {child} has a parent with no name.\")\n        if groupname not in inventory.groups:\n            inventory.add_group(groupname)\n        group = inventory.groups[groupname]\n        for (k, v) in parent.get('vars', {}).items():\n            group.set_variable(k, self.template(v, template_vars))\n        inventory.add_child(groupname, child)\n        self.add_parents(inventory, groupname, parent.get('parents', []), template_vars)", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\generator.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["config['hosts'].get", "config['layers'].keys", "config['layers'].values", "dict", "enumerate", "inventory.add_host", "product", "self._read_config_data", "self.add_parents", "self.template", "super", "super(InventoryModule, self).parse"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, path, cache=False):\n    \"\"\" parses the inventory file \"\"\"\n\n    super(InventoryModule, self).parse(inventory, loader, path, cache=cache)\n\n    config = self._read_config_data(path)\n\n    template_inputs = product(*config['layers'].values())\n    for item in template_inputs:\n        template_vars = dict()\n        for i, key in enumerate(config['layers'].keys()):\n            template_vars[key] = item[i]\n        host = self.template(config['hosts']['name'], template_vars)\n        inventory.add_host(host)\n        self.add_parents(inventory, host, config['hosts'].get('parents', []), template_vars)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\host_list.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "host_list", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "h.strip", "host_list.split", "parse_address", "self.display.vvv", "self.inventory.add_host", "super", "super(InventoryModule, self).parse", "to_native", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, host_list, cache=True):\n    \"\"\" parses the inventory file \"\"\"\n\n    super(InventoryModule, self).parse(inventory, loader, host_list)\n\n    try:\n        for h in host_list.split(','):\n            h = h.strip()\n            if h:\n                try:\n                    (host, port) = parse_address(h, allow_ranges=False)\n                except AnsibleError as e:\n                    self.display.vvv(\"Unable to parse address from hostname, leaving unchanged: %s\" % to_text(e))\n                    host = h\n                    port = None\n\n                if host not in self.inventory.hosts:\n                    self.inventory.add_host(host, group='ungrouped', port=port)\n    except Exception as e:\n        raise AnsibleParserError(\"Invalid data from string, could not parse: %s\" % to_native(e))", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\script.py", "class_name": null, "function_name": "detect_profile_name", "parameters": ["value"], "param_types": {"value": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "ValueError", "data.get", "isinstance", "json.loads", "meta.get", "native_type_name", "profile.startswith"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Detect (optional) JSON profile name from an inventory JSON document. Defaults to `inventory_legacy`.", "source_code": "def detect_profile_name(value: str) -> str:\n    \"\"\"\n    Detect (optional) JSON profile name from an inventory JSON document.\n    Defaults to `inventory_legacy`.\n    \"\"\"\n    try:\n        data = json.loads(value)\n    except Exception as ex:\n        raise ValueError('Value could not be parsed as JSON.') from ex\n\n    if not isinstance(data, dict):\n        raise TypeError(f'Value is {native_type_name(data)!r} instead of {native_type_name(dict)!r}.')\n\n    if (meta := data.get('_meta', ...)) is ...:\n        return _inventory_legacy.Decoder.profile_name\n\n    if not isinstance(meta, dict):\n        raise TypeError(f\"Value contains '_meta' which is {native_type_name(meta)!r} instead of {native_type_name(dict)!r}.\")\n\n    if (profile := meta.get('profile', ...)) is ...:\n        return _inventory_legacy.Decoder.profile_name\n\n    if not isinstance(profile, str):\n        raise TypeError(f\"Value contains '_meta.profile' which is {native_type_name(profile)!r} instead of {native_type_name(str)!r}.\")\n\n    if not profile.startswith('inventory_'):\n        raise ValueError(f\"Non-inventory profile {profile!r} is not allowed.\")\n\n    return profile", "loc": 29}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\script.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {"inventory": "InventoryData", "loader": "DataLoader", "path": "str", "cache": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleJSONParserError.handle_exception", "Origin", "TypeError", "data_from_meta.get", "detect_profile_name", "display.deprecated", "gdata.get", "get_decoder", "isinstance", "json.loads", "native_type_name", "processed.items", "run_command", "self._parse_group", "self._populate_host_vars", "self.display.error", "self.get_host_variables", "self.get_option", "self.set_options", "super", "super(InventoryModule, self).parse"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def parse(self, inventory: InventoryData, loader: DataLoader, path: str, cache: bool = False) -> None:\n    super(InventoryModule, self).parse(inventory, loader, path)\n\n    self.set_options()\n\n    origin = Origin(description=f'<inventory script output from {path!r}>')\n\n    data, stderr, stderr_help_text = run_command(path, ['--list'], origin)\n\n    try:\n        profile_name = detect_profile_name(data)\n        decoder = get_decoder(profile_name)\n    except Exception as ex:\n        raise AnsibleError(\n            message=\"Unable to get JSON decoder for inventory script result.\",\n            help_text=stderr_help_text,\n            # obj will be added by inventory manager\n        ) from ex\n\n    try:\n        try:\n            processed = json.loads(data, cls=decoder)\n        except Exception as json_ex:\n            AnsibleJSONParserError.handle_exception(json_ex, origin)\n    except Exception as ex:\n        raise AnsibleError(\n            message=\"Inventory script result could not be parsed as JSON.\",\n            help_text=stderr_help_text,\n            # obj will be added by inventory manager\n        ) from ex\n\n    # if no other errors happened, and you want to force displaying stderr, do so now\n    if stderr and self.get_option('always_show_stderr'):\n        self.display.error(msg=stderr)\n\n    data_from_meta: dict | None = None\n\n    # A \"_meta\" subelement may contain a variable \"hostvars\" which contains a hash for each host\n    # if this \"hostvars\" exists at all then do not call --host for each # host.\n    # This is for efficiency and scripts should still return data\n    # if called with --host for backwards compat with 1.2 and earlier.\n    for (group, gdata) in processed.items():\n        if group == '_meta':\n            data_from_meta = gdata.get('hostvars')\n\n            if not isinstance(data_from_meta, dict):\n                raise TypeError(f\"Value contains '_meta.hostvars' which is {native_type_name(data_from_meta)!r} instead of {native_type_name(dict)!r}.\")\n        else:\n            self._parse_group(group, gdata, origin)\n\n    if data_from_meta is None:\n        display.deprecated(\n            msg=\"Inventory scripts should always provide 'meta.hostvars'. \"\n                \"Host variables will be collected by running the inventory script with the '--host' option for each host.\",\n            version='2.23',\n            obj=origin,\n        )\n\n    for host in self._hosts:\n        if data_from_meta is None:\n            got = self.get_host_variables(path, host, origin)\n        else:\n            got = data_from_meta.get(host, {})\n\n        self._populate_host_vars([host], got)", "loc": 65}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\script.py", "class_name": "InventoryModule", "function_name": "get_host_variables", "parameters": ["path", "host", "origin"], "param_types": {"path": "str", "host": "str", "origin": "Origin"}, "return_type": "dict", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsibleJSONParserError.handle_exception", "data.strip", "json.loads", "origin.replace", "run_command"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Runs <script> --host <hostname>, to determine additional host variables.", "source_code": "def get_host_variables(path: str, host: str, origin: Origin) -> dict:\n    \"\"\"Runs <script> --host <hostname>, to determine additional host variables.\"\"\"\n    origin = origin.replace(description=f'{origin.description} for host {host!r}')\n\n    data, stderr, stderr_help_text = run_command(path, ['--host', host], origin)\n\n    if not data.strip():\n        return {}\n\n    try:\n        try:\n            # Use standard legacy trust inversion here.\n            # Unlike the normal inventory output, everything here is considered a variable and thus supports trust (and trust inversion).\n            processed = json.loads(data, cls=_legacy.Decoder)\n        except Exception as json_ex:\n            AnsibleJSONParserError.handle_exception(json_ex, origin)\n    except Exception as ex:\n        raise AnsibleError(\n            message=f\"Inventory script result for host {host!r} could not be parsed as JSON.\",\n            help_text=stderr_help_text,\n            # obj will be added by inventory manager\n        ) from ex\n\n    return processed", "loc": 24}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\toml.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "data.get", "self._load_file", "self._parse_group", "self.set_options", "super", "super(InventoryModule, self).parse"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, path, cache=True):\n    \"\"\" parses the inventory file \"\"\"\n    super(InventoryModule, self).parse(inventory, loader, path)\n    self.set_options()\n\n    try:\n        data = self._load_file(path)\n    except Exception as e:\n        raise AnsibleParserError(e)\n\n    if not data:\n        raise AnsibleParserError('Parsed empty TOML file')\n    elif data.get('plugin'):\n        raise AnsibleParserError('Plugin configuration TOML file, not TOML inventory')\n\n    for group_name in data:\n        self._parse_group(group_name, data[group_name])", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\yaml.py", "class_name": "InventoryModule", "function_name": "parse", "parameters": ["self", "inventory", "loader", "path", "cache"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "data.get", "isinstance", "self._parse_group", "self.loader.load_from_file", "self.set_options", "super", "super(InventoryModule, self).parse", "to_native", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "parses the inventory file", "source_code": "def parse(self, inventory, loader, path, cache=True):\n    \"\"\" parses the inventory file \"\"\"\n\n    super(InventoryModule, self).parse(inventory, loader, path)\n    self.set_options()\n\n    try:\n        data = self.loader.load_from_file(path, cache='none', trusted_as_template=True)\n    except Exception as e:\n        raise AnsibleParserError(e)\n\n    if not data:\n        raise AnsibleParserError('Parsed empty YAML file')\n    elif not isinstance(data, MutableMapping):\n        raise AnsibleParserError('YAML inventory has invalid structure, it should be a dictionary, got: %s' % type(data))\n    elif data.get('plugin'):\n        raise AnsibleParserError('Plugin configuration YAML file, not YAML inventory')\n\n    # We expect top level keys to correspond to groups, iterate over them\n    # to get host, vars and subgroups (which we iterate over recursively)\n    if isinstance(data, MutableMapping):\n        for group_name in data:\n            self._parse_group(group_name, data[group_name])\n    else:\n        raise AnsibleParserError(\"Invalid data from file, expected dictionary and got:\\n\\n%s\" % to_native(data))", "loc": 25}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\__init__.py", "class_name": null, "function_name": "expand_hostname_range", "parameters": ["line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "AnsibleError", "all_hosts.append", "all_hosts.extend", "detect_range", "expand_hostname_range", "fill", "int", "len", "line.replace", "line.replace('[', '|', 1).replace", "line.replace('[', '|', 1).replace(']', '|', 1).split", "list", "nrange.split", "range", "str", "str(x).zfill", "string.ascii_letters.index"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "A helper function that expands a given line that contains a pattern specified in top docstring, and returns a list that consists of the expanded version.", "source_code": "def expand_hostname_range(line=None):\n    \"\"\"\n    A helper function that expands a given line that contains a pattern\n    specified in top docstring, and returns a list that consists of the\n    expanded version.\n\n    The '[' and ']' characters are used to maintain the pseudo-code\n    appearance. They are replaced in this function with '|' to ease\n    string splitting.\n\n    References: https://docs.ansible.com/ansible/latest/user_guide/intro_inventory.html#hosts-and-groups\n    \"\"\"\n    all_hosts = []\n    if line:\n        # A hostname such as db[1:6]-node is considered to consists\n        # three parts:\n        # head: 'db'\n        # nrange: [1:6]; range() is a built-in. Can't use the name\n        # tail: '-node'\n\n        # Add support for multiple ranges in a host so:\n        # db[01:10:3]node-[01:10]\n        # - to do this we split off at the first [...] set, getting the list\n        #   of hosts and then repeat until none left.\n        # - also add an optional third parameter which contains the step. (Default: 1)\n        #   so range can be [01:10:2] -> 01 03 05 07 09\n\n        (head, nrange, tail) = line.replace('[', '|', 1).replace(']', '|', 1).split('|')\n        bounds = nrange.split(\":\")\n        if len(bounds) != 2 and len(bounds) != 3:\n            raise AnsibleError(\"host range must be begin:end or begin:end:step\")\n        beg = bounds[0]\n        end = bounds[1]\n        if len(bounds) == 2:\n            step = 1\n        else:\n            step = bounds[2]\n        if not beg:\n            beg = \"0\"\n        if not end:\n            raise AnsibleError(\"host range must specify end value\")\n        if beg[0] == '0' and len(beg) > 1:\n            rlen = len(beg)  # range length formatting hint\n            if rlen != len(end):\n                raise AnsibleError(\"host range must specify equal-length begin and end formats\")\n\n            def fill(x):\n                return str(x).zfill(rlen)  # range sequence\n\n        else:\n            fill = str\n\n        try:\n            i_beg = string.ascii_letters.index(beg)\n            i_end = string.ascii_letters.index(end)\n            if i_beg > i_end:\n                raise AnsibleError(\"host range must have begin <= end\")\n            seq = list(string.ascii_letters[i_beg:i_end + 1:int(step)])\n        except ValueError:  # not an alpha range\n            seq = range(int(beg), int(end) + 1, int(step))\n\n        for rseq in seq:\n            hname = ''.join((head, fill(rseq), tail))\n\n            if detect_range(hname):\n                all_hosts.extend(expand_hostname_range(hname))\n            else:\n                all_hosts.append(hname)\n\n        return all_hosts", "loc": 70}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\__init__.py", "class_name": null, "function_name": "get_cache_plugin", "parameters": ["plugin_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Unable to use cache plugin {0} for inventory. Cache options were provided but may not reconcile correctly unless set via set_options. Refer to the porting guide if the plugin derives user settings from ansible.constants.'.format", "AnsibleError", "CachePluginAdjudicator", "getattr", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cache_plugin(plugin_name, **kwargs):\n    if not plugin_name:\n        raise AnsibleError(\"A cache plugin must be configured to use inventory caching.\")\n\n    try:\n        cache = CachePluginAdjudicator(plugin_name, **kwargs)\n    except AnsibleError as e:\n        if 'fact_caching_connection' in to_native(e):\n            raise AnsibleError(\"error, '%s' inventory cache plugin requires the one of the following to be set \"\n                               \"to a writeable directory path:\\nansible.cfg:\\n[default]: fact_caching_connection,\\n\"\n                               \"[inventory]: cache_connection;\\nEnvironment:\\nANSIBLE_INVENTORY_CACHE_CONNECTION,\\n\"\n                               \"ANSIBLE_CACHE_PLUGIN_CONNECTION.\" % plugin_name)\n        else:\n            raise\n\n    if cache._plugin.ansible_name != 'ansible.builtin.memory' and kwargs and not getattr(cache._plugin, '_options', None):\n        raise AnsibleError('Unable to use cache plugin {0} for inventory. Cache options were provided but may not reconcile '\n                           'correctly unless set via set_options. Refer to the porting guide if the plugin derives user settings '\n                           'from ansible.constants.'.format(plugin_name))\n    return cache", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\inventory\\__init__.py", "class_name": "Cacheable", "function_name": "load_cache_plugin", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "get_cache_plugin", "self.get_option"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_cache_plugin(self) -> None:\n    plugin_name = self.get_option('cache_plugin')\n    cache_option_keys = [('_uri', 'cache_connection'), ('_timeout', 'cache_timeout'), ('_prefix', 'cache_prefix')]\n    cache_options = dict((opt[0], self.get_option(opt[1])) for opt in cache_option_keys if self.get_option(opt[1]) is not None)\n    self._cache = get_cache_plugin(plugin_name, **cache_options)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\config.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_get_global_config", "_get_plugin_config", "isinstance", "ret.append", "self._display.warning", "self.get_option", "self.set_options", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    self.set_options(var_options=variables, direct=kwargs)\n\n    missing = self.get_option('on_missing')\n    ptype = self.get_option('plugin_type')\n    pname = self.get_option('plugin_name')\n    show_origin = self.get_option('show_origin')\n\n    if (ptype or pname) and not (ptype and pname):\n        raise AnsibleError('Both plugin_type and plugin_name are required, cannot use one without the other.')\n\n    ret = []\n\n    for term in terms:\n        if not isinstance(term, str):\n            raise AnsibleError(f'Invalid setting identifier, {term!r} is not a {str}, its a {type(term)}.')\n\n        result = Sentinel\n        origin = None\n        try:\n            if pname:\n                result, origin = _get_plugin_config(pname, ptype, term, variables)\n            else:\n                result = _get_global_config(term)\n        except AnsibleUndefinedConfigEntry:\n            if missing == 'error':\n                raise\n            elif missing == 'warn':\n                self._display.warning(f\"Skipping, did not find setting {term!r}.\")\n            elif missing == 'skip':\n                pass  # this is not needed, but added to have all 3 options stated\n\n        if result is not Sentinel:\n            if show_origin:\n                ret.append([result, origin])\n            else:\n                ret.append(result)\n    return ret", "loc": 39}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\csvfile.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "ValueError", "isinstance", "kv.items", "parse_kv", "ret.append", "ret.extend", "self._deprecate_inline_kv", "self.find_file_in_search_path", "self.get_options", "self.read_csv", "self.set_option", "self.set_options"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    ret = []\n\n    self.set_options(var_options=variables, direct=kwargs)\n\n    # populate options\n    paramvals = self.get_options()\n\n    if not terms:\n        raise AnsibleError('Search key is required but was not found')\n\n    for term in terms:\n        kv = parse_kv(term)\n\n        if '_raw_params' not in kv:\n            raise AnsibleError('Search key is required but was not found')\n\n        key = kv['_raw_params']\n\n        # parameters override per term using k/v\n        reset_params = False\n        for name, value in kv.items():\n            if name == '_raw_params':\n                continue\n            if name not in paramvals:\n                raise ValueError(f'{name!r} is not a valid option')\n\n            self._deprecate_inline_kv()\n            self.set_option(name, value)\n            reset_params = True\n\n        if reset_params:\n            paramvals = self.get_options()\n\n        # default is just placeholder for real tab\n        if paramvals['delimiter'] == 'TAB':\n            paramvals['delimiter'] = \"\\t\"\n\n        lookupfile = self.find_file_in_search_path(variables, 'files', paramvals['file'])\n        var = self.read_csv(lookupfile, key, paramvals['delimiter'], paramvals['encoding'], paramvals['default'], paramvals['col'], paramvals['keycol'])\n        if var is not None:\n            if isinstance(var, MutableSequence):\n                ret.extend(var)\n            else:\n                ret.append(var)\n\n    return ret", "loc": 47}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\dict.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "isinstance", "results.extend", "self._flatten_hash_to_list", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    results = []\n    for term in terms:\n        # Expect any type of Mapping, notably hostvars\n        if not isinstance(term, Mapping):\n            raise AnsibleError(f\"the 'dict' lookup plugin expects a dictionary, got '{term}' of type {type(term)})\")\n\n        results.extend(self._flatten_hash_to_list(term))\n    return results", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\env.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Origin", "Origin(description=f'<environment variable {var!r}>').try_tag", "_undef", "os.environ.get", "ret.append", "self.get_option", "self.set_options", "term.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    self.set_options(var_options=variables, direct=kwargs)\n\n    ret = []\n    d = self.get_option('default')\n\n    for term in terms:\n        var = term.split()[0]\n        val = os.environ.get(var, d)\n\n        if val is _DEFAULT_UNDEF:\n            val = _undef(f'The environment variable {var!r} is not set.')\n        else:\n            val = Origin(description=f\"<environment variable {var!r}>\").try_tag(val)\n\n        ret.append(val)\n\n    return ret", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\file.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "contents.lstrip", "contents.rstrip", "display.debug", "display.vvvv", "ret.append", "self._loader.get_text_file_contents", "self.find_file_in_search_path", "self.get_option", "self.set_options"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    ret = []\n    self.set_options(var_options=variables, direct=kwargs)\n\n    for term in terms:\n        display.debug(\"File lookup term: %s\" % term)\n        # Find the file in the expected search path\n        try:\n            lookupfile = self.find_file_in_search_path(variables, 'files', term, ignore_missing=True)\n            display.vvvv(u\"File lookup using %s as file\" % lookupfile)\n            if lookupfile:\n                contents = self._loader.get_text_file_contents(lookupfile)\n                if self.get_option('lstrip'):\n                    contents = contents.lstrip()\n                if self.get_option('rstrip'):\n                    contents = contents.rstrip()\n                ret.append(contents)\n            else:\n                # TODO: only add search info if abs path?\n                raise AnsibleError(\"File not found. Use -vvvvv to see paths searched.\")\n        except AnsibleError as ex:\n            raise AnsibleError(f\"Unable to access the file {term!r}.\") from ex\n\n    return ret", "loc": 25}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\first_found.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {"terms": "list"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_jinja_plugins._LookupContext.current", "_recurse_terms", "_task.TaskContext.current", "_template.get_first_marker_arg", "first_marker.trip", "self._process_terms", "self.find_file_in_search_path", "self.get_option", "self.set_options", "unfrackpath"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms: list, variables=None, **kwargs):\n    if (first_marker := _template.get_first_marker_arg((), kwargs)) is not None:\n        first_marker.trip()\n\n    if _jinja_plugins._LookupContext.current().invoked_as_with:\n        # we're being invoked by TaskExecutor.get_loop_items(), special backwards compatibility behavior\n        terms = _recurse_terms(terms, omit_undefined=True)  # recursively drop undefined values from terms for backwards compatibility\n\n        # invoked_as_with shouldn't be possible outside a TaskContext\n        te_action = _task.TaskContext.current().task.action  # FIXME: this value has not been templated, it should be (historical problem)...\n\n        # based on the presence of `var`/`template`/`file` in the enclosing task action name, choose a subdir to search\n        for subdir in ['template', 'var', 'file']:\n            if subdir in te_action:\n                break\n\n        subdir += \"s\"  # convert to the matching directory name\n    else:\n        terms = _recurse_terms(terms, omit_undefined=False)  # undefined values are only omitted when invoked using `with`\n\n        subdir = None\n\n    self.set_options(var_options=variables, direct=kwargs)\n\n    if not terms:\n        terms = self.get_option('files')\n\n    total_search = self._process_terms(terms, variables)\n\n    # NOTE: during refactor noticed that the 'using a dict' as term\n    # is designed to only work with 'one' otherwise inconsistencies will appear.\n    # see other notes below.\n\n    for fn in total_search:\n        # get subdir if set by task executor, default to files otherwise\n        path = self.find_file_in_search_path(variables, subdir, fn, ignore_missing=True)\n\n        # exit if we find one!\n        if path is not None:\n            return [unfrackpath(path, follow=False)]\n\n    skip = self.get_option('skip')\n\n    # if we get here, no file was found\n    if skip:\n        # NOTE: global skip won't matter, only last 'skip' value in dict term\n        return []\n\n    raise AnsibleError(\"No file was found when using first_found.\")", "loc": 49}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\ini.py", "class_name": "LookupModule", "function_name": "get_value", "parameters": ["self", "key", "section", "dflt", "is_regexp"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.match", "self.cp.get", "self.cp.items"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_value(self, key, section, dflt, is_regexp):\n    # Retrieve all values from a section using a regexp\n    if is_regexp:\n        return [v for k, v in self.cp.items(section) if re.match(key, k)]\n    value = None\n    # Retrieve a single value\n    try:\n        value = self.cp.get(section, key)\n    except configparser.NoOptionError:\n        return dflt\n    return value", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\ini.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "StringIO", "ValueError", "_parse_params", "config.seek", "config.write", "configparser.BasicInterpolation", "configparser.ConfigParser", "isinstance", "param.split", "paramvals.get", "ret.append", "self._deprecate_inline_kv", "self._loader.get_text_file_contents", "self.cp.read_file", "self.find_file_in_search_path", "self.get_options", "self.get_value", "self.set_option", "self.set_options", "term.strip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    self.set_options(var_options=variables, direct=kwargs)\n    paramvals = self.get_options()\n\n    self.cp = configparser.ConfigParser(\n        allow_no_value=paramvals.get('allow_no_value', paramvals.get('allow_none')),\n        interpolation=configparser.BasicInterpolation() if paramvals.get('interpolation') else None,\n    )\n    if paramvals['case_sensitive']:\n        self.cp.optionxform = to_native\n\n    ret = []\n    for term in terms:\n\n        key = term\n        # parameters specified?\n        if '=' in term or ' ' in term.strip():\n            self._deprecate_inline_kv()\n            params = _parse_params(term, paramvals)\n            param = None\n            try:\n                updated_key = False\n                updated_options = False\n                for param in params:\n                    if '=' in param:\n                        name, value = param.split('=')\n                        if name not in paramvals:\n                            raise AnsibleError(f\"{name!r} is not a valid option.\")\n                        self.set_option(name, value)\n                        updated_options = True\n                    elif key == term:\n                        # only take first, this format never supported multiple keys inline\n                        key = param\n                        updated_key = True\n                if updated_options:\n                    paramvals = self.get_options()\n            except ValueError as ex:\n                # bad params passed\n                raise ValueError(f\"Could not use {param!r} from {params!r}.\") from ex\n            if not updated_key:\n                raise ValueError(f\"No key to look up was provided as first term within string inline options: {term}\")\n                # only passed options in inline string\n\n        # TODO: look to use cache to avoid redoing this for every term if they use same file\n        # Retrieve file path\n        path = self.find_file_in_search_path(variables, 'files', paramvals['file'])\n\n        # Create StringIO later used to parse ini\n        config = StringIO()\n        # Special case for java properties\n        if paramvals['type'] == \"properties\":\n            config.write(u'[java_properties]\\n')\n            paramvals['section'] = 'java_properties'\n\n        contents = self._loader.get_text_file_contents(path, encoding=paramvals['encoding'])\n        config.write(contents)\n        config.seek(0, os.SEEK_SET)\n\n        try:\n            self.cp.read_file(config)\n        except configparser.DuplicateOptionError as ex:\n            raise ValueError(f\"Duplicate option in {paramvals['file']!r}.\") from ex\n\n        try:\n            var = self.get_value(key, paramvals['section'], paramvals['default'], paramvals['re'])\n        except configparser.NoSectionError:\n            raise ValueError(f\"No section {paramvals['section']!r} in {paramvals['file']!r}.\") from None\n\n        if var is not None:\n            if isinstance(var, MutableSequence):\n                for v in var:\n                    ret.append(v)\n            else:\n                ret.append(var)\n\n    return ret", "loc": 77}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\inventory_hostnames.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["InventoryManager", "manager.add_group", "manager.add_host", "manager.get_hosts", "variables['groups'].items"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    manager = InventoryManager(self._loader, parse=False)\n    for group, hosts in variables['groups'].items():\n        manager.add_group(group)\n        for host in hosts:\n            manager.add_host(host, group=group)\n\n    try:\n        return [h.name for h in manager.get_hosts(pattern=terms)]\n    except AnsibleError:\n        return []", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\nested.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "len", "my_list.pop", "my_list.reverse", "new_result.append", "self._combine", "self._flatten"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    my_list = terms[:]\n    my_list.reverse()\n\n    if len(my_list) == 0:\n        raise AnsibleError(\"with_nested requires at least one element in the nested list\")\n    result = my_list.pop()\n    while len(my_list) > 0:\n        result2 = self._combine(result, my_list.pop())\n        result = result2\n    new_result = []\n    for x in result:\n        new_result.append(self._flatten(x))\n    return new_result", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\password.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_format_content", "_gen_candidate_chars", "_get_lock", "_parse_content", "_read_password_file", "_release_lock", "_write_password_file", "do_encrypt", "random_password", "random_salt", "ret.append", "self._loader.path_dwim", "self._parse_parameters", "self.set_options", "to_bytes"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    ret = []\n\n    for term in terms:\n\n        self.set_options(var_options=variables, direct=kwargs)\n\n        changed = None\n        relpath, params = self._parse_parameters(term)\n        path = self._loader.path_dwim(relpath)\n        b_path = to_bytes(path, errors='surrogate_or_strict')\n        chars = _gen_candidate_chars(params['chars'])\n        ident = None\n        first_process = None\n        lockfile = None\n\n        try:\n            # make sure only one process finishes all the job first\n            first_process, lockfile = _get_lock(b_path)\n\n            content = _read_password_file(b_path)\n\n            if content is None or b_path == to_bytes('/dev/null'):\n                plaintext_password = random_password(params['length'], chars, params['seed'])\n                salt = None\n                changed = True\n            else:\n                plaintext_password, salt, ident = _parse_content(content)\n\n            encrypt = params['encrypt']\n            if encrypt and not salt:\n                changed = True\n                try:\n                    salt = random_salt(BaseHash.algorithms[encrypt].salt_size)\n                except KeyError:\n                    salt = random_salt()\n\n            if not ident:\n                ident = params['ident']\n            elif params['ident'] and ident != params['ident']:\n                raise AnsibleError('The ident parameter provided (%s) does not match the stored one (%s).' % (ident, params['ident']))\n\n            if encrypt and not ident:\n                try:\n                    ident = BaseHash.algorithms[encrypt].implicit_ident\n                except KeyError:\n                    ident = None\n                if ident:\n                    changed = True\n\n            if changed and b_path != to_bytes('/dev/null'):\n                content = _format_content(plaintext_password, salt, encrypt=encrypt, ident=ident)\n                _write_password_file(b_path, content)\n\n        finally:\n            if first_process:\n                # let other processes continue\n                _release_lock(lockfile)\n\n        if encrypt:\n            password = do_encrypt(plaintext_password, encrypt, salt=salt, ident=ident)\n            ret.append(password)\n        else:\n            ret.append(plaintext_password)\n\n    return ret", "loc": 66}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\random_choice.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "secrets.choice", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    ret = terms\n    if terms:\n        try:\n            ret = [secrets.choice(terms)]\n        except Exception as e:\n            raise AnsibleError(\"Unable to choose random term: %s\" % to_native(e))\n\n    return ret", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\sequence.py", "class_name": "LookupModule", "function_name": "parse_kv_args", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "args.keys", "args.pop", "list", "self.set_option"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "parse key-value style arguments", "source_code": "def parse_kv_args(self, args):\n    \"\"\"parse key-value style arguments\"\"\"\n    for arg in FIELDS:\n        value = args.pop(arg, None)\n        if value is not None:\n            self.set_option(arg, value)\n    if args:\n        raise AnsibleError(\n            \"unrecognized arguments to with_sequence: %s\"\n            % list(args.keys())\n        )", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\sequence.py", "class_name": "LookupModule", "function_name": "parse_simple_args", "parameters": ["self", "term"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SHORTCUT.match", "locals", "locals().get", "match.groups", "self.set_option"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "parse the shortcut forms, return True/False", "source_code": "def parse_simple_args(self, term):\n    \"\"\"parse the shortcut forms, return True/False\"\"\"\n    match = SHORTCUT.match(term)\n    if not match:\n        return False\n\n    dummy, start, end, dummy, stride, dummy, format = match.groups()\n\n    for key in FIELDS:\n        value = locals().get(key, None)\n        if value is not None:\n            self.set_option(key, value)\n\n    return True", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\sequence.py", "class_name": "LookupModule", "function_name": "sanity_check", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "self.format.count"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sanity_check(self):\n    \"\"\"\n    Returns True if options comprise a valid sequence expression\n    Raises AnsibleError if options are an invalid expression\n    Returns false if options are valid but result in an empty sequence - these cases do not raise exceptions\n    in order to maintain historic behavior\n    \"\"\"\n    if self.count is None and self.end is None:\n        raise AnsibleError(\"must specify count or end in with_sequence\")\n    elif self.count is not None and self.end is not None:\n        raise AnsibleError(\"can't specify both count and end in with_sequence\")\n    elif self.count is not None:\n        # convert count to end\n        if self.count != 0:\n            self.end = self.start + self.count * self.stride - 1\n        else:\n            return False\n    if self.stride > 0 and self.end < self.start:\n        raise AnsibleError(\"to count backwards make stride negative\")\n    if self.stride < 0 and self.end > self.start:\n        raise AnsibleError(\"to count forward don't make stride negative\")\n    if self.stride == 0:\n        return False\n    if self.format.count('%') != 1:\n        raise AnsibleError(\"bad formatting string: %s\" % self.format)\n\n    return True", "loc": 27}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\sequence.py", "class_name": "LookupModule", "function_name": "generate_sequence", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "range"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate_sequence(self):\n    if self.stride >= 0:\n        adjust = 1\n    else:\n        adjust = -1\n    numbers = range(self.start, self.end + adjust, self.stride)\n\n    for i in numbers:\n        try:\n            formatted = self.format % i\n            yield formatted\n        except (ValueError, TypeError):\n            raise AnsibleError(\n                \"problem formatting %r with %r\" % (i, self.format)\n            )", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\sequence.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "parse_kv", "results.extend", "self.generate_sequence", "self.parse_kv_args", "self.parse_simple_args", "self.sanity_check", "self.set_fields", "self.set_options"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    results = []\n\n    if kwargs and not terms:\n        # All of the necessary arguments can be provided as keywords, but we still need something to loop over\n        terms = ['']\n\n    for term in terms:\n        try:\n            # set defaults/global\n            self.set_options(direct=kwargs)\n            try:\n                if not self.parse_simple_args(term):\n                    self.parse_kv_args(parse_kv(term))\n            except AnsibleError:\n                raise\n            except Exception as e:\n                raise AnsibleError(\"unknown error parsing with_sequence arguments: %r. Error was: %s\" % (term, e))\n\n            self.set_fields()\n            if self.sanity_check():\n                results.extend(self.generate_sequence())\n\n        except AnsibleError:\n            raise\n        except Exception as e:\n            raise AnsibleError(\n                \"unknown error generating sequence: %s\" % e\n            )\n\n    return results", "loc": 31}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\subelements.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_raise_terms_error", "all", "boolean", "elementlist.append", "flags.get", "isinstance", "item0.get", "len", "ret.append", "subvalue.pop", "terms[0].get", "terms[1].split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    def _raise_terms_error(msg=\"\"):\n        raise AnsibleError(\n            \"subelements lookup expects a list of two or three items, \" + msg)\n\n    # check lookup terms - check number of terms\n    if not isinstance(terms, list) or not 2 <= len(terms) <= 3:\n        _raise_terms_error()\n\n    # first term should be a list (or dict), second a string holding the subkey\n    if not isinstance(terms[0], (list, dict)) or not isinstance(terms[1], str):\n        _raise_terms_error(\"first a dict or a list, second a string pointing to the subkey\")\n    subelements = terms[1].split(\".\")\n\n    if isinstance(terms[0], dict):  # convert to list:\n        if terms[0].get('skipped', False) is not False:\n            # the registered result was completely skipped\n            return []\n        elementlist = []\n        for key in terms[0]:\n            elementlist.append(terms[0][key])\n    else:\n        elementlist = terms[0]\n\n    # check for optional flags in third term\n    flags = {}\n    if len(terms) == 3:\n        flags = terms[2]\n    if not isinstance(flags, dict) and not all(isinstance(key, str) and key in FLAGS for key in flags):\n        _raise_terms_error(\"the optional third item must be a dict with flags %s\" % FLAGS)\n\n    # build_items\n    ret = []\n    for item0 in elementlist:\n        if not isinstance(item0, dict):\n            raise AnsibleError(\"subelements lookup expects a dictionary, got '%s'\" % item0)\n        if item0.get('skipped', False) is not False:\n            # this particular item is to be skipped\n            continue\n\n        skip_missing = boolean(flags.get('skip_missing', False), strict=False)\n        subvalue = item0\n        lastsubkey = False\n        sublist = []\n        for subkey in subelements:\n            if subkey == subelements[-1]:\n                lastsubkey = True\n            if subkey not in subvalue:\n                if skip_missing:\n                    continue\n                else:\n                    raise AnsibleError(\"could not find '%s' key in iterated item '%s'\" % (subkey, subvalue))\n            if not lastsubkey:\n                if not isinstance(subvalue[subkey], dict):\n                    if skip_missing:\n                        continue\n                    else:\n                        raise AnsibleError(\"the key %s should point to a dictionary, got '%s'\" % (subkey, subvalue[subkey]))\n                else:\n                    subvalue = subvalue[subkey]\n            else:  # lastsubkey\n                if not isinstance(subvalue[subkey], list):\n                    raise AnsibleError(\"the key %s should point to a list, got '%s'\" % (subkey, subvalue[subkey]))\n                else:\n                    sublist = subvalue.pop(subkey, [])\n        for item1 in sublist:\n            ret.append((item0, item1))\n\n    return ret", "loc": 70}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\together.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "itertools.zip_longest", "len", "self._flatten"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    my_list = terms[:]\n    if len(my_list) == 0:\n        raise AnsibleError(\"with_together requires at least one element in each list\")\n\n    return [self._flatten(x) for x in itertools.zip_longest(*my_list, fillvalue=None)]", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\unvault.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "display.debug", "display.vvvv", "ret.append", "self._loader.get_text_file_contents", "self.find_file_in_search_path", "self.set_options"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    ret = []\n\n    self.set_options(var_options=variables, direct=kwargs)\n\n    for term in terms:\n        display.debug(\"Unvault lookup term: %s\" % term)\n\n        # Find the file in the expected search path\n        lookupfile = self.find_file_in_search_path(variables, 'files', term)\n        display.vvvv(u\"Unvault lookup found %s\" % lookupfile)\n        if lookupfile:\n            ret.append(self._loader.get_text_file_contents(lookupfile))\n        else:\n            raise AnsibleParserError('Unable to find file matching \"%s\" ' % term)\n\n    return ret", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\varnames.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "isinstance", "list", "name.search", "re.compile", "ret.append", "self.set_options", "to_native", "type", "variables.keys"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n\n    if variables is None:\n        raise AnsibleError('No variables available to search')\n\n    self.set_options(var_options=variables, direct=kwargs)\n\n    ret = []\n    variable_names = list(variables.keys())\n    for term in terms:\n\n        if not isinstance(term, str):\n            raise AnsibleError('Invalid setting identifier, \"%s\" is not a string, it is a %s' % (term, type(term)))\n\n        try:\n            name = re.compile(term)\n        except Exception as e:\n            raise AnsibleError('Unable to use \"%s\" as a search parameter: %s' % (term, to_native(e)))\n\n        for varname in variable_names:\n            if name.search(varname):\n                ret.append(varname)\n\n    return ret", "loc": 24}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\vars.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTypeError", "_jinja_bits._undef", "isinstance", "native_type_name", "ret.append", "self._templar._engine.template", "self.get_option", "self.set_options"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables, **kwargs):\n    self.set_options(var_options=variables, direct=kwargs)\n\n    default = self.get_option('default')\n\n    ret = []\n\n    for term in terms:\n        if not isinstance(term, str):\n            raise AnsibleTypeError(f'Variable name must be {native_type_name(str)!r} not {native_type_name(term)!r}.', obj=term)\n\n        try:\n            value = variables[term]\n        except KeyError:\n            if default is None:\n                value = _jinja_bits._undef(f'No variable named {term!r} was found.')\n            else:\n                value = default\n\n        ret.append(value)\n\n    return self._templar._engine.template(ret)", "loc": 22}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\__init__.py", "class_name": "LookupBase", "function_name": "get_basedir", "parameters": ["self", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._loader.get_basedir"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_basedir(self, variables):\n    if 'role_path' in variables:\n        return variables['role_path']\n    else:\n        return self._loader.get_basedir()", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\lookup\\__init__.py", "class_name": "LookupBase", "function_name": "find_file_in_search_path", "parameters": ["self", "myvars", "subdir", "needle", "ignore_missing"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "self._display.warning", "self._loader.path_dwim_relative_stack", "self.get_basedir"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Return a file (needle) in the task's expected search path.", "source_code": "def find_file_in_search_path(self, myvars, subdir, needle, ignore_missing=False):\n    \"\"\"\n    Return a file (needle) in the task's expected search path.\n    \"\"\"\n\n    if 'ansible_search_path' in myvars:\n        paths = myvars['ansible_search_path']\n    else:\n        paths = [self.get_basedir(myvars)]\n\n    result = None\n    try:\n        result = self._loader.path_dwim_relative_stack(paths, subdir, needle, is_role=bool('role_path' in myvars))\n    except AnsibleFileNotFound:\n        if not ignore_missing:\n            self._display.warning(\"Unable to find '%s' in expected paths (use -vvvvv to see paths)\" % needle)\n\n    return result", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": null, "function_name": "ensure_ncclient", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "func", "missing_required_lib", "to_native", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ensure_ncclient(func):\n    @wraps(func)\n    def wrapped(self, *args, **kwargs):\n        if not HAS_NCCLIENT:\n            raise AnsibleError(\"%s: %s\" % (missing_required_lib('ncclient'), to_native(NCCLIENT_IMP_ERR)))\n        return func(self, *args, **kwargs)\n    return wrapped", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "rpc", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {"name": "Name of rpc in string format"}, "return_doc": "Received rpc response from remote host", "raises_doc": [], "called_functions": ["Exception", "hasattr", "self.m.rpc", "to_ele", "to_xml"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "RPC to be execute on remote device", "source_code": "def rpc(self, name):\n    \"\"\"\n    RPC to be execute on remote device\n    :param name: Name of rpc in string format\n    :return: Received rpc response from remote host\n    \"\"\"\n    try:\n        obj = to_ele(name)\n        resp = self.m.rpc(obj)\n        return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml\n    except RPCError as exc:\n        msg = exc.xml\n        raise Exception(to_xml(msg))", "loc": 13}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "get_config", "parameters": ["self", "source", "filter"], "param_types": {}, "return_type": null, "param_doc": {"source": "Name of the configuration datastore being queried, defaults to running datastore", "filter": "This argument specifies the portion of the configuration data to retrieve"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "isinstance", "self.m.get_config", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Retrieve all or part of a specified configuration (by default entire configuration is retrieved).", "source_code": "def get_config(self, source=None, filter=None):\n    \"\"\"\n    Retrieve all or part of a specified configuration\n    (by default entire configuration is retrieved).\n    :param source: Name of the configuration datastore being queried, defaults to running datastore\n    :param filter: This argument specifies the portion of the configuration data to retrieve\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    if isinstance(filter, list):\n        filter = tuple(filter)\n\n    if not source:\n        source = 'running'\n    resp = self.m.get_config(source=source, filter=filter)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "get", "parameters": ["self", "filter", "with_defaults"], "param_types": {}, "return_type": null, "param_doc": {"filter": "This argument specifies the portion of the state data to retrieve", "with_defaults": "defines an explicit method of retrieving default values"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "isinstance", "self.m.get", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Retrieve device configuration and state information.", "source_code": "def get(self, filter=None, with_defaults=None):\n    \"\"\"\n    Retrieve device configuration and state information.\n    :param filter: This argument specifies the portion of the state data to retrieve\n                   (by default entire state data is retrieved)\n    :param with_defaults: defines an explicit method of retrieving default values\n                          from the configuration\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    if isinstance(filter, list):\n        filter = tuple(filter)\n    resp = self.m.get(filter=filter, with_defaults=with_defaults)\n    response = resp.data_xml if hasattr(resp, 'data_xml') else resp.xml\n    return response", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "edit_config", "parameters": ["self", "config", "format", "target", "default_operation", "test_option", "error_option"], "param_types": {}, "return_type": null, "param_doc": {"config": "Is the configuration, which must be rooted in the `config` element.", "format": "The format of configuration eg. xml, text", "target": "Is the name of the configuration datastore being edited", "default_operation": "If specified must be one of { `\"merge\"`, `\"replace\"`, or `\"none\"` }", "test_option": "If specified must be one of { `\"test_then_set\"`, `\"set\"` }", "error_option": "If specified must be one of { `\"stop-on-error\"`, `\"continue-on-error\"`, `\"rollback-on-error\"` }"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["ValueError", "hasattr", "self.m.edit_config"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Loads all or part of the specified *config* to the *target* configuration datastore.", "source_code": "def edit_config(self, config=None, format='xml', target='candidate', default_operation=None, test_option=None, error_option=None):\n    \"\"\"\n    Loads all or part of the specified *config* to the *target* configuration datastore.\n    :param config: Is the configuration, which must be rooted in the `config` element.\n                   It can be specified either as a string or an :class:`~xml.etree.ElementTree.Element`.\n    :param format: The format of configuration eg. xml, text\n    :param target: Is the name of the configuration datastore being edited\n    :param default_operation: If specified must be one of { `\"merge\"`, `\"replace\"`, or `\"none\"` }\n    :param test_option: If specified must be one of { `\"test_then_set\"`, `\"set\"` }\n    :param error_option: If specified must be one of { `\"stop-on-error\"`, `\"continue-on-error\"`, `\"rollback-on-error\"` }\n                         The `\"rollback-on-error\"` *error_option* depends on the `:rollback-on-error` capability.\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    if config is None:\n        raise ValueError('config value must be provided')\n    resp = self.m.edit_config(config, format=format, target=target, default_operation=default_operation, test_option=test_option,\n                              error_option=error_option)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 18}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "validate", "parameters": ["self", "source"], "param_types": {}, "return_type": null, "param_doc": {"source": "Is the name of the configuration datastore being validated or `config` element"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.validate"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Validate the contents of the specified configuration.", "source_code": "def validate(self, source='candidate'):\n    \"\"\"\n    Validate the contents of the specified configuration.\n    :param source: Is the name of the configuration datastore being validated or `config` element\n                   containing the configuration subtree to be validated\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.validate(source=source)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "copy_config", "parameters": ["self", "source", "target"], "param_types": {}, "return_type": null, "param_doc": {"source": "Is the name of the configuration datastore to use as the source of the copy operation or `config`", "target": "Is the name of the configuration datastore to use as the destination of the copy operation"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.copy_config"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create or replace an entire configuration datastore with the contents of another complete configuration datastore.", "source_code": "def copy_config(self, source, target):\n    \"\"\"\n    Create or replace an entire configuration datastore with the contents of another complete configuration datastore.\n    :param source: Is the name of the configuration datastore to use as the source of the copy operation or `config`\n                   element containing the configuration subtree to copy\n    :param target: Is the name of the configuration datastore to use as the destination of the copy operation\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.copy_config(source, target)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "dispatch", "parameters": ["self", "rpc_command", "source", "filter"], "param_types": {}, "return_type": null, "param_doc": {"rpc_command": "specifies rpc command to be dispatched either in plain text or in xml element format (depending on command)", "source": "name of the configuration datastore being queried", "filter": "specifies the portion of the configuration to retrieve (by default entire configuration is retrieved)"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["ValueError", "fromstring", "hasattr", "isinstance", "self.m.dispatch"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Execute rpc on the remote device eg. dispatch('clear-arp-table')", "source_code": "def dispatch(self, rpc_command=None, source=None, filter=None):\n    \"\"\"\n    Execute rpc on the remote device eg. dispatch('clear-arp-table')\n    :param rpc_command: specifies rpc command to be dispatched either in plain text or in xml element format (depending on command)\n    :param source: name of the configuration datastore being queried\n    :param filter: specifies the portion of the configuration to retrieve (by default entire configuration is retrieved)\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    if rpc_command is None:\n        raise ValueError('rpc_command value must be provided')\n\n    resp = self.m.dispatch(fromstring(rpc_command), source=source, filter=filter)\n\n    if isinstance(resp, NCElement):\n        # In case xml reply is transformed or namespace is removed in\n        # ncclient device specific handler return modified xml response\n        result = resp.data_xml\n    elif hasattr(resp, 'data_ele') and resp.data_ele:\n        # if data node is present in xml response return the xml string\n        # with data node as root\n        result = resp.data_xml\n    else:\n        # return raw xml string received from host with rpc-reply as the root node\n        result = resp.xml\n\n    return result", "loc": 26}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "lock", "parameters": ["self", "target"], "param_types": {}, "return_type": null, "param_doc": {"target": "is the name of the configuration datastore to lock,"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.lock"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Allows the client to lock the configuration system of a device.", "source_code": "def lock(self, target=\"candidate\"):\n    \"\"\"\n    Allows the client to lock the configuration system of a device.\n    :param target: is the name of the configuration datastore to lock,\n                    defaults to candidate datastore\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.lock(target=target)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "unlock", "parameters": ["self", "target"], "param_types": {}, "return_type": null, "param_doc": {"target": "is the name of the configuration datastore to unlock,"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.unlock"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Release a configuration lock, previously obtained with the lock operation.", "source_code": "def unlock(self, target=\"candidate\"):\n    \"\"\"\n    Release a configuration lock, previously obtained with the lock operation.\n    :param target: is the name of the configuration datastore to unlock,\n                   defaults to candidate datastore\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.unlock(target=target)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 9}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "discard_changes", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.discard_changes"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Revert the candidate configuration to the currently running configuration. Any uncommitted changes are discarded.", "source_code": "def discard_changes(self):\n    \"\"\"\n    Revert the candidate configuration to the currently running configuration.\n    Any uncommitted changes are discarded.\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.discard_changes()\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "commit", "parameters": ["self", "confirmed", "timeout", "persist"], "param_types": {}, "return_type": null, "param_doc": {"confirmed": "whether this is a confirmed commit", "timeout": "specifies the confirm timeout in seconds", "persist": "make the confirmed commit survive a session termination,"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.commit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Commit the candidate configuration as the device's new current configuration. Depends on the `:candidate` capability. A confirmed commit (i.e. if *confirmed* is `True`) is reverted if there is no followup commit within the *timeout* interval. If no timeout is specified the confirm timeout defaults to 600 seconds (10 minutes). A confirming commit may have the *confirmed* parameter but this is not required. Depends on the `:confirmed-commit` capability.", "source_code": "def commit(self, confirmed=False, timeout=None, persist=None):\n    \"\"\"\n    Commit the candidate configuration as the device's new current configuration.\n    Depends on the `:candidate` capability.\n    A confirmed commit (i.e. if *confirmed* is `True`) is reverted if there is no\n    followup commit within the *timeout* interval. If no timeout is specified the\n    confirm timeout defaults to 600 seconds (10 minutes).\n    A confirming commit may have the *confirmed* parameter but this is not required.\n    Depends on the `:confirmed-commit` capability.\n    :param confirmed: whether this is a confirmed commit\n    :param timeout: specifies the confirm timeout in seconds\n    :param persist: make the confirmed commit survive a session termination,\n                    and set a token on the ongoing confirmed commit\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.commit(confirmed=confirmed, timeout=timeout, persist=persist)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 17}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "get_schema", "parameters": ["self", "identifier", "version", "format"], "param_types": {}, "return_type": null, "param_doc": {"identifier": "name of the schema to be retrieved", "version": "version of schema to get", "format": "format of the schema to be retrieved, yang is the default"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.get_schema"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Retrieve a named schema, with optional revision and type.", "source_code": "def get_schema(self, identifier=None, version=None, format=None):\n    \"\"\"\n    Retrieve a named schema, with optional revision and type.\n    :param identifier: name of the schema to be retrieved\n    :param version: version of schema to get\n    :param format: format of the schema to be retrieved, yang is the default\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.get_schema(identifier, version=version, format=format)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 10}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "delete_config", "parameters": ["self", "target"], "param_types": {}, "return_type": null, "param_doc": {"target": "specifies the  name or URL of configuration datastore to delete"}, "return_doc": "Returns xml string containing the RPC response received from remote host", "raises_doc": [], "called_functions": ["hasattr", "self.m.delete_config"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "delete a configuration datastore", "source_code": "def delete_config(self, target):\n    \"\"\"\n    delete a configuration datastore\n    :param target: specifies the  name or URL of configuration datastore to delete\n    :return: Returns xml string containing the RPC response received from remote host\n    \"\"\"\n    resp = self.m.delete_config(target)\n    return resp.data_xml if hasattr(resp, 'data_xml') else resp.xml", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\netconf\\__init__.py", "class_name": "NetconfBase", "function_name": "get_device_operations", "parameters": ["self", "server_capabilities"], "param_types": {}, "return_type": null, "param_doc": {"server_capabilities": "Server capabilities received during Netconf session initialization"}, "return_doc": "Remote host capabilities in dictionary format", "raises_doc": [], "called_functions": ["'\\n'.join", "bool", "operations['lock_datastore'].append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Retrieve remote host capability from Netconf server hello message.", "source_code": "def get_device_operations(self, server_capabilities):\n    \"\"\"\n    Retrieve remote host capability from Netconf server hello message.\n    :param server_capabilities: Server capabilities received during Netconf session initialization\n    :return: Remote host capabilities in dictionary format\n    \"\"\"\n    operations = {}\n    capabilities = '\\n'.join(server_capabilities)\n    operations['supports_commit'] = ':candidate' in capabilities\n    operations['supports_defaults'] = ':with-defaults' in capabilities\n    operations['supports_confirm_commit'] = ':confirmed-commit' in capabilities\n    operations['supports_startup'] = ':startup' in capabilities\n    operations['supports_xpath'] = ':xpath' in capabilities\n    operations['supports_writable_running'] = ':writable-running' in capabilities\n    operations['supports_validate'] = ':validate' in capabilities\n\n    operations['lock_datastore'] = []\n    if operations['supports_writable_running']:\n        operations['lock_datastore'].append('running')\n\n    if operations['supports_commit']:\n        operations['lock_datastore'].append('candidate')\n\n    if operations['supports_startup']:\n        operations['lock_datastore'].append('startup')\n\n    operations['supports_lock'] = bool(operations['lock_datastore'])\n\n    return operations", "loc": 29}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\cmd.py", "class_name": "ShellModule", "function_name": "quote", "parameters": ["self", "cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_find_unsafe", "cmd.replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def quote(self, cmd):\n    # cmd does not support single quotes that the shlex_quote uses. We need to override the quoting behaviour to\n    # better match cmd.exe.\n    # https://blogs.msdn.microsoft.com/twistylittlepassagesallalike/2011/04/23/everyone-quotes-command-line-arguments-the-wrong-way/\n\n    # Return an empty argument\n    if not cmd:\n        return '\"\"'\n\n    if _find_unsafe(cmd) is None:\n        return cmd\n\n    # Escape the metachars as we are quoting the string to stop cmd from interpreting that metachar. For example\n    # 'file &whoami.exe' would result in 'file $(whoami.exe)' instead of the literal string\n    # https://stackoverflow.com/questions/3411771/multiple-character-replace-with-python\n    for c in '^()%!\"<>&|':  # '^' must be the first char that we scan and replace\n        if c in cmd:\n            # I can't find any docs that explicitly say this but to escape \", it needs to be prefixed with \\^.\n            cmd = cmd.replace(c, (\"\\\\^\" if c == '\"' else \"^\") + c)\n\n    return '^\"' + cmd + '^\"'", "loc": 21}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "join_path", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ntpath.join", "ntpath.normpath", "part.strip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def join_path(self, *args):\n    # use normpath() to remove doubled slashed and convert forward to backslashes\n    parts = [ntpath.normpath(arg) for arg in args]\n\n    # Because ntpath.join treats any component that begins with a backslash as an absolute path,\n    # we have to strip slashes from at least the beginning, otherwise join will ignore all previous\n    # path components except for the drive.\n    return ntpath.join(parts[0], *[part.strip('\\\\') for part in parts[1:]])", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "remove", "parameters": ["self", "path", "recurse"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._encode_script", "self._escape"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove(self, path, recurse=False):\n    quoted_path = self._escape(path)\n    if recurse:\n        return self._encode_script(\"\"\"Remove-Item '%s' -Force -Recurse;\"\"\" % quoted_path)\n    else:\n        return self._encode_script(\"\"\"Remove-Item '%s' -Force;\"\"\" % quoted_path)", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "mkdtemp", "parameters": ["self", "basefile", "system", "mode", "tmpdir"], "param_types": {"basefile": "str | None", "system": "bool", "mode": "int", "tmpdir": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["script.strip", "self.__class__._generate_temp_dir_name", "self._encode_script", "self._escape", "self.get_option"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mkdtemp(\n    self,\n    basefile: str | None = None,\n    system: bool = False,\n    mode: int = 0o700,\n    tmpdir: str | None = None,\n) -> str:\n    # This is not called in Ansible anymore but it is kept for backwards\n    # compatibility in case other action plugins outside Ansible calls this.\n    if not basefile:\n        basefile = self.__class__._generate_temp_dir_name()\n    basetmpdir = self._escape(tmpdir if tmpdir else self.get_option('remote_tmp'))\n\n    script = f\"\"\"\n    {self._CONSOLE_ENCODING}\n    $tmp_path = [System.Environment]::ExpandEnvironmentVariables('{basetmpdir}')\n    $tmp = New-Item -Type Directory -Path $tmp_path -Name '{basefile}'\n    Write-Output -InputObject $tmp.FullName\n    \"\"\"\n    return self._encode_script(script.strip())", "loc": 20}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "expand_user", "parameters": ["self", "user_home_path", "username"], "param_types": {"user_home_path": "str", "username": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._encode_script", "self._escape", "user_home_path.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def expand_user(\n    self,\n    user_home_path: str,\n    username: str = '',\n) -> str:\n    # This is not called in Ansible anymore but it is kept for backwards\n    # compatibility in case other actions plugins outside Ansible called this.\n    if user_home_path == '~':\n        script = 'Write-Output (Get-Location).Path'\n    elif user_home_path.startswith('~\\\\'):\n        script = \"Write-Output ((Get-Location).Path + '%s')\" % self._escape(user_home_path[1:])\n    else:\n        script = \"Write-Output '%s'\" % self._escape(user_home_path)\n    return self._encode_script(f\"{self._CONSOLE_ENCODING}; {script}\")", "loc": 14}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "exists", "parameters": ["self", "path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._encode_script", "self._escape"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exists(self, path):\n    path = self._escape(path)\n    script = \"\"\"\n        If (Test-Path '%s')\n        {\n            $res = 0;\n        }\n        Else\n        {\n            $res = 1;\n        }\n        Write-Output '$res';\n        Exit $res;\n     \"\"\" % path\n    return self._encode_script(script)", "loc": 15}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "checksum", "parameters": ["self", "path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "display.deprecated", "self._encode_script", "self._escape"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def checksum(self, path, *args, **kwargs):\n    display.deprecated(\n        msg=\"The `ShellModule.checksum` method is deprecated.\",\n        version=\"2.23\",\n        help_text=\"Use `ActionBase._execute_remote_stat()` instead.\",\n    )\n    path = self._escape(path)\n    script = \"\"\"\n        If (Test-Path -PathType Leaf '%(path)s')\n        {\n            $sp = new-object -TypeName System.Security.Cryptography.SHA1CryptoServiceProvider;\n            $fp = [System.IO.File]::Open('%(path)s', [System.IO.Filemode]::Open, [System.IO.FileAccess]::Read);\n            [System.BitConverter]::ToString($sp.ComputeHash($fp)).Replace(\"-\", \"\").ToLower();\n            $fp.Dispose();\n        }\n        ElseIf (Test-Path -PathType Container '%(path)s')\n        {\n            Write-Output \"3\";\n        }\n        Else\n        {\n            Write-Output \"1\";\n        }\n    \"\"\" % dict(path=path)\n    return self._encode_script(script)", "loc": 25}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\powershell.py", "class_name": "ShellModule", "function_name": "build_module_command", "parameters": ["self", "env_string", "shebang", "cmd", "arg_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "_get_powershell_script", "cmd_parts.append", "cmd_parts.insert", "list", "map", "script_path.lower", "script_path.lower().endswith", "self._encode_script", "shebang.lower", "shebang.startswith", "shlex.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_module_command(self, env_string, shebang, cmd, arg_path=None):\n    bootstrap_wrapper = _get_powershell_script(\"bootstrap_wrapper.ps1\")\n\n    # pipelining bypass\n    if cmd == '':\n        return self._encode_script(script=bootstrap_wrapper, strict_mode=False, preserve_rc=False)\n\n    # non-pipelining\n\n    cmd_parts = shlex.split(cmd, posix=False)\n    cmd_parts = list(map(to_text, cmd_parts))\n    if shebang and shebang.lower() == '#!powershell':\n        if arg_path:\n            # Running a module without the exec_wrapper and with an argument\n            # file.\n            script_path = cmd_parts[0]\n            if not script_path.lower().endswith('.ps1'):\n                script_path += '.ps1'\n\n            cmd_parts.insert(0, '-File')\n            cmd_parts[1] = f'\"{script_path}\"'\n            if arg_path:\n                cmd_parts.append(f'\"{arg_path}\"')\n\n            wrapper_cmd = \" \".join(_common_args + cmd_parts)\n            return wrapper_cmd\n\n        else:\n            # Running a module with ANSIBLE_KEEP_REMOTE_FILES=true, the script\n            # arg is actually the input manifest JSON to provide to the bootstrap\n            # wrapper.\n            wrapper_cmd = \"type \" + cmd_parts[0] + \" | \" + self._encode_script(script=bootstrap_wrapper, strict_mode=False, preserve_rc=False)\n            return wrapper_cmd\n\n    elif shebang and shebang.startswith('#!'):\n        cmd_parts.insert(0, shebang[2:])\n    elif not shebang:\n        # The module is assumed to be a binary\n        cmd_parts.append(arg_path)\n    script = \"\"\"\n        Try\n        {\n            %s\n            %s\n        }\n        Catch\n        {\n            $_obj = @{ failed = $true }\n            If ($_.Exception.GetType)\n            {\n                $_obj.Add('msg', $_.Exception.Message)\n            }\n            Else\n            {\n                $_obj.Add('msg', $_.ToString())\n            }\n            If ($_.InvocationInfo.PositionMessage)\n            {\n                $_obj.Add('exception', $_.InvocationInfo.PositionMessage)\n            }\n            ElseIf ($_.ScriptStackTrace)\n            {\n                $_obj.Add('exception', $_.ScriptStackTrace)\n            }\n            Try\n            {\n                $_obj.Add('error_record', ($_ | ConvertTo-Json | ConvertFrom-Json))\n            }\n            Catch\n            {\n            }\n            Echo $_obj | ConvertTo-Json -Compress -Depth 99\n            Exit 1\n        }\n    \"\"\" % (env_string, ' '.join(cmd_parts))\n    return self._encode_script(script, preserve_rc=False)", "loc": 76}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "set_options", "parameters": ["self", "task_keys", "var_options", "direct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._normalize_system_tmpdirs", "super", "super(ShellBase, self).set_options"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_options(self, task_keys=None, var_options=None, direct=None):\n\n    super(ShellBase, self).set_options(task_keys=task_keys, var_options=var_options, direct=direct)\n\n    # We can remove the try: except in the future when we make ShellBase a proper subset of\n    # *all* shells.  Right now powershell and third party shells which do not use the\n    # shell_common documentation fragment (and so do not have system_tmpdirs) will fail\n    try:\n        self._normalize_system_tmpdirs()\n    except KeyError:\n        pass", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "set_user_facl", "parameters": ["self", "paths", "user", "mode"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.extend", "self.join"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Only sets acls for users as that's really all we need", "source_code": "def set_user_facl(self, paths, user, mode):\n    \"\"\"Only sets acls for users as that's really all we need\"\"\"\n    cmd = ['setfacl', '-m', 'u:%s:%s' % (user, mode)]\n    cmd.extend(paths)\n    return self.join(cmd)", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "remove", "parameters": ["self", "path", "recurse"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.quote"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove(self, path, recurse=False):\n    path = self.quote(path)\n    cmd = 'rm -f '\n    if recurse:\n        cmd += '-r '\n    return cmd + \"%s %s\" % (path, self._SHELL_REDIRECT_ALLNULL)", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "mkdtemp", "parameters": ["self", "basefile", "system", "mode", "tmpdir"], "param_types": {"basefile": "str | None", "system": "bool", "mode": "int", "tmpdir": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.__class__._generate_temp_dir_name", "self.get_option", "self.join_path", "tmpdir.rstrip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mkdtemp(\n    self,\n    basefile: str | None = None,\n    system: bool = False,\n    mode: int = 0o700,\n    tmpdir: str | None = None,\n) -> str:\n    if not basefile:\n        basefile = self.__class__._generate_temp_dir_name()\n\n    # When system is specified we have to create this in a directory where\n    # other users can read and access the tmp directory.\n    # This is because we use system to create tmp dirs for unprivileged users who are\n    # sudo'ing to a second unprivileged user.\n    # The 'system_tmpdirs' setting defines directories we can use for this purpose\n    # the default are, /tmp and /var/tmp.\n    # So we only allow one of those locations if system=True, using the\n    # passed in tmpdir if it is valid or the first one from the setting if not.\n\n    if system:\n        if tmpdir:\n            tmpdir = tmpdir.rstrip('/')\n\n        if tmpdir in self.get_option('system_tmpdirs'):\n            basetmpdir = tmpdir\n        else:\n            basetmpdir = self.get_option('system_tmpdirs')[0]\n    else:\n        if tmpdir is None:\n            basetmpdir = self.get_option('remote_tmp')\n        else:\n            basetmpdir = tmpdir\n\n    basetmp = self.join_path(basetmpdir, basefile)\n\n    # use mkdir -p to ensure parents exist, but mkdir fullpath to ensure last one is created by us\n    cmd = 'mkdir -p %s echo %s %s' % (self._SHELL_SUB_LEFT, basetmpdir, self._SHELL_SUB_RIGHT)\n    cmd += '%s mkdir %s echo %s %s' % (self._SHELL_AND, self._SHELL_SUB_LEFT, basetmp, self._SHELL_SUB_RIGHT)\n    cmd += ' %s echo %s=%s echo %s %s' % (self._SHELL_AND, basefile, self._SHELL_SUB_LEFT, basetmp, self._SHELL_SUB_RIGHT)\n\n    # change the umask in a subshell to achieve the desired mode\n    # also for directories created with `mkdir -p`\n    if mode:\n        tmp_umask = 0o777 & ~mode\n        cmd = '%s umask %o %s %s %s' % (self._SHELL_GROUP_LEFT, tmp_umask, self._SHELL_AND, cmd, self._SHELL_GROUP_RIGHT)\n\n    return cmd", "loc": 47}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "expand_user", "parameters": ["self", "user_home_path", "username"], "param_types": {"user_home_path": "str", "username": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_USER_HOME_PATH_RE.match", "self.quote"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a command to expand tildes in a path It can be either \"~\" or \"~username\". We just ignore $HOME We use the POSIX definition of a username:", "source_code": "def expand_user(\n    self,\n    user_home_path: str,\n    username: str = '',\n) -> str:\n    \"\"\" Return a command to expand tildes in a path\n\n    It can be either \"~\" or \"~username\". We just ignore $HOME\n    We use the POSIX definition of a username:\n        http://pubs.opengroup.org/onlinepubs/000095399/basedefs/xbd_chap03.html#tag_03_426\n        http://pubs.opengroup.org/onlinepubs/000095399/basedefs/xbd_chap03.html#tag_03_276\n\n        Falls back to 'current working directory' as we assume 'home is where the remote user ends up'\n    \"\"\"\n\n    # Check that the user_path to expand is safe\n    if user_home_path != '~':\n        if not _USER_HOME_PATH_RE.match(user_home_path):\n            user_home_path = self.quote(user_home_path)\n    elif username:\n        # if present the user name is appended to resolve \"that user's home\"\n        user_home_path += username\n\n    return 'echo %s' % user_home_path", "loc": 24}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "build_module_command", "parameters": ["self", "env_string", "shebang", "cmd", "arg_path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "cmd.strip", "env_string.strip", "raw_cmd_part.strip", "self.join", "shebang.removeprefix", "shebang.removeprefix('#!').strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_module_command(self, env_string, shebang, cmd, arg_path=None):\n    env_string = env_string.strip()\n    if env_string:\n        env_string += ' '\n\n    if shebang is None:\n        shebang = ''\n\n    cmd_parts = [\n        shebang.removeprefix('#!').strip(),\n        cmd.strip(),\n        arg_path,\n    ]\n\n    cleaned_up_cmd = self.join(\n        stripped_cmd_part for raw_cmd_part in cmd_parts\n        if raw_cmd_part and (stripped_cmd_part := raw_cmd_part.strip())\n    )\n    return ''.join((env_string, cleaned_up_cmd))", "loc": 19}
{"file": "ansible\\lib\\ansible\\plugins\\shell\\__init__.py", "class_name": "ShellBase", "function_name": "append_command", "parameters": ["self", "cmd", "cmd_to_append"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Append an additional command if supported by the shell", "source_code": "def append_command(self, cmd, cmd_to_append):\n    \"\"\"Append an additional command if supported by the shell\"\"\"\n\n    if self._SHELL_AND:\n        cmd += ' %s %s' % (self._SHELL_AND, cmd_to_append)\n\n    return cmd", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": null, "function_name": "results_thread_main", "parameters": ["strategy"], "param_types": {"strategy": "StrategyBase"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "display.prompt_until", "display.warning", "dmethod", "getattr", "isinstance", "strategy._convert_wire_task_result_to_raw", "strategy._final_q.get", "strategy._results.append", "strategy._tqm.send_callback", "strategy._workers[result.worker_id].worker_queue.put", "type"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def results_thread_main(strategy: StrategyBase) -> None:\n    value: object\n\n    while True:\n        try:\n            result = strategy._final_q.get()\n            if isinstance(result, StrategySentinel):\n                break\n            elif isinstance(result, DisplaySend):\n                dmethod = getattr(display, result.method)\n                dmethod(*result.args, **result.kwargs)\n            elif isinstance(result, CallbackSend):\n                task_result = strategy._convert_wire_task_result_to_raw(result.wire_task_result)\n                strategy._tqm.send_callback(result.method_name, task_result)\n            elif isinstance(result, _WireTaskResult):\n                result = strategy._convert_wire_task_result_to_raw(result)\n                with strategy._results_lock:\n                    strategy._results.append(result)\n            elif isinstance(result, PromptSend):\n                try:\n                    value = display.prompt_until(\n                        result.prompt,\n                        private=result.private,\n                        seconds=result.seconds,\n                        complete_input=result.complete_input,\n                        interrupt_input=result.interrupt_input,\n                    )\n                except AnsibleError as e:\n                    value = e\n                except BaseException as e:\n                    # relay unexpected errors so bugs in display are reported and don't cause workers to hang\n                    try:\n                        raise AnsibleError(f\"{e}\") from e\n                    except AnsibleError as e:\n                        value = e\n                strategy._workers[result.worker_id].worker_queue.put(value)\n            else:\n                display.warning('Received an invalid object (%s) in the result queue: %r' % (type(result), result))\n        except (OSError, EOFError):\n            break\n        except queue.Empty:\n            pass", "loc": 42}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": null, "function_name": "debug_closure", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Debugger", "NextAction", "_processed_results.append", "_processed_results.extend", "add_internal_fqcns", "dbg.cmdloop", "debug_closure", "func", "functools.wraps", "getattr", "iterator._play._removed_hosts.remove", "iterator.get_host_state", "iterator.host_states.copy", "iterator.set_state_for_host", "prev_host_states.items", "result.is_failed", "result.needs_debugger", "self._queue_task", "self._queued_task_cache.pop", "self._tqm._stats.decrement", "self._tqm.clear_failed_hosts", "sys.exit"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Closure to wrap ``StrategyBase._process_pending_results`` and invoke the task debugger", "source_code": "def debug_closure(func):\n    \"\"\"Closure to wrap ``StrategyBase._process_pending_results`` and invoke the task debugger\"\"\"\n    @functools.wraps(func)\n    def inner(self, iterator: PlayIterator, one_pass: bool = False, max_passes: int | None = None) -> list[_RawTaskResult]:\n        status_to_stats_map = (\n            ('is_failed', 'failures'),\n            ('is_unreachable', 'dark'),\n            ('is_changed', 'changed'),\n            ('is_skipped', 'skipped'),\n        )\n\n        # We don't know the host yet, copy the previous states, for lookup after we process new results\n        prev_host_states = iterator.host_states.copy()\n\n        results: list[_RawTaskResult] = func(self, iterator, one_pass=one_pass, max_passes=max_passes)\n        _processed_results: list[_RawTaskResult] = []\n\n        for result in results:\n            task = result.task\n            host = result.host\n            _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n            task_vars = _queued_task_args['task_vars']\n            play_context = _queued_task_args['play_context']\n            # Try to grab the previous host state, if it doesn't exist use get_host_state to generate an empty state\n            try:\n                prev_host_state = prev_host_states[host.name]\n            except KeyError:\n                prev_host_state = iterator.get_host_state(host)\n\n            while result.needs_debugger(globally_enabled=self.debugger_active):\n                next_action = NextAction()\n                dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n                dbg.cmdloop()\n\n                if next_action.result == NextAction.REDO:\n                    # rollback host state\n                    self._tqm.clear_failed_hosts()\n                    if task.run_once and iterator._play.strategy in add_internal_fqcns(('linear',)) and result.is_failed():\n                        for host_name, state in prev_host_states.items():\n                            if host_name == host.name:\n                                continue\n                            iterator.set_state_for_host(host_name, state)\n                            iterator._play._removed_hosts.remove(host_name)\n                    iterator.set_state_for_host(host.name, prev_host_state)\n                    for method, what in status_to_stats_map:\n                        if getattr(result, method)():\n                            self._tqm._stats.decrement(what, host.name)\n                    self._tqm._stats.decrement('ok', host.name)\n\n                    # redo\n                    self._queue_task(host, task, task_vars, play_context)\n\n                    _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n                    break\n                elif next_action.result == NextAction.CONTINUE:\n                    _processed_results.append(result)\n                    break\n                elif next_action.result == NextAction.EXIT:\n                    # Matches KeyboardInterrupt from bin/ansible\n                    sys.exit(99)\n            else:\n                _processed_results.append(result)\n\n        return _processed_results\n    return inner", "loc": 65}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": null, "function_name": "inner", "parameters": ["self", "iterator", "one_pass", "max_passes"], "param_types": {"iterator": "PlayIterator", "one_pass": "bool", "max_passes": "int | None"}, "return_type": "list[_RawTaskResult]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Debugger", "NextAction", "_processed_results.append", "_processed_results.extend", "add_internal_fqcns", "dbg.cmdloop", "debug_closure", "func", "functools.wraps", "getattr", "iterator._play._removed_hosts.remove", "iterator.get_host_state", "iterator.host_states.copy", "iterator.set_state_for_host", "prev_host_states.items", "result.is_failed", "result.needs_debugger", "self._queue_task", "self._queued_task_cache.pop", "self._tqm._stats.decrement", "self._tqm.clear_failed_hosts", "sys.exit"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner(self, iterator: PlayIterator, one_pass: bool = False, max_passes: int | None = None) -> list[_RawTaskResult]:\n    status_to_stats_map = (\n        ('is_failed', 'failures'),\n        ('is_unreachable', 'dark'),\n        ('is_changed', 'changed'),\n        ('is_skipped', 'skipped'),\n    )\n\n    # We don't know the host yet, copy the previous states, for lookup after we process new results\n    prev_host_states = iterator.host_states.copy()\n\n    results: list[_RawTaskResult] = func(self, iterator, one_pass=one_pass, max_passes=max_passes)\n    _processed_results: list[_RawTaskResult] = []\n\n    for result in results:\n        task = result.task\n        host = result.host\n        _queued_task_args = self._queued_task_cache.pop((host.name, task._uuid), None)\n        task_vars = _queued_task_args['task_vars']\n        play_context = _queued_task_args['play_context']\n        # Try to grab the previous host state, if it doesn't exist use get_host_state to generate an empty state\n        try:\n            prev_host_state = prev_host_states[host.name]\n        except KeyError:\n            prev_host_state = iterator.get_host_state(host)\n\n        while result.needs_debugger(globally_enabled=self.debugger_active):\n            next_action = NextAction()\n            dbg = Debugger(task, host, task_vars, play_context, result, next_action)\n            dbg.cmdloop()\n\n            if next_action.result == NextAction.REDO:\n                # rollback host state\n                self._tqm.clear_failed_hosts()\n                if task.run_once and iterator._play.strategy in add_internal_fqcns(('linear',)) and result.is_failed():\n                    for host_name, state in prev_host_states.items():\n                        if host_name == host.name:\n                            continue\n                        iterator.set_state_for_host(host_name, state)\n                        iterator._play._removed_hosts.remove(host_name)\n                iterator.set_state_for_host(host.name, prev_host_state)\n                for method, what in status_to_stats_map:\n                    if getattr(result, method)():\n                        self._tqm._stats.decrement(what, host.name)\n                self._tqm._stats.decrement('ok', host.name)\n\n                # redo\n                self._queue_task(host, task, task_vars, play_context)\n\n                _processed_results.extend(debug_closure(func)(self, iterator, one_pass))\n                break\n            elif next_action.result == NextAction.CONTINUE:\n                _processed_results.append(result)\n                break\n            elif next_action.result == NextAction.EXIT:\n                # Matches KeyboardInterrupt from bin/ansible\n                sys.exit(99)\n        else:\n            _processed_results.append(result)\n\n    return _processed_results", "loc": 61}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "cleanup", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Connection", "conn.reset", "display.debug", "self._active_connections.values", "self._final_q.put", "self._results_thread.join"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cleanup(self):\n    # close active persistent connections\n    for sock in self._active_connections.values():\n        try:\n            conn = Connection(sock)\n            conn.reset()\n        except ConnectionError as e:\n            # most likely socket is already closed\n            display.debug(\"got an error while closing persistent connection: %s\" % e)\n    self._final_q.put(_sentinel)\n    self._results_thread.join()", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "run", "parameters": ["self", "iterator", "play_context", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "iterator.get_failed_hosts", "iterator.get_next_task_for_host", "len", "self._inventory.get_host", "self._tqm._unreachable_hosts.keys"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, iterator, play_context, result=0):\n    # execute one more pass through the iterator without peeking, to\n    # make sure that all of the hosts are advanced to their final task.\n    # This should be safe, as everything should be IteratingStates.COMPLETE by\n    # this point, though the strategy may not advance the hosts itself.\n\n    for host in self._hosts_cache:\n        if host not in self._tqm._unreachable_hosts:\n            try:\n                iterator.get_next_task_for_host(self._inventory.hosts[host])\n            except KeyError:\n                iterator.get_next_task_for_host(self._inventory.get_host(host))\n\n    # return the appropriate code, depending on the status hosts after the run\n    if not isinstance(result, bool) and result != self._tqm.RUN_OK:\n        return result\n    elif len(self._tqm._unreachable_hosts.keys()) > 0:\n        return self._tqm.RUN_UNREACHABLE_HOSTS\n    elif len(iterator.get_failed_hosts()) > 0:\n        return self._tqm.RUN_FAILED_HOSTS\n    else:\n        return self._tqm.RUN_OK", "loc": 22}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "add_tqm_variables", "parameters": ["self", "vars", "play"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_failed_hosts", "self.get_hosts_remaining"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Base class method to add extra variables/information to the list of task vars sent through the executor engine regarding the task queue manager state.", "source_code": "def add_tqm_variables(self, vars, play):\n    \"\"\"\n    Base class method to add extra variables/information to the list of task\n    vars sent through the executor engine regarding the task queue manager state.\n    \"\"\"\n    vars['ansible_current_hosts'] = self.get_hosts_remaining(play)\n    vars['ansible_failed_hosts'] = self.get_failed_hosts(play)", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "get_task_hosts", "parameters": ["self", "iterator", "task_host", "task"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_task_hosts(self, iterator, task_host, task):\n    if task.run_once:\n        host_list = [host for host in self._hosts_cache if host not in self._tqm._unreachable_hosts]\n    else:\n        host_list = [task_host.name]\n    return host_list", "loc": 6}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "search_handlers_by_notification", "parameters": ["self", "notification", "iterator"], "param_types": {"notification": "str", "iterator": "PlayIterator"}, "return_type": "t.Generator[Handler, None, None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateEngine", "display.warning", "handler.get_name", "reversed", "seen.add", "self._variable_manager.get_vars", "set", "templar.template", "to_text"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def search_handlers_by_notification(self, notification: str, iterator: PlayIterator) -> t.Generator[Handler, None, None]:\n    handlers = [h for b in reversed(iterator._play.handlers) for h in b.block]\n    # iterate in reversed order since last handler loaded with the same name wins\n    for handler in handlers:\n        if not handler.name:\n            continue\n\n        if not handler.cached_name:\n            def variables_factory() -> dict[str, t.Any]:\n                return self._variable_manager.get_vars(\n                    play=iterator._play,\n                    task=handler,\n                    _hosts=self._hosts_cache,\n                    _hosts_all=self._hosts_cache_all\n                )\n\n            templar = TemplateEngine(variables_factory=variables_factory)\n\n            try:\n                handler.name = templar.template(handler.name)\n            except AnsibleTemplateError as e:\n                # We skip this handler due to the fact that it may be using\n                # a variable in the name that was conditionally included via\n                # set_fact or some other method, and we don't want to error\n                # out unnecessarily\n                if not handler.listen:\n                    display.warning(\n                        \"Handler '%s' is unusable because it has no listen topics and \"\n                        \"the name could not be templated (host-specific variables are \"\n                        \"not supported in handler names). The error: %s\" % (handler.name, to_text(e))\n                    )\n                continue\n\n            handler.cached_name = True\n\n        # first we check with the full result of get_name(), which may\n        # include the role name (if the handler is from a role). If that\n        # is not found, we resort to the simple name field, which doesn't\n        # have anything extra added to it.\n        if notification in {\n            handler.name,\n            handler.get_name(include_role_fqcn=False),\n            handler.get_name(include_role_fqcn=True),\n        }:\n            yield handler\n            break\n\n    seen = set()\n    for handler in handlers:\n        if notification in handler.listen:\n            if handler.name and handler.name in seen:\n                continue\n            seen.add(handler.name)\n            yield handler", "loc": 54}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "get_hosts_left", "parameters": ["self", "iterator"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hosts_left.append", "self._inventory.get_host"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "returns list of available hosts for this iterator by filtering out unreachables", "source_code": "def get_hosts_left(self, iterator):\n    \"\"\" returns list of available hosts for this iterator by filtering out unreachables \"\"\"\n\n    hosts_left = []\n    for host in self._hosts_cache:\n        if host not in self._tqm._unreachable_hosts:\n            try:\n                hosts_left.append(self._inventory.hosts[host])\n            except KeyError:\n                hosts_left.append(self._inventory.get_host(host))\n    return hosts_left", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "StrategyBase", "function_name": "update_active_connections", "parameters": ["self", "results"], "param_types": {"results": "_c.Iterable[_RawTaskResult]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["r.task_fields['args'].get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "updates the current active persistent connections", "source_code": "def update_active_connections(self, results: _c.Iterable[_RawTaskResult]) -> None:\n    \"\"\" updates the current active persistent connections \"\"\"\n    for r in results:\n        if 'args' in r.task_fields:\n            socket_path = r.task_fields['args'].get('_ansible_socket')\n            if socket_path:\n                if r.host not in self._active_connections:\n                    self._active_connections[r.host] = socket_path", "loc": 8}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "cmdloop", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.Cmd.cmdloop"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cmdloop(self):\n    try:\n        cmd.Cmd.cmdloop(self)\n    except KeyboardInterrupt:\n        pass", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "do_update_task", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateEngine", "task.load_data", "task.post_validate"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Recreate the task from ``task._ds``, and template with updated ``task_vars``", "source_code": "def do_update_task(self, args):\n    \"\"\"Recreate the task from ``task._ds``, and template with updated ``task_vars``\"\"\"\n    templar = TemplateEngine(variables=self.scope['task_vars'])\n    task = self.scope['task']\n    task = task.load_data(task._ds)\n    task.post_validate(templar)\n    self.scope['task'] = task", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "evaluate", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "eval", "globals", "isinstance", "repr", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def evaluate(self, args):\n    try:\n        return eval(args, globals(), self.scope)\n    except Exception:\n        t, v = sys.exc_info()[:2]\n        if isinstance(t, str):\n            exc_type_name = t\n        else:\n            exc_type_name = t.__name__\n        display.display('***%s:%s' % (exc_type_name, repr(v)))\n        raise", "loc": 11}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "do_pprint", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.display", "pprint.pformat", "self.evaluate"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Pretty Print", "source_code": "def do_pprint(self, args):\n    \"\"\"Pretty Print\"\"\"\n    try:\n        result = self.evaluate(args)\n        display.display(pprint.pformat(result))\n    except Exception:\n        pass", "loc": 7}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "execute", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compile", "display.display", "exec", "globals", "isinstance", "repr", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def execute(self, args):\n    try:\n        code = compile(args + '\\n', '<stdin>', 'single')\n        exec(code, globals(), self.scope)\n    except Exception:\n        t, v = sys.exc_info()[:2]\n        if isinstance(t, str):\n            exc_type_name = t\n        else:\n            exc_type_name = t.__name__\n        display.display('***%s:%s' % (exc_type_name, repr(v)))\n        raise", "loc": 12}
{"file": "ansible\\lib\\ansible\\plugins\\strategy\\__init__.py", "class_name": "Debugger", "function_name": "default", "parameters": ["self", "line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.execute"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def default(self, line):\n    try:\n        self.execute(line)\n    except Exception:\n        pass", "loc": 5}
{"file": "ansible\\lib\\ansible\\plugins\\vars\\host_group_vars.py", "class_name": "VarsModule", "function_name": "load_found_files", "parameters": ["self", "loader", "data", "found_files"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "loader.load_from_file"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_found_files(self, loader, data, found_files):\n    for found in found_files:\n        new_data = loader.load_from_file(found, cache='all', unsafe=True, trusted_as_template=True)\n        if new_data:  # ignore empty files\n            data = combine_vars(data, new_data)\n    return data", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\color.py", "class_name": null, "function_name": "parsecolor", "parameters": ["color"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "matches.group", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "SGR parameter string for the specified color name.", "source_code": "def parsecolor(color):\n    \"\"\"SGR parameter string for the specified color name.\"\"\"\n    matches = re.match(r\"color(?P<color>[0-9]+)\"\n                       r\"|(?P<rgb>rgb(?P<red>[0-5])(?P<green>[0-5])(?P<blue>[0-5]))\"\n                       r\"|gray(?P<gray>[0-9]+)\", color)\n    if not matches:\n        return C.COLOR_CODES[color]\n    if matches.group('color'):\n        return u'38;5;%d' % int(matches.group('color'))\n    if matches.group('rgb'):\n        return u'38;5;%d' % (16 + 36 * int(matches.group('red')) +\n                             6 * int(matches.group('green')) +\n                             int(matches.group('blue')))\n    if matches.group('gray'):\n        return u'38;5;%d' % (232 + int(matches.group('gray')))", "loc": 15}
{"file": "ansible\\lib\\ansible\\utils\\color.py", "class_name": null, "function_name": "stringc", "parameters": ["text", "color", "wrap_nonvisible_chars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["parsecolor", "text.split", "u'\\n'.join"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "String in color.", "source_code": "def stringc(text, color, wrap_nonvisible_chars=False):\n    \"\"\"String in color.\"\"\"\n\n    if ANSIBLE_COLOR:\n        color_code = parsecolor(color)\n        fmt = u\"\\033[%sm%s\\033[0m\"\n        if wrap_nonvisible_chars:\n            # This option is provided for use in cases when the\n            # formatting of a command line prompt is needed, such as\n            # `ansible-console`. As said in `readline` sources:\n            # readline/display.c:321\n            # /* Current implementation:\n            #         \\001 (^A) start non-visible characters\n            #         \\002 (^B) end non-visible characters\n            #    all characters except \\001 and \\002 (following a \\001) are copied to\n            #    the returned string; all characters except those between \\001 and\n            #    \\002 are assumed to be `visible'. */\n            fmt = u\"\\001\\033[%sm\\002%s\\001\\033[0m\\002\"\n        return u\"\\n\".join([fmt % (color_code, t) for t in text.split(u'\\n')])\n    else:\n        return text", "loc": 21}
{"file": "ansible\\lib\\ansible\\utils\\color.py", "class_name": null, "function_name": "colorize", "parameters": ["lead", "num", "color"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["str", "stringc"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Print 'lead' = 'num' in 'color'", "source_code": "def colorize(lead, num, color):\n    \"\"\" Print 'lead' = 'num' in 'color' \"\"\"\n    s = u\"%s=%-4s\" % (lead, str(num))\n    if num != 0 and ANSIBLE_COLOR and color is not None:\n        s = stringc(s, color)\n    return s", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\color.py", "class_name": null, "function_name": "hostcolor", "parameters": ["host", "stats", "color"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["stringc"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def hostcolor(host, stats, color=True):\n    if ANSIBLE_COLOR and color:\n        if stats['failures'] != 0 or stats['unreachable'] != 0:\n            return u\"%-37s\" % stringc(host, C.COLOR_ERROR)\n        elif stats['changed'] != 0:\n            return u\"%-37s\" % stringc(host, C.COLOR_CHANGED)\n        else:\n            return u\"%-37s\" % stringc(host, C.COLOR_OK)\n    return u\"%-26s\" % host", "loc": 9}
{"file": "ansible\\lib\\ansible\\utils\\encrypt.py", "class_name": null, "function_name": "random_password", "parameters": ["length", "chars", "seed"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "isinstance", "random.Random", "random.SystemRandom", "random_generator.choice", "range", "type", "u''.join"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a random password string of length containing only chars :kwarg length: The number of characters in the new password.  Defaults to 20. :kwarg chars: The characters to choose from.  The default is all ascii", "source_code": "def random_password(length=DEFAULT_PASSWORD_LENGTH, chars=C.DEFAULT_PASSWORD_CHARS, seed=None):\n    \"\"\"Return a random password string of length containing only chars\n\n    :kwarg length: The number of characters in the new password.  Defaults to 20.\n    :kwarg chars: The characters to choose from.  The default is all ascii\n        letters, ascii digits, and these symbols ``.,:-_``\n    \"\"\"\n    if not isinstance(chars, str):\n        raise AnsibleAssertionError(f'{chars=!r} ({type(chars)}) is not a {type(str)}.')\n\n    if seed is None:\n        random_generator = random.SystemRandom()\n    else:\n        random_generator = random.Random(seed)\n    return u''.join(random_generator.choice(chars) for dummy in range(length))", "loc": 15}
{"file": "ansible\\lib\\ansible\\utils\\encrypt.py", "class_name": null, "function_name": "do_encrypt", "parameters": ["result", "encrypt", "salt_size", "salt", "ident", "rounds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "PasslibHash", "PasslibHash(encrypt).hash"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_encrypt(result, encrypt, salt_size=None, salt=None, ident=None, rounds=None):\n    if PASSLIB_AVAILABLE:\n        return PasslibHash(encrypt).hash(result, salt=salt, salt_size=salt_size, rounds=rounds, ident=ident)\n\n    raise AnsibleError(\"Unable to encrypt nor hash, passlib must be installed.\") from PASSLIB_E", "loc": 5}
{"file": "ansible\\lib\\ansible\\utils\\encrypt.py", "class_name": "PasslibHash", "function_name": "hash", "parameters": ["self", "secret", "salt", "salt_size", "rounds", "ident"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "self._clean_ident", "self._clean_rounds", "self._clean_salt", "self._hash"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def hash(self, secret, salt=None, salt_size=None, rounds=None, ident=None):\n    salt = self._clean_salt(salt)\n    rounds = self._clean_rounds(rounds)\n    ident = self._clean_ident(ident)\n    if salt_size is not None and not isinstance(salt_size, int):\n        raise TypeError(\"salt_size must be an integer\")\n    return self._hash(secret, salt=salt, salt_size=salt_size, rounds=rounds, ident=ident)", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\fqcn.py", "class_name": null, "function_name": "add_internal_fqcns", "parameters": ["names"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["result.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Given a sequence of action/module names, returns a list of these names with the same names with the prefixes `ansible.builtin.` and `ansible.legacy.` added for all names that are not already FQCNs.", "source_code": "def add_internal_fqcns(names):\n    \"\"\"\n    Given a sequence of action/module names, returns a list of these names\n    with the same names with the prefixes `ansible.builtin.` and\n    `ansible.legacy.` added for all names that are not already FQCNs.\n    \"\"\"\n    result = []\n    for name in names:\n        result.append(name)\n        if '.' not in name:\n            result.append('ansible.builtin.%s' % name)\n            result.append('ansible.legacy.%s' % name)\n    return result", "loc": 13}
{"file": "ansible\\lib\\ansible\\utils\\hashing.py", "class_name": null, "function_name": "secure_hash_s", "parameters": ["data", "hash_func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["digest.hexdigest", "digest.update", "hash_func", "to_bytes"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a secure hash hex digest of data.", "source_code": "def secure_hash_s(data, hash_func=sha1):\n    \"\"\" Return a secure hash hex digest of data. \"\"\"\n\n    digest = hash_func()\n    data = to_bytes(data, errors='surrogate_or_strict')\n    digest.update(data)\n    return digest.hexdigest()", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\helpers.py", "class_name": null, "function_name": "pct_to_int", "parameters": ["value", "num_items", "min_value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "isinstance", "value.endswith", "value.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts a given value to a percentage if specified as \"x%\", otherwise converts the given value to an integer.", "source_code": "def pct_to_int(value, num_items, min_value=1):\n    \"\"\"\n    Converts a given value to a percentage if specified as \"x%\",\n    otherwise converts the given value to an integer.\n    \"\"\"\n    if isinstance(value, str) and value.endswith('%'):\n        value_pct = int(value.replace(\"%\", \"\"))\n        return int((value_pct / 100.0) * num_items) or min_value\n    else:\n        return int(value)", "loc": 10}
{"file": "ansible\\lib\\ansible\\utils\\helpers.py", "class_name": null, "function_name": "object_to_dict", "parameters": ["obj", "exclude"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "dir", "getattr", "isinstance", "key.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts an object into a dict making the properties into keys, allows excluding certain keys", "source_code": "def object_to_dict(obj, exclude=None):\n    \"\"\"\n    Converts an object into a dict making the properties into keys, allows excluding certain keys\n    \"\"\"\n    if exclude is None or not isinstance(exclude, list):\n        exclude = []\n    return dict((key, getattr(obj, key)) for key in dir(obj) if not (key.startswith('_') or key in exclude))", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\helpers.py", "class_name": null, "function_name": "deduplicate_list", "parameters": ["original_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["seen.add", "set"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Creates a deduplicated list with the order in which each item is first found.", "source_code": "def deduplicate_list(original_list):\n    \"\"\"\n    Creates a deduplicated list with the order in which each item is first found.\n    \"\"\"\n    seen = set()\n    return [x for x in original_list if x not in seen and not seen.add(x)]", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\jsonrpc.py", "class_name": "JsonRpcServer", "function_name": "handle_request", "parameters": ["self", "request"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["delattr", "display.vvv", "getattr", "isinstance", "json.dumps", "json.loads", "method.startswith", "request.get", "rpc_method", "self.error", "self.internal_error", "self.invalid_request", "self.method_not_found", "self.response", "setattr", "to_text", "traceback.format_exc"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def handle_request(self, request):\n    request = json.loads(to_text(request, errors='surrogate_then_replace'))\n\n    method = request.get('method')\n\n    if method.startswith('rpc.') or method.startswith('_'):\n        error = self.invalid_request()\n        return json.dumps(error)\n\n    args, kwargs = request.get('params')\n    setattr(self, '_identifier', request.get('id'))\n\n    rpc_method = None\n    for obj in self._objects:\n        rpc_method = getattr(obj, method, None)\n        if rpc_method:\n            break\n\n    if not rpc_method:\n        error = self.method_not_found()\n        response = json.dumps(error)\n    else:\n        try:\n            result = rpc_method(*args, **kwargs)\n        except ConnectionError as exc:\n            display.vvv(traceback.format_exc())\n            try:\n                error = self.error(code=exc.code, message=to_text(exc))\n            except AttributeError:\n                error = self.internal_error(data=to_text(exc))\n            response = json.dumps(error)\n        except Exception as exc:\n            display.vvv(traceback.format_exc())\n            error = self.internal_error(data=to_text(exc, errors='surrogate_then_replace'))\n            response = json.dumps(error)\n        else:\n            if isinstance(result, dict) and 'jsonrpc' in result:\n                response = result\n            else:\n                response = self.response(result)\n\n            try:\n                response = json.dumps(response)\n            except Exception as exc:\n                display.vvv(traceback.format_exc())\n                error = self.internal_error(data=to_text(exc, errors='surrogate_then_replace'))\n                response = json.dumps(error)\n\n    delattr(self, '_identifier')\n\n    return response", "loc": 51}
{"file": "ansible\\lib\\ansible\\utils\\jsonrpc.py", "class_name": "JsonRpcServer", "function_name": "response", "parameters": ["self", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "pickle.dumps", "self.header", "to_text"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def response(self, result=None):\n    response = self.header()\n    if isinstance(result, bytes):\n        result = to_text(result)\n    if not isinstance(result, str):\n        response[\"result_type\"] = \"pickle\"\n        result = to_text(pickle.dumps(result), errors='surrogateescape')\n    response['result'] = result\n    return response", "loc": 9}
{"file": "ansible\\lib\\ansible\\utils\\jsonrpc.py", "class_name": "JsonRpcServer", "function_name": "error", "parameters": ["self", "code", "message", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.header"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def error(self, code, message, data=None):\n    response = self.header()\n    error = {'code': code, 'message': message}\n    if data:\n        error['data'] = data\n    response['error'] = error\n    return response", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\listify.py", "class_name": null, "function_name": "listify_lookup_plugin_terms", "parameters": ["terms", "templar", "fail_on_undefined"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "terms.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def listify_lookup_plugin_terms(terms, templar=None, fail_on_undefined=True):\n    # deprecated: description=\"Calling listify_lookup_plugin_terms function is not necessary; the function should be deprecated.\" core_version=\"2.23\"\n    # display.deprecated(\n    #     msg='The \"listify_lookup_plugin_terms\" function is not required for lookup terms to be templated.',\n    #     version='2.27',\n    #     help_text='If needed, implement custom `strip` or list-wrapping in the caller.',\n    # )\n\n    if isinstance(terms, str):\n        terms = terms.strip()\n\n    if isinstance(terms, str) or not isinstance(terms, Iterable):\n        terms = [terms]\n\n    return terms", "loc": 15}
{"file": "ansible\\lib\\ansible\\utils\\lock.py", "class_name": null, "function_name": "lock_decorator", "parameters": ["attr", "lock"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "getattr", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "This decorator is a generic implementation that allows you to either use a pre-defined instance attribute as the location of the lock, or to explicitly pass a lock object.", "source_code": "def lock_decorator(attr='missing_lock_attr', lock=None):\n    \"\"\"This decorator is a generic implementation that allows you\n    to either use a pre-defined instance attribute as the location\n    of the lock, or to explicitly pass a lock object.\n\n    This code was implemented with ``threading.Lock`` in mind, but\n    may work with other locks, assuming that they function as\n    context managers.\n\n    When using ``attr``, the assumption is the first argument to\n    the wrapped method, is ``self`` or ``cls``.\n\n    Examples:\n\n        @lock_decorator(attr='_callback_lock')\n        def send_callback(...):\n\n        @lock_decorator(lock=threading.Lock())\n        def some_method(...):\n    \"\"\"\n    def outer(func):\n        @wraps(func)\n        def inner(*args, **kwargs):\n            # Python2 doesn't have ``nonlocal``\n            # assign the actual lock to ``_lock``\n            if lock is None:\n                _lock = getattr(args[0], attr)\n            else:\n                _lock = lock\n            with _lock:\n                return func(*args, **kwargs)\n        return inner\n    return outer", "loc": 33}
{"file": "ansible\\lib\\ansible\\utils\\lock.py", "class_name": null, "function_name": "outer", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "getattr", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def outer(func):\n    @wraps(func)\n    def inner(*args, **kwargs):\n        # Python2 doesn't have ``nonlocal``\n        # assign the actual lock to ``_lock``\n        if lock is None:\n            _lock = getattr(args[0], attr)\n        else:\n            _lock = lock\n        with _lock:\n            return func(*args, **kwargs)\n    return inner", "loc": 12}
{"file": "ansible\\lib\\ansible\\utils\\lock.py", "class_name": null, "function_name": "inner", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "getattr", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner(*args, **kwargs):\n    # Python2 doesn't have ``nonlocal``\n    # assign the actual lock to ``_lock``\n    if lock is None:\n        _lock = getattr(args[0], attr)\n    else:\n        _lock = lock\n    with _lock:\n        return func(*args, **kwargs)", "loc": 9}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "merge_fragment", "parameters": ["target", "source"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "frozenset", "isinstance", "sorted", "source.items", "value.add", "value.update"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def merge_fragment(target, source):\n\n    for key, value in source.items():\n        if key in target:\n            # assumes both structures have same type\n            if isinstance(target[key], MutableMapping):\n                value.update(target[key])\n            elif isinstance(target[key], MutableSet):\n                value.add(target[key])\n            elif isinstance(target[key], MutableSequence):\n                value = sorted(frozenset(value + target[key]))\n            else:\n                raise Exception(\"Attempt to extend a documentation fragment, invalid type for %s\" % key)\n        target[key] = value", "loc": 14}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "add_collection_to_versions_and_dates", "parameters": ["fragment", "collection_name", "is_module", "return_docs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_process_versions_and_dates"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_collection_to_versions_and_dates(fragment, collection_name, is_module, return_docs=False):\n    def add(options, option, collection_name_field):\n        if collection_name_field not in options:\n            options[collection_name_field] = collection_name\n\n    _process_versions_and_dates(fragment, is_module, return_docs, add)", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "remove_current_collection_from_versions_and_dates", "parameters": ["fragment", "collection_name", "is_module", "return_docs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_process_versions_and_dates", "options.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_current_collection_from_versions_and_dates(fragment, collection_name, is_module, return_docs=False):\n    def remove(options, option, collection_name_field):\n        if options.get(collection_name_field) == collection_name:\n            del options[collection_name_field]\n\n    _process_versions_and_dates(fragment, is_module, return_docs, remove)", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "add_fragments", "parameters": ["doc", "filename", "fragment_loader", "is_module", "section"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "'.'.join", "'unknown doc_fragment(s) in file {0}: {1}'.format", "AnsibleError", "AnsibleFragmentError", "_tags.Origin", "_tags.Origin(path=filename).tag", "_tags.TrustedAsTemplate", "_tags.TrustedAsTemplate().tag", "add_collection_to_versions_and_dates", "doc.pop", "doc[doc_key].extend", "fragment.pop", "fragment_loader.get", "fragment_slug.rsplit", "fragment_slug.strip", "fragments.split", "getattr", "isinstance", "merge_fragment", "real_fragment_name.split", "splitname[1].upper", "to_native", "unknown_fragments.append", "yaml.load"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def add_fragments(doc, filename, fragment_loader, is_module=False, section='DOCUMENTATION'):\n\n    if section not in _FRAGMENTABLE:\n        raise AnsibleError(f\"Invalid fragment section ({section}) passed to render {filename}, it can only be one of {_FRAGMENTABLE!r}\")\n\n    fragments = doc.pop('extends_documentation_fragment', [])\n\n    if isinstance(fragments, str):\n        fragments = fragments.split(',')\n\n    unknown_fragments = []\n\n    # doc_fragments are allowed to specify a fragment var other than DOCUMENTATION or RETURN\n    # with a . separator; this is complicated by collections-hosted doc_fragments that\n    # use the same separator. Assume it's collection-hosted normally first, try to load\n    # as-specified. If failure, assume the right-most component is a var, split it off,\n    # and retry the load.\n    for fragment_slug in fragments:\n        fragment_name = fragment_slug.strip()\n        fragment_var = section\n\n        fragment_class = fragment_loader.get(fragment_name)\n        if fragment_class is None and '.' in fragment_slug:\n            splitname = fragment_slug.rsplit('.', 1)\n            fragment_name = splitname[0]\n            fragment_var = splitname[1].upper()\n            fragment_class = fragment_loader.get(fragment_name)\n\n        if fragment_class is None:\n            unknown_fragments.append(fragment_slug)\n            continue\n\n        # trust-tagged source propagates to loaded values; expressions and templates in config require trust\n        fragment_yaml = _tags.TrustedAsTemplate().tag(getattr(fragment_class, fragment_var, None))\n        if fragment_yaml is None:\n            if fragment_var not in _FRAGMENTABLE:\n                # if it's asking for something specific that's missing, that's an error\n                unknown_fragments.append(fragment_slug)\n                continue\n            else:\n                fragment_yaml = '{}'  # TODO: this is still an error later since we require 'options' below...\n\n        fragment = yaml.load(_tags.Origin(path=filename).tag(fragment_yaml), Loader=AnsibleLoader)\n\n        real_fragment_name = getattr(fragment_class, 'ansible_name')\n        real_collection_name = '.'.join(real_fragment_name.split('.')[0:2]) if '.' in real_fragment_name else ''\n        add_collection_to_versions_and_dates(fragment, real_collection_name, is_module=is_module, return_docs=(section == 'RETURN'))\n\n        if section == 'DOCUMENTATION':\n            # notes, seealso, options and attributes entries are specificly merged, but only occur in documentation section\n            for doc_key in ['notes', 'seealso']:\n                if doc_key in fragment:\n                    entries = fragment.pop(doc_key)\n                    if entries:\n                        if doc_key not in doc:\n                            doc[doc_key] = []\n                        doc[doc_key].extend(entries)\n\n            if 'options' not in fragment and 'attributes' not in fragment:\n                raise AnsibleFragmentError(\"missing options or attributes in fragment (%s), possibly misformatted?: %s\" % (fragment_name, filename))\n\n            # ensure options themselves are directly merged\n            for doc_key in ['options', 'attributes']:\n                if doc_key in fragment:\n                    if doc_key in doc:\n                        try:\n                            merge_fragment(doc[doc_key], fragment.pop(doc_key))\n                        except Exception as e:\n                            raise AnsibleFragmentError(\"%s %s (%s) of unknown type: %s\" % (to_native(e), doc_key, fragment_name, filename))\n                    else:\n                        doc[doc_key] = fragment.pop(doc_key)\n\n        # merge rest of the sections\n        try:\n            merge_fragment(doc, fragment)\n        except Exception as e:\n            raise AnsibleFragmentError(\"%s (%s) of unknown type: %s\" % (to_native(e), fragment_name, filename))\n\n    if unknown_fragments:\n        raise AnsibleFragmentError('unknown doc_fragment(s) in file {0}: {1}'.format(filename, to_native(', '.join(unknown_fragments))))", "loc": 80}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "get_docstring", "parameters": ["filename", "fragment_loader", "verbose", "ignore_errors", "collection_name", "is_module", "plugin_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_collection_to_versions_and_dates", "add_fragments", "data.get", "read_docstring"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "DOCUMENTATION can be extended using documentation fragments loaded by the PluginLoader from the doc_fragments plugins.", "source_code": "def get_docstring(filename, fragment_loader, verbose=False, ignore_errors=False, collection_name=None, is_module=None, plugin_type=None):\n    \"\"\"\n    DOCUMENTATION can be extended using documentation fragments loaded by the PluginLoader from the doc_fragments plugins.\n    \"\"\"\n\n    if is_module is None:\n        if plugin_type is None:\n            is_module = False\n        else:\n            is_module = (plugin_type == 'module')\n    else:\n        # TODO deprecate is_module argument, now that we have 'type'\n        pass\n\n    data = read_docstring(filename, verbose=verbose, ignore_errors=ignore_errors)\n\n    if data.get('doc', False):\n        # add collection name to versions and dates\n        if collection_name is not None:\n            add_collection_to_versions_and_dates(data['doc'], collection_name, is_module=is_module)\n\n        # add fragments to documentation\n        add_fragments(data['doc'], filename, fragment_loader=fragment_loader, is_module=is_module, section='DOCUMENTATION')\n\n    if data.get('returndocs', False):\n        # add collection name to versions and dates\n        if collection_name is not None:\n            add_collection_to_versions_and_dates(data['returndocs'], collection_name, is_module=is_module, return_docs=True)\n\n        # add fragments to return\n        add_fragments(data['returndocs'], filename, fragment_loader=fragment_loader, is_module=is_module, section='RETURN')\n\n    return data['doc'], data['plainexamples'], data['returndocs'], data['metadata']", "loc": 33}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "get_versioned_doclink", "parameters": ["path"], "param_types": {}, "return_type": null, "param_doc": {"path": "relative path to a document under docs/docsite/rst;"}, "return_doc": "absolute URL to the specified doc for the current version of Ansible", "raises_doc": [], "called_functions": ["'(unable to create versioned doc link for path {0}: {1})'.format", "'invalid version ({0})'.format", "'{0}.{1}'.format", "'{0}{1}/{2}'.format", "C.config.get_config_value", "RuntimeError", "ansible_version.split", "any", "base_url.endswith", "len", "path.startswith", "split_ver[2].startswith", "to_native"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "returns a versioned documentation link for the current Ansible major.minor version; used to generate in-product warning/error links to the configured DOCSITE_ROOT_URL (eg, https://docs.ansible.com/ansible/2.8/somepath/doc.html)", "source_code": "def get_versioned_doclink(path):\n    \"\"\"\n    returns a versioned documentation link for the current Ansible major.minor version; used to generate\n    in-product warning/error links to the configured DOCSITE_ROOT_URL\n    (eg, https://docs.ansible.com/ansible/2.8/somepath/doc.html)\n\n    :param path: relative path to a document under docs/docsite/rst;\n    :return: absolute URL to the specified doc for the current version of Ansible\n    \"\"\"\n    path = to_native(path)\n    try:\n        base_url = C.config.get_config_value('DOCSITE_ROOT_URL')\n        if not base_url.endswith('/'):\n            base_url += '/'\n        if path.startswith('/'):\n            path = path[1:]\n        split_ver = ansible_version.split('.')\n        if len(split_ver) < 3:\n            raise RuntimeError('invalid version ({0})'.format(ansible_version))\n\n        doc_version = '{0}.{1}'.format(split_ver[0], split_ver[1])\n\n        # check to see if it's a X.Y.0 non-rc prerelease or dev release, if so, assume devel (since the X.Y doctree\n        # isn't published until beta-ish)\n        if split_ver[2].startswith('0'):\n            # exclude rc; we should have the X.Y doctree live by rc1\n            if any((pre in split_ver[2]) for pre in ['a', 'b']) or len(split_ver) > 3 and 'dev' in split_ver[3]:\n                doc_version = 'devel'\n\n        return '{0}{1}/{2}'.format(base_url, doc_version, path)\n    except Exception as ex:\n        return '(unable to create versioned doc link for path {0}: {1})'.format(path, to_native(ex))", "loc": 32}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "find_plugin_docfile", "parameters": ["plugin", "plugin_type", "loader"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "AnsiblePluginNotFound", "Path", "_find_adjacent", "loader.find_plugin_with_context", "loader.get_with_context", "to_native"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "if the plugin lives in a non-python file (eg, win_X.ps1), require the corresponding 'sidecar' file for docs", "source_code": "def find_plugin_docfile(plugin, plugin_type, loader):\n    \"\"\"  if the plugin lives in a non-python file (eg, win_X.ps1), require the corresponding 'sidecar' file for docs \"\"\"\n\n    context = loader.find_plugin_with_context(plugin, ignore_deprecated=False, check_aliases=True)\n    if (not context or not context.resolved) and plugin_type in ('filter', 'test'):\n        # should only happen for filters/test\n        plugin_obj, context = loader.get_with_context(plugin)\n\n    if not context or not context.resolved:\n        raise AnsiblePluginNotFound('%s was not found' % (plugin), plugin_load_context=context)\n\n    docfile = Path(context.plugin_resolved_path)\n    if docfile.suffix not in C.DOC_EXTENSIONS:\n        # only look for adjacent if plugin file does not support documents\n        filenames = _find_adjacent(docfile, plugin, C.DOC_EXTENSIONS)\n        filename = filenames[0] if filenames else None\n    else:\n        filename = to_native(docfile)\n\n    if filename is None:\n        raise AnsibleError('%s cannot contain DOCUMENTATION nor does it have a companion documentation file' % (plugin))\n\n    return filename, context", "loc": 23}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "get_plugin_docs", "parameters": ["plugin", "plugin_type", "loader", "fragment_loader", "verbose"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "_find_adjacent", "find_plugin_docfile", "get_docstring"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_plugin_docs(plugin, plugin_type, loader, fragment_loader, verbose):\n\n    # find plugin doc file, if it doesn't exist this will throw error, we let it through\n    # can raise exception and short circuit when 'not found'\n    filename, context = find_plugin_docfile(plugin, plugin_type, loader)\n    collection_name = context.plugin_resolved_collection\n\n    try:\n        docs = get_docstring(filename, fragment_loader, verbose=verbose, collection_name=collection_name, plugin_type=plugin_type)\n    except Exception as ex:\n        raise AnsibleParserError(f'{plugin_type} plugin {plugin!r} did not contain a DOCUMENTATION attribute in {filename!r}.') from ex\n\n    # no good? try adjacent\n    if not docs[0]:\n        for newfile in _find_adjacent(filename, plugin, C.DOC_EXTENSIONS):\n            try:\n                docs = get_docstring(newfile, fragment_loader, verbose=verbose, collection_name=collection_name, plugin_type=plugin_type)\n                filename = newfile\n                if docs[0] is not None:\n                    break\n            except Exception as ex:\n                raise AnsibleParserError(f'{plugin_type} plugin {plugin!r} adjacent file did not contain a DOCUMENTATION attribute in {filename!r}.') from ex\n\n    # add extra data to docs[0] (aka 'DOCUMENTATION')\n    if docs[0] is None:\n        raise AnsibleParserError(f'No documentation available for {plugin_type} plugin {plugin!r} in {filename!r}.')\n\n    docs[0]['filename'] = filename\n    docs[0]['collection'] = collection_name\n    docs[0]['plugin_name'] = context.resolved_fqcn\n\n    return docs", "loc": 32}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "process_deprecation", "parameters": ["deprecation", "top_level"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_deprecation(deprecation, top_level=False):\n    collection_name = 'removed_from_collection' if top_level else 'collection_name'\n    if not isinstance(deprecation, MutableMapping):\n        return\n    if (is_module or top_level) and 'removed_in' in deprecation:  # used in module deprecations\n        callback(deprecation, 'removed_in', collection_name)\n    if 'removed_at_date' in deprecation:\n        callback(deprecation, 'removed_at_date', collection_name)\n    if not (is_module or top_level) and 'version' in deprecation:  # used in plugin option deprecations\n        callback(deprecation, 'version', collection_name)", "loc": 10}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "process_option_specifiers", "parameters": ["specifiers"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "isinstance", "process_deprecation", "specifier.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_option_specifiers(specifiers):\n    for specifier in specifiers:\n        if not isinstance(specifier, MutableMapping):\n            continue\n        if 'version_added' in specifier:\n            callback(specifier, 'version_added', 'version_added_collection')\n        if isinstance(specifier.get('deprecated'), MutableMapping):\n            process_deprecation(specifier['deprecated'])", "loc": 8}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "process_options", "parameters": ["options"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "isinstance", "option.get", "options.values", "process_deprecation", "process_option_specifiers", "process_options"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_options(options):\n    for option in options.values():\n        if not isinstance(option, MutableMapping):\n            continue\n        if 'version_added' in option:\n            callback(option, 'version_added', 'version_added_collection')\n        if not is_module:\n            if isinstance(option.get('env'), list):\n                process_option_specifiers(option['env'])\n            if isinstance(option.get('ini'), list):\n                process_option_specifiers(option['ini'])\n            if isinstance(option.get('vars'), list):\n                process_option_specifiers(option['vars'])\n            if isinstance(option.get('deprecated'), MutableMapping):\n                process_deprecation(option['deprecated'])\n        if isinstance(option.get('suboptions'), MutableMapping):\n            process_options(option['suboptions'])", "loc": 17}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "process_return_values", "parameters": ["return_values"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "isinstance", "process_return_values", "return_value.get", "return_values.values"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_return_values(return_values):\n    for return_value in return_values.values():\n        if not isinstance(return_value, MutableMapping):\n            continue\n        if 'version_added' in return_value:\n            callback(return_value, 'version_added', 'version_added_collection')\n        if isinstance(return_value.get('contains'), MutableMapping):\n            process_return_values(return_value['contains'])", "loc": 8}
{"file": "ansible\\lib\\ansible\\utils\\plugin_docs.py", "class_name": null, "function_name": "process_attributes", "parameters": ["attributes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["attributes.values", "callback", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_attributes(attributes):\n    for attribute in attributes.values():\n        if not isinstance(attribute, MutableMapping):\n            continue\n        if 'version_added' in attribute:\n            callback(attribute, 'version_added', 'version_added_collection')", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\unicode.py", "class_name": null, "function_name": "unicode_wrap", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "to_text"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "If a function returns a string, force it to be a text string. Use with partial to ensure that filter plugins will return text values.", "source_code": "def unicode_wrap(func, *args, **kwargs):\n    \"\"\"If a function returns a string, force it to be a text string.\n\n    Use with partial to ensure that filter plugins will return text values.\n    \"\"\"\n    return to_text(func(*args, **kwargs), nonstring='passthru')", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\unsafe_proxy.py", "class_name": null, "function_name": "wrap_var", "parameters": ["v"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleUnsafeBytes", "AnsibleUnsafeText", "_wrap_dict", "_wrap_sequence", "_wrap_set", "is_sequence", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrap_var(v):\n    # maintain backward compat by recursively *un* marking TrustedAsTemplate\n    if v is None or isinstance(v, AnsibleUnsafe):\n        return v\n\n    if isinstance(v, Mapping):\n        v = _wrap_dict(v)\n    elif isinstance(v, Set):\n        v = _wrap_set(v)\n    elif is_sequence(v):\n        v = _wrap_sequence(v)\n    elif isinstance(v, bytes):\n        v = AnsibleUnsafeBytes(v)\n    elif isinstance(v, str):\n        v = AnsibleUnsafeText(v)\n\n    return v", "loc": 17}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "combine_vars", "parameters": ["a", "b", "merge"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_validate_mutable_mappings", "merge_hash"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a copy of dictionaries of variables based on configured hash behavior", "source_code": "def combine_vars(a, b, merge=None):\n    \"\"\"\n    Return a copy of dictionaries of variables based on configured hash behavior\n    \"\"\"\n\n    if merge or merge is None and C.DEFAULT_HASH_BEHAVIOUR == \"merge\":\n        return merge_hash(a, b)\n\n    # HASH_BEHAVIOUR == 'replace'\n    _validate_mutable_mappings(a, b)\n    result = a | b\n    return result", "loc": 12}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "merge_hash", "parameters": ["x", "y", "recursive", "list_merge"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_validate_mutable_mappings", "isinstance", "merge_hash", "x.copy", "x.update", "y.copy", "y.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a new dictionary result of the merges of y into x, so that keys from y take precedence over keys from x. (x and y aren't modified)", "source_code": "def merge_hash(x, y, recursive=True, list_merge='replace'):\n    \"\"\"\n    Return a new dictionary result of the merges of y into x,\n    so that keys from y take precedence over keys from x.\n    (x and y aren't modified)\n    \"\"\"\n    if list_merge not in ('replace', 'keep', 'append', 'prepend', 'append_rp', 'prepend_rp'):\n        raise AnsibleError(\"merge_hash: 'list_merge' argument can only be equal to 'replace', 'keep', 'append', 'prepend', 'append_rp' or 'prepend_rp'\")\n\n    # verify x & y are dicts\n    _validate_mutable_mappings(x, y)\n\n    # to speed things up: if x is empty or equal to y, return y\n    # (this `if` can be remove without impact on the function\n    #  except performance)\n    if x == {} or x == y:\n        return y.copy()\n    if y == {}:\n        return x\n\n    # in the following we will copy elements from y to x, but\n    # we don't want to modify x, so we create a copy of it\n    x = x.copy()\n\n    # to speed things up: use dict.update if possible\n    # (this `if` can be remove without impact on the function\n    #  except performance)\n    if not recursive and list_merge == 'replace':\n        x.update(y)\n        return x\n\n    # insert each element of y in x, overriding the one in x\n    # (as y has higher priority)\n    # we copy elements from y to x instead of x to y because\n    # there is a high probability x will be the \"default\" dict the user\n    # want to \"patch\" with y\n    # therefore x will have much more elements than y\n    for key, y_value in y.items():\n        # if `key` isn't in x\n        # update x and move on to the next element of y\n        if key not in x:\n            x[key] = y_value\n            continue\n        # from this point we know `key` is in x\n\n        x_value = x[key]\n\n        # if both x's element and y's element are dicts\n        # recursively \"combine\" them or override x's with y's element\n        # depending on the `recursive` argument\n        # and move on to the next element of y\n        if isinstance(x_value, MutableMapping) and isinstance(y_value, MutableMapping):\n            if recursive:\n                x[key] = merge_hash(x_value, y_value, recursive, list_merge)\n            else:\n                x[key] = y_value\n            continue\n\n        # if both x's element and y's element are lists\n        # \"merge\" them depending on the `list_merge` argument\n        # and move on to the next element of y\n        if isinstance(x_value, MutableSequence) and isinstance(y_value, MutableSequence):\n            if list_merge == 'replace':\n                # replace x value by y's one as it has higher priority\n                x[key] = y_value\n            elif list_merge == 'append':\n                x[key] = x_value + y_value\n            elif list_merge == 'prepend':\n                x[key] = y_value + x_value\n            elif list_merge == 'append_rp':\n                # append all elements from y_value (high prio) to x_value (low prio)\n                # and remove x_value elements that are also in y_value\n                # we don't remove elements from x_value nor y_value that were already in double\n                # (we assume that there is a reason if there where such double elements)\n                # _rp stands for \"remove present\"\n                x[key] = [z for z in x_value if z not in y_value] + y_value\n            elif list_merge == 'prepend_rp':\n                # same as 'append_rp' but y_value elements are prepend\n                x[key] = y_value + [z for z in x_value if z not in y_value]\n            # else 'keep'\n            #   keep x value even if y it's of higher priority\n            #   it's done by not changing x[key]\n            continue\n\n        # else just override x's element with y's one\n        x[key] = y_value\n\n    return x", "loc": 88}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "load_extra_vars", "parameters": ["loader"], "param_types": {"loader": "DataLoader"}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleOptionsError", "combine_vars", "context.CLIARGS.get", "extra_vars_opt.startswith", "getattr", "isinstance", "loader.load", "loader.load_from_file", "parse_kv", "to_text", "tuple"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_extra_vars(loader: DataLoader) -> dict[str, t.Any]:\n\n    if not getattr(load_extra_vars, 'extra_vars', None):\n        extra_vars: dict[str, t.Any] = {}\n        for extra_vars_opt in context.CLIARGS.get('extra_vars', tuple()):\n            extra_vars_opt = to_text(extra_vars_opt, errors='surrogate_or_strict')\n            if extra_vars_opt is None or not extra_vars_opt:\n                continue\n\n            if extra_vars_opt.startswith(u\"@\"):\n                # Argument is a YAML file (JSON is a subset of YAML)\n                data = loader.load_from_file(extra_vars_opt[1:], trusted_as_template=True)\n            elif extra_vars_opt[0] in [u'/', u'.']:\n                raise AnsibleOptionsError(\"Please prepend extra_vars filename '%s' with '@'\" % extra_vars_opt)\n            elif extra_vars_opt[0] in [u'[', u'{']:\n                # Arguments as YAML\n                data = loader.load(extra_vars_opt)\n            else:\n                # Arguments as Key-value\n                data = parse_kv(extra_vars_opt)\n\n            if isinstance(data, MutableMapping):\n                extra_vars = combine_vars(extra_vars, data)\n            else:\n                raise AnsibleOptionsError(\"Invalid extra vars data supplied. '%s' could not be made into a dictionary\" % extra_vars_opt)\n\n        load_extra_vars.extra_vars = extra_vars\n\n    return load_extra_vars.extra_vars", "loc": 29}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "load_options_vars", "parameters": ["version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["attrs.items", "context.CLIARGS.get", "getattr", "setattr"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_options_vars(version):\n\n    if not getattr(load_options_vars, 'options_vars', None):\n        if version is None:\n            version = 'Unknown'\n        options_vars = {'ansible_version': version}\n        attrs = {'check': 'check_mode',\n                 'diff': 'diff_mode',\n                 'forks': 'forks',\n                 'inventory': 'inventory_sources',\n                 'skip_tags': 'skip_tags',\n                 'subset': 'limit',\n                 'tags': 'run_tags',\n                 'verbosity': 'verbosity'}\n\n        for attr, alias in attrs.items():\n            opt = context.CLIARGS.get(attr)\n            if opt is not None:\n                options_vars['ansible_%s' % alias] = opt\n\n        setattr(load_options_vars, 'options_vars', options_vars)\n\n    return load_options_vars.options_vars", "loc": 23}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "isidentifier", "parameters": ["ident"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ident.isascii", "ident.isidentifier", "isinstance", "keyword.iskeyword"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Determine if string is valid identifier. The purpose of this function is to be used to validate any variables created in a play to be valid Python identifiers and to not conflict with Python keywords", "source_code": "def isidentifier(ident):\n    \"\"\"Determine if string is valid identifier.\n\n    The purpose of this function is to be used to validate any variables created in\n    a play to be valid Python identifiers and to not conflict with Python keywords\n    to prevent unexpected behavior. Since Python 2 and Python 3 differ in what\n    a valid identifier is, this function unifies the validation so playbooks are\n    portable between the two. The following changes were made:\n\n        * disallow non-ascii characters (Python 3 allows for them as opposed to Python 2)\n\n    :arg ident: A text string of identifier to check. Note: It is callers\n        responsibility to convert ident to text if it is not already.\n\n    Originally posted at https://stackoverflow.com/a/29586366\n    \"\"\"\n    # deprecated: description='Use validate_variable_name instead.' core_version='2.23'\n\n    if not isinstance(ident, str):\n        return False\n\n    if not ident.isascii():\n        return False\n\n    if not ident.isidentifier():\n        return False\n\n    if keyword.iskeyword(ident):\n        return False\n\n    return True", "loc": 31}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "validate_variable_name", "parameters": ["name"], "param_types": {"name": "object"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "isinstance", "name.isascii", "name.isidentifier", "native_type_name", "str", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validate the given variable name is valid, raising an AnsibleError if it is not.", "source_code": "def validate_variable_name(name: object) -> None:\n    \"\"\"Validate the given variable name is valid, raising an AnsibleError if it is not.\"\"\"\n    if isinstance(name, str) and name.isidentifier() and name.isascii() and name not in _jinja_bits.JINJA_KEYWORDS:\n        return\n\n    if isinstance(name, (str, int, float, bool, type(None))):\n        key_description = f'name {str(name)!r}'  # show common scalar key names as strings\n    else:\n        key_description = 'name'\n\n    if not isinstance(name, str):\n        key_description += f' of type {native_type_name(name)!r}'  # show the type name of all non-string keys\n\n    raise AnsibleError(\n        message=f'Invalid variable {key_description}.',\n        help_text='Variable names must be strings starting with a letter or underscore character, and contain only letters, numbers and underscores.',\n        obj=name,\n    )", "loc": 18}
{"file": "ansible\\lib\\ansible\\utils\\vars.py", "class_name": null, "function_name": "transform_to_native_types", "parameters": ["value", "redact"], "param_types": {"value": "object", "redact": "bool"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_json.AnsibleVariableVisitor", "avv.visit"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Recursively transform the given value to Python native types. Potentially sensitive values such as individually vaulted variables will be redacted unless ``redact=False`` is passed. Which values are considered potentially sensitive may change in future releases.", "source_code": "def transform_to_native_types(\n    value: object,\n    redact: bool = True,\n) -> t.Any:\n    \"\"\"\n    Recursively transform the given value to Python native types.\n    Potentially sensitive values such as individually vaulted variables will be redacted unless ``redact=False`` is passed.\n    Which values are considered potentially sensitive may change in future releases.\n    Types which cannot be converted to Python native types will result in an error.\n    \"\"\"\n    avv = _json.AnsibleVariableVisitor(\n        convert_mapping_to_dict=True,\n        convert_sequence_to_list=True,\n        convert_custom_scalars=True,\n        convert_to_native_values=True,\n        apply_transforms=True,\n        visit_keys=True,  # ensure that keys are also converted\n        encrypted_string_behavior=_json.EncryptedStringBehavior.REDACT if redact else _json.EncryptedStringBehavior.DECRYPT,\n    )\n\n    return avv.visit(value)", "loc": 21}
{"file": "ansible\\lib\\ansible\\utils\\version.py", "class_name": "SemanticVersion", "function_name": "from_loose_version", "parameters": ["loose_version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "SemanticVersion", "ValueError", "extra.group", "isinstance", "len", "re.search", "set", "str", "type", "version.index"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "This method is designed to take a ``LooseVersion`` and attempt to construct a ``SemanticVersion`` from it This is useful where you want to do simple version math", "source_code": "def from_loose_version(loose_version):\n    \"\"\"This method is designed to take a ``LooseVersion``\n    and attempt to construct a ``SemanticVersion`` from it\n\n    This is useful where you want to do simple version math\n    without requiring users to provide a compliant semver.\n    \"\"\"\n    if not isinstance(loose_version, LooseVersion):\n        raise ValueError(\"%r is not a LooseVersion\" % loose_version)\n\n    try:\n        version = loose_version.version[:]\n    except AttributeError:\n        raise ValueError(\"%r is not a LooseVersion\" % loose_version)\n\n    extra_idx = 3\n    for marker in ('-', '+'):\n        try:\n            idx = version.index(marker)\n        except ValueError:\n            continue\n        else:\n            if idx < extra_idx:\n                extra_idx = idx\n    version = version[:extra_idx]\n\n    if version and set(type(v) for v in version) != set((int,)):\n        raise ValueError(\"Non integer values in %r\" % loose_version)\n\n    # Extra is everything to the right of the core version\n    extra = re.search('[+-].+$', loose_version.vstring)\n\n    version = version + [0] * (3 - len(version))\n    return SemanticVersion(\n        '%s%s' % (\n            '.'.join(str(v) for v in version),\n            extra.group(0) if extra else ''\n        )\n    )", "loc": 39}
{"file": "ansible\\lib\\ansible\\utils\\version.py", "class_name": "SemanticVersion", "function_name": "parse", "parameters": ["self", "vstring"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SEMVER_RE.match", "ValueError", "_Alpha", "_Numeric", "buildmetadata.split", "int", "match.group", "prerelease.split", "tuple", "x.isdigit"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(self, vstring):\n    match = SEMVER_RE.match(vstring)\n    if not match:\n        raise ValueError(\"invalid semantic version '%s'\" % vstring)\n\n    (major, minor, patch, prerelease, buildmetadata) = match.group(1, 2, 3, 4, 5)\n    self.vstring = vstring\n    self.major = int(major)\n    self.minor = int(minor)\n    self.patch = int(patch)\n\n    if prerelease:\n        self.prerelease = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in prerelease.split('.'))\n    if buildmetadata:\n        self.buildmetadata = tuple(_Numeric(x) if x.isdigit() else _Alpha(x) for x in buildmetadata.split('.'))", "loc": 15}
{"file": "ansible\\lib\\ansible\\utils\\_junit_xml.py", "class_name": "TestResult", "function_name": "get_xml_element", "parameters": ["self"], "param_types": {}, "return_type": "ET.Element", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ET.Element", "self.get_attributes"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return an XML element representing this instance.", "source_code": "def get_xml_element(self) -> ET.Element:\n    \"\"\"Return an XML element representing this instance.\"\"\"\n    element = ET.Element(self.tag, self.get_attributes())\n    element.text = self.output\n\n    return element", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\_junit_xml.py", "class_name": "TestCase", "function_name": "get_xml_element", "parameters": ["self"], "param_types": {}, "return_type": "ET.Element", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ET.Element", "ET.SubElement", "element.extend", "error.get_xml_element", "failure.get_xml_element", "self.get_attributes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return an XML element representing this instance.", "source_code": "def get_xml_element(self) -> ET.Element:\n    \"\"\"Return an XML element representing this instance.\"\"\"\n    element = ET.Element('testcase', self.get_attributes())\n\n    if self.skipped:\n        ET.SubElement(element, 'skipped').text = self.skipped\n\n    element.extend([error.get_xml_element() for error in self.errors])\n    element.extend([failure.get_xml_element() for failure in self.failures])\n\n    if self.system_out:\n        ET.SubElement(element, 'system-out').text = self.system_out\n\n    if self.system_err:\n        ET.SubElement(element, 'system-err').text = self.system_err\n\n    return element", "loc": 17}
{"file": "ansible\\lib\\ansible\\utils\\_junit_xml.py", "class_name": "TestSuite", "function_name": "get_attributes", "parameters": ["self"], "param_types": {}, "return_type": "dict[str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_attributes", "self.timestamp.replace", "self.timestamp.replace(tzinfo=None).isoformat"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a dictionary of attributes for this instance.", "source_code": "def get_attributes(self) -> dict[str, str]:\n    \"\"\"Return a dictionary of attributes for this instance.\"\"\"\n    return _attributes(\n        disabled=self.disabled,\n        errors=self.errors,\n        failures=self.failures,\n        hostname=self.hostname,\n        id=self.id,\n        name=self.name,\n        package=self.package,\n        skipped=self.skipped,\n        tests=self.tests,\n        time=self.time,\n        timestamp=self.timestamp.replace(tzinfo=None).isoformat(timespec='seconds') if self.timestamp else None,\n    )", "loc": 15}
{"file": "ansible\\lib\\ansible\\utils\\_junit_xml.py", "class_name": "TestSuite", "function_name": "get_xml_element", "parameters": ["self"], "param_types": {}, "return_type": "ET.Element", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ET.Element", "ET.SubElement", "ET.SubElement(element, 'properties').extend", "dict", "element.extend", "self.get_attributes", "self.properties.items", "test_case.get_xml_element"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return an XML element representing this instance.", "source_code": "def get_xml_element(self) -> ET.Element:\n    \"\"\"Return an XML element representing this instance.\"\"\"\n    element = ET.Element('testsuite', self.get_attributes())\n\n    if self.properties:\n        ET.SubElement(element, 'properties').extend([ET.Element('property', dict(name=name, value=value)) for name, value in self.properties.items()])\n\n    element.extend([test_case.get_xml_element() for test_case in self.cases])\n\n    if self.system_out:\n        ET.SubElement(element, 'system-out').text = self.system_out\n\n    if self.system_err:\n        ET.SubElement(element, 'system-err').text = self.system_err\n\n    return element", "loc": 16}
{"file": "ansible\\lib\\ansible\\utils\\_junit_xml.py", "class_name": "TestSuites", "function_name": "get_xml_element", "parameters": ["self"], "param_types": {}, "return_type": "ET.Element", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ET.Element", "element.extend", "self.get_attributes", "suite.get_xml_element"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return an XML element representing this instance.", "source_code": "def get_xml_element(self) -> ET.Element:\n    \"\"\"Return an XML element representing this instance.\"\"\"\n    element = ET.Element('testsuites', self.get_attributes())\n    element.extend([suite.get_xml_element() for suite in self.suites])\n\n    return element", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_config.py", "class_name": "_EventSource", "function_name": "fire", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["h", "self._on_exception"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fire(self, *args, **kwargs):\n    for h in self._handlers:\n        try:\n            h(*args, **kwargs)\n        except Exception as ex:\n            if self._on_exception(h, ex, *args, **kwargs):\n                raise", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_config.py", "class_name": "_AnsibleCollectionConfig", "function_name": "collection_finder", "parameters": ["cls", "value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def collection_finder(cls, value):\n    if cls._collection_finder:\n        raise ValueError('an AnsibleCollectionFinder has already been configured')\n\n    cls._collection_finder = value", "loc": 5}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleTraversableResources", "function_name": "files", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "_AnsibleNSTraversable", "find_spec", "isinstance", "len", "package.split", "pathlib.Path", "self._ensure_package", "self._get_path", "spec_from_loader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def files(self):\n    package = self._package\n    parts = package.split('.')\n    is_ns = parts[0] == 'ansible_collections' and len(parts) < 3\n\n    if isinstance(package, str):\n        if is_ns:\n            # Don't use ``spec_from_loader`` here, because that will point\n            # to exactly 1 location for a namespace. Use ``find_spec``\n            # to get a list of all locations for the namespace\n            package = find_spec(package)\n        else:\n            package = spec_from_loader(package, self._loader)\n    elif not isinstance(package, ModuleType):\n        raise TypeError('Expected string or module, got %r' % package.__class__.__name__)\n\n    self._ensure_package(package)\n    if is_ns:\n        return _AnsibleNSTraversable(*package.submodule_search_locations)\n    return pathlib.Path(self._get_path(package)).parent", "loc": 20}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionFinder", "function_name": "find_spec", "parameters": ["self", "fullname", "path", "target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self._get_loader", "spec_from_loader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_spec(self, fullname, path, target=None):\n    loader = self._get_loader(fullname, path)\n\n    if loader is None:\n        return None\n\n    spec = spec_from_loader(fullname, loader)\n    if spec is not None and hasattr(loader, '_subpackage_search_paths'):\n        spec.submodule_search_locations = loader._subpackage_search_paths\n    return spec", "loc": 10}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsiblePathHookFinder", "function_name": "find_module", "parameters": ["self", "fullname", "path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["finder.find_module", "isinstance", "self._get_finder"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_module(self, fullname, path=None):\n    # we ignore the passed in path here- use what we got from the path hook init\n    finder = self._get_finder(fullname)\n\n    if finder is None:\n        return None\n    elif isinstance(finder, FileFinder):\n        # this codepath is erroneously used under some cases in py3,\n        # and the find_module method on FileFinder does not accept the path arg\n        # see https://github.com/pypa/setuptools/pull/2918\n        return finder.find_module(fullname)\n    else:\n        return finder.find_module(fullname, path=[self._pathctx])", "loc": 13}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsiblePathHookFinder", "function_name": "find_spec", "parameters": ["self", "fullname", "target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["finder.find_spec", "fullname.split", "self._get_finder"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_spec(self, fullname, target=None):\n    split_name = fullname.split('.')\n    toplevel_pkg = split_name[0]\n\n    finder = self._get_finder(fullname)\n\n    if finder is None:\n        return None\n    elif toplevel_pkg == 'ansible_collections':\n        return finder.find_spec(fullname, path=[self._pathctx])\n    else:\n        return finder.find_spec(fullname)", "loc": 12}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionPkgLoaderBase", "function_name": "exec_module", "parameters": ["self", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["exec", "self.get_code"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_module(self, module):\n    # short-circuit redirect; avoid reinitializing existing modules\n    if self._redirect_module:\n        return\n\n    # execute the module's code in its namespace\n    code_obj = self.get_code(self._fullname)\n    if code_obj is not None:  # things like NS packages that can't have code on disk will return None\n        exec(code_obj, module.__dict__)", "loc": 9}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionPkgLoaderBase", "function_name": "create_module", "parameters": ["self", "spec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_module(self, spec):\n    # short-circuit redirect; we've already imported the redirected module, so just alias it and return it\n    if self._redirect_module:\n        return self._redirect_module\n    else:\n        return None", "loc": 6}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionPkgLoaderBase", "function_name": "load_module", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "exec", "self._new_or_existing_module", "self.get_code", "self.get_filename"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_module(self, fullname):\n    # short-circuit redirect; we've already imported the redirected module, so just alias it and return it\n    if self._redirect_module:\n        sys.modules[self._fullname] = self._redirect_module\n        return self._redirect_module\n\n    # we're actually loading a module/package\n    module_attrs = dict(\n        __loader__=self,\n        __file__=self.get_filename(fullname),\n        __package__=self._parent_package_name  # sane default for non-packages\n    )\n\n    # eg, I am a package\n    if self._subpackage_search_paths is not None:  # empty is legal\n        module_attrs['__path__'] = self._subpackage_search_paths\n        module_attrs['__package__'] = fullname  # per PEP366\n\n    with self._new_or_existing_module(fullname, **module_attrs) as module:\n        # execute the module's code in its namespace\n        code_obj = self.get_code(fullname)\n        if code_obj is not None:  # things like NS packages that can't have code on disk will return None\n            exec(code_obj, module.__dict__)\n\n        return module", "loc": 25}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionPkgLoaderBase", "function_name": "get_source", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'this loader cannot load source for {0}, only {1}'.format", "ValueError", "self.get_data"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_source(self, fullname):\n    if self._decoded_source:\n        return self._decoded_source\n    if fullname != self._fullname:\n        raise ValueError('this loader cannot load source for {0}, only {1}'.format(fullname, self._fullname))\n    if not self._source_code_path:\n        return None\n    # FIXME: what do we want encoding/newline requirements to be?\n    self._decoded_source = self.get_data(self._source_code_path)\n    return self._decoded_source", "loc": 10}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleCollectionPkgLoaderBase", "function_name": "get_code", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compile", "self.get_filename", "self.get_source"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_code(self, fullname):\n    if self._compiled_code:\n        return self._compiled_code\n\n    # this may or may not be an actual filename, but it's the value we'll use for __file__\n    filename = self.get_filename(fullname)\n    if not filename:\n        filename = '<string>'\n\n    source_code = self.get_source(fullname)\n\n    # for things like synthetic modules that really have no source on disk, don't return a code object at all\n    # vs things like an empty package init (which has an empty string source on disk)\n    if source_code is None:\n        return None\n\n    self._compiled_code = compile(source=source_code, filename=filename, mode='exec', flags=0, dont_inherit=True)\n\n    return self._compiled_code", "loc": 19}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleInternalRedirectLoader", "function_name": "exec_module", "parameters": ["self", "module"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'no redirect found for {0}'.format", "ValueError", "import_module"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_module(self, module):\n    # should never see this\n    if not self._redirect:\n        raise ValueError('no redirect found for {0}'.format(module.__spec__.name))\n\n    # Replace the module with the redirect\n    sys.modules[module.__spec__.name] = import_module(self._redirect)", "loc": 7}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "_AnsibleInternalRedirectLoader", "function_name": "load_module", "parameters": ["self", "fullname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'no redirect found for {0}'.format", "ValueError", "import_module"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_module(self, fullname):\n    # since we're delegating to other loaders, this should only be called for internal redirects where we answered\n    # find_module with this loader, in which case we'll just directly import the redirection target, insert it into\n    # sys.modules under the name it was requested by, and return the original module.\n\n    # should never see this\n    if not self._redirect:\n        raise ValueError('no redirect found for {0}'.format(fullname))\n\n    # FIXME: smuggle redirection context, provide warning/error that we tried and failed to redirect\n    mod = import_module(self._redirect)\n    sys.modules[fullname] = mod\n    return mod", "loc": 13}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "AnsibleCollectionRef", "function_name": "from_fqcr", "parameters": ["ref", "ref_type"], "param_types": {}, "return_type": null, "param_doc": {"ref": "collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')", "ref_type": "the type of the reference, eg 'module', 'role', 'doc_fragment'"}, "return_doc": "a populated AnsibleCollectionRef object", "raises_doc": [], "called_functions": ["'{0} is not a valid collection reference'.format", "AnsibleCollectionRef", "AnsibleCollectionRef.is_valid_fqcr", "ValueError", "_to_text", "len", "package_remnant.split", "ref.endswith", "ref.rsplit", "u'.'.join"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse a string as a fully-qualified collection reference, raises ValueError if invalid", "source_code": "def from_fqcr(ref, ref_type):\n    \"\"\"\n    Parse a string as a fully-qualified collection reference, raises ValueError if invalid\n    :param ref: collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n    :param ref_type: the type of the reference, eg 'module', 'role', 'doc_fragment'\n    :return: a populated AnsibleCollectionRef object\n    \"\"\"\n    # assuming the fq_name is of the form (ns).(coll).(optional_subdir_N).(resource_name),\n    # we split the resource name off the right, split ns and coll off the left, and we're left with any optional\n    # subdirs that need to be added back below the plugin-specific subdir we'll add. So:\n    # ns.coll.resource -> ansible_collections.ns.coll.plugins.(plugintype).resource\n    # ns.coll.subdir1.resource -> ansible_collections.ns.coll.plugins.subdir1.(plugintype).resource\n    # ns.coll.rolename -> ansible_collections.ns.coll.roles.rolename\n    if not AnsibleCollectionRef.is_valid_fqcr(ref):\n        raise ValueError('{0} is not a valid collection reference'.format(_to_text(ref)))\n\n    ref = _to_text(ref, strict=True)\n    ref_type = _to_text(ref_type, strict=True)\n    ext = ''\n\n    if ref_type == u'playbook' and ref.endswith(PB_EXTENSIONS):\n        resource_splitname = ref.rsplit(u'.', 2)\n        package_remnant = resource_splitname[0]\n        resource = resource_splitname[1]\n        ext = '.' + resource_splitname[2]\n    else:\n        resource_splitname = ref.rsplit(u'.', 1)\n        package_remnant = resource_splitname[0]\n        resource = resource_splitname[1]\n\n    # split the left two components of the collection package name off, anything remaining is plugin-type\n    # specific subdirs to be added back on below the plugin type\n    package_splitname = package_remnant.split(u'.', 2)\n    if len(package_splitname) == 3:\n        subdirs = package_splitname[2]\n    else:\n        subdirs = u''\n\n    collection_name = u'.'.join(package_splitname[0:2])\n\n    return AnsibleCollectionRef(collection_name, subdirs, resource + ext, ref_type)", "loc": 41}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "AnsibleCollectionRef", "function_name": "try_parse_fqcr", "parameters": ["ref", "ref_type"], "param_types": {}, "return_type": null, "param_doc": {"ref": "collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')", "ref_type": "the type of the reference, eg 'module', 'role', 'doc_fragment'"}, "return_doc": "a populated AnsibleCollectionRef object on successful parsing, else None", "raises_doc": [], "called_functions": ["AnsibleCollectionRef.from_fqcr"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Attempt to parse a string as a fully-qualified collection reference, returning None on failure (instead of raising an error)", "source_code": "def try_parse_fqcr(ref, ref_type):\n    \"\"\"\n    Attempt to parse a string as a fully-qualified collection reference, returning None on failure (instead of raising an error)\n    :param ref: collection reference to parse (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n    :param ref_type: the type of the reference, eg 'module', 'role', 'doc_fragment'\n    :return: a populated AnsibleCollectionRef object on successful parsing, else None\n    \"\"\"\n    try:\n        return AnsibleCollectionRef.from_fqcr(ref, ref_type)\n    except ValueError:\n        pass", "loc": 11}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "AnsibleCollectionRef", "function_name": "legacy_plugin_dir_to_plugin_type", "parameters": ["legacy_plugin_dir_name"], "param_types": {}, "return_type": null, "param_doc": {"legacy_plugin_dir_name": "PluginLoader dir name (eg, 'action_plugins', 'library')"}, "return_doc": "the corresponding plugin ref_type (eg, 'action', 'role')", "raises_doc": [], "called_functions": ["ValueError", "_to_text", "legacy_plugin_dir_name.removesuffix"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Utility method to convert from a PluginLoader dir name to a plugin ref_type", "source_code": "def legacy_plugin_dir_to_plugin_type(legacy_plugin_dir_name):\n    \"\"\"\n    Utility method to convert from a PluginLoader dir name to a plugin ref_type\n    :param legacy_plugin_dir_name: PluginLoader dir name (eg, 'action_plugins', 'library')\n    :return: the corresponding plugin ref_type (eg, 'action', 'role')\n    \"\"\"\n    if legacy_plugin_dir_name is None:\n        plugin_type = None\n    else:\n        legacy_plugin_dir_name = _to_text(legacy_plugin_dir_name)\n\n        plugin_type = legacy_plugin_dir_name.removesuffix(u'_plugins')\n\n    if plugin_type == u'library':\n        plugin_type = u'modules'\n\n    if plugin_type not in AnsibleCollectionRef.VALID_REF_TYPES:\n        raise ValueError(f'{legacy_plugin_dir_name!r} cannot be mapped to a valid collection ref type')\n\n    return plugin_type", "loc": 20}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "AnsibleCollectionRef", "function_name": "is_valid_fqcr", "parameters": ["ref", "ref_type"], "param_types": {}, "return_type": null, "param_doc": {"ref": "candidate collection reference to validate (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')", "ref_type": "optional reference type to enable deeper validation, eg 'module', 'role', 'doc_fragment'"}, "return_doc": "True if the collection ref passed is well-formed, False otherwise", "raises_doc": [], "called_functions": ["AnsibleCollectionRef.try_parse_fqcr", "_to_text", "bool", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validates if is string is a well-formed fully-qualified collection reference (does not look up the collection itself)", "source_code": "def is_valid_fqcr(ref, ref_type=None):\n    \"\"\"\n    Validates if is string is a well-formed fully-qualified collection reference (does not look up the collection itself)\n    :param ref: candidate collection reference to validate (a valid ref is of the form 'ns.coll.resource' or 'ns.coll.subdir1.subdir2.resource')\n    :param ref_type: optional reference type to enable deeper validation, eg 'module', 'role', 'doc_fragment'\n    :return: True if the collection ref passed is well-formed, False otherwise\n    \"\"\"\n\n    ref = _to_text(ref)\n\n    if not ref_type:\n        return bool(re.match(AnsibleCollectionRef.VALID_FQCR_RE, ref))\n\n    return bool(AnsibleCollectionRef.try_parse_fqcr(ref, ref_type))", "loc": 14}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\_collection_finder.py", "class_name": "AnsibleCollectionRef", "function_name": "is_valid_collection_name", "parameters": ["collection_name"], "param_types": {}, "return_type": null, "param_doc": {"collection_name": "candidate collection name to validate (a valid name is of the form 'ns.collname')"}, "return_doc": "True if the collection name passed is well-formed, False otherwise", "raises_doc": [], "called_functions": ["_to_text", "all", "collection_name.count", "collection_name.split", "iskeyword", "ns_or_name.isidentifier"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Validates if the given string is a well-formed collection name (does not look up the collection itself)", "source_code": "def is_valid_collection_name(collection_name):\n    \"\"\"\n    Validates if the given string is a well-formed collection name (does not look up the collection itself)\n    :param collection_name: candidate collection name to validate (a valid name is of the form 'ns.collname')\n    :return: True if the collection name passed is well-formed, False otherwise\n    \"\"\"\n\n    collection_name = _to_text(collection_name)\n\n    if collection_name.count(u'.') != 1:\n        return False\n\n    return all(\n        # NOTE: keywords and identifiers are different in different Pythons\n        not iskeyword(ns_or_name) and ns_or_name.isidentifier()\n        for ns_or_name in collection_name.split(u'.')\n    )", "loc": 17}
{"file": "ansible\\lib\\ansible\\utils\\collection_loader\\__init__.py", "class_name": null, "function_name": "resource_from_fqcr", "parameters": ["ref"], "param_types": {}, "return_type": null, "param_doc": {"ref": "collection reference to parse"}, "return_doc": "the resource as a unicode string", "raises_doc": [], "called_functions": ["_to_text", "ref.split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return resource from a fully-qualified collection reference, or from a simple resource name. For fully-qualified collection references, this is equivalent to ``AnsibleCollectionRef.from_fqcr(ref).resource``.", "source_code": "def resource_from_fqcr(ref):\n    \"\"\"\n    Return resource from a fully-qualified collection reference,\n    or from a simple resource name.\n    For fully-qualified collection references, this is equivalent to\n    ``AnsibleCollectionRef.from_fqcr(ref).resource``.\n    :param ref: collection reference to parse\n    :return: the resource as a unicode string\n    \"\"\"\n    ref = _to_text(ref, strict=True)\n    return ref.split(u'.')[-1]", "loc": 11}
{"file": "ansible\\lib\\ansible\\vars\\clean.py", "class_name": null, "function_name": "module_response_deepcopy", "parameters": ["v"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "isinstance", "module_response_deepcopy", "ret.items", "v.copy"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Function to create a deep copy of module response data Designed to be used within the Ansible \"engine\" to improve performance issues where ``copy.deepcopy`` was used previously, largely with CPU", "source_code": "def module_response_deepcopy(v):\n    \"\"\"Function to create a deep copy of module response data\n\n    Designed to be used within the Ansible \"engine\" to improve performance\n    issues where ``copy.deepcopy`` was used previously, largely with CPU\n    and memory contention.\n\n    This only supports the following data types, and was designed to only\n    handle specific workloads:\n\n    * ``dict``\n    * ``list``\n\n    The data we pass here will come from a serialization such\n    as JSON, so we shouldn't have need for other data types such as\n    ``set`` or ``tuple``.\n\n    Take note that this function should not be used extensively as a\n    replacement for ``deepcopy`` due to the naive way in which this\n    handles other data types.\n\n    Do not expect uses outside of those listed below to maintain\n    backwards compatibility, in case we need to extend this function\n    to handle our specific needs:\n\n    * ``ansible.executor.task_result._RawTaskResult.as_callback_task_result``\n    * ``ansible.vars.clean.clean_facts``\n    * ``ansible.vars.namespace_facts``\n    \"\"\"\n    if isinstance(v, dict):\n        ret = v.copy()\n        items = ret.items()\n    elif isinstance(v, list):\n        ret = v[:]\n        items = enumerate(ret)\n    else:\n        return v\n\n    for key, value in items:\n        if isinstance(value, (dict, list)):\n            ret[key] = module_response_deepcopy(value)\n        else:\n            ret[key] = value\n\n    return ret", "loc": 45}
{"file": "ansible\\lib\\ansible\\vars\\clean.py", "class_name": null, "function_name": "strip_internal_keys", "parameters": ["dirty", "exceptions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "dirty.keys", "isinstance", "k.startswith", "list", "strip_internal_keys", "tuple", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def strip_internal_keys(dirty, exceptions=None):\n    # All keys starting with _ansible_ are internal, so change the 'dirty' mapping and remove them.\n\n    if exceptions is None:\n        exceptions = tuple()\n\n    if isinstance(dirty, MutableSequence):\n\n        for element in dirty:\n            if isinstance(element, (MutableMapping, MutableSequence)):\n                strip_internal_keys(element, exceptions=exceptions)\n\n    elif isinstance(dirty, MutableMapping):\n\n        # listify to avoid updating dict while iterating over it\n        for k in list(dirty.keys()):\n            if isinstance(k, str):\n                if k.startswith('_ansible_') and k not in exceptions:\n                    del dirty[k]\n                    continue\n\n            if isinstance(dirty[k], (MutableMapping, MutableSequence)):\n                strip_internal_keys(dirty[k], exceptions=exceptions)\n    else:\n        raise AnsibleError(\"Cannot strip invalid keys from %s\" % type(dirty))\n\n    return dirty", "loc": 27}
{"file": "ansible\\lib\\ansible\\vars\\clean.py", "class_name": null, "function_name": "remove_internal_keys", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.get", "data.get('ansible_facts', {}).keys", "data.keys", "display.warning", "key.startswith", "list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "More nuanced version of strip_internal_keys", "source_code": "def remove_internal_keys(data):\n    \"\"\"\n    More nuanced version of strip_internal_keys\n    \"\"\"\n    for key in list(data.keys()):\n        if (key.startswith('_ansible_') and key != '_ansible_parsed') or key in C.INTERNAL_RESULT_KEYS:\n            display.warning(\"Removed unexpected internal key in module return: %s = %s\" % (key, data[key]))\n            del data[key]\n\n    # remove bad/empty internal keys\n    for key in ['warnings', 'deprecations']:\n        if key in data and not data[key]:\n            del data[key]\n\n    # cleanse fact values that are allowed from actions but not modules\n    for key in list(data.get('ansible_facts', {}).keys()):\n        if key.startswith('discovered_interpreter_') or key.startswith('ansible_discovered_interpreter_'):\n            del data['ansible_facts'][key]", "loc": 18}
{"file": "ansible\\lib\\ansible\\vars\\clean.py", "class_name": null, "function_name": "namespace_facts", "parameters": ["facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["k.startswith", "module_response_deepcopy"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "return all facts inside 'ansible_facts' w/o an ansible_ prefix", "source_code": "def namespace_facts(facts):\n    \"\"\" return all facts inside 'ansible_facts' w/o an ansible_ prefix \"\"\"\n    deprefixed = {}\n    for k in facts:\n        if k.startswith('ansible_') and k not in ('ansible_local',):\n            deprefixed[k[8:]] = module_response_deepcopy(facts[k])\n        else:\n            deprefixed[k] = module_response_deepcopy(facts[k])\n\n    return {'ansible_facts': deprefixed}", "loc": 10}
{"file": "ansible\\lib\\ansible\\vars\\hostvars.py", "class_name": "HostVars", "function_name": "raw_get", "parameters": ["self", "host_name"], "param_types": {"host_name": "str"}, "return_type": "dict[str, t.Any] | Marker", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_jinja_bits._undef", "self._inventory.get_host", "self._variable_manager.get_vars"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Similar to __getitem__, however the returned data is not run through the templating engine to expand variables in the hostvars.", "source_code": "def raw_get(self, host_name: str) -> dict[str, t.Any] | Marker:\n    \"\"\"\n    Similar to __getitem__, however the returned data is not run through\n    the templating engine to expand variables in the hostvars.\n    \"\"\"\n    # does not use inventory.hosts, so it can create localhost on demand\n    host = self._inventory.get_host(host_name)\n\n    if host is None:\n        from ansible._internal._templating import _jinja_bits\n\n        return _jinja_bits._undef(f\"hostvars[{host_name!r}]\")\n\n    return self._variable_manager.get_vars(host=host, include_hostvars=False)", "loc": 14}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": null, "function_name": "preprocess_vars", "parameters": ["a"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleParserError", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Ensures that vars contained in the parameter passed in are returned as a list of dictionaries, to ensure for instance that vars loaded from a file conform to an expected state.", "source_code": "def preprocess_vars(a):\n    \"\"\"\n    Ensures that vars contained in the parameter passed in are\n    returned as a list of dictionaries, to ensure for instance\n    that vars loaded from a file conform to an expected state.\n    \"\"\"\n\n    # FIXME: this does not properly handle omit, undefined, or dynamic structure from templated `vars` ; templating should be done earlier\n    if a is None:\n        return None\n    elif not isinstance(a, list):\n        data = [a]\n    else:\n        data = a\n\n    for item in data:\n        if not isinstance(item, MutableMapping):\n            raise AnsibleParserError(\n                message=\"Invalid variable file contents.\",\n                obj=item,\n                help_text=\"Variable files must contain either a dictionary of variables, or a list of dictionaries.\",\n            )\n\n    return data", "loc": 24}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": "VariableManager", "function_name": "get_delegated_vars_and_hostname", "parameters": ["self", "templar", "task", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Host", "self._inventory.get_host", "self._inventory.get_hosts", "self.get_vars", "task.get_play", "templar.template", "variables.get"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Get the delegated_vars for an individual task invocation, which may be in the context of an individual loop iteration. Not used directly be VariableManager, but used primarily within TaskExecutor", "source_code": "def get_delegated_vars_and_hostname(self, templar, task, variables):\n    \"\"\"Get the delegated_vars for an individual task invocation, which may be in the context\n    of an individual loop iteration.\n\n    Not used directly be VariableManager, but used primarily within TaskExecutor\n    \"\"\"\n    delegated_vars = {}\n    delegated_host_name = ...  # sentinel value distinct from empty/None, which are errors\n\n    if task.delegate_to:\n        try:\n            delegated_host_name = templar.template(task.delegate_to)\n        except AnsibleValueOmittedError:\n            pass\n\n    # bypass for unspecified value/omit\n    if delegated_host_name is ...:\n        return delegated_vars, None\n\n    if not delegated_host_name:\n        raise AnsibleError('Empty hostname produced from delegate_to: \"%s\"' % task.delegate_to)\n\n    delegated_host = self._inventory.get_host(delegated_host_name)\n    if delegated_host is None:\n        for h in self._inventory.get_hosts(ignore_limits=True, ignore_restrictions=True):\n            # check if the address matches, or if both the delegated_to host\n            # and the current host are in the list of localhost aliases\n            if h.address == delegated_host_name:\n                delegated_host = h\n                break\n        else:\n            delegated_host = Host(name=delegated_host_name)\n\n    delegated_vars['ansible_delegated_vars'] = {\n        delegated_host_name: self.get_vars(\n            play=task.get_play(),\n            host=delegated_host,\n            task=task,\n            include_hostvars=True,\n        )\n    }\n    delegated_vars['ansible_delegated_vars'][delegated_host_name]['inventory_hostname'] = variables.get('inventory_hostname')\n\n    return delegated_vars, delegated_host_name", "loc": 44}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": "VariableManager", "function_name": "clear_facts", "parameters": ["self", "hostname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._fact_cache.delete"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Clears the facts for a host", "source_code": "def clear_facts(self, hostname):\n    \"\"\"\n    Clears the facts for a host\n    \"\"\"\n    try:\n        self._fact_cache.delete(hostname)\n    except KeyError:\n        pass", "loc": 8}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": "VariableManager", "function_name": "set_host_facts", "parameters": ["self", "host", "facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'The object retrieved for {0} must be a MutableMapping but was a {1}'.format", "AnsibleAssertionError", "TypeError", "isinstance", "self._fact_cache.get", "self._fact_cache.set", "type", "warn_if_reserved"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Sets or updates the given facts for a host in the fact cache.", "source_code": "def set_host_facts(self, host, facts):\n    \"\"\"\n    Sets or updates the given facts for a host in the fact cache.\n    \"\"\"\n\n    if not isinstance(facts, Mapping):\n        raise AnsibleAssertionError(\"the type of 'facts' to set for host_facts should be a Mapping but is a %s\" % type(facts))\n\n    warn_if_reserved(facts)\n\n    try:\n        host_cache = self._fact_cache.get(host)\n    except KeyError:\n        # We get to set this as new\n        host_cache = facts\n    else:\n        if not isinstance(host_cache, MutableMapping):\n            raise TypeError('The object retrieved for {0} must be a MutableMapping but was'\n                            ' a {1}'.format(host, type(host_cache)))\n        # Update the existing facts\n        host_cache |= facts\n\n    # Save the facts back to the backing store\n    self._fact_cache.set(host, host_cache)", "loc": 24}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": "VariableManager", "function_name": "set_nonpersistent_facts", "parameters": ["self", "host", "facts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAssertionError", "isinstance", "type", "warn_if_reserved"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Sets or updates the given facts for a host in the fact cache.", "source_code": "def set_nonpersistent_facts(self, host, facts):\n    \"\"\"\n    Sets or updates the given facts for a host in the fact cache.\n    \"\"\"\n\n    if not isinstance(facts, Mapping):\n        raise AnsibleAssertionError(\"the type of 'facts' to set for nonpersistent_facts should be a Mapping but is a %s\" % type(facts))\n\n    warn_if_reserved(facts)\n\n    try:\n        self._nonpersistent_fact_cache[host] |= facts\n    except KeyError:\n        self._nonpersistent_fact_cache[host] = facts", "loc": 14}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": "VariableManager", "function_name": "set_host_variable", "parameters": ["self", "host", "varname", "value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["combine_vars", "dict", "isinstance", "warn_if_reserved"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets a value in the vars_cache for a host.", "source_code": "def set_host_variable(self, host, varname, value):\n    \"\"\"\n    Sets a value in the vars_cache for a host.\n    \"\"\"\n\n    warn_if_reserved([varname])\n\n    if host not in self._vars_cache:\n        self._vars_cache[host] = dict()\n\n    if varname in self._vars_cache[host] and isinstance(self._vars_cache[host][varname], MutableMapping) and isinstance(value, MutableMapping):\n        self._vars_cache[host] = combine_vars(self._vars_cache[host], {varname: value})\n    else:\n        self._vars_cache[host][varname] = value", "loc": 14}
{"file": "ansible\\lib\\ansible\\vars\\manager.py", "class_name": null, "function_name": "plugins_by_groups", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_combine_and_track", "_plugins_inventory", "_plugins_play"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "merges all plugin sources by group, This should be used instead, NOT in combination with the other groups_plugins* functions", "source_code": "def plugins_by_groups():\n    \"\"\"\n        merges all plugin sources by group,\n        This should be used instead, NOT in combination with the other groups_plugins* functions\n    \"\"\"\n    data = {}\n    for group in host_groups:\n        data[group] = _combine_and_track(data[group], _plugins_inventory(group), \"inventory group_vars for '%s'\" % group)\n        data[group] = _combine_and_track(data[group], _plugins_play(group), \"playbook group_vars for '%s'\" % group)\n    return data", "loc": 10}
{"file": "ansible\\lib\\ansible\\vars\\plugins.py", "class_name": null, "function_name": "get_vars_from_path", "parameters": ["loader", "path", "entities", "stage"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_plugin_should_run", "_prime_vars_loader", "combine_vars", "display.warning", "hasattr", "plugin.ansible_name.startswith", "plugin.get_vars", "vars_loader.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_vars_from_path(loader, path, entities, stage):\n    data = {}\n    if vars_loader._paths is None:\n        # cache has been reset, reload all()\n        _prime_vars_loader()\n\n    for plugin_name in vars_loader._plugin_instance_cache:\n        if (plugin := vars_loader.get(plugin_name)) is None:\n            continue\n\n        collection = '.' in plugin.ansible_name and not plugin.ansible_name.startswith('ansible.builtin.')\n        # Warn if a collection plugin has REQUIRES_ENABLED because it has no effect.\n        if collection and hasattr(plugin, 'REQUIRES_ENABLED'):\n            display.warning(\n                \"Vars plugins in collections must be enabled to be loaded, REQUIRES_ENABLED is not supported. \"\n                \"This should be removed from the plugin %s.\" % plugin.ansible_name\n            )\n\n        if not _plugin_should_run(plugin, stage):\n            continue\n\n        if (new_vars := plugin.get_vars(loader, path, entities)) != {}:\n            data = combine_vars(data, new_vars)\n\n    return data", "loc": 25}
{"file": "ansible\\lib\\ansible\\vars\\reserved.py", "class_name": null, "function_name": "get_reserved_names", "parameters": ["include_private"], "param_types": {"include_private": "bool"}, "return_type": "set[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateEngine", "TemplateEngine().environment.globals.keys", "aclass.fattributes.items", "private.add", "public.add", "public.union", "result.discard", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "this function returns the list of reserved names associated with play objects", "source_code": "def get_reserved_names(include_private: bool = True) -> set[str]:\n    \"\"\" this function returns the list of reserved names associated with play objects\"\"\"\n\n    public = set(TemplateEngine().environment.globals.keys())\n    private = set()\n\n    # FIXME: find a way to 'not hardcode', possibly need role deps/includes\n    class_list = [Play, Role, Block, Task]\n\n    for aclass in class_list:\n        # build ordered list to loop over and dict with attributes\n        for name, attr in aclass.fattributes.items():\n            if attr.private:\n                private.add(name)\n            else:\n                public.add(name)\n\n    # local_action is implicit with action\n    if 'action' in public:\n        public.add('local_action')\n\n    # loop implies with_\n    # FIXME: remove after with_ is not only deprecated but removed\n    if 'loop' in private or 'loop' in public:\n        public.add('with_')\n\n    if include_private:\n        result = public.union(private)\n    else:\n        result = public\n\n    result.discard('gather_subset')\n\n    return result", "loc": 34}
{"file": "ansible\\lib\\ansible\\vars\\reserved.py", "class_name": null, "function_name": "warn_if_reserved", "parameters": ["myvars", "additional"], "param_types": {"myvars": "c.Iterable[str]", "additional": "c.Iterable[str] | None"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_RESERVED_NAMES.union", "display.warning", "set", "varnames.discard", "varnames.intersection"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Issue a warning for any variable which conflicts with an internally reserved name.", "source_code": "def warn_if_reserved(myvars: c.Iterable[str], additional: c.Iterable[str] | None = None) -> None:\n    \"\"\"Issue a warning for any variable which conflicts with an internally reserved name.\"\"\"\n    if additional is None:\n        reserved = _RESERVED_NAMES\n    else:\n        reserved = _RESERVED_NAMES.union(additional)\n\n    varnames = set(myvars)\n    varnames.discard('vars')  # we add this one internally, so safe to ignore\n\n    if conflicts := varnames.intersection(reserved):\n        # Ensure the varname used for obj is the tagged one from myvars and not the untagged one from reserved.\n        # This can occur because tags do not affect value equality, and intersection can return values from either the left or right side.\n        for varname in (name for name in myvars if name in conflicts):\n            display.warning(f'Found variable using reserved name {varname!r}.', obj=varname)", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_display_utils.py", "class_name": null, "function_name": "format_message", "parameters": ["summary", "include_traceback"], "param_types": {"summary": "_messages.SummaryBase", "include_traceback": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_event_formatting.format_event", "dataclasses.replace", "get_deprecation_message_with_plugin_info", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_message(summary: _messages.SummaryBase, include_traceback: bool) -> str:\n    if isinstance(summary, _messages.DeprecationSummary):\n        deprecation_message = get_deprecation_message_with_plugin_info(\n            msg=summary.event.msg,\n            version=summary.version,\n            date=summary.date,\n            deprecator=summary.deprecator,\n        )\n\n        event = dataclasses.replace(summary.event, msg=deprecation_message)\n    else:\n        event = summary.event\n\n    return _event_formatting.format_event(event, include_traceback)", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_display_utils.py", "class_name": null, "function_name": "get_deprecation_message_with_plugin_info", "parameters": [], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'.'.join", "deprecator.resolved_name.split", "join_sentences", "len", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Internal use only. Return a deprecation message and help text for display.", "source_code": "def get_deprecation_message_with_plugin_info(\n    *,\n    msg: str,\n    version: str | None,\n    removed: bool = False,\n    date: str | None,\n    deprecator: _messages.PluginInfo | None,\n) -> str:\n    \"\"\"Internal use only. Return a deprecation message and help text for display.\"\"\"\n    # DTFIX-FUTURE: the logic for omitting date/version doesn't apply to the payload, so it shows up in vars in some cases when it should not\n\n    if removed:\n        removal_fragment = 'This feature was removed'\n    else:\n        removal_fragment = 'This feature will be removed'\n\n    if not deprecator or not deprecator.type:\n        # indeterminate has no resolved_name or type\n        # collections have a resolved_name but no type\n        collection = deprecator.resolved_name if deprecator else None\n        plugin_fragment = ''\n    elif deprecator.resolved_name == 'ansible.builtin':\n        # core deprecations from base classes (the API) have no plugin name, only 'ansible.builtin'\n        plugin_type_name = str(deprecator.type) if deprecator.type is _messages.PluginType.MODULE else f'{deprecator.type} plugin'\n\n        collection = deprecator.resolved_name\n        plugin_fragment = f'the {plugin_type_name} API'\n    else:\n        parts = deprecator.resolved_name.split('.')\n        plugin_name = parts[-1]\n        plugin_type_name = str(deprecator.type) if deprecator.type is _messages.PluginType.MODULE else f'{deprecator.type} plugin'\n\n        collection = '.'.join(parts[:2]) if len(parts) > 2 else None\n        plugin_fragment = f'{plugin_type_name} {plugin_name!r}'\n\n    if collection and plugin_fragment:\n        plugin_fragment += ' in'\n\n    if collection == 'ansible.builtin':\n        collection_fragment = 'ansible-core'\n    elif collection:\n        collection_fragment = f'collection {collection!r}'\n    else:\n        collection_fragment = ''\n\n    if not collection:\n        when_fragment = 'in the future' if not removed else ''\n    elif date:\n        when_fragment = f'in a release after {date}'\n    elif version:\n        when_fragment = f'version {version}'\n    else:\n        when_fragment = 'in a future release' if not removed else ''\n\n    if plugin_fragment or collection_fragment:\n        from_fragment = 'from'\n    else:\n        from_fragment = ''\n\n    deprecation_msg = ' '.join(f for f in [removal_fragment, from_fragment, plugin_fragment, collection_fragment, when_fragment] if f) + '.'\n\n    return join_sentences(msg, deprecation_msg)", "loc": 62}
{"file": "ansible\\lib\\ansible\\_internal\\_display_utils.py", "class_name": null, "function_name": "join_sentences", "parameters": ["first", "second"], "param_types": {"first": "str | None", "second": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "first or ''.strip", "second or ''.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Join two sentences together.", "source_code": "def join_sentences(first: str | None, second: str | None) -> str:\n    \"\"\"Join two sentences together.\"\"\"\n    first = (first or '').strip()\n    second = (second or '').strip()\n\n    if first and first[-1] not in ('!', '?', '.'):\n        first += '.'\n\n    if second and second[-1] not in ('!', '?', '.'):\n        second += '.'\n\n    if first and not second:\n        return first\n\n    if not first and second:\n        return second\n\n    return ' '.join((first, second))", "loc": 18}
{"file": "ansible\\lib\\ansible\\_internal\\_display_utils.py", "class_name": "DeferredWarningContext", "function_name": "capture", "parameters": ["self", "warning"], "param_types": {"warning": "_messages.WarningSummary"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._deprecation_warnings.append", "self._seen.add", "self._warnings.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add the warning/deprecation to the context if it has not already been seen by this context.", "source_code": "def capture(self, warning: _messages.WarningSummary) -> None:\n    \"\"\"Add the warning/deprecation to the context if it has not already been seen by this context.\"\"\"\n    if warning in self._seen:\n        return\n\n    self._seen.add(warning)\n\n    if isinstance(warning, _messages.DeprecationSummary):\n        self._deprecation_warnings.append(warning)\n    else:\n        self._warnings.append(warning)", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_event_formatting.py", "class_name": null, "function_name": "format_event", "parameters": ["event", "include_traceback"], "param_types": {"event": "_messages.Event", "include_traceback": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_event_traceback", "format_event_verbose_message", "msg.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Format an event into a verbose message and traceback.", "source_code": "def format_event(event: _messages.Event, include_traceback: bool) -> str:\n    \"\"\"Format an event into a verbose message and traceback.\"\"\"\n    msg = format_event_verbose_message(event)\n\n    if include_traceback:\n        msg += '\\n' + format_event_traceback(event)\n\n    msg = msg.strip()\n\n    if '\\n' in msg:\n        msg += '\\n\\n'\n    else:\n        msg += '\\n'\n\n    return msg", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_event_formatting.py", "class_name": null, "function_name": "format_event_traceback", "parameters": ["event"], "param_types": {"event": "_messages.Event"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "_format_event_children", "format_event_traceback", "reversed", "segments.append"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Format an event into a traceback.", "source_code": "def format_event_traceback(event: _messages.Event) -> str:\n    \"\"\"Format an event into a traceback.\"\"\"\n    segments: list[str] = []\n\n    while event:\n        segment = event.formatted_traceback or '(traceback missing)\\n'\n\n        if event.events:\n            child_tracebacks = [format_event_traceback(child) for child in event.events]\n            segment += _format_event_children(\"Sub-Traceback\", child_tracebacks)\n\n        segments.append(segment)\n\n        if event.chain:\n            segments.append(f'\\n{event.chain.traceback_reason}\\n\\n')\n\n            event = event.chain.event\n        else:\n            event = None\n\n    return ''.join(reversed(segments))", "loc": 21}
{"file": "ansible\\lib\\ansible\\_internal\\_event_formatting.py", "class_name": null, "function_name": "format_event_verbose_message", "parameters": ["event"], "param_types": {"event": "_messages.Event"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "'\\n'.join", "_event_utils.deduplicate_message_parts", "_event_utils.format_event_brief_message", "_format_event_children", "_get_message_lines", "format_event_verbose_message", "len", "messages.append", "segments.append", "segments.insert"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Format an event into a verbose message. Help text, contextual information and sub-events will be included.", "source_code": "def format_event_verbose_message(event: _messages.Event) -> str:\n    \"\"\"\n    Format an event into a verbose message.\n    Help text, contextual information and sub-events will be included.\n    \"\"\"\n    segments: list[str] = []\n    original_event = event\n\n    while event:\n        messages = [event.msg]\n        chain: _messages.EventChain = event.chain\n\n        while chain and chain.follow:\n            if chain.event.events:\n                break  # do not collapse a chained event with sub-events, since they would be lost\n\n            if chain.event.formatted_source_context or chain.event.help_text:\n                if chain.event.formatted_source_context != event.formatted_source_context or chain.event.help_text != event.help_text:\n                    break  # do not collapse a chained event with different details, since they would be lost\n\n            if chain.event.chain and chain.msg_reason != chain.event.chain.msg_reason:\n                break  # do not collapse a chained event which has a chain with a different msg_reason\n\n            messages.append(chain.event.msg)\n\n            chain = chain.event.chain\n\n        msg = _event_utils.deduplicate_message_parts(messages)\n        segment = '\\n'.join(_get_message_lines(msg, event.help_text, event.formatted_source_context)) + '\\n'\n\n        if event.events:\n            child_msgs = [format_event_verbose_message(child) for child in event.events]\n            segment += _format_event_children(\"Sub-Event\", child_msgs)\n\n        segments.append(segment)\n\n        if chain and chain.follow:\n            segments.append(f'\\n{chain.msg_reason}\\n\\n')\n\n            event = chain.event\n        else:\n            event = None\n\n    if len(segments) > 1:\n        segments.insert(0, _event_utils.format_event_brief_message(original_event) + '\\n\\n')\n\n    return ''.join(segments)", "loc": 47}
{"file": "ansible\\lib\\ansible\\_internal\\_task.py", "class_name": "TaskArgsFinalizer", "function_name": "finalize", "parameters": ["self", "callback", "context"], "param_types": {"callback": "TaskArgsFinalizerCallback", "context": "t.Any"}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "Display", "Display().warning", "TaskArgsChainTemplar", "_engine.TemplateOptions", "constants.config.get_config_value", "ct.as_dict", "isinstance", "native_type_name", "resolved_layers.append", "reversed", "self._templar.resolve_to_container"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finalize(self, callback: TaskArgsFinalizerCallback, context: t.Any) -> dict[str, t.Any]:\n    resolved_layers: list[c.Mapping[str, t.Any]] = []\n\n    for layer in self._args_layers:\n        if isinstance(layer, (str, _vault.EncryptedString)):  # EncryptedString can hide a template\n            if constants.config.get_config_value('INJECT_FACTS_AS_VARS'):\n                Display().warning(\n                    \"Using a template for task args is unsafe in some situations \"\n                    \"(see https://docs.ansible.com/ansible/devel/reference_appendices/faq.html#argsplat-unsafe).\",\n                    obj=layer,\n                )\n\n            resolved_layer = self._templar.resolve_to_container(layer, options=_engine.TemplateOptions(value_for_omit={}))\n        else:\n            resolved_layer = layer\n\n        if not isinstance(resolved_layer, dict):\n            raise AnsibleError(f'Task args must resolve to a {native_type_name(dict)!r} not {native_type_name(resolved_layer)!r}.', obj=layer)\n\n        resolved_layers.append(resolved_layer)\n\n    ct = TaskArgsChainTemplar(*reversed(resolved_layers), templar=self._templar, callback=callback, context=context)\n\n    return ct.as_dict()", "loc": 24}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\action\\debug.py", "class_name": "ActionModule", "function_name": "finalize_task_arg", "parameters": ["cls", "name", "value", "templar", "context"], "param_types": {"name": "str", "value": "t.Any", "templar": "TemplateEngine", "context": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().finalize_task_arg"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finalize_task_arg(cls, name: str, value: t.Any, templar: TemplateEngine, context: t.Any) -> t.Any:\n    if name == 'expression':\n        return value\n\n    return super().finalize_task_arg(name, value, templar, context)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\action\\debug.py", "class_name": "ActionModule", "function_name": "run", "parameters": ["self", "tmp", "task_vars"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ReplacingMarkerBehavior.warning_context", "dict", "self._templar._engine.extend", "self.validate_argument_spec", "templar.evaluate_expression"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, tmp=None, task_vars=None):\n    # accepts a list of literal expressions (no templating), evaluates with no failure on undefined, returns all results\n    _vr, args = self.validate_argument_spec(\n        argument_spec=dict(\n            expression=dict(type=_check_type_list_strict, elements=_check_type_str_no_conversion, required=True),\n        ),\n    )\n\n    with ReplacingMarkerBehavior.warning_context() as replacing_behavior:\n        templar = self._templar._engine.extend(marker_behavior=replacing_behavior)\n\n        return dict(\n            _ansible_verbose_always=True,\n            expression_result=[templar.evaluate_expression(expression) for expression in args['expression']],\n        )", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\filter\\apply_trust.py", "class_name": null, "function_name": "apply_trust", "parameters": ["value"], "param_types": {"value": "object"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TrustedAsTemplate", "TrustedAsTemplate().tag", "isinstance"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Filter that returns a tagged copy of the input string with TrustedAsTemplate. Containers and other non-string values are returned unmodified.", "source_code": "def apply_trust(value: object) -> object:\n    \"\"\"\n    Filter that returns a tagged copy of the input string with TrustedAsTemplate.\n    Containers and other non-string values are returned unmodified.\n    \"\"\"\n    return TrustedAsTemplate().tag(value) if isinstance(value, str) else value", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\filter\\dump_object.py", "class_name": null, "function_name": "dump_object", "parameters": ["value"], "param_types": {"value": "t.Any"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dataclasses.asdict", "dataclasses.is_dataclass", "dict", "isinstance", "value._as_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Internal filter to convert objects not supported by JSON to types which are.", "source_code": "def dump_object(value: t.Any) -> object:\n    \"\"\"Internal filter to convert objects not supported by JSON to types which are.\"\"\"\n    if dataclasses.is_dataclass(value):\n        return dataclasses.asdict(value)  # type: ignore[arg-type]\n\n    if isinstance(value, ExceptionMarker):\n        return dict(\n            exception=value._as_exception(),\n        )\n\n    return value", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\filter\\origin.py", "class_name": null, "function_name": "origin", "parameters": ["value"], "param_types": {"value": "object"}, "return_type": "str | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Origin.get_tag", "str"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return the origin of the value, if any, otherwise `None`.", "source_code": "def origin(value: object) -> str | None:\n    \"\"\"Return the origin of the value, if any, otherwise `None`.\"\"\"\n    origin_tag = Origin.get_tag(value)\n\n    return str(origin_tag) if origin_tag else None", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\filter\\python_literal_eval.py", "class_name": null, "function_name": "python_literal_eval", "parameters": ["value", "ignore_errors"], "param_types": {"value": "object"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTypeError", "ast.literal_eval", "isinstance"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def python_literal_eval(value: object, ignore_errors=False) -> object:\n    try:\n        if isinstance(value, str):\n            return ast.literal_eval(value)\n\n        raise AnsibleTypeError(\"The `value` to eval must be a string.\", obj=value)\n    except Exception:\n        if ignore_errors:\n            return value\n\n        raise", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\filter\\unmask.py", "class_name": null, "function_name": "unmask", "parameters": ["value", "type_names"], "param_types": {"value": "object", "type_names": "str | list[str]"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "AnsibleError", "copy.copy", "dataclasses.replace", "frozenset", "isinstance", "validate_arg_type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Internal filter to suppress automatic type transformation in Jinja (e.g., WarningSummary, DeprecationSummary, ErrorSummary). Lazy collection caching is in play - the first attempt to access a value in a given lazy container must be with unmasking in place, or the transformed value will already be cached.", "source_code": "def unmask(value: object, type_names: str | list[str]) -> object:\n    \"\"\"\n    Internal filter to suppress automatic type transformation in Jinja (e.g., WarningSummary, DeprecationSummary, ErrorSummary).\n    Lazy collection caching is in play - the first attempt to access a value in a given lazy container must be with unmasking in place, or the transformed value\n    will already be cached.\n    \"\"\"\n    validate_arg_type(\"type_names\", type_names, (str, list))\n\n    if isinstance(type_names, str):\n        check_type_names = [type_names]\n    else:\n        check_type_names = type_names\n\n    valid_type_names = {key.__name__ for key in _type_transform_mapping}\n    invalid_type_names = [type_name for type_name in check_type_names if type_name not in valid_type_names]\n\n    if invalid_type_names:\n        raise AnsibleError(f'Unknown type name(s): {\", \".join(invalid_type_names)}', obj=type_names)\n\n    result: object\n\n    if isinstance(value, _AnsibleLazyTemplateMixin):\n        result = copy.copy(value)\n        result._lazy_options = dataclasses.replace(\n            result._lazy_options,\n            unmask_type_names=result._lazy_options.unmask_type_names | frozenset(check_type_names),\n        )\n    else:\n        result = value\n\n    return result", "loc": 31}
{"file": "ansible\\lib\\ansible\\_internal\\ansible_collections\\ansible\\_protomatter\\plugins\\lookup\\config.py", "class_name": "LookupModule", "function_name": "run", "parameters": ["self", "terms", "variables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "traceback_for"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, terms, variables=None, **kwargs):\n    if not terms or not (config_name := terms[0]):\n        raise ValueError(\"config name is required\")\n\n    match config_name:\n        case 'DISPLAY_TRACEBACK':\n            # since config can't expand this yet, we need the post-processed version\n            from ansible.module_utils._internal._traceback import traceback_for\n\n            return traceback_for()\n        # DTFIX-FUTURE: plumb through normal config fallback\n        case _:\n            raise ValueError(f\"Unknown config name {config_name!r}.\")", "loc": 13}
{"file": "ansible\\lib\\ansible\\_internal\\_ansiballz\\_wrapper.py", "class_name": null, "function_name": "invoke_module", "parameters": ["modlib_path", "json_params"], "param_types": {"modlib_path": "str", "json_params": "bytes"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_loader.run_module", "date_time.utctimetuple", "sitecustomize.encode", "sys.path.insert", "z.close", "z.writestr", "zipfile.ZipFile", "zipfile.ZipInfo"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def invoke_module(modlib_path: str, json_params: bytes) -> None:\n    # When installed via setuptools (including python setup.py install),\n    # ansible may be installed with an easy-install.pth file.  That file\n    # may load the system-wide install of ansible rather than the one in\n    # the module.  sitecustomize is the only way to override that setting.\n    z = zipfile.ZipFile(modlib_path, mode='a')\n\n    # py3: modlib_path will be text, py2: it's bytes.  Need bytes at the end\n    sitecustomize = u'import sys\\\\nsys.path.insert(0,\"%s\")\\\\n' % modlib_path\n    sitecustomize = sitecustomize.encode('utf-8')\n    # Use a ZipInfo to work around zipfile limitation on hosts with\n    # clocks set to a pre-1980 year (for instance, Raspberry Pi)\n    zinfo = zipfile.ZipInfo()\n    zinfo.filename = 'sitecustomize.py'\n    zinfo.date_time = date_time.utctimetuple()[:6]\n    z.writestr(zinfo, sitecustomize)\n    z.close()\n\n    # Put the zipped up module_utils we got from the controller first in the python path so that we\n    # can monkeypatch the right basic\n    sys.path.insert(0, modlib_path)\n\n    from ansible.module_utils._internal._ansiballz import _loader\n\n    _loader.run_module(\n        json_params=json_params,\n        profile=profile,\n        module_fqn=module_fqn,\n        modlib_path=modlib_path,\n        extensions=extensions,\n    )", "loc": 31}
{"file": "ansible\\lib\\ansible\\_internal\\_datatag\\_tags.py", "class_name": "Origin", "function_name": "get_or_create_tag", "parameters": ["cls", "value", "path"], "param_types": {"value": "t.Any", "path": "str | os.PathLike | None"}, "return_type": "Origin", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Origin", "cls.get_tag", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return the tag from the given value, creating a tag from the provided path if no tag was found.", "source_code": "def get_or_create_tag(cls, value: t.Any, path: str | os.PathLike | None) -> Origin:\n    \"\"\"Return the tag from the given value, creating a tag from the provided path if no tag was found.\"\"\"\n    if not (origin := cls.get_tag(value)):\n        if path:\n            origin = Origin(path=str(path))  # convert tagged strings and path-like values to a native str\n        else:\n            origin = Origin.UNKNOWN\n\n    return origin", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_datatag\\_tags.py", "class_name": "Origin", "function_name": "replace", "parameters": ["self", "path", "description", "line_num", "col_num"], "param_types": {"path": "str | types.EllipsisType", "description": "str | types.EllipsisType", "line_num": "int | None | types.EllipsisType", "col_num": "int | None | types.EllipsisType"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dataclasses.replace", "dict", "dict(path=path, description=description, line_num=line_num, col_num=col_num).items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a new origin based on an existing one, with the given fields replaced.", "source_code": "def replace(\n    self,\n    path: str | types.EllipsisType = ...,\n    description: str | types.EllipsisType = ...,\n    line_num: int | None | types.EllipsisType = ...,\n    col_num: int | None | types.EllipsisType = ...,\n) -> t.Self:\n    \"\"\"Return a new origin based on an existing one, with the given fields replaced.\"\"\"\n    return dataclasses.replace(\n        self,\n        **{\n            key: value\n            for key, value in dict(\n                path=path,\n                description=description,\n                line_num=line_num,\n                col_num=col_num,\n            ).items()\n            if value is not ...\n        },  # type: ignore[arg-type]\n    )", "loc": 21}
{"file": "ansible\\lib\\ansible\\_internal\\_datatag\\_utils.py", "class_name": null, "function_name": "str_problematic_strip", "parameters": ["value"], "param_types": {"value": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag_copy", "value.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a copy of `value` with leading and trailing whitespace removed. Used where `str.strip` is needed, but tags must be preserved *AND* the stripping behavior likely shouldn't exist. If the stripping behavior is non-problematic, use `AnsibleTagHelper.tag_copy` around `str.strip` instead.", "source_code": "def str_problematic_strip(value: str) -> str:\n    \"\"\"\n    Return a copy of `value` with leading and trailing whitespace removed.\n    Used where `str.strip` is needed, but tags must be preserved *AND* the stripping behavior likely shouldn't exist.\n    If the stripping behavior is non-problematic, use `AnsibleTagHelper.tag_copy` around `str.strip` instead.\n    \"\"\"\n    if (stripped_value := value.strip()) == value:\n        return value\n\n    # FUTURE: consider deprecating some/all usages of this method; they generally imply a code smell or pattern we shouldn't be supporting\n\n    stripped_value = AnsibleTagHelper.tag_copy(value, stripped_value)\n\n    return stripped_value", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_alarm_timeout.py", "class_name": "AnsibleTimeoutError", "function_name": "alarm_timeout", "parameters": ["cls", "timeout"], "param_types": {"timeout": "int | None"}, "return_type": "_t.Iterator[None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "TypeError", "ValueError", "cls", "datatag.native_type_name", "isinstance", "signal.alarm", "signal.signal"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Context for running code under an optional timeout.", "source_code": "def alarm_timeout(cls, timeout: int | None) -> _t.Iterator[None]:\n    \"\"\"\n    Context for running code under an optional timeout.\n    Raises an instance of this class if the timeout occurs.\n\n    New usages of this timeout mechanism are discouraged.\n    \"\"\"\n    if timeout is not None:\n        if not isinstance(timeout, int):\n            raise TypeError(f\"Timeout requires 'int' argument, not {datatag.native_type_name(timeout)!r}.\")\n\n        if timeout < 0 or timeout > cls._MAX_TIMEOUT:\n            # On BSD based systems, alarm is implemented using setitimer.\n            # If out-of-bounds values are passed to alarm, they will return -1, which would be interpreted as an existing timer being set.\n            # To avoid that, bounds checking is performed in advance.\n            raise ValueError(f'Timeout {timeout} is invalid, it must be between 0 and {cls._MAX_TIMEOUT}.')\n\n    if not timeout:\n        yield  # execute the context manager's body\n        return  # no timeout to deal with, exit immediately\n\n    def on_alarm(_signal: int, _frame: types.FrameType) -> None:\n        raise cls(timeout)\n\n    if signal.signal(signal.SIGALRM, on_alarm):\n        raise RuntimeError(\"An existing alarm handler was present.\")\n\n    try:\n        try:\n            if signal.alarm(timeout):\n                raise RuntimeError(\"An existing alarm was set.\")\n\n            yield  # execute the context manager's body\n        finally:\n            # Disable the alarm.\n            # If the alarm fires inside this finally block, the alarm is still disabled.\n            # This guarantees the cleanup code in the outer finally block runs without risk of encountering the `TaskTimeoutError` from the alarm.\n            signal.alarm(0)\n    finally:\n        signal.signal(signal.SIGALRM, signal.SIG_DFL)", "loc": 40}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_captured.py", "class_name": "AnsibleResultCapturedError", "function_name": "normalize_result_exception", "parameters": ["cls", "result"], "param_types": {"result": "dict[str, t.Any]"}, "return_type": "CapturedErrorSummary | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CapturedErrorSummary", "TypeError", "_messages.Event", "cls._normalize_traceback", "isinstance", "result.get", "result.pop", "result.update", "str", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Normalize the result `exception`, if any, to be a `CapturedErrorSummary` instance. If a new `CapturedErrorSummary` was created, the `error_type` will be `cls`. The `exception` key will be removed if falsey.", "source_code": "def normalize_result_exception(cls, result: dict[str, t.Any]) -> CapturedErrorSummary | None:\n    \"\"\"\n    Normalize the result `exception`, if any, to be a `CapturedErrorSummary` instance.\n    If a new `CapturedErrorSummary` was created, the `error_type` will be `cls`.\n    The `exception` key will be removed if falsey.\n    A `CapturedErrorSummary` instance will be returned if `failed` is truthy.\n    \"\"\"\n    if type(cls) is AnsibleResultCapturedError:  # pylint: disable=unidiomatic-typecheck\n        raise TypeError('The normalize_result_exception method cannot be called on the AnsibleCapturedError base type, use a derived type.')\n\n    if not isinstance(result, dict):\n        raise TypeError(f'Malformed result. Received {type(result)} instead of {dict}.')\n\n    failed = result.get('failed')  # DTFIX-FUTURE: warn if failed is present and not a bool, or exception is present without failed being True\n    exception = result.pop('exception', None)\n\n    if not failed and not exception:\n        return None\n\n    if isinstance(exception, CapturedErrorSummary):\n        error_summary = exception\n    elif isinstance(exception, _messages.ErrorSummary):\n        error_summary = CapturedErrorSummary(\n            event=exception.event,\n            error_type=cls,\n        )\n    else:\n        # translate non-ErrorDetail errors\n        error_summary = CapturedErrorSummary(\n            event=_messages.Event(\n                msg=str(result.get('msg', 'Unknown error.')),\n                formatted_traceback=cls._normalize_traceback(exception),\n            ),\n            error_type=cls,\n        )\n\n    result.update(exception=error_summary)\n\n    return error_summary if failed else None  # even though error detail was normalized, only return it if the result indicated failure", "loc": 39}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_error_utils.py", "class_name": null, "function_name": "result_dict_from_exception", "parameters": ["exception", "accept_result_contribution"], "param_types": {"exception": "BaseException", "accept_result_contribution": "bool"}, "return_type": "dict[str, object]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_error_factory.ControllerEventFactory.from_exception", "_event_utils.format_event_brief_message", "_messages.ErrorSummary", "_traceback.is_traceback_enabled", "dict", "isinstance", "result.pop", "result.update"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Return a failed task result dict from the given exception.", "source_code": "def result_dict_from_exception(exception: BaseException, accept_result_contribution: bool = False) -> dict[str, object]:\n    \"\"\"Return a failed task result dict from the given exception.\"\"\"\n    event = _error_factory.ControllerEventFactory.from_exception(exception, _traceback.is_traceback_enabled(_traceback.TracebackEvent.ERROR))\n\n    result: dict[str, object] = {}\n    omit_failed_key = False\n    omit_exception_key = False\n\n    if accept_result_contribution:\n        while exception:\n            if isinstance(exception, ContributesToTaskResult):\n                result = dict(exception.result_contribution)\n                omit_failed_key = exception.omit_failed_key\n                omit_exception_key = exception.omit_exception_key\n                break\n\n            exception = exception.__cause__\n\n    if omit_failed_key:\n        result.pop('failed', None)\n    else:\n        result.update(failed=True)\n\n    if omit_exception_key:\n        result.pop('exception', None)\n    else:\n        result.update(exception=_messages.ErrorSummary(event=event))\n\n    if 'msg' not in result:\n        # if nothing contributed `msg`, generate one from the exception messages\n        result.update(msg=_event_utils.format_event_brief_message(event))\n\n    return result", "loc": 33}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_error_utils.py", "class_name": null, "function_name": "result_dict_from_captured_errors", "parameters": ["msg"], "param_types": {"msg": "str"}, "return_type": "dict[str, object]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_event_utils.format_event_brief_message", "_messages.ErrorSummary", "_messages.Event", "_traceback.maybe_capture_traceback", "dict", "tuple"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a failed task result dict from the given error message and captured errors.", "source_code": "def result_dict_from_captured_errors(\n    msg: str,\n    *,\n    errors: list[_messages.ErrorSummary] | None = None,\n) -> dict[str, object]:\n    \"\"\"Return a failed task result dict from the given error message and captured errors.\"\"\"\n    _skip_stackwalk = True\n\n    event = _messages.Event(\n        msg=msg,\n        formatted_traceback=_traceback.maybe_capture_traceback(msg, _traceback.TracebackEvent.ERROR),\n        events=tuple(error.event for error in errors) if errors else None,\n    )\n\n    result = dict(\n        failed=True,\n        exception=_messages.ErrorSummary(\n            event=event,\n        ),\n        msg=_event_utils.format_event_brief_message(event),\n    )\n\n    return result", "loc": 23}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_error_utils.py", "class_name": "SourceContext", "function_name": "from_value", "parameters": ["cls", "value"], "param_types": {"value": "t.Any"}, "return_type": "SourceContext | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Origin.get_tag", "RedactAnnotatedSourceContext.current", "SourceContext", "cls.error", "cls.from_origin", "isinstance", "str", "textwrap.shorten"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Attempt to retrieve source and render a contextual indicator from the value's origin (if any).", "source_code": "def from_value(cls, value: t.Any) -> SourceContext | None:\n    \"\"\"Attempt to retrieve source and render a contextual indicator from the value's origin (if any).\"\"\"\n    if value is None:\n        return None\n\n    if isinstance(value, Origin):\n        origin = value\n        value = None\n    else:\n        origin = Origin.get_tag(value)\n\n    if RedactAnnotatedSourceContext.current(optional=True):\n        return cls.error('content redacted')\n\n    if origin and origin.path:\n        return cls.from_origin(origin)\n\n    if value is None:\n        truncated_value = None\n        annotated_source_lines = []\n    else:\n        # DTFIX-FUTURE: cleanup/share width\n        try:\n            value = str(value)\n        except Exception as ex:\n            value = f'<< context unavailable: {ex} >>'\n\n        truncated_value = textwrap.shorten(value, width=120)\n        annotated_source_lines = [truncated_value]\n\n    return SourceContext(\n        origin=origin or Origin.UNKNOWN,\n        annotated_source_lines=annotated_source_lines,\n        target_line=truncated_value,\n    )", "loc": 35}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_handler.py", "class_name": "ErrorHandler", "function_name": "handle", "parameters": ["self"], "param_types": {}, "return_type": "t.Iterator[None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "_SkipException", "display.error_as_warning"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Handle the specified exception(s) using the defined error action. If `skip_on_ignore` is `True`, the body of the context manager will be skipped for `ErrorAction.IGNORE`. Use of `skip_on_ignore` requires enclosure within the `Skippable` context manager.", "source_code": "def handle(self, *args: type[BaseException], skip_on_ignore: bool = False) -> t.Iterator[None]:\n    \"\"\"\n    Handle the specified exception(s) using the defined error action.\n    If `skip_on_ignore` is `True`, the body of the context manager will be skipped for `ErrorAction.IGNORE`.\n    Use of `skip_on_ignore` requires enclosure within the `Skippable` context manager.\n    \"\"\"\n    if not args:\n        raise ValueError('At least one exception type is required.')\n\n    if skip_on_ignore and self.action == ErrorAction.IGNORE:\n        raise _SkipException()  # skipping ignored action\n\n    try:\n        yield\n    except args as ex:\n        match self.action:\n            case ErrorAction.WARNING:\n                display.error_as_warning(msg=None, exception=ex)\n            case ErrorAction.ERROR:\n                raise\n            case _:  # ErrorAction.IGNORE\n                pass\n\n    if skip_on_ignore:\n        raise _SkipException()  # completed skippable action, ensures the `Skippable` context was used", "loc": 25}
{"file": "ansible\\lib\\ansible\\_internal\\_errors\\_task_timeout.py", "class_name": "TaskTimeoutError", "function_name": "result_contribution", "parameters": ["self"], "param_types": {}, "return_type": "_c.Mapping[str, object]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["deprecate_value", "dict"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def result_contribution(self) -> _c.Mapping[str, object]:\n    help_text = \"Configure `DISPLAY_TRACEBACK` to see a traceback on timeout errors.\"\n\n    frame = deprecate_value(\n        value=help_text,\n        msg=\"The `timedout.frame` task result key is deprecated.\",\n        help_text=help_text,\n        version=\"2.23\",\n    )\n\n    return dict(timedout=dict(frame=frame, period=self.timeout))", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_legacy_encoder.py", "class_name": "LegacyControllerJSONEncoder", "function_name": "default", "parameters": ["self", "o"], "param_types": {"o": "_t.Any"}, "return_type": "_t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "o.decode", "str", "super", "super().default", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Hooked default that can conditionally bypass base encoder behavior based on this instance's config.", "source_code": "def default(self, o: _t.Any) -> _t.Any:\n    \"\"\"Hooked default that can conditionally bypass base encoder behavior based on this instance's config.\"\"\"\n    if type(o) is _profiles._WrappedValue:  # pylint: disable=unidiomatic-typecheck\n        o = o.wrapped\n\n    if not self._preprocess_unsafe and type(o) is _legacy._Untrusted:  # pylint: disable=unidiomatic-typecheck\n        return o.value  # if not emitting unsafe markers, bypass custom unsafe serialization and just return the raw value\n\n    if self._vault_to_text and type(o) is _vault.EncryptedString:  # pylint: disable=unidiomatic-typecheck\n        return str(o)  # decrypt and return the plaintext (or fail trying)\n\n    if self._decode_bytes and isinstance(o, bytes):\n        return o.decode(errors='surrogateescape')  # backward compatibility with `ansible.module_utils.basic.jsonify`\n\n    return super().default(o)", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_profiles\\_legacy.py", "class_name": "_Profile", "function_name": "serialize_tagged_str", "parameters": ["cls", "value"], "param_types": {"value": "_datatag.AnsibleTaggedObject"}, "return_type": "_t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_datatag.AnsibleTagHelper.untag", "_vault.VaultHelper.get_ciphertext", "dict"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def serialize_tagged_str(cls, value: _datatag.AnsibleTaggedObject) -> _t.Any:\n    if ciphertext := _vault.VaultHelper.get_ciphertext(value, with_tags=False):\n        return dict(\n            __ansible_vault=ciphertext,\n        )\n\n    return _datatag.AnsibleTagHelper.untag(value)", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_profiles\\_legacy.py", "class_name": "_Profile", "function_name": "deserialize_unsafe", "parameters": ["cls", "value"], "param_types": {"value": "dict[str, _t.Any]"}, "return_type": "_Untrusted", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "_Untrusted", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deserialize_unsafe(cls, value: dict[str, _t.Any]) -> _Untrusted:\n    ansible_unsafe = value['__ansible_unsafe']\n\n    if type(ansible_unsafe) is not str:  # pylint: disable=unidiomatic-typecheck\n        raise TypeError(f\"__ansible_unsafe is {type(ansible_unsafe)} not {str}\")\n\n    return _Untrusted(ansible_unsafe)", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_profiles\\_legacy.py", "class_name": "_Profile", "function_name": "deserialize_vault", "parameters": ["cls", "value"], "param_types": {"value": "dict[str, _t.Any]"}, "return_type": "_vault.EncryptedString", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "_vault.EncryptedString", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deserialize_vault(cls, value: dict[str, _t.Any]) -> _vault.EncryptedString:\n    ansible_vault = value['__ansible_vault']\n\n    if type(ansible_vault) is not str:  # pylint: disable=unidiomatic-typecheck\n        raise TypeError(f\"__ansible_vault is {type(ansible_vault)} not {str}\")\n\n    encrypted_string = _vault.EncryptedString(ciphertext=ansible_vault)\n\n    return encrypted_string", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_profiles\\_legacy.py", "class_name": "_Profile", "function_name": "pre_serialize", "parameters": ["cls", "encoder", "o"], "param_types": {"encoder": "Encoder", "o": "_t.Any"}, "return_type": "_t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["avv.visit", "cls.visitor_type"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pre_serialize(cls, encoder: Encoder, o: _t.Any) -> _t.Any:\n    # DTFIX7: these conversion args probably aren't needed\n    avv = cls.visitor_type(invert_trust=True, convert_mapping_to_dict=True, convert_sequence_to_list=True, convert_custom_scalars=True)\n\n    return avv.visit(o)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_json\\_profiles\\_legacy.py", "class_name": "Decoder", "function_name": "raw_decode", "parameters": ["self", "s", "idx"], "param_types": {"s": "str", "idx": "int"}, "return_type": "tuple[_t.Any, int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_tags.Origin.get_tag", "_tags.TrustedAsTemplate.is_tagged_on", "super", "super().raw_decode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def raw_decode(self, s: str, idx: int = 0) -> tuple[_t.Any, int]:\n    self._origin = _tags.Origin.get_tag(s)\n    self._trusted_as_template = _tags.TrustedAsTemplate.is_tagged_on(s)\n\n    return super().raw_decode(s, idx)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_agent_launch.py", "class_name": null, "function_name": "launch_ssh_agent", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "_launch_ssh_agent"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "If configured via `SSH_AGENT`, launch an ssh-agent for Ansible's use and/or verify access to an existing one.", "source_code": "def launch_ssh_agent() -> None:\n    \"\"\"If configured via `SSH_AGENT`, launch an ssh-agent for Ansible's use and/or verify access to an existing one.\"\"\"\n    try:\n        _launch_ssh_agent()\n    except Exception as ex:\n        raise AnsibleError(\"Failed to launch ssh agent.\") from ex", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": null, "function_name": "key_data_into_crypto_objects", "parameters": ["key_data", "passphrase"], "param_types": {"key_data": "bytes", "passphrase": "bytes | None"}, "return_type": "tuple[CryptoPrivateKey, CryptoPublicKey, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PublicKeyMsg.from_public_key", "private_key.public_key", "serialization.ssh.load_ssh_private_key"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def key_data_into_crypto_objects(key_data: bytes, passphrase: bytes | None) -> tuple[CryptoPrivateKey, CryptoPublicKey, str]:\n    private_key = serialization.ssh.load_ssh_private_key(key_data, passphrase)\n    public_key = private_key.public_key()\n    fingerprint = PublicKeyMsg.from_public_key(public_key).fingerprint\n\n    return private_key, public_key, fingerprint", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "VariableSized", "function_name": "consume_from_blob", "parameters": ["cls", "blob"], "param_types": {"blob": "memoryview | bytes"}, "return_type": "tuple[t.Self, memoryview | bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_split_blob", "cls.from_blob", "uint32.from_blob"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def consume_from_blob(cls, blob: memoryview | bytes) -> tuple[t.Self, memoryview | bytes]:\n    length = uint32.from_blob(blob[:4])\n    blob = blob[4:]\n    data, rest = _split_blob(blob, length)\n    return cls.from_blob(data), rest", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "mpint", "function_name": "to_blob", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "bytearray", "len", "self.bit_length", "self.to_bytes", "uint32", "uint32(len(ret)).to_blob"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_blob(self) -> bytes:\n    if self < 0:\n        raise ValueError(\"negative mpint not allowed\")\n    if not self:\n        return b\"\"\n    nbytes = (self.bit_length() + 8) // 8\n    ret = bytearray(self.to_bytes(length=nbytes, byteorder='big'))\n    ret[:0] = uint32(len(ret)).to_blob()\n    return ret", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "binary_string", "function_name": "to_blob", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "uint32", "uint32(length).to_blob"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_blob(self) -> bytes:\n    if length := len(self):\n        return uint32(length).to_blob() + self\n    else:\n        return b\"\"", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "unicode_string", "function_name": "to_blob", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.encode", "uint32", "uint32(length).to_blob"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_blob(self) -> bytes:\n    val = self.encode('utf-8')\n    if length := len(val):\n        return uint32(length).to_blob() + val\n    else:\n        return b\"\"", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "Msg", "function_name": "to_blob", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["NotImplementedError", "bytearray", "dataclasses.fields", "fv.to_blob", "getattr", "isinstance", "rv.extend"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_blob(self) -> bytes:\n    rv = bytearray()\n    for field in dataclasses.fields(self):\n        fv = getattr(self, field.name)\n        if isinstance(fv, SupportsToBlob):\n            rv.extend(fv.to_blob())\n        else:\n            raise NotImplementedError(field.type)\n    return rv", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "Msg", "function_name": "from_blob", "parameters": ["cls", "blob"], "param_types": {"blob": "memoryview | bytes"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["NotImplementedError", "args.append", "cls", "field_type.consume_from_blob", "isinstance", "str", "t.get_type_hints", "t.get_type_hints(cls).items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_blob(cls, blob: memoryview | bytes) -> t.Self:\n    args: list[t.Any] = []\n    for _field_name, field_type in t.get_type_hints(cls).items():\n        if isinstance(field_type, SupportsFromBlob):\n            fv, blob = field_type.consume_from_blob(blob)\n            args.append(fv)\n        else:\n            raise NotImplementedError(str(field_type))\n    return cls(*args)", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "PrivateKeyMsg", "function_name": "from_private_key", "parameters": ["private_key"], "param_types": {"private_key": "CryptoPrivateKey"}, "return_type": "PrivateKeyMsg", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DSAPrivateKeyMsg", "EcdsaPrivateKeyMsg", "Ed25519PrivateKeyMsg", "NotImplementedError", "RSAPrivateKeyMsg", "binary_string", "getattr", "mpint", "private_key.private_bytes", "private_key.private_numbers", "private_key.public_key", "private_key.public_key().public_bytes", "serialization.NoEncryption", "unicode_string"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_private_key(private_key: CryptoPrivateKey) -> PrivateKeyMsg:\n    match private_key:\n        case RSAPrivateKey():\n            rsa_pn: RSAPrivateNumbers = private_key.private_numbers()\n            return RSAPrivateKeyMsg(\n                KeyAlgo.RSA,\n                mpint(rsa_pn.public_numbers.n),\n                mpint(rsa_pn.public_numbers.e),\n                mpint(rsa_pn.d),\n                mpint(rsa_pn.iqmp),\n                mpint(rsa_pn.p),\n                mpint(rsa_pn.q),\n            )\n        case DSAPrivateKey():\n            dsa_pn: DSAPrivateNumbers = private_key.private_numbers()\n            return DSAPrivateKeyMsg(\n                KeyAlgo.DSA,\n                mpint(dsa_pn.public_numbers.parameter_numbers.p),\n                mpint(dsa_pn.public_numbers.parameter_numbers.q),\n                mpint(dsa_pn.public_numbers.parameter_numbers.g),\n                mpint(dsa_pn.public_numbers.y),\n                mpint(dsa_pn.x),\n            )\n        case EllipticCurvePrivateKey():\n            ecdsa_pn: EllipticCurvePrivateNumbers = private_key.private_numbers()\n            key_size = private_key.key_size\n            return EcdsaPrivateKeyMsg(\n                getattr(KeyAlgo, f'ECDSA{key_size}'),\n                unicode_string(f'nistp{key_size}'),\n                binary_string(\n                    private_key.public_key().public_bytes(\n                        encoding=serialization.Encoding.X962,\n                        format=serialization.PublicFormat.UncompressedPoint,\n                    )\n                ),\n                mpint(ecdsa_pn.private_value),\n            )\n        case Ed25519PrivateKey():\n            public_bytes = private_key.public_key().public_bytes(\n                encoding=serialization.Encoding.Raw,\n                format=serialization.PublicFormat.Raw,\n            )\n            private_bytes = private_key.private_bytes(\n                encoding=serialization.Encoding.Raw,\n                format=serialization.PrivateFormat.Raw,\n                encryption_algorithm=serialization.NoEncryption(),\n            )\n            return Ed25519PrivateKeyMsg(\n                KeyAlgo.ED25519,\n                binary_string(public_bytes),\n                binary_string(private_bytes + public_bytes),\n            )\n        case _:\n            raise NotImplementedError(private_key)", "loc": 54}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "PublicKeyMsg", "function_name": "public_key", "parameters": ["self"], "param_types": {}, "return_type": "CryptoPublicKey", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DSAParameterNumbers", "DSAPublicNumbers", "DSAPublicNumbers(self.y, DSAParameterNumbers(self.p, self.q, self.g)).public_key", "Ed25519PublicKey.from_public_bytes", "EllipticCurvePublicKey.from_encoded_point", "KeyAlgo", "NotImplementedError", "RSAPublicNumbers", "RSAPublicNumbers(self.e, self.n).public_key", "curve"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def public_key(self) -> CryptoPublicKey:\n    type: KeyAlgo = self.type\n    match type:\n        case KeyAlgo.RSA:\n            return RSAPublicNumbers(self.e, self.n).public_key()\n        case KeyAlgo.ECDSA256 | KeyAlgo.ECDSA384 | KeyAlgo.ECDSA521:\n            curve = _ECDSA_KEY_TYPE[KeyAlgo(type)]\n            return EllipticCurvePublicKey.from_encoded_point(curve(), self.Q)\n        case KeyAlgo.ED25519:\n            return Ed25519PublicKey.from_public_bytes(self.enc_a)\n        case KeyAlgo.DSA:\n            return DSAPublicNumbers(self.y, DSAParameterNumbers(self.p, self.q, self.g)).public_key()\n        case _:\n            raise NotImplementedError(type)", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "PublicKeyMsg", "function_name": "from_public_key", "parameters": ["public_key"], "param_types": {"public_key": "CryptoPublicKey"}, "return_type": "PublicKeyMsg", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DSAPublicKeyMsg", "EcdsaPublicKeyMsg", "Ed25519PublicKeyMsg", "NotImplementedError", "RSAPublicKeyMsg", "binary_string", "getattr", "mpint", "public_key.public_bytes", "public_key.public_numbers", "unicode_string"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_public_key(public_key: CryptoPublicKey) -> PublicKeyMsg:\n    match public_key:\n        case DSAPublicKey():\n            dsa_pn: DSAPublicNumbers = public_key.public_numbers()\n            return DSAPublicKeyMsg(\n                KeyAlgo.DSA,\n                mpint(dsa_pn.parameter_numbers.p),\n                mpint(dsa_pn.parameter_numbers.q),\n                mpint(dsa_pn.parameter_numbers.g),\n                mpint(dsa_pn.y),\n            )\n        case EllipticCurvePublicKey():\n            return EcdsaPublicKeyMsg(\n                getattr(KeyAlgo, f'ECDSA{public_key.curve.key_size}'),\n                unicode_string(f'nistp{public_key.curve.key_size}'),\n                binary_string(\n                    public_key.public_bytes(\n                        encoding=serialization.Encoding.X962,\n                        format=serialization.PublicFormat.UncompressedPoint,\n                    )\n                ),\n            )\n        case Ed25519PublicKey():\n            return Ed25519PublicKeyMsg(\n                KeyAlgo.ED25519,\n                binary_string(\n                    public_key.public_bytes(\n                        encoding=serialization.Encoding.Raw,\n                        format=serialization.PublicFormat.Raw,\n                    )\n                ),\n            )\n        case RSAPublicKey():\n            rsa_pn: RSAPublicNumbers = public_key.public_numbers()\n            return RSAPublicKeyMsg(KeyAlgo.RSA, mpint(rsa_pn.e), mpint(rsa_pn.n))\n        case _:\n            raise NotImplementedError(public_key)", "loc": 37}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "PublicKeyMsg", "function_name": "fingerprint", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["binascii.b2a_base64", "binascii.b2a_base64(digest.digest(), newline=False).rstrip", "binascii.b2a_base64(digest.digest(), newline=False).rstrip(b'=').decode", "copy.copy", "digest.digest", "digest.update", "hashlib.sha256", "msg.to_blob", "unicode_string"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fingerprint(self) -> str:\n    digest = hashlib.sha256()\n    msg = copy.copy(self)\n    msg.comments = unicode_string('')\n    k = msg.to_blob()\n    digest.update(k)\n    return binascii.b2a_base64(digest.digest(), newline=False).rstrip(b'=').decode('utf-8')", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "PublicKeyMsgList", "function_name": "consume_from_blob", "parameters": ["cls", "blob"], "param_types": {"blob": "memoryview | bytes"}, "return_type": "tuple[t.Self, memoryview | bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyAlgo", "PublicKeyMsg.get_dataclass", "args.append", "bytes", "bytes(peek_key_algo).decode", "cls", "cls._consume_field", "pub_key_msg_cls.from_blob"], "control_structures": ["While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def consume_from_blob(cls, blob: memoryview | bytes) -> tuple[t.Self, memoryview | bytes]:\n    args: list[PublicKeyMsg] = []\n    while blob:\n        prev_blob = blob\n        key_blob, key_blob_length, comment_blob = cls._consume_field(blob)\n\n        peek_key_algo, _length, _blob = cls._consume_field(key_blob)\n        pub_key_msg_cls = PublicKeyMsg.get_dataclass(KeyAlgo(bytes(peek_key_algo).decode('utf-8')))\n\n        _fv, comment_blob_length, blob = cls._consume_field(comment_blob)\n        key_plus_comment = prev_blob[4 : (4 + key_blob_length) + (4 + comment_blob_length)]\n\n        args.append(pub_key_msg_cls.from_blob(key_plus_comment))\n    return cls(args), b\"\"", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "SshAgentClient", "function_name": "send", "parameters": ["self", "msg"], "param_types": {"msg": "bytes"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SshAgentFailure", "len", "self._sock.recv", "self._sock.sendall", "uint32", "uint32(len(msg)).to_blob", "uint32.from_blob"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def send(self, msg: bytes) -> bytes:\n    length = uint32(len(msg)).to_blob()\n    self._sock.sendall(length + msg)\n    bufsize = uint32.from_blob(self._sock.recv(4))\n    resp = self._sock.recv(bufsize)\n    if resp[0] == ProtocolMsgNumbers.SSH_AGENT_FAILURE:\n        raise SshAgentFailure('agent: failure')\n    return resp", "loc": 8}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "SshAgentClient", "function_name": "add", "parameters": ["self", "private_key", "comments", "lifetime", "confirm"], "param_types": {"private_key": "CryptoPrivateKey", "comments": "str | None", "lifetime": "int | None", "confirm": "bool | None"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PrivateKeyMsg.from_private_key", "ProtocolMsgNumbers.SSH_AGENTC_ADD_IDENTITY.to_blob", "ProtocolMsgNumbers.SSH_AGENTC_ADD_ID_CONSTRAINED.to_blob", "constraints", "constraints([ProtocolMsgNumbers.SSH_AGENT_CONSTRAIN_CONFIRM]).to_blob", "constraints([ProtocolMsgNumbers.SSH_AGENT_CONSTRAIN_LIFETIME]).to_blob", "key_msg.to_blob", "self.send", "uint32", "uint32(lifetime).to_blob", "unicode_string"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add(\n    self,\n    private_key: CryptoPrivateKey,\n    comments: str | None = None,\n    lifetime: int | None = None,\n    confirm: bool | None = None,\n) -> None:\n    key_msg = PrivateKeyMsg.from_private_key(private_key)\n    key_msg.comments = unicode_string(comments or '')\n    if lifetime:\n        key_msg.constraints += constraints([ProtocolMsgNumbers.SSH_AGENT_CONSTRAIN_LIFETIME]).to_blob() + uint32(lifetime).to_blob()\n    if confirm:\n        key_msg.constraints += constraints([ProtocolMsgNumbers.SSH_AGENT_CONSTRAIN_CONFIRM]).to_blob()\n\n    if key_msg.constraints:\n        msg = ProtocolMsgNumbers.SSH_AGENTC_ADD_ID_CONSTRAINED.to_blob()\n    else:\n        msg = ProtocolMsgNumbers.SSH_AGENTC_ADD_IDENTITY.to_blob()\n    msg += key_msg.to_blob()\n    self.send(msg)", "loc": 20}
{"file": "ansible\\lib\\ansible\\_internal\\_ssh\\_ssh_agent.py", "class_name": "SshAgentClient", "function_name": "list", "parameters": ["self"], "param_types": {}, "return_type": "KeyList", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyList.from_blob", "ProtocolMsgNumbers.SSH_AGENTC_REQUEST_IDENTITIES.to_blob", "SshAgentFailure", "bytearray", "memoryview", "self.send"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list(self) -> KeyList:\n    req = ProtocolMsgNumbers.SSH_AGENTC_REQUEST_IDENTITIES.to_blob()\n    r = memoryview(bytearray(self.send(req)))\n    if r[0] != ProtocolMsgNumbers.SSH_AGENT_IDENTITIES_ANSWER:\n        raise SshAgentFailure('agent: non-identities answer received for identities list')\n    return KeyList.from_blob(r[1:])", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_access.py", "class_name": "AnsibleAccessContext", "function_name": "current", "parameters": [], "param_types": {}, "return_type": "AnsibleAccessContext", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAccessContext", "AnsibleAccessContext._contextvar.get", "AnsibleAccessContext._contextvar.set"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Creates or retrieves an `AnsibleAccessContext` for the current logical callstack.", "source_code": "def current() -> AnsibleAccessContext:\n    \"\"\"Creates or retrieves an `AnsibleAccessContext` for the current logical callstack.\"\"\"\n    try:\n        ctx: AnsibleAccessContext = AnsibleAccessContext._contextvar.get()\n    except LookupError:\n        # didn't exist; create it\n        ctx = AnsibleAccessContext()\n        AnsibleAccessContext._contextvar.set(ctx)  # we ignore the token, since this should live for the life of the thread/async ctx\n\n    return ctx", "loc": 10}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_access.py", "class_name": "AnsibleAccessContext", "function_name": "access", "parameters": ["self", "value"], "param_types": {"value": "t.Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag_types", "ctx._notify", "ctx._type_interest.intersection", "frozenset", "masked.add", "reversed", "set", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Notify all contexts which have registered interest in the given value that it is being accessed.", "source_code": "def access(self, value: t.Any) -> None:\n    \"\"\"Notify all contexts which have registered interest in the given value that it is being accessed.\"\"\"\n    if not self._notify_contexts:\n        return\n\n    value_types = AnsibleTagHelper.tag_types(value) | frozenset((type(value),))\n    masked: set[type] = set()\n\n    for ctx in reversed(self._notify_contexts):\n        if ctx._mask:\n            if (ctx_type := type(ctx)) in masked:\n                continue\n\n            masked.add(ctx_type)\n\n        # noinspection PyProtectedMember\n        if ctx._type_interest.intersection(value_types):\n            ctx._notify(value)", "loc": 18}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_chain_templar.py", "class_name": "ChainTemplar", "function_name": "get", "parameters": ["self", "key"], "param_types": {"key": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "KeyError", "self.template"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Get the value for the given key, templating the result before returning it.", "source_code": "def get(self, key: t.Any) -> t.Any:\n    \"\"\"Get the value for the given key, templating the result before returning it.\"\"\"\n    for source in self.sources:\n        if key not in source:\n            continue\n\n        value = source[key]\n\n        try:\n            return self.template(key, value)\n        except AnsibleValueOmittedError:\n            break  # omit == obliterate - matches historical behavior where dict layers were squashed before templating was applied\n        except Exception as ex:\n            raise AnsibleError(f'Error while resolving value for {key!r}.', obj=value) from ex\n\n    raise KeyError(key)", "loc": 16}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_chain_templar.py", "class_name": "ChainTemplar", "function_name": "keys", "parameters": ["self"], "param_types": {}, "return_type": "t.Iterable[t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["itertools.chain.from_iterable", "set", "sorted"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def keys(self) -> t.Iterable[t.Any]:\n    \"\"\"\n    Returns a sorted iterable of all keys present in all source layers, without templating associated values.\n    Values that resolve to `omit` are thus included.\n    \"\"\"\n    return sorted(set(itertools.chain.from_iterable(self.sources)))", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_chain_templar.py", "class_name": "ChainTemplar", "function_name": "items", "parameters": ["self"], "param_types": {}, "return_type": "t.Iterable[t.Tuple[t.Any, t.Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get", "self.keys"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def items(self) -> t.Iterable[t.Tuple[t.Any, t.Any]]:\n    \"\"\"\n    Returns a sorted iterable of (key, templated value) tuples.\n    Any tuple where the templated value resolves to `omit` will not be included in the result.\n    \"\"\"\n    for key in self.keys():\n        try:\n            yield key, self.get(key)\n        except KeyError:\n            pass", "loc": 10}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_datatag.py", "class_name": "DeprecatedAccessAuditContext", "function_name": "when", "parameters": [], "param_types": {}, "return_type": "t.Self | _contextlib.nullcontext", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_contextlib.nullcontext", "cls"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def when(cls, condition: bool, /) -> t.Self | _contextlib.nullcontext:\n    \"\"\"Returns a new instance if `condition` is True (usually `TemplateContext.is_top_level`), otherwise a `nullcontext` instance.\"\"\"\n    if condition:\n        return cls()\n\n    return _contextlib.nullcontext()", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "extend", "parameters": ["self", "marker_behavior"], "param_types": {"marker_behavior": "MarkerBehavior | None"}, "return_type": "t.Self", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extend(self, marker_behavior: MarkerBehavior | None = None) -> t.Self:\n    new_templar = type(self)(\n        loader=self._loader,\n        variables=self._variables,\n        variables_factory=self._variables_factory,\n        marker_behavior=marker_behavior or self._marker_behavior,\n    )\n\n    if self._environment:\n        new_templar._environment = self._environment\n\n    return new_templar", "loc": 12}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "environment", "parameters": ["self"], "param_types": {}, "return_type": "AnsibleEnvironment", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleEnvironment"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def environment(self) -> AnsibleEnvironment:\n    if not self._environment:\n        self._environment = AnsibleEnvironment(ansible_basedir=self.basedir)\n\n    return self._environment", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "available_variables", "parameters": ["self"], "param_types": {}, "return_type": "dict[str, t.Any] | ChainMap[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._variables_factory"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Available variables this instance will use when templating.", "source_code": "def available_variables(self) -> dict[str, t.Any] | ChainMap[str, t.Any]:\n    \"\"\"Available variables this instance will use when templating.\"\"\"\n    # DTFIX3: ensure that we're always accessing this as a shallow container-level snapshot, and eliminate uses of anything\n    #  that directly mutates this value. _new_context may resolve this for us?\n    if self._variables is None:\n        self._variables = self._variables_factory() if self._variables_factory else {}\n\n    return self._variables", "loc": 8}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "resolve_variable_expression", "parameters": ["self", "expression"], "param_types": {"expression": "str"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleError", "TrustedAsTemplate", "TrustedAsTemplate().tag", "re.fullmatch", "re.split", "self.evaluate_expression", "validate_variable_name"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Resolve a potentially untrusted string variable expression consisting only of valid identifiers, integers, dots, and indexing containing these. Optional local variables may be provided, which can only be referenced directly by the given expression. Valid: x, x.y, x[y].z, x[1], 1, x[y.z]", "source_code": "def resolve_variable_expression(\n    self,\n    expression: str,\n    *,\n    local_variables: dict[str, t.Any] | None = None,\n) -> t.Any:\n    \"\"\"\n    Resolve a potentially untrusted string variable expression consisting only of valid identifiers, integers, dots, and indexing containing these.\n    Optional local variables may be provided, which can only be referenced directly by the given expression.\n    Valid: x, x.y, x[y].z, x[1], 1, x[y.z]\n    Error: 'x', x['y'], q('env')\n    \"\"\"\n    components = re.split(r'[.\\[\\]]', expression)\n\n    try:\n        for component in components:\n            if re.fullmatch('[0-9]*', component):\n                continue  # allow empty strings and integers\n\n            validate_variable_name(component)\n    except Exception as ex:\n        raise AnsibleError(f'Invalid variable expression: {expression}', obj=expression) from ex\n\n    return self.evaluate_expression(TrustedAsTemplate().tag(expression), local_variables=local_variables)", "loc": 24}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "template", "parameters": ["self", "variable"], "param_types": {"variable": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTemplateTransformLimitError", "DeprecatedAccessAuditContext.when", "JinjaCallContext", "TemplateContext", "TemplateContext.current", "TemplateEncountered", "_AnsibleLazyTemplateMixin._try_create", "_type_transform_mapping.get", "compiled_template", "defer_template_error", "is_possibly_template", "isinstance", "range", "self._compile_template", "self._finalize_top_level_template_result", "self._post_render_mutation", "self._trust_check", "transform", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Templates (possibly recursively) any given data as input.", "source_code": "def template(\n    self,\n    variable: t.Any,  # DTFIX-FUTURE: once we settle the new/old API boundaries, rename this (here and in other methods)\n    *,\n    options: TemplateOptions = TemplateOptions.DEFAULT,\n    mode: TemplateMode = TemplateMode.DEFAULT,\n    lazy_options: LazyOptions = LazyOptions.DEFAULT,\n) -> t.Any:\n    \"\"\"Templates (possibly recursively) any given data as input.\"\"\"\n    original_variable = variable\n\n    for _attempt in range(TRANSFORM_CHAIN_LIMIT):\n        if variable is None or (value_type := type(variable)) in IGNORE_SCALAR_VAR_TYPES:\n            return variable  # quickly ignore supported scalar types which are not be templated\n\n        value_is_str = isinstance(variable, str)\n\n        if template_ctx := TemplateContext.current(optional=True):\n            stop_on_template = template_ctx.stop_on_template\n        else:\n            stop_on_template = False\n\n        if mode is TemplateMode.STOP_ON_TEMPLATE:\n            stop_on_template = True\n\n        with (\n            TemplateContext(template_value=variable, templar=self, options=options, stop_on_template=stop_on_template) as ctx,\n            DeprecatedAccessAuditContext.when(ctx.is_top_level),\n            JinjaCallContext(accept_lazy_markers=True),  # let default Jinja marker behavior apply, since we're descending into a new template\n        ):\n            try:\n                if not value_is_str:\n                    # transforms are currently limited to non-str types as an optimization\n                    if (transform := _type_transform_mapping.get(value_type)) and value_type.__name__ not in lazy_options.unmask_type_names:\n                        variable = transform(variable)\n                        continue\n\n                    template_result = _AnsibleLazyTemplateMixin._try_create(variable, lazy_options)\n                elif not lazy_options.template:\n                    template_result = variable\n                elif not is_possibly_template(variable, options.overrides):\n                    template_result = variable\n                elif not self._trust_check(variable, skip_handler=stop_on_template):\n                    template_result = variable\n                elif stop_on_template:\n                    raise TemplateEncountered()\n                else:\n                    compiled_template = self._compile_template(variable, options)\n\n                    template_result = compiled_template(self.available_variables)\n                    template_result = self._post_render_mutation(variable, template_result, options)\n            except TemplateEncountered:\n                raise\n            except Exception as ex:\n                template_result = defer_template_error(ex, variable, is_expression=False)\n\n            if ctx.is_top_level or mode is TemplateMode.ALWAYS_FINALIZE:\n                template_result = self._finalize_top_level_template_result(\n                    variable, options, template_result, stop_on_container=mode is TemplateMode.STOP_ON_CONTAINER\n                )\n\n        return template_result\n\n    raise AnsibleTemplateTransformLimitError(obj=original_variable)", "loc": 64}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "is_template", "parameters": ["self", "data", "overrides"], "param_types": {"data": "t.Any", "overrides": "TemplateOverrides"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateOptions", "self.template"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Evaluate the input data to determine if it contains a template, even if that template is invalid. Containers will be recursively searched. Objects subject to template-time transforms that do not yield a template are not considered templates by this method. Gating a conditional call to `template` with this method is redundant and inefficient -- request templating unconditionally instead.", "source_code": "def is_template(self, data: t.Any, overrides: TemplateOverrides = TemplateOverrides.DEFAULT) -> bool:\n    \"\"\"\n    Evaluate the input data to determine if it contains a template, even if that template is invalid. Containers will be recursively searched.\n    Objects subject to template-time transforms that do not yield a template are not considered templates by this method.\n    Gating a conditional call to `template` with this method is redundant and inefficient -- request templating unconditionally instead.\n    \"\"\"\n    options = TemplateOptions(overrides=overrides) if overrides is not TemplateOverrides.DEFAULT else TemplateOptions.DEFAULT\n\n    try:\n        self.template(data, options=options, mode=TemplateMode.STOP_ON_TEMPLATE)\n    except TemplateEncountered:\n        return True\n    else:\n        return False", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "evaluate_expression", "parameters": ["self", "expression"], "param_types": {"expression": "str"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ChainMap", "DeprecatedAccessAuditContext.when", "TemplateContext", "TemplateOptions", "TemplateTrustCheckFailedError", "TrustedAsTemplate.is_tagged_on", "TypeError", "compiled_template", "defer_template_error", "isinstance", "self._compile_expression", "self._finalize_top_level_template_result", "self._post_render_mutation", "type"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Evaluate a trusted string expression and return its result. Optional local variables may be provided, which can only be referenced directly by the given expression.", "source_code": "def evaluate_expression(\n    self,\n    expression: str,\n    *,\n    local_variables: dict[str, t.Any] | None = None,\n    escape_backslashes: bool = True,\n    _render_jinja_const_template: bool = False,\n) -> t.Any:\n    \"\"\"\n    Evaluate a trusted string expression and return its result.\n    Optional local variables may be provided, which can only be referenced directly by the given expression.\n    \"\"\"\n    if not isinstance(expression, str):\n        raise TypeError(f\"Expressions must be {str!r}, got {type(expression)!r}.\")\n\n    options = TemplateOptions(escape_backslashes=escape_backslashes, preserve_trailing_newlines=False)\n\n    with (\n        TemplateContext(template_value=expression, templar=self, options=options, _render_jinja_const_template=_render_jinja_const_template) as ctx,\n        DeprecatedAccessAuditContext.when(ctx.is_top_level),\n    ):\n        try:\n            if not TrustedAsTemplate.is_tagged_on(expression):\n                raise TemplateTrustCheckFailedError(obj=expression)\n\n            template_variables = ChainMap(local_variables, self.available_variables) if local_variables else self.available_variables\n            compiled_template = self._compile_expression(expression, options)\n\n            template_result = compiled_template(template_variables)\n            template_result = self._post_render_mutation(expression, template_result, options)\n        except Exception as ex:\n            template_result = defer_template_error(ex, expression, is_expression=True)\n\n        return self._finalize_top_level_template_result(expression, options, template_result, is_expression=True)", "loc": 34}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_engine.py", "class_name": "TemplateEngine", "function_name": "evaluate_conditional", "parameters": ["self", "conditional"], "param_types": {"conditional": "str | bool"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleBrokenConditionalError", "Origin.get_tag", "_display.deprecated", "bool", "isinstance", "native_type_name", "self._normalize_and_evaluate_conditional", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Evaluate a trusted string expression or boolean and return its boolean result. A non-boolean result will raise `AnsibleBrokenConditionalError`. The ALLOW_BROKEN_CONDITIONALS configuration option can temporarily relax this requirement, allowing truthy conditionals to succeed.", "source_code": "def evaluate_conditional(self, conditional: str | bool) -> bool:\n    \"\"\"\n    Evaluate a trusted string expression or boolean and return its boolean result. A non-boolean result will raise `AnsibleBrokenConditionalError`.\n    The ALLOW_BROKEN_CONDITIONALS configuration option can temporarily relax this requirement, allowing truthy conditionals to succeed.\n    \"\"\"\n    result = self._normalize_and_evaluate_conditional(conditional)\n\n    if isinstance(result, bool):\n        return result\n\n    bool_result = bool(result)\n\n    result_origin = Origin.get_tag(result) or Origin.UNKNOWN\n\n    msg = (\n        f'Conditional result ({bool_result}) was derived from value of type {native_type_name(result)!r} at {str(result_origin)!r}. '\n        'Conditionals must have a boolean result.'\n    )\n\n    if _TemplateConfig.allow_broken_conditionals:\n        _display.deprecated(\n            msg=msg,\n            obj=conditional,\n            help_text=self._BROKEN_CONDITIONAL_ALLOWED_FRAGMENT,\n            version='2.23',\n        )\n\n        return bool_result\n\n    raise AnsibleBrokenConditionalError(msg, obj=conditional)", "loc": 30}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": null, "function_name": "defer_template_error", "parameters": ["ex", "variable"], "param_types": {"ex": "Exception", "variable": "t.Any"}, "return_type": "Marker", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AssertionError", "CapturedExceptionMarker", "create_template_error", "isinstance"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def defer_template_error(ex: Exception, variable: t.Any, *, is_expression: bool) -> Marker:\n    if not ex.__traceback__:\n        raise AssertionError('ex must be a previously raised exception')\n\n    if isinstance(ex, MarkerError):\n        return ex.source\n\n    exception_to_raise = create_template_error(ex, variable, is_expression)\n\n    if exception_to_raise is ex:\n        return CapturedExceptionMarker(ex)  # capture the previously raised exception\n\n    try:\n        raise exception_to_raise from ex  # raise the newly synthesized exception before capturing it\n    except Exception as captured_ex:\n        return CapturedExceptionMarker(captured_ex)", "loc": 16}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": null, "function_name": "create_template_error", "parameters": ["ex", "variable", "is_expression"], "param_types": {"ex": "Exception", "variable": "t.Any", "is_expression": "bool"}, "return_type": "AnsibleTemplateError", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TemplateContext.current", "ex_type", "is_possibly_template", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_template_error(ex: Exception, variable: t.Any, is_expression: bool) -> AnsibleTemplateError:\n    if isinstance(ex, AnsibleTemplateError):\n        exception_to_raise = ex\n    else:\n        kind = \"expression\" if is_expression else \"template\"\n        ex_type = AnsibleTemplateError  # always raise an AnsibleTemplateError/subclass\n\n        if isinstance(ex, RecursionError):\n            msg = f\"Recursive loop detected in {kind}.\"\n        elif isinstance(ex, TemplateSyntaxError):\n            msg = f\"Syntax error in {kind}.\"\n\n            if is_expression and is_possibly_template(variable):\n                msg += \" Template delimiters are not supported in expressions.\"\n\n            ex_type = AnsibleTemplateSyntaxError\n        else:\n            msg = f\"Error rendering {kind}.\"\n\n        exception_to_raise = ex_type(msg, obj=variable)\n\n    if exception_to_raise.obj is None:\n        exception_to_raise.obj = TemplateContext.current().template_value\n\n    # DTFIX-FUTURE: Look through the TemplateContext hierarchy to find the most recent non-template\n    #   caller and use that for origin when no origin is available on obj. This could be useful for situations where the template\n    #   was embedded in a plugin, or a plugin is otherwise responsible for losing the origin and/or trust. We can't just use the first\n    #   non-template caller as that will lead to false positives for re-entrant calls (e.g. template plugins that call into templar).\n\n    return exception_to_raise", "loc": 30}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": null, "function_name": "is_possibly_template", "parameters": ["value", "overrides"], "param_types": {"value": "str", "overrides": "TemplateOverrides"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["overrides._contains_start_string", "value.startswith"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "A lightweight check to determine if the given string looks like it contains a template, even if that template is invalid.", "source_code": "def is_possibly_template(value: str, overrides: TemplateOverrides = TemplateOverrides.DEFAULT):\n    \"\"\"\n    A lightweight check to determine if the given string looks like it contains a template, even if that template is invalid.\n    Returns `True` if the given string starts with a Jinja overrides header or if it contains template start strings.\n    \"\"\"\n    return value.startswith(JINJA2_OVERRIDE) or overrides._contains_start_string(value)", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": null, "function_name": "is_possibly_all_template", "parameters": ["value", "overrides"], "param_types": {"value": "str", "overrides": "TemplateOverrides"}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["overrides._starts_and_ends_with_jinja_delimiters", "value.startswith"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "A lightweight check to determine if the given string looks like it contains *only* a template, even if that template is invalid.", "source_code": "def is_possibly_all_template(value: str, overrides: TemplateOverrides = TemplateOverrides.DEFAULT):\n    \"\"\"\n    A lightweight check to determine if the given string looks like it contains *only* a template, even if that template is invalid.\n    Returns `True` if the given string starts with a Jinja overrides header or if it starts and ends with Jinja template delimiters.\n    \"\"\"\n    return value.startswith(JINJA2_OVERRIDE) or overrides._starts_and_ends_with_jinja_delimiters(value)", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "TemplateOverrides", "function_name": "overlay_kwargs", "parameters": ["self"], "param_types": {}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dataclasses.fields", "getattr"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a dictionary of arguments for passing to Environment.overlay. The dictionary will be empty if all fields have their default value.", "source_code": "def overlay_kwargs(self) -> dict[str, t.Any]:\n    \"\"\"\n    Return a dictionary of arguments for passing to Environment.overlay.\n    The dictionary will be empty if all fields have their default value.\n    \"\"\"\n    # DTFIX-FUTURE: calculate default/non-default during __post_init__\n    fields = [(field, getattr(self, field.name)) for field in dataclasses.fields(self)]\n    kwargs = {field.name: value for field, value in fields if value != field.default}\n\n    return kwargs", "loc": 10}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "TemplateOverrides", "function_name": "merge", "parameters": [], "param_types": {}, "return_type": "TemplateOverrides", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dataclasses.asdict", "self.from_kwargs"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a new instance based on the current instance with the given kwargs overridden.", "source_code": "def merge(self, kwargs: dict[str, t.Any] | None, /) -> TemplateOverrides:\n    \"\"\"Return a new instance based on the current instance with the given kwargs overridden.\"\"\"\n    if kwargs:\n        return self.from_kwargs(dataclasses.asdict(self) | kwargs)\n\n    return self", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "TemplateOverrides", "function_name": "from_kwargs", "parameters": [], "param_types": {}, "return_type": "TemplateOverrides", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "value.overlay_kwargs"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "TemplateOverrides instance factory; instances resolving to all default values will instead return the DEFAULT singleton for optimization.", "source_code": "def from_kwargs(cls, kwargs: dict[str, t.Any] | None, /) -> TemplateOverrides:\n    \"\"\"TemplateOverrides instance factory; instances resolving to all default values will instead return the DEFAULT singleton for optimization.\"\"\"\n    if kwargs:\n        value = cls(**kwargs)\n\n        if value.overlay_kwargs():\n            return value\n\n    return cls.DEFAULT", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleContext", "function_name": "get_all", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ChainMap", "layers.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Override Jinja's default get_all to return all vars in the context as a ChainMap with a mutable layer at the bottom. This provides some isolation against accidental changes to inherited variable contexts without requiring copies.", "source_code": "def get_all(self):\n    \"\"\"\n    Override Jinja's default get_all to return all vars in the context as a ChainMap with a mutable layer at the bottom.\n    This provides some isolation against accidental changes to inherited variable contexts without requiring copies.\n    \"\"\"\n    layers = []\n\n    if self.vars:\n        layers.append(self.vars)\n    if self.parent:\n        layers.append(self.parent)\n\n    # HACK: always include a sacrificial plain-dict on the bottom layer, since Jinja's debug and stacktrace rewrite code invokes\n    # `__setitem__` outside a call context; this will ensure that it always occurs on a plain dict instead of a lazy one.\n    return ChainMap({}, *layers)", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleContext", "function_name": "derived", "parameters": ["self", "locals"], "param_types": {"locals": "t.Optional[t.Dict[str, t.Any]]"}, "return_type": "Context", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_new_context", "context.blocks.update", "list", "self.blocks.items", "self.get_all"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def derived(self, locals: t.Optional[t.Dict[str, t.Any]] = None) -> Context:\n    # this is a clone of Jinja's impl of derived, but using our lazy-aware _new_context\n\n    context = _new_context(\n        environment=self.environment,\n        template_name=self.name,\n        blocks={},\n        shared=True,\n        jinja_locals=locals,\n        jinja_vars=self.get_all(),\n    )\n    context.eval_ctx = self.eval_ctx\n    context.blocks.update((k, list(v)) for k, v in self.blocks.items())\n    return context", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "ArgSmuggler", "function_name": "extract_jinja_vars", "parameters": ["cls", "maybe_smuggled_vars"], "param_types": {"maybe_smuggled_vars": "c.Mapping[str, t.Any] | None"}, "return_type": "c.Mapping[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "maybe_smuggled_vars.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "If the supplied vars dict contains an ArgSmuggler instance with the expected key, unwrap it and return the smuggled value. Otherwise, return the supplied dict as-is.", "source_code": "def extract_jinja_vars(cls, maybe_smuggled_vars: c.Mapping[str, t.Any] | None) -> c.Mapping[str, t.Any]:\n    \"\"\"\n    If the supplied vars dict contains an ArgSmuggler instance with the expected key, unwrap it and return the smuggled value.\n    Otherwise, return the supplied dict as-is.\n    \"\"\"\n    if maybe_smuggled_vars and ((smuggler := maybe_smuggled_vars.get('_smuggled_vars')) and isinstance(smuggler, ArgSmuggler)):\n        return smuggler.jinja_vars\n\n    return maybe_smuggled_vars", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleTemplate", "function_name": "new_context", "parameters": ["self", "vars", "shared", "locals"], "param_types": {"vars": "c.Mapping[str, t.Any] | None", "shared": "bool", "locals": "c.Mapping[str, t.Any] | None"}, "return_type": "Context", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ArgSmuggler.extract_jinja_vars", "_new_context"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def new_context(\n    self,\n    vars: c.Mapping[str, t.Any] | None = None,\n    shared: bool = False,\n    locals: c.Mapping[str, t.Any] | None = None,\n) -> Context:\n    return _new_context(\n        environment=self.environment,\n        template_name=self.name,\n        blocks=self.blocks,\n        shared=shared,\n        jinja_locals=locals,\n        jinja_vars=ArgSmuggler.extract_jinja_vars(vars),\n        jinja_globals=self.globals,\n    )", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleCodeGenerator", "function_name": "visit_Const", "parameters": ["self", "node", "frame"], "param_types": {"node": "Const", "frame": "Frame"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_possibly_template", "node.as_const", "repr", "self.write", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Override Jinja's visit_Const to inject a runtime call to AnsibleEnvironment._access_const for constant strings that are possibly templates, which may require special handling at runtime. See that method for details. An example that hits this path: {{ lookup(\"file\", \"{{ output_dir }}/bla\") }}", "source_code": "def visit_Const(self, node: Const, frame: Frame) -> None:\n    \"\"\"\n    Override Jinja's visit_Const to inject a runtime call to AnsibleEnvironment._access_const for constant strings that are possibly templates, which\n    may require special handling at runtime. See that method for details. An example that hits this path:\n    {{ lookup(\"file\", \"{{ output_dir }}/bla\") }}\n    \"\"\"\n    value = node.as_const(frame.eval_ctx)\n\n    if _TemplateConfig.allow_embedded_templates and type(value) is str and is_possibly_template(value):  # pylint: disable=unidiomatic-typecheck\n        # deprecated: description='embedded Jinja constant string template support' core_version='2.23'\n        self.write(f'environment._access_const({value!r})')\n    else:\n        # NB: This is actually more efficient than Jinja's visit_Const, which contains obsolete (as of Py2.7/3.1) float conversion instance checks. Before\n        #     removing this override entirely, ensure that upstream Jinja has removed the obsolete code.\n        #     See https://docs.python.org/release/2.7/whatsnew/2.7.html#python-3-1-features for more details.\n        self.write(repr(value))", "loc": 16}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleLexer", "function_name": "tokeniter", "parameters": ["self"], "param_types": {}, "return_type": "t.Iterator[t.Tuple[int, str, str]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_TemplateCompileContext.current", "super", "super().tokeniter", "token[2].replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Pre-escape backslashes in expression ({{ }}) raw string constants before Jinja's Lexer.wrap() can interpret them as ASCII escape sequences.", "source_code": "def tokeniter(self, *args, **kwargs) -> t.Iterator[t.Tuple[int, str, str]]:\n    \"\"\"Pre-escape backslashes in expression ({{ }}) raw string constants before Jinja's Lexer.wrap() can interpret them as ASCII escape sequences.\"\"\"\n    token_stream = super().tokeniter(*args, **kwargs)\n\n    # if we have no context, Jinja's doing a nested compile at runtime (eg, import/include); historically, no backslash escaping is performed\n    if not (tcc := _TemplateCompileContext.current(optional=True)) or not tcc.escape_backslashes:\n        yield from token_stream\n        return\n\n    in_variable = False\n\n    for token in token_stream:\n        token_type = token[1]\n\n        if token_type == TOKEN_VARIABLE_BEGIN:\n            in_variable = True\n        elif token_type == TOKEN_VARIABLE_END:\n            in_variable = False\n        elif in_variable and token_type == TOKEN_STRING:\n            token = token[0], token_type, token[2].replace('\\\\', '\\\\\\\\')\n\n        yield token", "loc": 22}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "get_template", "parameters": ["self", "name", "parent", "globals"], "param_types": {"name": "str | Template", "parent": "str | None", "globals": "c.MutableMapping[str, t.Any] | None"}, "return_type": "Template", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CompileStateSmugglingCtx.when", "isinstance", "super", "super().get_template", "t.cast"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Ensures that templates built via `get_template` are also source debuggable.", "source_code": "def get_template(\n    self,\n    name: str | Template,\n    parent: str | None = None,\n    globals: c.MutableMapping[str, t.Any] | None = None,\n) -> Template:\n    \"\"\"Ensures that templates built via `get_template` are also source debuggable.\"\"\"\n    with _CompileStateSmugglingCtx.when(self._debuggable_template_source) as ctx:\n        template_obj = t.cast(AnsibleTemplate, super().get_template(name, parent, globals))\n\n        if isinstance(ctx, _CompileStateSmugglingCtx):  # only present if debugging is enabled\n            template_obj._python_source_temp_path = ctx.python_source_temp_path  # facilitate deletion of the temp file when template_obj is deleted\n\n        return template_obj", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "is_safe_attribute", "parameters": ["self", "obj", "attr", "value"], "param_types": {"obj": "t.Any", "attr": "str", "value": "t.Any"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._allowed_unsafe_attributes.get", "super", "super().is_safe_attribute"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_safe_attribute(self, obj: t.Any, attr: str, value: t.Any) -> bool:\n    # deprecated: description=\"remove relaxed template sandbox mode support\" core_version=\"2.23\"\n    if _TemplateConfig.sandbox_mode == _SandboxMode.ALLOW_UNSAFE_ATTRIBUTES:\n        return True\n\n    if (type_or_tuple := self._allowed_unsafe_attributes.get(attr)) and isinstance(obj, type_or_tuple):\n        return True\n\n    return super().is_safe_attribute(obj, attr, value)", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "lexer", "parameters": ["self"], "param_types": {}, "return_type": "AnsibleLexer", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleLexer", "getattr", "self._lexer_cache.get", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return/cache an AnsibleLexer with settings from the current AnsibleEnvironment", "source_code": "def lexer(self) -> AnsibleLexer:\n    \"\"\"Return/cache an AnsibleLexer with settings from the current AnsibleEnvironment\"\"\"\n    # DTFIX-FUTURE: optimization - we should pre-generate the default cached lexer before forking, not leave it to chance (e.g. simple playbooks)\n    key = tuple(getattr(self, name) for name in _TEMPLATE_OVERRIDE_FIELD_NAMES)\n\n    lex = self._lexer_cache.get(key)\n\n    if lex is None:\n        self._lexer_cache[key] = lex = AnsibleLexer(self)\n\n    return lex", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "call_filter", "parameters": ["self", "name", "value", "args", "kwargs", "context", "eval_ctx"], "param_types": {"name": "str", "value": "t.Any", "args": "c.Sequence[t.Any] | None", "kwargs": "c.Mapping[str, t.Any] | None", "context": "Context | None", "eval_ctx": "EvalContext | None"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_AnsibleLazyTemplateMixin._try_create", "list", "super", "super().call_filter"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Ensure that filters directly invoked by plugins will see non-templating lazy containers. Without this, `_wrap_filter` will wrap `args` and `kwargs` in templating lazy containers. This provides consistency with plugin output handling by preventing auto-templating of trusted templates passed in native containers.", "source_code": "def call_filter(\n    self,\n    name: str,\n    value: t.Any,\n    args: c.Sequence[t.Any] | None = None,\n    kwargs: c.Mapping[str, t.Any] | None = None,\n    context: Context | None = None,\n    eval_ctx: EvalContext | None = None,\n) -> t.Any:\n    \"\"\"\n    Ensure that filters directly invoked by plugins will see non-templating lazy containers.\n    Without this, `_wrap_filter` will wrap `args` and `kwargs` in templating lazy containers.\n    This provides consistency with plugin output handling by preventing auto-templating of trusted templates passed in native containers.\n    \"\"\"\n    # DTFIX-FUTURE: need better logic to handle non-list/non-dict inputs for args/kwargs\n    args = _AnsibleLazyTemplateMixin._try_create(list(args or []), LazyOptions.SKIP_TEMPLATES)\n    kwargs = _AnsibleLazyTemplateMixin._try_create(kwargs, LazyOptions.SKIP_TEMPLATES)\n\n    return super().call_filter(name, value, args, kwargs, context, eval_ctx)", "loc": 19}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "call_test", "parameters": ["self", "name", "value", "args", "kwargs", "context", "eval_ctx"], "param_types": {"name": "str", "value": "t.Any", "args": "c.Sequence[t.Any] | None", "kwargs": "c.Mapping[str, t.Any] | None", "context": "Context | None", "eval_ctx": "EvalContext | None"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_AnsibleLazyTemplateMixin._try_create", "list", "super", "super().call_test"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Ensure that tests directly invoked by plugins will see non-templating lazy containers. Without this, `_wrap_test` will wrap `args` and `kwargs` in templating lazy containers. This provides consistency with plugin output handling by preventing auto-templating of trusted templates passed in native containers.", "source_code": "def call_test(\n    self,\n    name: str,\n    value: t.Any,\n    args: c.Sequence[t.Any] | None = None,\n    kwargs: c.Mapping[str, t.Any] | None = None,\n    context: Context | None = None,\n    eval_ctx: EvalContext | None = None,\n) -> t.Any:\n    \"\"\"\n    Ensure that tests directly invoked by plugins will see non-templating lazy containers.\n    Without this, `_wrap_test` will wrap `args` and `kwargs` in templating lazy containers.\n    This provides consistency with plugin output handling by preventing auto-templating of trusted templates passed in native containers.\n    \"\"\"\n    # DTFIX-FUTURE: need better logic to handle non-list/non-dict inputs for args/kwargs\n    args = _AnsibleLazyTemplateMixin._try_create(list(args or []), LazyOptions.SKIP_TEMPLATES)\n    kwargs = _AnsibleLazyTemplateMixin._try_create(kwargs, LazyOptions.SKIP_TEMPLATES)\n\n    return super().call_test(name, value, args, kwargs, context, eval_ctx)", "loc": 19}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "compile_expression", "parameters": ["self", "source"], "param_types": {"source": "str"}, "return_type": "TemplateExpression", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CompileStateSmugglingCtx.when", "isinstance", "super", "super().compile_expression"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compile_expression(self, source: str, *args, **kwargs) -> TemplateExpression:\n    # compile_expression parses and passes the tree to from_string; for debug support, activate the context here to capture the intermediate results\n    with _CompileStateSmugglingCtx.when(self._debuggable_template_source) as ctx:\n        if isinstance(ctx, _CompileStateSmugglingCtx):  # only present if debugging is enabled\n            ctx.template_source = source\n\n        return super().compile_expression(source, *args, **kwargs)", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "from_string", "parameters": ["self", "source"], "param_types": {"source": "str | jinja2.nodes.Template"}, "return_type": "AnsibleTemplate", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CompileStateSmugglingCtx.current", "_CompileStateSmugglingCtx.when", "isinstance", "super", "super().from_string", "t.cast"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_string(self, source: str | jinja2.nodes.Template, *args, **kwargs) -> AnsibleTemplate:\n    # if debugging is enabled, use existing context when present (e.g., from compile_expression)\n    current_ctx = _CompileStateSmugglingCtx.current(optional=True) if self._debuggable_template_source else None\n\n    with _CompileStateSmugglingCtx.when(self._debuggable_template_source and not current_ctx) as new_ctx:\n        template_obj = t.cast(AnsibleTemplate, super().from_string(source, *args, **kwargs))\n\n        if isinstance(ctx := current_ctx or new_ctx, _CompileStateSmugglingCtx):  # only present if debugging is enabled\n            template_obj._python_source_temp_path = ctx.python_source_temp_path  # facilitate deletion of the temp file when template_obj is deleted\n\n    return template_obj", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "concat", "parameters": ["nodes"], "param_types": {"nodes": "t.Iterable[t.Any]"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "_finalize_template_result", "_flatten_nodes", "len", "list", "to_text"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def concat(nodes: t.Iterable[t.Any]) -> t.Any:  # type: ignore[override]\n    node_list = list(_flatten_nodes(nodes))\n\n    if not node_list:\n        return None\n\n    # this code is complemented by our tweaked CodeGenerator _output_const_repr that ensures that literal constants\n    # in templates aren't double-repr'd in the generated code\n    if len(node_list) == 1:\n        return node_list[0]\n\n    # In order to ensure that all markers are tripped, do a recursive finalize before we repr (otherwise we can end up\n    # repr'ing a Marker). This requires two passes, but avoids the need for a parallel reimplementation of all repr methods.\n    try:\n        node_list = _finalize_template_result(node_list, FinalizeMode.CONCAT)\n    except MarkerError as ex:\n        return ex.source  # return the first Marker encountered\n\n    return ''.join([to_text(v) for v in node_list])", "loc": 19}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "getitem", "parameters": ["self", "obj", "argument"], "param_types": {"obj": "t.Any", "argument": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAccessContext.current", "AnsibleAccessContext.current().access", "super", "super().getitem"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getitem(self, obj: t.Any, argument: t.Any) -> t.Any:\n    value = super().getitem(obj, argument)\n\n    AnsibleAccessContext.current().access(value)\n\n    return value", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "getattr", "parameters": ["self", "obj", "attribute"], "param_types": {"obj": "t.Any", "attribute": "str"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleAccessContext.current", "AnsibleAccessContext.current().access", "getattr", "self.is_safe_attribute", "self.undefined", "self.unsafe_undefined"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Get `attribute` from the attributes of `obj`, falling back to items in `obj`. If no item was found, return a sandbox-specific `UndefinedMarker` if `attribute` is protected by the sandbox, otherwise return a normal `UndefinedMarker` instance.", "source_code": "def getattr(self, obj: t.Any, attribute: str) -> t.Any:\n    \"\"\"\n    Get `attribute` from the attributes of `obj`, falling back to items in `obj`.\n    If no item was found, return a sandbox-specific `UndefinedMarker` if `attribute` is protected by the sandbox,\n    otherwise return a normal `UndefinedMarker` instance.\n    This differs from the built-in Jinja behavior which will not fall back to items if `attribute` is protected by the sandbox.\n    \"\"\"\n    # example template that uses this: \"{{ some.thing }}\" -- obj is the \"some\" dict, attribute is \"thing\"\n\n    is_safe = True\n\n    try:\n        value = getattr(obj, attribute)\n    except AttributeError:\n        value = _sentinel\n    else:\n        if not (is_safe := self.is_safe_attribute(obj, attribute, value)):\n            value = _sentinel\n\n    if value is _sentinel:\n        try:\n            value = obj[attribute]\n        except (TypeError, LookupError):\n            value = self.undefined(obj=obj, name=attribute) if is_safe else self.unsafe_undefined(obj, attribute)\n\n    AnsibleAccessContext.current().access(value)\n\n    return value", "loc": 28}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_bits.py", "class_name": "AnsibleEnvironment", "function_name": "call", "parameters": ["self", "__context", "__obj"], "param_types": {"__context": "Context", "__obj": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CapturedExceptionMarker", "JinjaCallContext", "_DirectCall.is_marked", "_wrap_plugin_output", "get_first_marker_arg", "isinstance", "lazify_container_args", "lazify_container_kwargs", "super", "super().call"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def call(\n    self,\n    __context: Context,\n    __obj: t.Any,\n    *args: t.Any,\n    **kwargs: t.Any,\n) -> t.Any:\n    try:\n        if _DirectCall.is_marked(__obj):\n            # Both `_lookup` and `_query` handle arg proxying and `Marker` args internally.\n            # Performing either before calling them will interfere with that processing.\n            return super().call(__context, __obj, *args, **kwargs)\n\n        # Jinja's generated macro code handles Markers, so preemptive raise on Marker args and lazy retrieval should be disabled for the macro invocation.\n        is_macro = isinstance(__obj, Macro)\n\n        if not is_macro and (first_marker := get_first_marker_arg(args, kwargs)) is not None:\n            return first_marker\n\n        with JinjaCallContext(accept_lazy_markers=is_macro):\n            call_res = super().call(__context, __obj, *lazify_container_args(args), **lazify_container_kwargs(kwargs))\n\n            if __obj is range:\n                # Preserve the ability to do `range(1000000000) | random` by not converting range objects to lists.\n                # Historically, range objects were only converted on Jinja finalize and filter outputs, so they've always been floating around in templating\n                # code and visible to user plugins.\n                return call_res\n\n            return _wrap_plugin_output(call_res)\n\n    except MarkerError as ex:\n        return ex.source\n    except Exception as ex:\n        return CapturedExceptionMarker(ex)", "loc": 34}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_common.py", "class_name": null, "function_name": "iter_marker_args", "parameters": ["args", "kwargs"], "param_types": {"args": "c.Sequence", "kwargs": "dict[str, t.Any]"}, "return_type": "t.Generator[Marker]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "itertools.chain", "kwargs.values"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Utility method to iterate plugin args and yield any `Marker` encountered.", "source_code": "def iter_marker_args(args: c.Sequence, kwargs: dict[str, t.Any]) -> t.Generator[Marker]:\n    \"\"\"Utility method to iterate plugin args and yield any `Marker` encountered.\"\"\"\n    for arg in itertools.chain(args, kwargs.values()):\n        if isinstance(arg, Marker):\n            yield arg", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_jinja_common.py", "class_name": null, "function_name": "validate_arg_type", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' or '.join", "AnsibleTypeError", "isinstance", "native_type_name", "repr", "value.trip"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Validate the type of the given argument while preserving context for Marker values.", "source_code": "def validate_arg_type(name: str, value: t.Any, allowed_type_or_types: type | tuple[type, ...], /) -> None:\n    \"\"\"Validate the type of the given argument while preserving context for Marker values.\"\"\"\n    # DTFIX-FUTURE: find a home for this as a general-purpose utliity method and expose it after some API review\n    if isinstance(value, allowed_type_or_types):\n        return\n\n    if isinstance(allowed_type_or_types, type):\n        arg_type_description = repr(native_type_name(allowed_type_or_types))\n    else:\n        arg_type_description = ' or '.join(repr(native_type_name(item)) for item in allowed_type_or_types)\n\n    if isinstance(value, Marker):\n        try:\n            value.trip()\n        except Exception as ex:\n            raise AnsibleTypeError(f\"The {name!r} argument must be of type {arg_type_description}.\", obj=value) from ex\n\n    raise AnsibleTypeError(f\"The {name!r} argument must be of type {arg_type_description}, not {native_type_name(value)!r}.\", obj=value)", "loc": 18}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_lazy_containers.py", "class_name": "_AnsibleLazyTemplateDict", "function_name": "get", "parameters": ["self", "key", "default"], "param_types": {"key": "t.Any", "default": "t.Any"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._proxy_or_render_lazy_value", "super", "super().get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self, key: t.Any, default: t.Any = None) -> t.Any:\n    if (value := super().get(key, _NoKeySentinel)) is _NoKeySentinel:\n        return default\n\n    return self._proxy_or_render_lazy_value(key, value)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_lazy_containers.py", "class_name": "_AnsibleLazyTemplateDict", "function_name": "setdefault", "parameters": [], "param_types": {}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get", "super", "super().__setitem__"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def setdefault(self, key, default=None, /) -> t.Any:\n    if (value := self.get(key, _NoKeySentinel)) is not _NoKeySentinel:\n        return value\n\n    super().__setitem__(key, default)\n\n    return default", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_lazy_containers.py", "class_name": "_AnsibleLazyTemplateDict", "function_name": "pop", "parameters": [], "param_types": {}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "self._proxy_or_render_lazy_value", "super", "super().get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pop(self, key, default=_NoKeySentinel, /) -> t.Any:\n    if (value := super().get(key, _NoKeySentinel)) is _NoKeySentinel:\n        if default is _NoKeySentinel:\n            raise KeyError(key)\n\n        return default\n\n    value = self._proxy_or_render_lazy_value(_NoKeySentinel, value)\n\n    del self[key]\n\n    return value", "loc": 12}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_lazy_containers.py", "class_name": "_AnsibleLazyTemplateDict", "function_name": "popitem", "parameters": ["self"], "param_types": {}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "next", "reversed", "self._proxy_or_render_lazy_value"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def popitem(self) -> t.Any:\n    try:\n        key = next(reversed(self))\n    except StopIteration:\n        raise KeyError(\"popitem(): dictionary is empty\")\n\n    value = self._proxy_or_render_lazy_value(_NoKeySentinel, self[key])\n\n    del self[key]\n\n    return key, value", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_lazy_containers.py", "class_name": "_AnsibleLazyTemplateList", "function_name": "pop", "parameters": [], "param_types": {}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IndexError", "self._proxy_or_render_lazy_value"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pop(self, idx: t.SupportsIndex = -1, /) -> t.Any:\n    if not self:\n        raise IndexError('pop from empty list')\n\n    try:\n        value = self[idx]\n    except IndexError:\n        raise IndexError('pop index out of range')\n\n    value = self._proxy_or_render_lazy_value(_NoKeySentinel, value)\n\n    del self[idx]\n\n    return value", "loc": 14}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_marker_behaviors.py", "class_name": "ReplacingMarkerBehavior", "function_name": "record_marker", "parameters": ["self", "value"], "param_types": {"value": "Marker"}, "return_type": "t.Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_MarkerTracker", "len", "self._trackers.append"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Assign a sequence number to the given value and record it for later generation of warnings.", "source_code": "def record_marker(self, value: Marker) -> t.Any:\n    \"\"\"Assign a sequence number to the given value and record it for later generation of warnings.\"\"\"\n    number = len(self._trackers) + 1\n\n    self._trackers.append(_MarkerTracker(number=number, value=value))\n\n    return number", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_marker_behaviors.py", "class_name": "ReplacingMarkerBehavior", "function_name": "emit_warnings", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Display", "display.warning", "item.value._as_message", "itertools.groupby", "len", "list"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Emit warning messages caused by Marker values, aggregated by unique template.", "source_code": "def emit_warnings(self) -> None:\n    \"\"\"Emit warning messages caused by Marker values, aggregated by unique template.\"\"\"\n\n    display = Display()\n    grouped_templates = itertools.groupby(self._trackers, key=lambda tracker: tracker.value._marker_template_source)\n\n    for template, items in grouped_templates:\n        item_list = list(items)\n\n        msg = f'Encountered {len(item_list)} template error{\"s\" if len(item_list) > 1 else \"\"}.'\n\n        for item in item_list:\n            msg += f'\\nerror {item.number} - {item.value._as_message()}'\n\n        display.warning(msg=msg, obj=template)", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_marker_behaviors.py", "class_name": "ReplacingMarkerBehavior", "function_name": "warning_context", "parameters": ["cls"], "param_types": {}, "return_type": "t.Generator[t.Self, None, None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "instance.emit_warnings"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Collect warnings for `Marker` values and emit warnings when the context exits.", "source_code": "def warning_context(cls) -> t.Generator[t.Self, None, None]:\n    \"\"\"Collect warnings for `Marker` values and emit warnings when the context exits.\"\"\"\n    instance = cls()\n\n    try:\n        yield instance\n    finally:\n        instance.emit_warnings()", "loc": 8}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_transform.py", "class_name": null, "function_name": "error_summary", "parameters": ["value"], "param_types": {"value": "_messages.ErrorSummary"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_event_formatting.format_event_traceback", "_traceback._is_traceback_enabled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Render ErrorSummary as a formatted traceback for backward-compatibility with pre-2.19 TaskResult.exception.", "source_code": "def error_summary(value: _messages.ErrorSummary) -> str:\n    \"\"\"Render ErrorSummary as a formatted traceback for backward-compatibility with pre-2.19 TaskResult.exception.\"\"\"\n    if _traceback._is_traceback_enabled(_traceback.TracebackEvent.ERROR):\n        return _event_formatting.format_event_traceback(value.event)\n\n    return '(traceback unavailable)'", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_transform.py", "class_name": null, "function_name": "deprecation_summary", "parameters": ["value"], "param_types": {"value": "_messages.DeprecationSummary"}, "return_type": "dict[str, t.Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_event_utils.deprecation_as_dict", "transformed.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Render DeprecationSummary as dict values for backward-compatibility with pre-2.19 TaskResult.deprecations.", "source_code": "def deprecation_summary(value: _messages.DeprecationSummary) -> dict[str, t.Any]:\n    \"\"\"Render DeprecationSummary as dict values for backward-compatibility with pre-2.19 TaskResult.deprecations.\"\"\"\n    transformed = _event_utils.deprecation_as_dict(value)\n    transformed.update(deprecator=value.deprecator)\n\n    return transformed", "loc": 6}
{"file": "ansible\\lib\\ansible\\_internal\\_templating\\_transform.py", "class_name": null, "function_name": "encrypted_string", "parameters": ["value"], "param_types": {"value": "EncryptedString"}, "return_type": "str | VaultExceptionMarker", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["VaultExceptionMarker", "VaultHelper.get_ciphertext", "_error_factory.ControllerEventFactory.from_exception", "_traceback.is_traceback_enabled", "value._decrypt"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Decrypt an encrypted string and return its value, or a VaultExceptionMarker if decryption fails.", "source_code": "def encrypted_string(value: EncryptedString) -> str | VaultExceptionMarker:\n    \"\"\"Decrypt an encrypted string and return its value, or a VaultExceptionMarker if decryption fails.\"\"\"\n    try:\n        return value._decrypt()\n    except Exception as ex:\n        return VaultExceptionMarker(\n            ciphertext=VaultHelper.get_ciphertext(value, with_tags=True),\n            event=_error_factory.ControllerEventFactory.from_exception(ex, _traceback.is_traceback_enabled(_traceback.TracebackEvent.ERROR)),\n        )", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_yaml_map", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.update", "self._node_position_info", "self._node_position_info(node).tag", "self.construct_mapping"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_map(self, node):\n    data = self._node_position_info(node).tag({})  # always an ordered dictionary on py3.7+\n    yield data\n    value = self.construct_mapping(node)\n    data.update(value)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_mapping", "parameters": ["self", "node", "deep"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleConstructorError", "display.warning", "keys.add", "self.construct_object", "set", "super", "super().construct_mapping"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_mapping(self, node, deep=False):\n    # Delegate to built-in implementation to construct the mapping.\n    # This is done before checking for duplicates to leverage existing error checking on the input node.\n    mapping = super().construct_mapping(node, deep)\n    keys = set()\n\n    # Now that the node is known to be a valid mapping, handle any duplicate keys.\n    for key_node, _value_node in node.value:\n        if (key := self.construct_object(key_node, deep=deep)) in keys:\n            msg = f'Found duplicate mapping key {key!r}.'\n\n            if self._duplicate_key_mode == 'error':\n                raise AnsibleConstructorError(problem=msg, problem_mark=key_node.start_mark)\n\n            if self._duplicate_key_mode == 'warn':\n                display.warning(msg=msg, obj=key, help_text='Using last defined value only.')\n\n        keys.add(key)\n\n    return mapping", "loc": 20}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_yaml_omap", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "list", "origin.tag", "self._node_position_info", "super", "super().construct_yaml_omap"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_omap(self, node):\n    origin = self._node_position_info(node)\n    display.deprecated(\n        msg='Use of the YAML `!!omap` tag is deprecated.',\n        version='2.23',\n        obj=origin,\n        help_text='Use a standard mapping instead, as key order is always preserved.',\n    )\n    items = list(super().construct_yaml_omap(node))[0]\n    items = [origin.tag(item) for item in items]\n    yield origin.tag(items)", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_yaml_pairs", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "list", "origin.tag", "self._node_position_info", "super", "super().construct_yaml_pairs"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_pairs(self, node):\n    origin = self._node_position_info(node)\n    display.deprecated(\n        msg='Use of the YAML `!!pairs` tag is deprecated.',\n        version='2.23',\n        obj=origin,\n        help_text='Use a standard mapping instead.',\n    )\n    items = list(super().construct_yaml_pairs(node))[0]\n    items = [origin.tag(item) for item in items]\n    yield origin.tag(items)", "loc": 11}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_yaml_str", "parameters": ["self", "node"], "param_types": {"node": "ScalarNode"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag", "self._node_position_info", "self.construct_scalar", "tags.append", "to_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_str(self, node: ScalarNode) -> str:\n    # Override the default string handling function\n    # to always return unicode objects\n    # DTFIX-FUTURE: is this to_text conversion still necessary under Py3?\n    value = to_text(self.construct_scalar(node))\n\n    tags: list[AnsibleDatatagBase] = [self._node_position_info(node)]\n\n    if self.trusted_as_template:\n        # NB: since we're not context aware, this will happily add trust to dictionary keys; this is actually necessary for\n        #  certain backward compat scenarios, though might be accomplished in other ways if we wanted to avoid trusting keys in\n        #  the general scenario\n        tags.append(_TRUSTED_AS_TEMPLATE)\n\n    return AnsibleTagHelper.tag(value, tags)", "loc": 15}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleInstrumentedConstructor", "function_name": "construct_yaml_set", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.tag", "data.update", "self._node_position_info", "self.construct_mapping", "set"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_set(self, node):\n    data = AnsibleTagHelper.tag(set(), self._node_position_info(node))\n    yield data\n    value = self.construct_mapping(node)\n    data.update(value)", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleConstructor", "function_name": "construct_yaml_unsafe", "parameters": ["self", "node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._resolve_and_construct_object"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_unsafe(self, node):\n    self._unsafe_depth += 1\n\n    try:\n        return self._resolve_and_construct_object(node)\n    finally:\n        self._unsafe_depth -= 1", "loc": 7}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleConstructor", "function_name": "construct_yaml_vault", "parameters": ["self", "node"], "param_types": {"node": "Node"}, "return_type": "EncryptedString", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleConstructorError", "AnsibleTagHelper.tag_copy", "AnsibleTagHelper.untag", "EncryptedString", "isinstance", "self._resolve_and_construct_object"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_vault(self, node: Node) -> EncryptedString:\n    ciphertext = self._resolve_and_construct_object(node)\n\n    if not isinstance(ciphertext, str):\n        raise AnsibleConstructorError(problem=f\"the {node.tag!r} tag requires a string value\", problem_mark=node.start_mark)\n\n    encrypted_string = AnsibleTagHelper.tag_copy(ciphertext, EncryptedString(ciphertext=AnsibleTagHelper.untag(ciphertext)))\n\n    return encrypted_string", "loc": 9}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_constructor.py", "class_name": "AnsibleConstructor", "function_name": "construct_yaml_vault_encrypted", "parameters": ["self", "node"], "param_types": {"node": "Node"}, "return_type": "EncryptedString", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display.deprecated", "self._node_position_info", "self.construct_yaml_vault"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def construct_yaml_vault_encrypted(self, node: Node) -> EncryptedString:\n    origin = self._node_position_info(node)\n    display.deprecated(\n        msg='Use of the YAML `!vault-encrypted` tag is deprecated.',\n        version='2.23',\n        obj=origin,\n        help_text='Use the `!vault` tag instead.',\n    )\n\n    return self.construct_yaml_vault(node)", "loc": 10}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_dumper.py", "class_name": "AnsibleDumper", "function_name": "get_node_from_ciphertext", "parameters": ["self", "data"], "param_types": {"data": "object"}, "return_type": "ScalarNode | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["VaultHelper.get_ciphertext", "self.represent_scalar"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_node_from_ciphertext(self, data: object) -> ScalarNode | None:\n    if ciphertext := VaultHelper.get_ciphertext(data, with_tags=False):\n        return self.represent_scalar('!vault', ciphertext, style='|')\n\n    return None", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_dumper.py", "class_name": "AnsibleDumper", "function_name": "represent_vault_exception_marker", "parameters": ["self", "data"], "param_types": {"data": "_jinja_common.VaultExceptionMarker"}, "return_type": "ScalarNode", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.trip", "self.get_node_from_ciphertext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def represent_vault_exception_marker(self, data: _jinja_common.VaultExceptionMarker) -> ScalarNode:\n    if node := self.get_node_from_ciphertext(data):\n        return node\n\n    data.trip()", "loc": 5}
{"file": "ansible\\lib\\ansible\\_internal\\_yaml\\_dumper.py", "class_name": "AnsibleDumper", "function_name": "represent_ansible_tagged_object", "parameters": ["self", "data"], "param_types": {"data": "AnsibleTaggedObject"}, "return_type": "Node", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AnsibleTagHelper.as_native_type", "_internal.is_intermediate_iterable", "_internal.is_intermediate_mapping", "self.get_node_from_ciphertext", "self.represent_data", "self.represent_dict", "self.represent_list"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def represent_ansible_tagged_object(self, data: AnsibleTaggedObject) -> Node:\n    if _internal.is_intermediate_mapping(data):\n        return self.represent_dict(data)\n\n    if _internal.is_intermediate_iterable(data):\n        return self.represent_list(data)\n\n    if node := self.get_node_from_ciphertext(data):\n        return node\n\n    return self.represent_data(AnsibleTagHelper.as_native_type(data))  # automatically decrypts encrypted strings", "loc": 11}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argcomplete.autocomplete", "argparse.ArgumentParser", "args.func", "getattr", "inspect.signature", "json_parser.add_argument", "json_parser.set_defaults", "man_parser.add_argument", "man_parser.set_defaults", "parser.add_subparsers", "parser.parse_args", "rst_parser.add_argument", "rst_parser.set_defaults", "str", "subparsers.add_parser", "sys.path.insert"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Main program entry point.", "source_code": "def main() -> None:\n    \"\"\"Main program entry point.\"\"\"\n    parser = argparse.ArgumentParser(description=__doc__)\n    subparsers = parser.add_subparsers(required=True, metavar='command')\n\n    man_parser = subparsers.add_parser('man', description=build_man.__doc__, help=build_man.__doc__)\n    man_parser.add_argument('--output-dir', required=True, type=pathlib.Path, metavar='DIR', help='output directory')\n    man_parser.add_argument('--template-file', default=SCRIPT_DIR / 'man.j2', type=pathlib.Path, metavar='FILE', help='template file')\n    man_parser.set_defaults(func=build_man)\n\n    rst_parser = subparsers.add_parser('rst', description=build_rst.__doc__, help=build_rst.__doc__)\n    rst_parser.add_argument('--output-dir', required=True, type=pathlib.Path, metavar='DIR', help='output directory')\n    rst_parser.add_argument('--template-file', default=SCRIPT_DIR / 'rst.j2', type=pathlib.Path, metavar='FILE', help='template file')\n    rst_parser.set_defaults(func=build_rst)\n\n    json_parser = subparsers.add_parser('json', description=build_json.__doc__, help=build_json.__doc__)\n    json_parser.add_argument('--output-file', required=True, type=pathlib.Path, metavar='FILE', help='output file')\n    json_parser.set_defaults(func=build_json)\n\n    try:\n        # noinspection PyUnresolvedReferences\n        import argcomplete\n    except ImportError:\n        pass\n    else:\n        argcomplete.autocomplete(parser)\n\n    args = parser.parse_args()\n    kwargs = {name: getattr(args, name) for name in inspect.signature(args.func).parameters}\n\n    sys.path.insert(0, str(SOURCE_DIR / 'lib'))\n\n    args.func(**kwargs)", "loc": 33}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "build_man", "parameters": ["output_dir", "template_file"], "param_types": {"output_dir": "pathlib.Path", "template_file": "pathlib.Path"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["docutils.core.publish_file", "docutils.writers.manpage.Writer", "generate_rst", "generate_rst(template_file).items", "io.StringIO", "output_dir.mkdir", "template_file.resolve", "template_file.resolve().is_relative_to", "warnings.warn"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Build man pages for ansible-core CLI programs.", "source_code": "def build_man(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:\n    \"\"\"Build man pages for ansible-core CLI programs.\"\"\"\n    if not template_file.resolve().is_relative_to(SCRIPT_DIR):\n        warnings.warn(\"Custom templates are intended for debugging purposes only. The data model may change in future releases without notice.\")\n\n    import docutils.core\n    import docutils.writers.manpage\n\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for cli_name, source in generate_rst(template_file).items():\n        with io.StringIO(source) as source_file:\n            docutils.core.publish_file(\n                source=source_file,\n                destination_path=output_dir / f'{cli_name}.1',\n                writer=docutils.writers.manpage.Writer(),\n            )", "loc": 17}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "build_rst", "parameters": ["output_dir", "template_file"], "param_types": {"output_dir": "pathlib.Path", "template_file": "pathlib.Path"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["generate_rst", "generate_rst(template_file).items", "output_dir / f'{cli_name}.rst'.write_text", "output_dir.mkdir", "template_file.resolve", "template_file.resolve().is_relative_to", "warnings.warn"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Build RST documentation for ansible-core CLI programs.", "source_code": "def build_rst(output_dir: pathlib.Path, template_file: pathlib.Path) -> None:\n    \"\"\"Build RST documentation for ansible-core CLI programs.\"\"\"\n    if not template_file.resolve().is_relative_to(SCRIPT_DIR):\n        warnings.warn(\"Custom templates are intended for debugging purposes only. The data model may change in future releases without notice.\")\n\n    output_dir.mkdir(exist_ok=True, parents=True)\n\n    for cli_name, source in generate_rst(template_file).items():\n        (output_dir / f'{cli_name}.rst').write_text(source)", "loc": 9}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "build_json", "parameters": ["output_file"], "param_types": {"output_file": "pathlib.Path"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collect_programs", "json.dumps", "output_file.parent.mkdir", "output_file.write_text", "warnings.warn"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "Build JSON documentation for ansible-core CLI programs.", "source_code": "def build_json(output_file: pathlib.Path) -> None:\n    \"\"\"Build JSON documentation for ansible-core CLI programs.\"\"\"\n    warnings.warn(\"JSON output is intended for debugging purposes only. The data model may change in future releases without notice.\")\n\n    output_file.parent.mkdir(exist_ok=True, parents=True)\n    output_file.write_text(json.dumps(collect_programs(), indent=4))", "loc": 6}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "collect_programs", "parameters": [], "param_types": {}, "return_type": "dict[str, dict[str, t.Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SOURCE_DIR / 'lib/ansible/cli'.glob", "dict", "generate_options_docs", "programs.append", "source_file.name.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return information about CLI programs.", "source_code": "def collect_programs() -> dict[str, dict[str, t.Any]]:\n    \"\"\"Return information about CLI programs.\"\"\"\n    programs: list[tuple[str, dict[str, t.Any]]] = []\n    cli_bin_name_list: list[str] = []\n\n    for source_file in (SOURCE_DIR / 'lib/ansible/cli').glob('*.py'):\n        if not source_file.name.startswith('_'):\n            programs.append(generate_options_docs(source_file, cli_bin_name_list))\n\n    return dict(programs)", "loc": 10}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "generate_options_docs", "parameters": ["source_file", "cli_bin_name_list"], "param_types": {"source_file": "pathlib.Path", "cli_bin_name_list": "list[str]"}, "return_type": "tuple[str, dict[str, t.Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cli.init_parser", "cli_bin_name_list.append", "cli_class", "cli_module_name.capitalize", "dict", "get_action_docs", "getattr", "importlib.import_module", "itertools.chain.from_iterable", "parser.format_usage", "populate_subparser_actions", "source_file.relative_to", "source_file.with_suffix", "str", "str(source_file).endswith", "trim_docstring", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Generate doc structure from CLI module options.", "source_code": "def generate_options_docs(source_file: pathlib.Path, cli_bin_name_list: list[str]) -> tuple[str, dict[str, t.Any]]:\n    \"\"\"Generate doc structure from CLI module options.\"\"\"\n    import ansible.release\n\n    if str(source_file).endswith('/lib/ansible/cli/adhoc.py'):\n        cli_name = 'ansible'\n        cli_class_name = 'AdHocCLI'\n        cli_module_fqn = 'ansible.cli.adhoc'\n    else:\n        cli_module_name = source_file.with_suffix('').name\n        cli_name = f'ansible-{cli_module_name}'\n        cli_class_name = f'{cli_module_name.capitalize()}CLI'\n        cli_module_fqn = f'ansible.cli.{cli_module_name}'\n\n    cli_bin_name_list.append(cli_name)\n\n    cli_module = importlib.import_module(cli_module_fqn)\n    cli_class: type[CLI] = getattr(cli_module, cli_class_name)\n\n    cli = cli_class([cli_name])\n    cli.init_parser()\n\n    parser: argparse.ArgumentParser = cli.parser\n    long_desc = cli.__doc__\n    arguments: dict[str, str] | None = getattr(cli, 'ARGUMENTS', None)\n\n    action_docs = get_action_docs(parser)\n    option_names: tuple[str, ...] = tuple(itertools.chain.from_iterable(opt.options for opt in action_docs))\n    actions: dict[str, dict[str, t.Any]] = {}\n\n    content_depth = populate_subparser_actions(parser, option_names, actions)\n\n    docs = dict(\n        version=ansible.release.__version__,\n        source=str(source_file.relative_to(SOURCE_DIR)),\n        cli_name=cli_name,\n        usage=parser.format_usage(),\n        short_desc=parser.description,\n        long_desc=trim_docstring(long_desc),\n        actions=actions,\n        options=[item.__dict__ for item in action_docs],\n        arguments=arguments,\n        option_names=option_names,\n        cli_bin_name_list=cli_bin_name_list,\n        content_depth=content_depth,\n        inventory='-i' in option_names,\n        library='-M' in option_names,\n    )\n\n    return cli_name, docs", "loc": 50}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "populate_subparser_actions", "parameters": ["parser", "shared_option_names", "actions"], "param_types": {"parser": "argparse.ArgumentParser", "shared_option_names": "tuple[str, ...]", "actions": "dict[str, dict[str, t.Any]]"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "get_action_docs", "list", "populate_subparser_actions", "set", "subparser.get_default", "subparser_action_docs.add", "subparser_option_names.add", "subparsers.items", "trim_docstring"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Generate doc structure from CLI module subparser options.", "source_code": "def populate_subparser_actions(parser: argparse.ArgumentParser, shared_option_names: tuple[str, ...], actions: dict[str, dict[str, t.Any]]) -> int:\n    \"\"\"Generate doc structure from CLI module subparser options.\"\"\"\n    try:\n        # noinspection PyProtectedMember\n        subparsers: dict[str, argparse.ArgumentParser] = parser._subparsers._group_actions[0].choices  # type: ignore\n    except AttributeError:\n        subparsers = {}\n\n    depth = 0\n\n    for subparser_action, subparser in subparsers.items():\n        subparser_option_names: set[str] = set()\n        subparser_action_docs: set[ActionDoc] = set()\n        subparser_actions: dict[str, dict[str, t.Any]] = {}\n\n        for action_doc in get_action_docs(subparser):\n            for option_alias in action_doc.options:\n                if option_alias in shared_option_names:\n                    continue\n\n                subparser_option_names.add(option_alias)\n                subparser_action_docs.add(action_doc)\n\n        depth = populate_subparser_actions(subparser, shared_option_names, subparser_actions)\n\n        actions[subparser_action] = dict(\n            option_names=list(subparser_option_names),\n            options=[item.__dict__ for item in subparser_action_docs],\n            actions=subparser_actions,\n            name=subparser_action,\n            desc=trim_docstring(subparser.get_default(\"func\").__doc__),\n        )\n\n    return depth + 1", "loc": 34}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "get_action_docs", "parameters": ["parser"], "param_types": {"parser": "argparse.ArgumentParser"}, "return_type": "list[ActionDoc]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ActionDoc", "action.dest.upper", "action_docs.append", "isinstance", "tuple"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Get action documentation from the given argument parser.", "source_code": "def get_action_docs(parser: argparse.ArgumentParser) -> list[ActionDoc]:\n    \"\"\"Get action documentation from the given argument parser.\"\"\"\n    action_docs = []\n\n    # noinspection PyProtectedMember\n    for action in parser._actions:\n        if action.help == argparse.SUPPRESS:\n            continue\n\n        # noinspection PyProtectedMember, PyUnresolvedReferences\n        args = action.dest.upper() if isinstance(action, argparse._StoreAction) else None\n\n        if args or action.option_strings:\n            action_docs.append(\n                ActionDoc(\n                    desc=action.help,\n                    options=tuple(action.option_strings),\n                    arg=args,\n                )\n            )\n\n    return action_docs", "loc": 22}
{"file": "ansible\\packaging\\cli-doc\\build.py", "class_name": null, "function_name": "trim_docstring", "parameters": ["docstring"], "param_types": {"docstring": "str | None"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "docstring.expandtabs", "docstring.expandtabs().splitlines", "len", "line.lstrip", "line[indent:].rstrip", "lines[0].strip", "min", "trimmed.append", "trimmed.pop"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Trim and return the given docstring using the implementation from https://peps.python.org/pep-0257/#handling-docstring-indentation.", "source_code": "def trim_docstring(docstring: str | None) -> str:\n    \"\"\"Trim and return the given docstring using the implementation from https://peps.python.org/pep-0257/#handling-docstring-indentation.\"\"\"\n    if not docstring:\n        return ''  # pragma: nocover\n\n    # Convert tabs to spaces (following the normal Python rules) and split into a list of lines\n    lines = docstring.expandtabs().splitlines()\n\n    # Determine minimum indentation (first line doesn't count)\n    indent = sys.maxsize\n\n    for line in lines[1:]:\n        stripped = line.lstrip()\n\n        if stripped:\n            indent = min(indent, len(line) - len(stripped))\n\n    # Remove indentation (first line is special)\n    trimmed = [lines[0].strip()]\n\n    if indent < sys.maxsize:\n        for line in lines[1:]:\n            trimmed.append(line[indent:].rstrip())\n\n    # Strip off trailing and leading blank lines\n    while trimmed and not trimmed[-1]:\n        trimmed.pop()\n\n    while trimmed and not trimmed[0]:\n        trimmed.pop(0)\n\n    # Return a single string\n    return '\\n'.join(trimmed)", "loc": 33}
{"file": "cookiecutter\\cookiecutter\\cli.py", "class_name": null, "function_name": "validate_extra_context", "parameters": ["_ctx", "_param", "value"], "param_types": {"_ctx": "Context", "_param": "Parameter", "value": "Iterable[str]"}, "return_type": "OrderedDict[str, str] | None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OrderedDict", "click.BadParameter", "s.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Validate extra context.", "source_code": "def validate_extra_context(\n    _ctx: Context, _param: Parameter, value: Iterable[str]\n) -> OrderedDict[str, str] | None:\n    \"\"\"Validate extra context.\"\"\"\n    for string in value:\n        if '=' not in string:\n            msg = (\n                f\"EXTRA_CONTEXT should contain items of the form key=value; \"\n                f\"'{string}' doesn't match that form\"\n            )\n            raise click.BadParameter(msg)\n\n    # Convert tuple -- e.g.: ('program_name=foobar', 'startsecs=66')\n    # to dict -- e.g.: {'program_name': 'foobar', 'startsecs': '66'}\n    return OrderedDict(s.split('=', 1) for s in value) or None", "loc": 15}
{"file": "cookiecutter\\cookiecutter\\cli.py", "class_name": null, "function_name": "main", "parameters": ["template", "extra_context", "no_input", "checkout", "verbose", "replay", "overwrite_if_exists", "output_dir", "config_file", "default_config", "debug_file", "directory", "skip_if_file_exists", "accept_hooks", "replay_file", "list_installed", "keep_project_on_failure"], "param_types": {"template": "str", "extra_context": "dict[str, Any]", "no_input": "bool", "checkout": "str", "verbose": "bool", "replay": "bool | str", "overwrite_if_exists": "bool", "output_dir": "str", "config_file": "str | None", "default_config": "bool", "debug_file": "str | None", "directory": "str", "skip_if_file_exists": "bool", "accept_hooks": "Literal['yes', 'ask', 'no']", "replay_file": "str | None", "list_installed": "bool", "keep_project_on_failure": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["click.Choice", "click.Path", "click.argument", "click.command", "click.confirm", "click.echo", "click.get_current_context", "click.get_current_context().get_help", "click.option", "click.version_option", "configure_logger", "cookiecutter", "json.dumps", "list_installed_templates", "os.environ.get", "sys.exit", "template.lower", "version_msg"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Create a project from a Cookiecutter project template (TEMPLATE). Cookiecutter is free and open source software, developed and managed by volunteers. If you would like to help out or fund the project, please get", "source_code": "def main(\n    template: str,\n    extra_context: dict[str, Any],\n    no_input: bool,\n    checkout: str,\n    verbose: bool,\n    replay: bool | str,\n    overwrite_if_exists: bool,\n    output_dir: str,\n    config_file: str | None,\n    default_config: bool,\n    debug_file: str | None,\n    directory: str,\n    skip_if_file_exists: bool,\n    accept_hooks: Literal['yes', 'ask', 'no'],\n    replay_file: str | None,\n    list_installed: bool,\n    keep_project_on_failure: bool,\n) -> None:\n    \"\"\"Create a project from a Cookiecutter project template (TEMPLATE).\n\n    Cookiecutter is free and open source software, developed and managed by\n    volunteers. If you would like to help out or fund the project, please get\n    in touch at https://github.com/cookiecutter/cookiecutter.\n    \"\"\"\n    # Commands that should work without arguments\n    if list_installed:\n        list_installed_templates(default_config, config_file)\n        sys.exit(0)\n\n    # Raising usage, after all commands that should work without args.\n    if not template or template.lower() == 'help':\n        click.echo(click.get_current_context().get_help())\n        sys.exit(0)\n\n    configure_logger(stream_level='DEBUG' if verbose else 'INFO', debug_file=debug_file)\n\n    # If needed, prompt the user to ask whether or not they want to execute\n    # the pre/post hooks.\n    if accept_hooks == \"ask\":\n        _accept_hooks = click.confirm(\"Do you want to execute hooks?\")\n    else:\n        _accept_hooks = accept_hooks == \"yes\"\n\n    if replay_file:\n        replay = replay_file\n\n    try:\n        cookiecutter(\n            template,\n            checkout,\n            no_input,\n            extra_context=extra_context,\n            replay=replay,\n            overwrite_if_exists=overwrite_if_exists,\n            output_dir=output_dir,\n            config_file=config_file,\n            default_config=default_config,\n            password=os.environ.get('COOKIECUTTER_REPO_PASSWORD'),\n            directory=directory,\n            skip_if_file_exists=skip_if_file_exists,\n            accept_hooks=_accept_hooks,\n            keep_project_on_failure=keep_project_on_failure,\n        )\n    except (\n        ContextDecodingException,\n        OutputDirExistsException,\n        EmptyDirNameException,\n        InvalidModeException,\n        FailedHookException,\n        UnknownExtension,\n        InvalidZipRepository,\n        RepositoryNotFound,\n        RepositoryCloneFailed,\n    ) as e:\n        click.echo(e)\n        sys.exit(1)\n    except UndefinedVariableInTemplate as undefined_err:\n        click.echo(f'{undefined_err.message}')\n        click.echo(f'Error message: {undefined_err.error.message}')\n\n        context_str = json.dumps(undefined_err.context, indent=4, sort_keys=True)\n        click.echo(f'Context: {context_str}')\n        sys.exit(1)", "loc": 84}
{"file": "cookiecutter\\cookiecutter\\config.py", "class_name": null, "function_name": "merge_configs", "parameters": ["default", "overwrite"], "param_types": {"default": "dict[str, Any]", "overwrite": "dict[str, Any]"}, "return_type": "dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["copy.deepcopy", "default.get", "isinstance", "merge_configs", "overwrite.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Recursively update a dict with the key/value pair of another. Dict values that are dictionaries themselves will be updated, whilst preserving existing keys.", "source_code": "def merge_configs(default: dict[str, Any], overwrite: dict[str, Any]) -> dict[str, Any]:\n    \"\"\"Recursively update a dict with the key/value pair of another.\n\n    Dict values that are dictionaries themselves will be updated, whilst\n    preserving existing keys.\n    \"\"\"\n    new_config = copy.deepcopy(default)\n\n    for k, v in overwrite.items():\n        # Make sure to preserve existing items in\n        # nested dicts, for example `abbreviations`\n        if isinstance(v, dict):\n            new_config[k] = merge_configs(default.get(k, {}), v)\n        else:\n            new_config[k] = v\n\n    return new_config", "loc": 17}
{"file": "cookiecutter\\cookiecutter\\config.py", "class_name": null, "function_name": "get_config", "parameters": ["config_path"], "param_types": {"config_path": "Path | str"}, "return_type": "dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ConfigDoesNotExistException", "InvalidConfiguration", "_expand_path", "isinstance", "logger.debug", "merge_configs", "open", "os.path.exists", "yaml.safe_load"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Retrieve the config from the specified path, returning a config dict.", "source_code": "def get_config(config_path: Path | str) -> dict[str, Any]:\n    \"\"\"Retrieve the config from the specified path, returning a config dict.\"\"\"\n    if not os.path.exists(config_path):\n        msg = f'Config file {config_path} does not exist.'\n        raise ConfigDoesNotExistException(msg)\n\n    logger.debug('config_path is %s', config_path)\n    with open(config_path, encoding='utf-8') as file_handle:\n        try:\n            yaml_dict = yaml.safe_load(file_handle) or {}\n        except yaml.YAMLError as e:\n            msg = f'Unable to parse YAML file {config_path}.'\n            raise InvalidConfiguration(msg) from e\n        if not isinstance(yaml_dict, dict):\n            msg = f'Top-level element of YAML file {config_path} should be an object.'\n            raise InvalidConfiguration(msg)\n\n    config_dict = merge_configs(DEFAULT_CONFIG, yaml_dict)\n\n    raw_replay_dir = config_dict['replay_dir']\n    config_dict['replay_dir'] = _expand_path(raw_replay_dir)\n\n    raw_cookies_dir = config_dict['cookiecutters_dir']\n    config_dict['cookiecutters_dir'] = _expand_path(raw_cookies_dir)\n\n    return config_dict", "loc": 26}
{"file": "cookiecutter\\cookiecutter\\extensions.py", "class_name": "TimeExtension", "function_name": "parse", "parameters": ["self", "parser"], "param_types": {"parser": "Parser"}, "return_type": "nodes.Output", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "next", "nodes.Const", "nodes.Output", "parser.parse_expression", "parser.stream.skip_if", "self.call_method"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse datetime template and add datetime value.", "source_code": "def parse(self, parser: Parser) -> nodes.Output:\n    \"\"\"Parse datetime template and add datetime value.\"\"\"\n    lineno = next(parser.stream).lineno\n\n    node = parser.parse_expression()\n\n    if parser.stream.skip_if('comma'):\n        datetime_format = parser.parse_expression()\n    else:\n        datetime_format = nodes.Const(None)\n\n    if isinstance(node, nodes.Add):\n        call_method = self.call_method(\n            '_datetime',\n            [node.left, nodes.Const('+'), node.right, datetime_format],\n            lineno=lineno,\n        )\n    elif isinstance(node, nodes.Sub):\n        call_method = self.call_method(\n            '_datetime',\n            [node.left, nodes.Const('-'), node.right, datetime_format],\n            lineno=lineno,\n        )\n    else:\n        call_method = self.call_method(\n            '_now',\n            [node, datetime_format],\n            lineno=lineno,\n        )\n    return nodes.Output([call_method], lineno=lineno)", "loc": 30}
{"file": "cookiecutter\\cookiecutter\\extensions.py", "class_name": null, "function_name": "random_ascii_string", "parameters": ["length", "punctuation"], "param_types": {"length": "int", "punctuation": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "choice", "range"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def random_ascii_string(length: int, punctuation: bool = False) -> str:\n    if punctuation:\n        corpus = f'{string.ascii_letters}{string.punctuation}'\n    else:\n        corpus = string.ascii_letters\n    return \"\".join(choice(corpus) for _ in range(length))", "loc": 6}
{"file": "cookiecutter\\cookiecutter\\find.py", "class_name": null, "function_name": "find_template", "parameters": ["repo_dir", "env"], "param_types": {"repo_dir": "Path | str", "env": "Environment"}, "return_type": "Path", "param_doc": {"repo_dir": "Local directory of newly cloned repo."}, "return_doc": "Relative path to project template.", "raises_doc": [], "called_functions": ["Path", "logger.debug", "os.listdir"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Determine which child directory of ``repo_dir`` is the project template.", "source_code": "def find_template(repo_dir: Path | str, env: Environment) -> Path:\n    \"\"\"Determine which child directory of ``repo_dir`` is the project template.\n\n    :param repo_dir: Local directory of newly cloned repo.\n    :return: Relative path to project template.\n    \"\"\"\n    logger.debug('Searching %s for the project template.', repo_dir)\n\n    for str_path in os.listdir(repo_dir):\n        if (\n            'cookiecutter' in str_path\n            and env.variable_start_string in str_path\n            and env.variable_end_string in str_path\n        ):\n            project_template = Path(repo_dir, str_path)\n            break\n    else:\n        raise NonTemplatedInputDirException\n\n    logger.debug('The project template appears to be %s', project_template)\n    return project_template", "loc": 21}
{"file": "cookiecutter\\cookiecutter\\generate.py", "class_name": null, "function_name": "is_copy_only_path", "parameters": ["path", "context"], "param_types": {"path": "str", "context": "dict[str, Any]"}, "return_type": "bool", "param_doc": {"path": "A file-system path referring to a file or dir that", "context": "cookiecutter context."}, "return_doc": "", "raises_doc": [], "called_functions": ["fnmatch.fnmatch"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Check whether the given `path` should only be copied and not rendered.", "source_code": "def is_copy_only_path(path: str, context: dict[str, Any]) -> bool:\n    \"\"\"Check whether the given `path` should only be copied and not rendered.\n\n    Returns True if `path` matches a pattern in the given `context` dict,\n    otherwise False.\n\n    :param path: A file-system path referring to a file or dir that\n        should be rendered or just copied.\n    :param context: cookiecutter context.\n    \"\"\"\n    try:\n        for dont_render in context['cookiecutter']['_copy_without_render']:\n            if fnmatch.fnmatch(path, dont_render):\n                return True\n    except KeyError:\n        return False\n\n    return False", "loc": 18}
{"file": "cookiecutter\\cookiecutter\\generate.py", "class_name": null, "function_name": "apply_overwrites_to_context", "parameters": ["context", "overwrite_context"], "param_types": {"context": "dict[str, Any]", "overwrite_context": "dict[str, Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "YesNoPrompt", "YesNoPrompt().process_response", "apply_overwrites_to_context", "context_value.insert", "context_value.remove", "isinstance", "overwrite_context.items", "set", "set(overwrite).issubset"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Modify the given context in place based on the overwrite_context.", "source_code": "def apply_overwrites_to_context(\n    context: dict[str, Any],\n    overwrite_context: dict[str, Any],\n    *,\n    in_dictionary_variable: bool = False,\n) -> None:\n    \"\"\"Modify the given context in place based on the overwrite_context.\"\"\"\n    for variable, overwrite in overwrite_context.items():\n        if variable not in context:\n            if not in_dictionary_variable:\n                # We are dealing with a new variable on first level, ignore\n                continue\n            # We are dealing with a new dictionary variable in a deeper level\n            context[variable] = overwrite\n\n        context_value = context[variable]\n        if isinstance(context_value, list):\n            if in_dictionary_variable:\n                context[variable] = overwrite\n                continue\n            if isinstance(overwrite, list):\n                # We are dealing with a multichoice variable\n                # Let's confirm all choices are valid for the given context\n                if set(overwrite).issubset(set(context_value)):\n                    context[variable] = overwrite\n                else:\n                    msg = (\n                        f\"{overwrite} provided for multi-choice variable \"\n                        f\"{variable}, but valid choices are {context_value}\"\n                    )\n                    raise ValueError(msg)\n            else:\n                # We are dealing with a choice variable\n                if overwrite in context_value:\n                    # This overwrite is actually valid for the given context\n                    # Let's set it as default (by definition first item in list)\n                    # see ``cookiecutter.prompt.prompt_choice_for_config``\n                    context_value.remove(overwrite)\n                    context_value.insert(0, overwrite)\n                else:\n                    msg = (\n                        f\"{overwrite} provided for choice variable \"\n                        f\"{variable}, but the choices are {context_value}.\"\n                    )\n                    raise ValueError(msg)\n        elif isinstance(context_value, dict) and isinstance(overwrite, dict):\n            # Partially overwrite some keys in original dict\n            apply_overwrites_to_context(\n                context_value, overwrite, in_dictionary_variable=True\n            )\n            context[variable] = context_value\n        elif isinstance(context_value, bool) and isinstance(overwrite, str):\n            # We are dealing with a boolean variable\n            # Convert overwrite to its boolean counterpart\n            try:\n                context[variable] = YesNoPrompt().process_response(overwrite)\n            except InvalidResponse as err:\n                msg = (\n                    f\"{overwrite} provided for variable \"\n                    f\"{variable} could not be converted to a boolean.\"\n                )\n                raise ValueError(msg) from err\n        else:\n            # Simply overwrite the value for this variable\n            context[variable] = overwrite", "loc": 65}
{"file": "cookiecutter\\cookiecutter\\generate.py", "class_name": null, "function_name": "generate_context", "parameters": ["context_file", "default_context", "extra_context"], "param_types": {"context_file": "str", "default_context": "dict[str, Any] | None", "extra_context": "dict[str, Any] | None"}, "return_type": "dict[str, Any]", "param_doc": {"context_file": "JSON file containing key/value pairs for populating", "default_context": "Dictionary containing config to take into account.", "extra_context": "Dictionary containing configuration overrides"}, "return_doc": "", "raises_doc": [], "called_functions": ["ContextDecodingException", "OrderedDict", "apply_overwrites_to_context", "file_name.split", "json.load", "logger.debug", "open", "os.path.abspath", "os.path.split", "str", "warnings.warn"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Generate the context for a Cookiecutter project template. Loads the JSON file as a Python object, with key being the JSON filename.", "source_code": "def generate_context(\n    context_file: str = 'cookiecutter.json',\n    default_context: dict[str, Any] | None = None,\n    extra_context: dict[str, Any] | None = None,\n) -> dict[str, Any]:\n    \"\"\"Generate the context for a Cookiecutter project template.\n\n    Loads the JSON file as a Python object, with key being the JSON filename.\n\n    :param context_file: JSON file containing key/value pairs for populating\n        the cookiecutter's variables.\n    :param default_context: Dictionary containing config to take into account.\n    :param extra_context: Dictionary containing configuration overrides\n    \"\"\"\n    context = OrderedDict([])\n\n    try:\n        with open(context_file, encoding='utf-8') as file_handle:\n            obj = json.load(file_handle, object_pairs_hook=OrderedDict)\n    except ValueError as e:\n        # JSON decoding error.  Let's throw a new exception that is more\n        # friendly for the developer or user.\n        full_fpath = os.path.abspath(context_file)\n        json_exc_message = str(e)\n        our_exc_message = (\n            f\"JSON decoding error while loading '{full_fpath}'. \"\n            f\"Decoding error details: '{json_exc_message}'\"\n        )\n        raise ContextDecodingException(our_exc_message) from e\n\n    # Add the Python object to the context dictionary\n    file_name = os.path.split(context_file)[1]\n    file_stem = file_name.split('.')[0]\n    context[file_stem] = obj\n\n    # Overwrite context variable defaults with the default context from the\n    # user's global config, if available\n    if default_context:\n        try:\n            apply_overwrites_to_context(obj, default_context)\n        except ValueError as error:\n            warnings.warn(f\"Invalid default received: {error}\")\n    if extra_context:\n        apply_overwrites_to_context(obj, extra_context)\n\n    logger.debug('Context generated is %s', context)\n    return context", "loc": 47}
{"file": "cookiecutter\\cookiecutter\\generate.py", "class_name": null, "function_name": "render_and_create_dir", "parameters": ["dirname", "context", "output_dir", "environment", "overwrite_if_exists"], "param_types": {"dirname": "str", "context": "dict[str, Any]", "output_dir": "Path | str", "environment": "Environment", "overwrite_if_exists": "bool"}, "return_type": "tuple[Path, bool]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["EmptyDirNameException", "OutputDirExistsException", "Path", "dir_to_create.exists", "environment.from_string", "logger.debug", "make_sure_path_exists", "name_tmpl.render"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Render name of a directory, create the directory, return its path.", "source_code": "def render_and_create_dir(\n    dirname: str,\n    context: dict[str, Any],\n    output_dir: Path | str,\n    environment: Environment,\n    overwrite_if_exists: bool = False,\n) -> tuple[Path, bool]:\n    \"\"\"Render name of a directory, create the directory, return its path.\"\"\"\n    if not dirname or dirname == \"\":\n        msg = 'Error: directory name is empty'\n        raise EmptyDirNameException(msg)\n\n    name_tmpl = environment.from_string(dirname)\n    rendered_dirname = name_tmpl.render(**context)\n\n    dir_to_create = Path(output_dir, rendered_dirname)\n\n    logger.debug(\n        'Rendered dir %s must exist in output_dir %s', dir_to_create, output_dir\n    )\n\n    output_dir_exists = dir_to_create.exists()\n\n    if output_dir_exists:\n        if overwrite_if_exists:\n            logger.debug(\n                'Output directory %s already exists, overwriting it', dir_to_create\n            )\n        else:\n            msg = f'Error: \"{dir_to_create}\" directory already exists'\n            raise OutputDirExistsException(msg)\n    else:\n        make_sure_path_exists(dir_to_create)\n\n    return dir_to_create, not output_dir_exists", "loc": 35}
{"file": "cookiecutter\\cookiecutter\\hooks.py", "class_name": null, "function_name": "run_hook", "parameters": ["hook_name", "project_dir", "context"], "param_types": {"hook_name": "str", "project_dir": "Path | str", "context": "dict[str, Any]"}, "return_type": "None", "param_doc": {"hook_name": "The hook to execute.", "project_dir": "The directory to execute the script from.", "context": "Cookiecutter project context."}, "return_doc": "", "raises_doc": [], "called_functions": ["find_hook", "logger.debug", "run_script_with_context"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Try to find and execute a hook from the specified project directory.", "source_code": "def run_hook(hook_name: str, project_dir: Path | str, context: dict[str, Any]) -> None:\n    \"\"\"\n    Try to find and execute a hook from the specified project directory.\n\n    :param hook_name: The hook to execute.\n    :param project_dir: The directory to execute the script from.\n    :param context: Cookiecutter project context.\n    \"\"\"\n    scripts = find_hook(hook_name)\n    if not scripts:\n        logger.debug('No %s hook found', hook_name)\n        return\n    logger.debug('Running hook %s', hook_name)\n    for script in scripts:\n        run_script_with_context(script, project_dir, context)", "loc": 15}
{"file": "cookiecutter\\cookiecutter\\hooks.py", "class_name": null, "function_name": "run_pre_prompt_hook", "parameters": ["repo_dir"], "param_types": {"repo_dir": "Path | str"}, "return_type": "Path | str", "param_doc": {"repo_dir": "Project template input directory."}, "return_doc": "", "raises_doc": [], "called_functions": ["FailedHookException", "create_tmp_repo_dir", "find_hook", "run_script", "str", "work_in"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Run pre_prompt hook from repo directory.", "source_code": "def run_pre_prompt_hook(repo_dir: Path | str) -> Path | str:\n    \"\"\"Run pre_prompt hook from repo directory.\n\n    :param repo_dir: Project template input directory.\n    \"\"\"\n    # Check if we have a valid pre_prompt script\n    with work_in(repo_dir):\n        scripts = find_hook('pre_prompt')\n        if not scripts:\n            return repo_dir\n\n    # Create a temporary directory\n    repo_dir = create_tmp_repo_dir(repo_dir)\n    with work_in(repo_dir):\n        scripts = find_hook('pre_prompt') or []\n        for script in scripts:\n            try:\n                run_script(script, str(repo_dir))\n            except FailedHookException as e:  # noqa: PERF203\n                msg = 'Pre-Prompt Hook script failed'\n                raise FailedHookException(msg) from e\n    return repo_dir", "loc": 22}
{"file": "cookiecutter\\cookiecutter\\log.py", "class_name": null, "function_name": "configure_logger", "parameters": ["stream_level", "debug_file"], "param_types": {"stream_level": "str", "debug_file": "str | None"}, "return_type": "logging.Logger", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["file_handler.setFormatter", "file_handler.setLevel", "logger.addHandler", "logger.setLevel", "logging.FileHandler", "logging.Formatter", "logging.StreamHandler", "logging.getLogger", "stream_handler.setFormatter", "stream_handler.setLevel"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Configure logging for cookiecutter. Set up logging to stdout with given level. If ``debug_file`` is given set up logging to file with DEBUG level.", "source_code": "def configure_logger(\n    stream_level: str = 'DEBUG', debug_file: str | None = None\n) -> logging.Logger:\n    \"\"\"Configure logging for cookiecutter.\n\n    Set up logging to stdout with given level. If ``debug_file`` is given set\n    up logging to file with DEBUG level.\n    \"\"\"\n    # Set up 'cookiecutter' logger\n    logger = logging.getLogger('cookiecutter')\n    logger.setLevel(logging.DEBUG)\n\n    # Remove all attached handlers, in case there was\n    # a logger with using the name 'cookiecutter'\n    del logger.handlers[:]\n\n    # Create a file handler if a log file is provided\n    if debug_file is not None:\n        debug_formatter = logging.Formatter(LOG_FORMATS['DEBUG'])\n        file_handler = logging.FileHandler(debug_file)\n        file_handler.setLevel(LOG_LEVELS['DEBUG'])\n        file_handler.setFormatter(debug_formatter)\n        logger.addHandler(file_handler)\n\n    # Get settings based on the given stream_level\n    log_formatter = logging.Formatter(LOG_FORMATS[stream_level])\n    log_level = LOG_LEVELS[stream_level]\n\n    # Create a stream handler\n    stream_handler = logging.StreamHandler(stream=sys.stdout)\n    stream_handler.setLevel(log_level)\n    stream_handler.setFormatter(log_formatter)\n    logger.addHandler(stream_handler)\n\n    return logger", "loc": 35}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "process_json", "parameters": ["user_value"], "param_types": {"user_value": "str"}, "return_type": null, "param_doc": {"user_value": "User-supplied value to load as a JSON dict"}, "return_doc": "", "raises_doc": [], "called_functions": ["InvalidResponse", "isinstance", "json.loads"], "control_structures": ["If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Load user-supplied value as a JSON dict.", "source_code": "def process_json(user_value: str):\n    \"\"\"Load user-supplied value as a JSON dict.\n\n    :param user_value: User-supplied value to load as a JSON dict\n    \"\"\"\n    try:\n        user_dict = json.loads(user_value, object_pairs_hook=OrderedDict)\n    except Exception as error:\n        # Leave it up to click to ask the user again\n        msg = 'Unable to decode to JSON.'\n        raise InvalidResponse(msg) from error\n\n    if not isinstance(user_dict, dict):\n        # Leave it up to click to ask the user again\n        msg = 'Requires JSON dict.'\n        raise InvalidResponse(msg)\n\n    return user_dict", "loc": 18}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "render_variable", "parameters": ["env", "raw", "cookiecutter_dict"], "param_types": {"env": "Environment", "raw": "_Raw", "cookiecutter_dict": "dict[str, Any]"}, "return_type": "str", "param_doc": {"raw": "The next value to be prompted for by the user."}, "return_doc": "The rendered value for the default variable.", "raises_doc": [], "called_functions": ["env.from_string", "isinstance", "raw.items", "render_variable", "str", "template.render"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Render the next variable to be displayed in the user prompt. Inside the prompting taken from the cookiecutter.json file, this renders the next variable. For example, if a project_name is \"Peanut Butter Cookie\", the repo_name could be be rendered with: `{{ cookiecutter.project_name.replace(\" \", \"_\") }}`. This is then presented to the user as the default.", "source_code": "def render_variable(\n    env: Environment,\n    raw: _Raw,\n    cookiecutter_dict: dict[str, Any],\n) -> str:\n    \"\"\"Render the next variable to be displayed in the user prompt.\n\n    Inside the prompting taken from the cookiecutter.json file, this renders\n    the next variable. For example, if a project_name is \"Peanut Butter\n    Cookie\", the repo_name could be be rendered with:\n\n        `{{ cookiecutter.project_name.replace(\" \", \"_\") }}`.\n\n    This is then presented to the user as the default.\n\n    :param Environment env: A Jinja2 Environment object.\n    :param raw: The next value to be prompted for by the user.\n    :param dict cookiecutter_dict: The current context as it's gradually\n        being populated with variables.\n    :return: The rendered value for the default variable.\n    \"\"\"\n    if raw is None or isinstance(raw, bool):\n        return raw\n    if isinstance(raw, dict):\n        return {\n            render_variable(env, k, cookiecutter_dict): render_variable(\n                env, v, cookiecutter_dict\n            )\n            for k, v in raw.items()\n        }\n    if isinstance(raw, list):\n        return [render_variable(env, v, cookiecutter_dict) for v in raw]\n    if not isinstance(raw, str):\n        raw = str(raw)\n\n    template = env.from_string(raw)\n\n    return template.render(cookiecutter=cookiecutter_dict)", "loc": 38}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "prompt_choice_for_template", "parameters": ["key", "options", "no_input"], "param_types": {"key": "str", "options": "dict", "no_input": "bool"}, "return_type": "OrderedDict[str, Any]", "param_doc": {"no_input": "Do not prompt for user input and return the first available option."}, "return_doc": "", "raises_doc": [], "called_functions": ["_prompts_from_options", "list", "options.keys", "read_user_choice"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Prompt user with a set of options to choose from.", "source_code": "def prompt_choice_for_template(\n    key: str, options: dict, no_input: bool\n) -> OrderedDict[str, Any]:\n    \"\"\"Prompt user with a set of options to choose from.\n\n    :param no_input: Do not prompt for user input and return the first available option.\n    \"\"\"\n    opts = list(options.keys())\n    prompts = {\"templates\": _prompts_from_options(options)}\n    return opts[0] if no_input else read_user_choice(key, opts, prompts, \"\")", "loc": 10}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "prompt_choice_for_config", "parameters": ["cookiecutter_dict", "env", "key", "options", "no_input", "prompts", "prefix"], "param_types": {"cookiecutter_dict": "dict[str, Any]", "env": "Environment", "key": "str", "no_input": "bool", "prefix": "str"}, "return_type": "OrderedDict[str, Any] | str", "param_doc": {"no_input": "Do not prompt for user input and return the first available option."}, "return_doc": "", "raises_doc": [], "called_functions": ["read_user_choice", "render_variable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Prompt user with a set of options to choose from.", "source_code": "def prompt_choice_for_config(\n    cookiecutter_dict: dict[str, Any],\n    env: Environment,\n    key: str,\n    options,\n    no_input: bool,\n    prompts=None,\n    prefix: str = \"\",\n) -> OrderedDict[str, Any] | str:\n    \"\"\"Prompt user with a set of options to choose from.\n\n    :param no_input: Do not prompt for user input and return the first available option.\n    \"\"\"\n    rendered_options = [render_variable(env, raw, cookiecutter_dict) for raw in options]\n    if no_input:\n        return rendered_options[0]\n    return read_user_choice(key, rendered_options, prompts, prefix)", "loc": 17}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "prompt_for_config", "parameters": ["context", "no_input"], "param_types": {"context": "dict[str, Any]", "no_input": "bool"}, "return_type": "OrderedDict[str, Any]", "param_doc": {"no_input": "Do not prompt for user input and use only values from context."}, "return_doc": "", "raises_doc": [], "called_functions": ["OrderedDict", "UndefinedVariableInTemplate", "context['cookiecutter'].items", "context['cookiecutter'].pop", "create_env_with_context", "isinstance", "k.startswith", "key.startswith", "len", "prompt_choice_for_config", "read_user_dict", "read_user_variable", "read_user_yes_no", "render_variable"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Prompt user to enter a new config.", "source_code": "def prompt_for_config(\n    context: dict[str, Any], no_input: bool = False\n) -> OrderedDict[str, Any]:\n    \"\"\"Prompt user to enter a new config.\n\n    :param dict context: Source for field names and sample values.\n    :param no_input: Do not prompt for user input and use only values from context.\n    \"\"\"\n    cookiecutter_dict = OrderedDict([])\n    env = create_env_with_context(context)\n    prompts = context['cookiecutter'].pop('__prompts__', {})\n\n    # First pass: Handle simple and raw variables, plus choices.\n    # These must be done first because the dictionaries keys and\n    # values might refer to them.\n    count = 0\n    all_prompts = context['cookiecutter'].items()\n    visible_prompts = [k for k, _ in all_prompts if not k.startswith(\"_\")]\n    size = len(visible_prompts)\n    for key, raw in all_prompts:\n        if key.startswith('_') and not key.startswith('__'):\n            cookiecutter_dict[key] = raw\n            continue\n        if key.startswith('__'):\n            cookiecutter_dict[key] = render_variable(env, raw, cookiecutter_dict)\n            continue\n\n        if not isinstance(raw, dict):\n            count += 1\n            prefix = f\"  [dim][{count}/{size}][/] \"\n\n        try:\n            if isinstance(raw, list):\n                # We are dealing with a choice variable\n                val = prompt_choice_for_config(\n                    cookiecutter_dict, env, key, raw, no_input, prompts, prefix\n                )\n                cookiecutter_dict[key] = val\n            elif isinstance(raw, bool):\n                # We are dealing with a boolean variable\n                if no_input:\n                    cookiecutter_dict[key] = render_variable(\n                        env, raw, cookiecutter_dict\n                    )\n                else:\n                    cookiecutter_dict[key] = read_user_yes_no(key, raw, prompts, prefix)\n            elif not isinstance(raw, dict):\n                # We are dealing with a regular variable\n                val = render_variable(env, raw, cookiecutter_dict)\n\n                if not no_input:\n                    val = read_user_variable(key, val, prompts, prefix)\n\n                cookiecutter_dict[key] = val\n        except UndefinedError as err:\n            msg = f\"Unable to render variable '{key}'\"\n            raise UndefinedVariableInTemplate(msg, err, context) from err\n\n    # Second pass; handle the dictionaries.\n    for key, raw in context['cookiecutter'].items():\n        # Skip private type dicts not to be rendered.\n        if key.startswith('_') and not key.startswith('__'):\n            continue\n\n        try:\n            if isinstance(raw, dict):\n                # We are dealing with a dict variable\n                count += 1\n                prefix = f\"  [dim][{count}/{size}][/] \"\n                val = render_variable(env, raw, cookiecutter_dict)\n\n                if not no_input and not key.startswith('__'):\n                    val = read_user_dict(key, val, prompts, prefix)\n\n                cookiecutter_dict[key] = val\n        except UndefinedError as err:\n            msg = f\"Unable to render variable '{key}'\"\n            raise UndefinedVariableInTemplate(msg, err, context) from err\n\n    return cookiecutter_dict", "loc": 80}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": null, "function_name": "choose_nested_template", "parameters": ["context", "repo_dir", "no_input"], "param_types": {"context": "dict[str, Any]", "repo_dir": "Path | str", "no_input": "bool"}, "return_type": "str", "param_doc": {"context": "Source for field names and sample values.", "repo_dir": "Repository directory.", "no_input": "Do not prompt for user input and use only values from context."}, "return_doc": "Path to the selected template.", "raises_doc": [], "called_functions": ["OrderedDict", "Path", "Path(repo_dir).resolve", "ValueError", "context['cookiecutter'].get", "context['cookiecutter'].pop", "create_env_with_context", "prompt_choice_for_config", "prompt_choice_for_template", "re.search", "re.search('\\\\((.+)\\\\)', val).group", "repo_dir / template.resolve", "template.is_absolute"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Prompt user to select the nested template to use.", "source_code": "def choose_nested_template(\n    context: dict[str, Any], repo_dir: Path | str, no_input: bool = False\n) -> str:\n    \"\"\"Prompt user to select the nested template to use.\n\n    :param context: Source for field names and sample values.\n    :param repo_dir: Repository directory.\n    :param no_input: Do not prompt for user input and use only values from context.\n    :returns: Path to the selected template.\n    \"\"\"\n    cookiecutter_dict: OrderedDict[str, Any] = OrderedDict([])\n    env = create_env_with_context(context)\n    prefix = \"\"\n    prompts = context['cookiecutter'].pop('__prompts__', {})\n    key = \"templates\"\n    config = context['cookiecutter'].get(key, {})\n    if config:\n        # Pass\n        val = prompt_choice_for_template(key, config, no_input)\n        template = config[val][\"path\"]\n    else:\n        # Old style\n        key = \"template\"\n        config = context['cookiecutter'].get(key, [])\n        val = prompt_choice_for_config(\n            cookiecutter_dict, env, key, config, no_input, prompts, prefix\n        )\n        template = re.search(r'\\((.+)\\)', val).group(1)\n\n    template = Path(template) if template else None\n    if not (template and not template.is_absolute()):\n        msg = \"Illegal template path\"\n        raise ValueError(msg)\n\n    repo_dir = Path(repo_dir).resolve()\n    template_path = (repo_dir / template).resolve()\n    # Return path as string\n    return f\"{template_path}\"", "loc": 38}
{"file": "cookiecutter\\cookiecutter\\prompt.py", "class_name": "YesNoPrompt", "function_name": "process_response", "parameters": ["self", "value"], "param_types": {"value": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["InvalidResponse", "value.strip", "value.strip().lower"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert choices to a bool.", "source_code": "def process_response(self, value: str) -> bool:\n    \"\"\"Convert choices to a bool.\"\"\"\n    value = value.strip().lower()\n    if value in self.yes_choices:\n        return True\n    if value in self.no_choices:\n        return False\n    raise InvalidResponse(self.validate_error_message)", "loc": 8}
{"file": "cookiecutter\\cookiecutter\\replay.py", "class_name": null, "function_name": "dump", "parameters": ["replay_dir", "template_name", "context"], "param_types": {"replay_dir": "Path | str", "template_name": "str", "context": "dict[str, Any]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "get_file_name", "json.dump", "make_sure_path_exists", "open"], "control_structures": ["If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Write json data to file.", "source_code": "def dump(replay_dir: Path | str, template_name: str, context: dict[str, Any]) -> None:\n    \"\"\"Write json data to file.\"\"\"\n    make_sure_path_exists(replay_dir)\n\n    if 'cookiecutter' not in context:\n        msg = 'Context is required to contain a cookiecutter key'\n        raise ValueError(msg)\n\n    replay_file = get_file_name(replay_dir, template_name)\n\n    with open(replay_file, 'w', encoding=\"utf-8\") as outfile:\n        json.dump(context, outfile, indent=2)", "loc": 12}
{"file": "cookiecutter\\cookiecutter\\replay.py", "class_name": null, "function_name": "load", "parameters": ["replay_dir", "template_name"], "param_types": {"replay_dir": "Path | str", "template_name": "str"}, "return_type": "dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "get_file_name", "json.load", "open"], "control_structures": ["If"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Read json data from file.", "source_code": "def load(replay_dir: Path | str, template_name: str) -> dict[str, Any]:\n    \"\"\"Read json data from file.\"\"\"\n    replay_file = get_file_name(replay_dir, template_name)\n\n    with open(replay_file, encoding=\"utf-8\") as infile:\n        context: dict[str, Any] = json.load(infile)\n\n    if 'cookiecutter' not in context:\n        msg = 'Context is required to contain a cookiecutter key'\n        raise ValueError(msg)\n\n    return context", "loc": 12}
{"file": "cookiecutter\\cookiecutter\\repository.py", "class_name": null, "function_name": "expand_abbreviations", "parameters": ["template", "abbreviations"], "param_types": {"template": "str", "abbreviations": "dict[str, str]"}, "return_type": "str", "param_doc": {"template": "The project template name.", "abbreviations": "Abbreviation definitions."}, "return_doc": "", "raises_doc": [], "called_functions": ["abbreviations[prefix].format", "template.partition"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Expand abbreviations in a template name.", "source_code": "def expand_abbreviations(template: str, abbreviations: dict[str, str]) -> str:\n    \"\"\"Expand abbreviations in a template name.\n\n    :param template: The project template name.\n    :param abbreviations: Abbreviation definitions.\n    \"\"\"\n    if template in abbreviations:\n        return abbreviations[template]\n\n    # Split on colon. If there is no colon, rest will be empty\n    # and prefix will be the whole template\n    prefix, _sep, rest = template.partition(':')\n    if prefix in abbreviations:\n        return abbreviations[prefix].format(rest)\n\n    return template", "loc": 16}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "force_delete", "parameters": ["func", "path", "_exc_info"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "os.chmod"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Error handler for `shutil.rmtree()` equivalent to `rm -rf`. Usage: `shutil.rmtree(path, onerror=force_delete)` From https://docs.python.org/3/library/shutil.html#rmtree-example", "source_code": "def force_delete(func, path, _exc_info) -> None:  # type: ignore[no-untyped-def]\n    \"\"\"Error handler for `shutil.rmtree()` equivalent to `rm -rf`.\n\n    Usage: `shutil.rmtree(path, onerror=force_delete)`\n    From https://docs.python.org/3/library/shutil.html#rmtree-example\n    \"\"\"\n    os.chmod(path, stat.S_IWRITE)\n    func(path)", "loc": 8}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "make_sure_path_exists", "parameters": ["path"], "param_types": {"path": "Path | str"}, "return_type": "None", "param_doc": {"path": "A directory tree path for creation."}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "Path", "Path(path).mkdir", "logger.debug"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Ensure that a directory exists.", "source_code": "def make_sure_path_exists(path: Path | str) -> None:\n    \"\"\"Ensure that a directory exists.\n\n    :param path: A directory tree path for creation.\n    \"\"\"\n    logger.debug('Making sure path exists (creates tree if not exist): %s', path)\n    try:\n        Path(path).mkdir(parents=True, exist_ok=True)\n    except OSError as error:\n        msg = f'Unable to create directory at {path}'\n        raise OSError(msg) from error", "loc": 11}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "work_in", "parameters": ["dirname"], "param_types": {"dirname": "Path | str | None"}, "return_type": "Iterator[None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.chdir", "os.getcwd"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Context manager version of os.chdir. When exited, returns to the working directory prior to entering.", "source_code": "def work_in(dirname: Path | str | None = None) -> Iterator[None]:\n    \"\"\"Context manager version of os.chdir.\n\n    When exited, returns to the working directory prior to entering.\n    \"\"\"\n    curdir = os.getcwd()\n    try:\n        if dirname is not None:\n            os.chdir(dirname)\n        yield\n    finally:\n        os.chdir(curdir)", "loc": 12}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "make_executable", "parameters": ["script_path"], "param_types": {"script_path": "Path | str"}, "return_type": "None", "param_doc": {"script_path": "The file to change"}, "return_doc": "", "raises_doc": [], "called_functions": ["os.chmod", "os.stat"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Make `script_path` executable.", "source_code": "def make_executable(script_path: Path | str) -> None:\n    \"\"\"Make `script_path` executable.\n\n    :param script_path: The file to change\n    \"\"\"\n    status = os.stat(script_path)\n    os.chmod(script_path, status.st_mode | stat.S_IEXEC)", "loc": 7}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "simple_filter", "parameters": ["filter_function"], "param_types": {}, "return_type": "type[Extension]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().__init__"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Decorate a function to wrap it in a simplified jinja2 extension.", "source_code": "def simple_filter(filter_function) -> type[Extension]:  # type: ignore[no-untyped-def]\n    \"\"\"Decorate a function to wrap it in a simplified jinja2 extension.\"\"\"\n\n    class SimpleFilterExtension(Extension):\n        def __init__(self, environment: Environment) -> None:\n            super().__init__(environment)\n            environment.filters[filter_function.__name__] = filter_function\n\n    SimpleFilterExtension.__name__ = filter_function.__name__\n    return SimpleFilterExtension", "loc": 10}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "create_tmp_repo_dir", "parameters": ["repo_dir"], "param_types": {"repo_dir": "Path | str"}, "return_type": "Path", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Path", "Path(repo_dir).resolve", "logger.debug", "shutil.copytree", "tempfile.mkdtemp"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create a temporary dir with a copy of the contents of repo_dir.", "source_code": "def create_tmp_repo_dir(repo_dir: Path | str) -> Path:\n    \"\"\"Create a temporary dir with a copy of the contents of repo_dir.\"\"\"\n    repo_dir = Path(repo_dir).resolve()\n    base_dir = tempfile.mkdtemp(prefix='cookiecutter')\n    new_dir = f\"{base_dir}/{repo_dir.name}\"\n    logger.debug(f'Copying repo_dir from {repo_dir} to {new_dir}')\n    shutil.copytree(repo_dir, new_dir)\n    return Path(new_dir)", "loc": 8}
{"file": "cookiecutter\\cookiecutter\\utils.py", "class_name": null, "function_name": "create_env_with_context", "parameters": ["context"], "param_types": {"context": "dict[str, Any]"}, "return_type": "StrictEnvironment", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["StrictEnvironment", "context.get", "context.get('cookiecutter', {}).get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Create a jinja environment using the provided context.", "source_code": "def create_env_with_context(context: dict[str, Any]) -> StrictEnvironment:\n    \"\"\"Create a jinja environment using the provided context.\"\"\"\n    envvars = context.get('cookiecutter', {}).get('_jinja2_env_vars', {})\n\n    return StrictEnvironment(context=context, keep_trailing_newline=True, **envvars)", "loc": 5}
{"file": "cookiecutter\\cookiecutter\\vcs.py", "class_name": null, "function_name": "identify_repo", "parameters": ["repo_url"], "param_types": {"repo_url": "str"}, "return_type": "tuple[Literal['git', 'hg'], str]", "param_doc": {"repo_url": "Repo URL of unknown type."}, "return_doc": "('git', repo_url), ('hg', repo_url), or None.", "raises_doc": [], "called_functions": ["len", "repo_url.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Determine if `repo_url` should be treated as a URL to a git or hg repo. Repos can be identified by prepending \"hg+\" or \"git+\" to the repo URL.", "source_code": "def identify_repo(repo_url: str) -> tuple[Literal[\"git\", \"hg\"], str]:\n    \"\"\"Determine if `repo_url` should be treated as a URL to a git or hg repo.\n\n    Repos can be identified by prepending \"hg+\" or \"git+\" to the repo URL.\n\n    :param repo_url: Repo URL of unknown type.\n    :returns: ('git', repo_url), ('hg', repo_url), or None.\n    \"\"\"\n    repo_url_values = repo_url.split('+')\n    if len(repo_url_values) == 2:\n        repo_type = repo_url_values[0]\n        if repo_type in [\"git\", \"hg\"]:\n            return repo_type, repo_url_values[1]  # type: ignore[return-value]\n        raise UnknownRepoType\n    if 'git' in repo_url:\n        return 'git', repo_url\n    if 'bitbucket' in repo_url:\n        return 'hg', repo_url\n    raise UnknownRepoType", "loc": 19}
{"file": "cookiecutter\\cookiecutter\\vcs.py", "class_name": null, "function_name": "is_vcs_installed", "parameters": ["repo_type"], "param_types": {"repo_type": "str"}, "return_type": "bool", "param_doc": {"repo_type": ""}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "which"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Check if the version control system for a repo type is installed.", "source_code": "def is_vcs_installed(repo_type: str) -> bool:\n    \"\"\"\n    Check if the version control system for a repo type is installed.\n\n    :param repo_type:\n    \"\"\"\n    return bool(which(repo_type))", "loc": 7}
{"file": "PySnooper\\misc\\generate_authors.py", "class_name": null, "function_name": "drop_recurrences", "parameters": ["iterable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["s.add", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def drop_recurrences(iterable):\n    s = set()\n    for item in iterable:\n        if item not in s:\n            s.add(item)\n            yield item", "loc": 6}
{"file": "PySnooper\\misc\\generate_authors.py", "class_name": null, "function_name": "iterate_authors_by_chronological_order", "parameters": ["branch"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["drop_recurrences", "line.strip", "line.strip().split", "log_call.stdout.decode", "log_call.stdout.decode('utf-8').split", "subprocess.run", "tuple"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def iterate_authors_by_chronological_order(branch):\n    log_call = subprocess.run(\n        (\n            'git', 'log', branch, '--encoding=utf-8', '--full-history',\n            '--reverse', '--format=format:%at;%an;%ae'\n        ),\n        stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n    )\n    log_lines = log_call.stdout.decode('utf-8').split('\\n')\n\n    authors = tuple(line.strip().split(\";\")[1] for line in log_lines)\n    authors = (author for author in authors if author not in deny_list)\n    return drop_recurrences(authors)", "loc": 13}
{"file": "PySnooper\\pysnooper\\pycompat.py", "class_name": null, "function_name": "timedelta_parse", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime_module.timedelta", "map", "s.replace", "s.replace('.', ':').split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def timedelta_parse(s):\n    hours, minutes, seconds, microseconds = map(\n        int,\n        s.replace('.', ':').split(':')\n    )\n    return datetime_module.timedelta(hours=hours, minutes=minutes,\n                                     seconds=seconds,\n                                     microseconds=microseconds)", "loc": 8}
{"file": "PySnooper\\pysnooper\\pycompat.py", "class_name": null, "function_name": "time_isoformat", "parameters": ["time", "timespec"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{:02d}:{:02d}:{:02d}.{:06d}'.format", "isinstance", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def time_isoformat(time, timespec='microseconds'):\n    assert isinstance(time, datetime_module.time)\n    if timespec != 'microseconds':\n        raise NotImplementedError\n    result = '{:02d}:{:02d}:{:02d}.{:06d}'.format(\n        time.hour, time.minute, time.second, time.microsecond\n    )\n    assert len(result) == 15\n    return result", "loc": 9}
{"file": "PySnooper\\pysnooper\\tracer.py", "class_name": null, "function_name": "get_local_reprs", "parameters": ["frame", "watch", "custom_repr", "max_length", "normalize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collections.OrderedDict", "frame.f_locals.items", "frame.f_locals.keys", "result.update", "result_items.sort", "sorted", "tuple", "utils.get_shortish_repr", "variable.items", "vars_order.index"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_local_reprs(frame, watch=(), custom_repr=(), max_length=None, normalize=False):\n    code = frame.f_code\n    vars_order = (code.co_varnames + code.co_cellvars + code.co_freevars +\n                  tuple(frame.f_locals.keys()))\n\n    result_items = [(key, utils.get_shortish_repr(value, custom_repr,\n                                                  max_length, normalize))\n                    for key, value in frame.f_locals.items()]\n    result_items.sort(key=lambda key_value: vars_order.index(key_value[0]))\n    result = collections.OrderedDict(result_items)\n\n    for variable in watch:\n        result.update(sorted(variable.items(frame, normalize)))\n    return result", "loc": 14}
{"file": "PySnooper\\pysnooper\\tracer.py", "class_name": null, "function_name": "get_write_function", "parameters": ["output", "overwrite"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "FileWriter", "callable", "isinstance", "output.write", "stderr.write", "utils.shitcode"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_write_function(output, overwrite):\n    is_path = isinstance(output, (pycompat.PathLike, str))\n    if overwrite and not is_path:\n        raise Exception('`overwrite=True` can only be used when writing '\n                        'content to file.')\n    if output is None:\n        def write(s):\n            stderr = sys.stderr\n            try:\n                stderr.write(s)\n            except UnicodeEncodeError:\n                # God damn Python 2\n                stderr.write(utils.shitcode(s))\n    elif is_path:\n        return FileWriter(output, overwrite).write\n    elif callable(output):\n        write = output\n    else:\n        assert isinstance(output, utils.WritableStream)\n\n        def write(s):\n            output.write(s)\n    return write", "loc": 23}
{"file": "PySnooper\\pysnooper\\tracer.py", "class_name": "Tracer", "function_name": "set_thread_info_padding", "parameters": ["self", "thread_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "max", "thread_info.ljust"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_thread_info_padding(self, thread_info):\n    current_thread_len = len(thread_info)\n    self.thread_info_padding = max(self.thread_info_padding,\n                                   current_thread_len)\n    return thread_info.ljust(self.thread_info_padding)", "loc": 5}
{"file": "PySnooper\\pysnooper\\tracer.py", "class_name": null, "function_name": "write", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["stderr.write", "utils.shitcode"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write(s):\n    stderr = sys.stderr\n    try:\n        stderr.write(s)\n    except UnicodeEncodeError:\n        # God damn Python 2\n        stderr.write(utils.shitcode(s))", "loc": 7}
{"file": "PySnooper\\pysnooper\\tracer.py", "class_name": null, "function_name": "generator_wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["function", "functools.wraps", "method"], "control_structures": ["Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generator_wrapper(*args, **kwargs):\n    gen = function(*args, **kwargs)\n    method, incoming = gen.send, None\n    while True:\n        with self:\n            try:\n                outgoing = method(incoming)\n            except StopIteration:\n                return\n        try:\n            method, incoming = gen.send, (yield outgoing)\n        except Exception as e:\n            method, incoming = gen.throw, e", "loc": 13}
{"file": "PySnooper\\pysnooper\\utils.py", "class_name": null, "function_name": "get_repr_function", "parameters": ["item", "custom_repr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["condition", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_repr_function(item, custom_repr):\n    for condition, action in custom_repr:\n        if isinstance(condition, type):\n            condition = lambda x, y=condition: isinstance(x, y)\n        if condition(item):\n            return action\n    return repr", "loc": 7}
{"file": "PySnooper\\pysnooper\\utils.py", "class_name": null, "function_name": "get_shortish_repr", "parameters": ["item", "custom_repr", "max_length", "normalize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_repr_function", "normalize_repr", "r.replace", "r.replace('\\r', '').replace", "repr_function", "truncate"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_shortish_repr(item, custom_repr=(), max_length=None, normalize=False):\n    repr_function = get_repr_function(item, custom_repr)\n    try:\n        r = repr_function(item)\n    except Exception:\n        r = 'REPR FAILED'\n    r = r.replace('\\r', '').replace('\\n', '')\n    if normalize:\n        r = normalize_repr(r)\n    if max_length:\n        r = truncate(r, max_length)\n    return r", "loc": 12}
{"file": "PySnooper\\pysnooper\\utils.py", "class_name": null, "function_name": "truncate", "parameters": ["string", "max_length"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "u'{}...{}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def truncate(string, max_length):\n    if (max_length is None) or (len(string) <= max_length):\n        return string\n    else:\n        left = (max_length - 3) // 2\n        right = max_length - 3 - left\n        return u'{}...{}'.format(string[:left], string[-right:])", "loc": 7}
{"file": "PySnooper\\pysnooper\\utils.py", "class_name": null, "function_name": "ensure_tuple", "parameters": ["x"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ensure_tuple(x):\n    if isinstance(x, collections_abc.Iterable) and \\\n                                               not isinstance(x, string_types):\n        return tuple(x)\n    else:\n        return (x,)", "loc": 6}
{"file": "PySnooper\\pysnooper\\variables.py", "class_name": null, "function_name": "needs_parentheses", "parameters": ["source"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'({}).x'.format", "'{}.x'.format", "code", "compile"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def needs_parentheses(source):\n    def code(s):\n        return compile(s, '<variable>', 'eval').co_code\n\n    return code('{}.x'.format(source)) != code('({}).x'.format(source))", "loc": 5}
{"file": "PySnooper\\pysnooper\\variables.py", "class_name": "BaseVariable", "function_name": "items", "parameters": ["self", "frame", "normalize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["eval", "self._items"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def items(self, frame, normalize=False):\n    try:\n        main_value = eval(self.code, frame.f_globals or {}, frame.f_locals)\n    except Exception:\n        return ()\n    return self._items(main_value, normalize)", "loc": 6}
{"file": "thefuck\\fastentrypoints.py", "class_name": null, "function_name": "get_args", "parameters": ["cls", "dist", "header"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "TEMPLATE.format", "ValueError", "cls._get_script_args", "cls.get_header", "dist.as_requirement", "dist.get_entry_map", "dist.get_entry_map(group).items", "re.search", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yield write_script() argument tuples for a distribution's console_scripts and gui_scripts entry points.", "source_code": "def get_args(cls, dist, header=None):\n    \"\"\"\n    Yield write_script() argument tuples for a distribution's\n    console_scripts and gui_scripts entry points.\n    \"\"\"\n    if header is None:\n        header = cls.get_header()\n    spec = str(dist.as_requirement())\n    for type_ in 'console', 'gui':\n        group = type_ + '_scripts'\n        for name, ep in dist.get_entry_map(group).items():\n            # ensure_safe_name\n            if re.search(r'[\\\\/]', name):\n                raise ValueError(\"Path separators not allowed in script names\")\n            script_text = TEMPLATE.format(\n                          ep.module_name, ep.attrs[0], '.'.join(ep.attrs),\n                          spec, group, name)\n            args = cls._get_script_args(type_, name, header, script_text)\n            for res in args:\n                yield res", "loc": 20}
{"file": "thefuck\\thefuck\\conf.py", "class_name": null, "function_name": "load_source", "parameters": ["name", "pathname", "_file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["importlib.util.module_from_spec", "importlib.util.spec_from_file_location", "module_spec.loader.exec_module"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load_source(name, pathname, _file=None):\n    module_spec = importlib.util.spec_from_file_location(name, pathname)\n    module = importlib.util.module_from_spec(module_spec)\n    module_spec.loader.exec_module(module)\n    return module", "loc": 5}
{"file": "thefuck\\thefuck\\conf.py", "class_name": "Settings", "function_name": "init", "parameters": ["self", "args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["exception", "self._init_settings_file", "self._settings_from_args", "self._settings_from_env", "self._settings_from_file", "self._setup_user_dir", "self.update", "sys.exc_info"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Fills `settings` with values from `settings.py` and env.", "source_code": "def init(self, args=None):\n    \"\"\"Fills `settings` with values from `settings.py` and env.\"\"\"\n    from .logs import exception\n\n    self._setup_user_dir()\n    self._init_settings_file()\n\n    try:\n        self.update(self._settings_from_file())\n    except Exception:\n        exception(\"Can't load settings from file\", sys.exc_info())\n\n    try:\n        self.update(self._settings_from_env())\n    except Exception:\n        exception(\"Can't load settings from env\", sys.exc_info())\n\n    self.update(self._settings_from_args(args))", "loc": 18}
{"file": "thefuck\\thefuck\\corrector.py", "class_name": null, "function_name": "get_loaded_rules", "parameters": ["rules_paths"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Rule.from_path"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yields all available rules. :type rules_paths: [Path] :rtype: Iterable[Rule]", "source_code": "def get_loaded_rules(rules_paths):\n    \"\"\"Yields all available rules.\n\n    :type rules_paths: [Path]\n    :rtype: Iterable[Rule]\n\n    \"\"\"\n    for path in rules_paths:\n        if path.name != '__init__.py':\n            rule = Rule.from_path(path)\n            if rule and rule.is_enabled:\n                yield rule", "loc": 12}
{"file": "thefuck\\thefuck\\corrector.py", "class_name": null, "function_name": "get_rules_import_paths", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Path", "Path(__file__).parent.joinpath", "Path(path).glob", "contrib_module.joinpath", "contrib_rules.is_dir", "settings.user_dir.joinpath"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Yields all rules import paths. :rtype: Iterable[Path]", "source_code": "def get_rules_import_paths():\n    \"\"\"Yields all rules import paths.\n\n    :rtype: Iterable[Path]\n\n    \"\"\"\n    # Bundled rules:\n    yield Path(__file__).parent.joinpath('rules')\n    # Rules defined by user:\n    yield settings.user_dir.joinpath('rules')\n    # Packages with third-party rules:\n    for path in sys.path:\n        for contrib_module in Path(path).glob('thefuck_contrib_*'):\n            contrib_rules = contrib_module.joinpath('rules')\n            if contrib_rules.is_dir():\n                yield contrib_rules", "loc": 16}
{"file": "thefuck\\thefuck\\corrector.py", "class_name": null, "function_name": "get_rules", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_loaded_rules", "get_rules_import_paths", "path.glob", "sorted"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_rules():\n    \"\"\"Returns all enabled rules.\n\n    :rtype: [Rule]\n\n    \"\"\"\n    paths = [rule_path for path in get_rules_import_paths()\n             for rule_path in sorted(path.glob('*.py'))]\n    return sorted(get_loaded_rules(paths),\n                  key=lambda rule: rule.priority)", "loc": 10}
{"file": "thefuck\\thefuck\\corrector.py", "class_name": null, "function_name": "organize_commands", "parameters": ["corrected_commands"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "logs.debug", "next", "sorted", "u'Corrected commands: {}'.format", "u'{}'.format"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Yields sorted commands without duplicates. :type corrected_commands: Iterable[thefuck.types.CorrectedCommand] :rtype: Iterable[thefuck.types.CorrectedCommand]", "source_code": "def organize_commands(corrected_commands):\n    \"\"\"Yields sorted commands without duplicates.\n\n    :type corrected_commands: Iterable[thefuck.types.CorrectedCommand]\n    :rtype: Iterable[thefuck.types.CorrectedCommand]\n\n    \"\"\"\n    try:\n        first_command = next(corrected_commands)\n        yield first_command\n    except StopIteration:\n        return\n\n    without_duplicates = {\n        command for command in sorted(\n            corrected_commands, key=lambda command: command.priority)\n        if command != first_command}\n\n    sorted_commands = sorted(\n        without_duplicates,\n        key=lambda corrected_command: corrected_command.priority)\n\n    logs.debug(u'Corrected commands: {}'.format(\n        ', '.join(u'{}'.format(cmd) for cmd in [first_command] + sorted_commands)))\n\n    for command in sorted_commands:\n        yield command", "loc": 27}
{"file": "thefuck\\thefuck\\corrector.py", "class_name": null, "function_name": "get_corrected_commands", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_rules", "organize_commands", "rule.get_corrected_commands", "rule.is_match"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_corrected_commands(command):\n    \"\"\"Returns generator with sorted and unique corrected commands.\n\n    :type command: thefuck.types.Command\n    :rtype: Iterable[thefuck.types.CorrectedCommand]\n\n    \"\"\"\n    corrected_commands = (\n        corrected for rule in get_rules()\n        if rule.is_match(command)\n        for corrected in rule.get_corrected_commands(command))\n    return organize_commands(corrected_commands)", "loc": 12}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "color", "parameters": ["color_"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Utility for ability to disabling colored output.", "source_code": "def color(color_):\n    \"\"\"Utility for ability to disabling colored output.\"\"\"\n    if settings.no_colors:\n        return ''\n    else:\n        return color_", "loc": 6}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "warn", "parameters": ["title"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "sys.stderr.write", "u'{warn}[WARN] {title}{reset}\\n'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def warn(title):\n    sys.stderr.write(u'{warn}[WARN] {title}{reset}\\n'.format(\n        warn=color(colorama.Back.RED + colorama.Fore.WHITE\n                   + colorama.Style.BRIGHT),\n        reset=color(colorama.Style.RESET_ALL),\n        title=title))", "loc": 6}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "exception", "parameters": ["title", "exc_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "color", "format_exception", "sys.stderr.write", "u'{warn}[WARN] {title}:{reset}\\n{trace}{warn}----------------------------{reset}\\n\\n'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exception(title, exc_info):\n    sys.stderr.write(\n        u'{warn}[WARN] {title}:{reset}\\n{trace}'\n        u'{warn}----------------------------{reset}\\n\\n'.format(\n            warn=color(colorama.Back.RED + colorama.Fore.WHITE\n                       + colorama.Style.BRIGHT),\n            reset=color(colorama.Style.RESET_ALL),\n            title=title,\n            trace=''.join(format_exception(*exc_info))))", "loc": 9}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "failed", "parameters": ["msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "sys.stderr.write", "u'{red}{msg}{reset}\\n'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def failed(msg):\n    sys.stderr.write(u'{red}{msg}{reset}\\n'.format(\n        msg=msg,\n        red=color(colorama.Fore.RED),\n        reset=color(colorama.Style.RESET_ALL)))", "loc": 5}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "show_corrected_command", "parameters": ["corrected_command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "sys.stderr.write", "u'{prefix}{bold}{script}{reset}{side_effect}\\n'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def show_corrected_command(corrected_command):\n    sys.stderr.write(u'{prefix}{bold}{script}{reset}{side_effect}\\n'.format(\n        prefix=const.USER_COMMAND_MARK,\n        script=corrected_command.script,\n        side_effect=u' (+side effect)' if corrected_command.side_effect else u'',\n        bold=color(colorama.Style.BRIGHT),\n        reset=color(colorama.Style.RESET_ALL)))", "loc": 7}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "confirm_text", "parameters": ["corrected_command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "sys.stderr.write", "u'{prefix}{clear}{bold}{script}{reset}{side_effect} [{green}enter{reset}/{blue}↑{reset}/{blue}↓{reset}/{red}ctrl+c{reset}]'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def confirm_text(corrected_command):\n    sys.stderr.write(\n        (u'{prefix}{clear}{bold}{script}{reset}{side_effect} '\n         u'[{green}enter{reset}/{blue}↑{reset}/{blue}↓{reset}'\n         u'/{red}ctrl+c{reset}]').format(\n            prefix=const.USER_COMMAND_MARK,\n            script=corrected_command.script,\n            side_effect=' (+side effect)' if corrected_command.side_effect else '',\n            clear='\\033[1K\\r',\n            bold=color(colorama.Style.BRIGHT),\n            green=color(colorama.Fore.GREEN),\n            red=color(colorama.Fore.RED),\n            reset=color(colorama.Style.RESET_ALL),\n            blue=color(colorama.Fore.BLUE)))", "loc": 14}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "debug", "parameters": ["msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "sys.stderr.write", "u'{blue}{bold}DEBUG:{reset} {msg}\\n'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def debug(msg):\n    if settings.debug:\n        sys.stderr.write(u'{blue}{bold}DEBUG:{reset} {msg}\\n'.format(\n            msg=msg,\n            reset=color(colorama.Style.RESET_ALL),\n            blue=color(colorama.Fore.BLUE),\n            bold=color(colorama.Style.BRIGHT)))", "loc": 7}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "debug_time", "parameters": ["msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.now", "debug", "u'{} took: {}'.format"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def debug_time(msg):\n    started = datetime.now()\n    try:\n        yield\n    finally:\n        debug(u'{} took: {}'.format(msg, datetime.now() - started))", "loc": 6}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "how_to_configure_alias", "parameters": ["configuration_details"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "configuration_details._asdict", "print", "u\"Seems like {bold}fuck{reset} alias isn't configured!\".format", "u'Or run {bold}fuck{reset} a second time to configure it automatically.'.format", "u'Please put {bold}{content}{reset} in your {bold}{path}{reset} and apply changes with {bold}{reload}{reset} or restart your shell.'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def how_to_configure_alias(configuration_details):\n    print(u\"Seems like {bold}fuck{reset} alias isn't configured!\".format(\n        bold=color(colorama.Style.BRIGHT),\n        reset=color(colorama.Style.RESET_ALL)))\n\n    if configuration_details:\n        print(\n            u\"Please put {bold}{content}{reset} in your \"\n            u\"{bold}{path}{reset} and apply \"\n            u\"changes with {bold}{reload}{reset} or restart your shell.\".format(\n                bold=color(colorama.Style.BRIGHT),\n                reset=color(colorama.Style.RESET_ALL),\n                **configuration_details._asdict()))\n\n        if configuration_details.can_configure_automatically:\n            print(\n                u\"Or run {bold}fuck{reset} a second time to configure\"\n                u\" it automatically.\".format(\n                    bold=color(colorama.Style.BRIGHT),\n                    reset=color(colorama.Style.RESET_ALL)))\n\n    print(u'More details - https://github.com/nvbn/thefuck#manual-installation')", "loc": 22}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "already_configured", "parameters": ["configuration_details"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "print", "u'Seems like {bold}fuck{reset} alias already configured!\\nFor applying changes run {bold}{reload}{reset} or restart your shell.'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def already_configured(configuration_details):\n    print(\n        u\"Seems like {bold}fuck{reset} alias already configured!\\n\"\n        u\"For applying changes run {bold}{reload}{reset}\"\n        u\" or restart your shell.\".format(\n            bold=color(colorama.Style.BRIGHT),\n            reset=color(colorama.Style.RESET_ALL),\n            reload=configuration_details.reload))", "loc": 8}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "configured_successfully", "parameters": ["configuration_details"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["color", "print", "u'{bold}fuck{reset} alias configured successfully!\\nFor applying changes run {bold}{reload}{reset} or restart your shell.'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def configured_successfully(configuration_details):\n    print(\n        u\"{bold}fuck{reset} alias configured successfully!\\n\"\n        u\"For applying changes run {bold}{reload}{reset}\"\n        u\" or restart your shell.\".format(\n            bold=color(colorama.Style.BRIGHT),\n            reset=color(colorama.Style.RESET_ALL),\n            reload=configuration_details.reload))", "loc": 8}
{"file": "thefuck\\thefuck\\logs.py", "class_name": null, "function_name": "version", "parameters": ["thefuck_version", "python_version", "shell_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["sys.stderr.write", "u'The Fuck {} using Python {} and {}\\n'.format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def version(thefuck_version, python_version, shell_info):\n    sys.stderr.write(\n        u'The Fuck {} using Python {} and {}\\n'.format(thefuck_version,\n                                                       python_version,\n                                                       shell_info))", "loc": 5}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Command", "function_name": "script_parts", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "logs.debug", "shell.split_command", "sys.exc_info", "u\"Can't split command script {} because:\\n {}\".format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def script_parts(self):\n    if not hasattr(self, '_script_parts'):\n        try:\n            self._script_parts = shell.split_command(self.script)\n        except Exception:\n            logs.debug(u\"Can't split command script {} because:\\n {}\".format(\n                self, sys.exc_info()))\n            self._script_parts = []\n\n    return self._script_parts", "loc": 10}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Command", "function_name": "update", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Command", "kwargs.setdefault"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update(self, **kwargs):\n    \"\"\"Returns new command with replaced fields.\n\n    :rtype: Command\n\n    \"\"\"\n    kwargs.setdefault('script', self.script)\n    kwargs.setdefault('output', self.output)\n    return Command(**kwargs)", "loc": 9}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Command", "function_name": "from_raw_script", "parameters": ["cls", "raw_script"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "format_raw_script", "get_output", "shell.from_shell"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Creates instance of `Command` from a list of script parts. :type raw_script: [basestring] :rtype: Command", "source_code": "def from_raw_script(cls, raw_script):\n    \"\"\"Creates instance of `Command` from a list of script parts.\n\n    :type raw_script: [basestring]\n    :rtype: Command\n    :raises: EmptyCommand\n\n    \"\"\"\n    script = format_raw_script(raw_script)\n    if not script:\n        raise EmptyCommand\n\n    expanded = shell.from_shell(script)\n    output = get_output(script, expanded)\n    return cls(expanded, output)", "loc": 15}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Rule", "function_name": "from_path", "parameters": ["cls", "path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "getattr", "load_source", "logs.debug", "logs.debug_time", "logs.exception", "settings.priority.get", "str", "sys.exc_info", "u'Ignoring excluded rule: {}'.format", "u'Importing rule: {};'.format", "u'Rule {} failed to load'.format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Creates rule instance from path. :type path: pathlib.Path :rtype: Rule", "source_code": "def from_path(cls, path):\n    \"\"\"Creates rule instance from path.\n\n    :type path: pathlib.Path\n    :rtype: Rule\n\n    \"\"\"\n    name = path.name[:-3]\n    if name in settings.exclude_rules:\n        logs.debug(u'Ignoring excluded rule: {}'.format(name))\n        return\n    with logs.debug_time(u'Importing rule: {};'.format(name)):\n        try:\n            rule_module = load_source(name, str(path))\n        except Exception:\n            logs.exception(u\"Rule {} failed to load\".format(name), sys.exc_info())\n            return\n    priority = getattr(rule_module, 'priority', DEFAULT_PRIORITY)\n    return cls(name, rule_module.match,\n               rule_module.get_new_command,\n               getattr(rule_module, 'enabled_by_default', True),\n               getattr(rule_module, 'side_effect', None),\n               settings.priority.get(name, priority),\n               getattr(rule_module, 'requires_output', True))", "loc": 24}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Rule", "function_name": "is_match", "parameters": ["self", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["logs.debug_time", "logs.rule_failed", "self.match", "sys.exc_info", "u'Trying rule: {};'.format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_match(self, command):\n    \"\"\"Returns `True` if rule matches the command.\n\n    :type command: Command\n    :rtype: bool\n\n    \"\"\"\n    if command.output is None and self.requires_output:\n        return False\n\n    try:\n        with logs.debug_time(u'Trying rule: {};'.format(self.name)):\n            if self.match(command):\n                return True\n    except Exception:\n        logs.rule_failed(self, sys.exc_info())", "loc": 16}
{"file": "thefuck\\thefuck\\types.py", "class_name": "Rule", "function_name": "get_corrected_commands", "parameters": ["self", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CorrectedCommand", "enumerate", "isinstance", "self.get_new_command"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_corrected_commands(self, command):\n    \"\"\"Returns generator with corrected commands.\n\n    :type command: Command\n    :rtype: Iterable[CorrectedCommand]\n\n    \"\"\"\n    new_commands = self.get_new_command(command)\n    if not isinstance(new_commands, list):\n        new_commands = (new_commands,)\n    for n, new_command in enumerate(new_commands):\n        yield CorrectedCommand(script=new_command,\n                               side_effect=self.side_effect,\n                               priority=(n + 1) * self.priority)", "loc": 14}
{"file": "thefuck\\thefuck\\types.py", "class_name": "CorrectedCommand", "function_name": "run", "parameters": ["self", "old_cmd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["logs.debug", "os.environ.get", "self._get_script", "self.side_effect", "shell.put_to_history", "sys.stdout.write", "u'PYTHONIOENCODING: {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Runs command from rule for passed command. :type old_cmd: Command", "source_code": "def run(self, old_cmd):\n    \"\"\"Runs command from rule for passed command.\n\n    :type old_cmd: Command\n\n    \"\"\"\n    if self.side_effect:\n        self.side_effect(old_cmd, self.script)\n    if settings.alter_history:\n        shell.put_to_history(self.script)\n    # This depends on correct setting of PYTHONIOENCODING by the alias:\n    logs.debug(u'PYTHONIOENCODING: {}'.format(\n        os.environ.get('PYTHONIOENCODING', '!!not-set!!')))\n\n    sys.stdout.write(self._get_script())", "loc": 15}
{"file": "thefuck\\thefuck\\ui.py", "class_name": null, "function_name": "read_actions", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_key"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Yields actions for pressed keys.", "source_code": "def read_actions():\n    \"\"\"Yields actions for pressed keys.\"\"\"\n    while True:\n        key = get_key()\n\n        # Handle arrows, j/k (qwerty), and n/e (colemak)\n        if key in (const.KEY_UP, const.KEY_CTRL_N, 'k', 'e'):\n            yield const.ACTION_PREVIOUS\n        elif key in (const.KEY_DOWN, const.KEY_CTRL_P, 'j', 'n'):\n            yield const.ACTION_NEXT\n        elif key in (const.KEY_CTRL_C, 'q'):\n            yield const.ACTION_ABORT\n        elif key in ('\\n', '\\r'):\n            yield const.ACTION_SELECT", "loc": 14}
{"file": "thefuck\\thefuck\\ui.py", "class_name": null, "function_name": "select_command", "parameters": ["corrected_commands"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CommandSelector", "get_alias", "logs.confirm_text", "logs.failed", "logs.show_corrected_command", "read_actions", "selector.next", "selector.previous", "sys.stderr.write"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select_command(corrected_commands):\n    \"\"\"Returns:\n\n     - the first command when confirmation disabled;\n     - None when ctrl+c pressed;\n     - selected command.\n\n    :type corrected_commands: Iterable[thefuck.types.CorrectedCommand]\n    :rtype: thefuck.types.CorrectedCommand | None\n\n    \"\"\"\n    try:\n        selector = CommandSelector(corrected_commands)\n    except NoRuleMatched:\n        logs.failed('No fucks given' if get_alias() == 'fuck'\n                    else 'Nothing found')\n        return\n\n    if not settings.require_confirmation:\n        logs.show_corrected_command(selector.value)\n        return selector.value\n\n    logs.confirm_text(selector.value)\n\n    for action in read_actions():\n        if action == const.ACTION_SELECT:\n            sys.stderr.write('\\n')\n            return selector.value\n        elif action == const.ACTION_ABORT:\n            logs.failed('\\nAborted')\n            return\n        elif action == const.ACTION_PREVIOUS:\n            selector.previous()\n            logs.confirm_text(selector.value)\n        elif action == const.ACTION_NEXT:\n            selector.next()\n            logs.confirm_text(selector.value)", "loc": 37}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "memoize", "parameters": ["fn"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fn", "pickle.dumps", "wraps"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "Caches previous calls to the function.", "source_code": "def memoize(fn):\n    \"\"\"Caches previous calls to the function.\"\"\"\n    memo = {}\n\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        if not memoize.disabled:\n            key = pickle.dumps((args, kwargs))\n            if key not in memo:\n                memo[key] = fn(*args, **kwargs)\n            value = memo[key]\n        else:\n            # Memoize is disabled, call the function\n            value = fn(*args, **kwargs)\n\n        return value\n\n    return wrapper", "loc": 18}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "default_settings", "parameters": ["params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["decorator", "fn", "params.items", "settings.setdefault"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Adds default values to settings if it not presented. Usage: @default_settings({'apt': '/usr/bin/apt'})", "source_code": "def default_settings(params):\n    \"\"\"Adds default values to settings if it not presented.\n\n    Usage:\n\n        @default_settings({'apt': '/usr/bin/apt'})\n        def match(command):\n            print(settings.apt)\n\n    \"\"\"\n    def _default_settings(fn, command):\n        for k, w in params.items():\n            settings.setdefault(k, w)\n        return fn(command)\n    return decorator(_default_settings)", "loc": 15}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_closest", "parameters": ["word", "possibilities", "cutoff", "fallback_to_first"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["difflib_get_close_matches", "list"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_closest(word, possibilities, cutoff=0.6, fallback_to_first=True):\n    \"\"\"Returns closest match or just first from possibilities.\"\"\"\n    possibilities = list(possibilities)\n    try:\n        return difflib_get_close_matches(word, possibilities, 1, cutoff)[0]\n    except IndexError:\n        if fallback_to_first:\n            return possibilities[0]", "loc": 8}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_close_matches", "parameters": ["word", "possibilities", "n", "cutoff"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["difflib_get_close_matches"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Overrides `difflib.get_close_match` to control argument `n`.", "source_code": "def get_close_matches(word, possibilities, n=None, cutoff=0.6):\n    \"\"\"Overrides `difflib.get_close_match` to control argument `n`.\"\"\"\n    if n is None:\n        n = settings.num_close_matches\n    return difflib_get_close_matches(word, possibilities, n, cutoff)", "loc": 5}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_all_executables", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Path", "Path(path).iterdir", "_safe", "alias.decode", "exe.name.decode", "fn", "get_alias", "include_path_in_search", "list", "os.environ.get", "os.environ.get('PATH', '').split", "shell.get_aliases"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_executables():\n    from thefuck.shells import shell\n\n    def _safe(fn, fallback):\n        try:\n            return fn()\n        except OSError:\n            return fallback\n\n    tf_alias = get_alias()\n    tf_entry_points = ['thefuck', 'fuck']\n\n    bins = [exe.name.decode('utf8') if six.PY2 else exe.name\n            for path in os.environ.get('PATH', '').split(os.pathsep)\n            if include_path_in_search(path)\n            for exe in _safe(lambda: list(Path(path).iterdir()), [])\n            if not _safe(exe.is_dir, True)\n            and exe.name not in tf_entry_points]\n    aliases = [alias.decode('utf8') if six.PY2 else alias\n               for alias in shell.get_aliases() if alias != tf_alias]\n\n    return bins + aliases", "loc": 22}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "replace_argument", "parameters": ["script", "from_", "to"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.escape", "re.sub", "script.replace", "u' {} '.format", "u' {}$'.format", "u' {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Replaces command line argument.", "source_code": "def replace_argument(script, from_, to):\n    \"\"\"Replaces command line argument.\"\"\"\n    replaced_in_the_end = re.sub(u' {}$'.format(re.escape(from_)), u' {}'.format(to),\n                                 script, count=1)\n    if replaced_in_the_end != script:\n        return replaced_in_the_end\n    else:\n        return script.replace(\n            u' {} '.format(from_), u' {} '.format(to), 1)", "loc": 9}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_all_matched_commands", "parameters": ["stderr", "separator"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "line.strip", "stderr.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all_matched_commands(stderr, separator='Did you mean'):\n    if not isinstance(separator, list):\n        separator = [separator]\n    should_yield = False\n    for line in stderr.split('\\n'):\n        for sep in separator:\n            if sep in line:\n                should_yield = True\n                break\n        else:\n            if should_yield and line:\n                yield line.strip()", "loc": 12}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "replace_command", "parameters": ["command", "broken", "matched"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_close_matches", "new_cmd.strip", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Helper for *_no_command rules.", "source_code": "def replace_command(command, broken, matched):\n    \"\"\"Helper for *_no_command rules.\"\"\"\n    new_cmds = get_close_matches(broken, matched, cutoff=0.1)\n    return [replace_argument(command.script, broken, new_cmd.strip())\n            for new_cmd in new_cmds]", "loc": 5}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "for_app", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["decorator", "fn", "is_app"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Specifies that matching script is for one of app names.", "source_code": "def for_app(*app_names, **kwargs):\n    \"\"\"Specifies that matching script is for one of app names.\"\"\"\n    def _for_app(fn, command):\n        if is_app(command, *app_names, **kwargs):\n            return fn(command)\n        else:\n            return False\n\n    return decorator(_for_app)", "loc": 9}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "cache", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_cache.get_value", "fn", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Caches function result in temporary file. Cache will be expired when modification date of files from `depends_on` will be changed.", "source_code": "def cache(*depends_on):\n    \"\"\"Caches function result in temporary file.\n\n    Cache will be expired when modification date of files from `depends_on`\n    will be changed.\n\n    Only functions should be wrapped in `cache`, not methods.\n\n    \"\"\"\n    def cache_decorator(fn):\n        @memoize\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            if cache.disabled:\n                return fn(*args, **kwargs)\n            else:\n                return _cache.get_value(fn, depends_on, args, kwargs)\n\n        return wrapper\n\n    return cache_decorator", "loc": 21}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_installation_version", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["pkg_resources.require", "version"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_installation_version():\n    try:\n        from importlib.metadata import version\n\n        return version('thefuck')\n    except ImportError:\n        import pkg_resources\n\n        return pkg_resources.require('thefuck')[0].version", "loc": 9}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "get_valid_history_without_current", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_not_corrected", "get_alias", "get_all_executables", "line.split", "line.startswith", "set", "set(get_all_executables()).union", "shell.get_builtin_commands", "shell.get_history"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_valid_history_without_current(command):\n    def _not_corrected(history, tf_alias):\n        \"\"\"Returns all lines from history except that comes before `fuck`.\"\"\"\n        previous = None\n        for line in history:\n            if previous is not None and line != tf_alias:\n                yield previous\n            previous = line\n        if history:\n            yield history[-1]\n\n    from thefuck.shells import shell\n    history = shell.get_history()\n    tf_alias = get_alias()\n    executables = set(get_all_executables())\\\n        .union(shell.get_builtin_commands())\n\n    return [line for line in _not_corrected(history, tf_alias)\n            if not line.startswith(tf_alias) and not line == command.script\n            and line.split(' ')[0] in executables]", "loc": 20}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "format_raw_script", "parameters": ["raw_script"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "arg.decode", "script.lstrip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Creates single script from a list of script parts. :type raw_script: [basestring] :rtype: basestring", "source_code": "def format_raw_script(raw_script):\n    \"\"\"Creates single script from a list of script parts.\n\n    :type raw_script: [basestring]\n    :rtype: basestring\n\n    \"\"\"\n    if six.PY2:\n        script = ' '.join(arg.decode('utf-8') for arg in raw_script)\n    else:\n        script = ' '.join(raw_script)\n\n    return script.lstrip()", "loc": 13}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fn", "pickle.dumps", "wraps"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    if not memoize.disabled:\n        key = pickle.dumps((args, kwargs))\n        if key not in memo:\n            memo[key] = fn(*args, **kwargs)\n        value = memo[key]\n    else:\n        # Memoize is disabled, call the function\n        value = fn(*args, **kwargs)\n\n    return value", "loc": 11}
{"file": "thefuck\\thefuck\\utils.py", "class_name": "Cache", "function_name": "get_value", "parameters": ["self", "fn", "depends_on", "args", "kwargs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "Path", "Path(name).expanduser", "Path(name).expanduser().absolute", "Path(name).expanduser().absolute().as_posix", "fn", "self._db.get", "self._db.get(key, {}).get", "self._get_key", "self._get_mtime", "self._init_db"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_value(self, fn, depends_on, args, kwargs):\n    if self._db is None:\n        self._init_db()\n\n    depends_on = [Path(name).expanduser().absolute().as_posix()\n                  for name in depends_on]\n    key = self._get_key(fn, depends_on, args, kwargs)\n    etag = '.'.join(self._get_mtime(path) for path in depends_on)\n\n    if self._db.get(key, {}).get('etag') == etag:\n        return self._db[key]['value']\n    else:\n        value = fn(*args, **kwargs)\n        self._db[key] = {'etag': etag, 'value': value}\n        return value", "loc": 15}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "cache_decorator", "parameters": ["fn"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_cache.get_value", "fn", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cache_decorator(fn):\n    @memoize\n    @wraps(fn)\n    def wrapper(*args, **kwargs):\n        if cache.disabled:\n            return fn(*args, **kwargs)\n        else:\n            return _cache.get_value(fn, depends_on, args, kwargs)\n\n    return wrapper", "loc": 10}
{"file": "thefuck\\thefuck\\utils.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_cache.get_value", "fn", "wraps"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    if cache.disabled:\n        return fn(*args, **kwargs)\n    else:\n        return _cache.get_value(fn, depends_on, args, kwargs)", "loc": 5}
{"file": "thefuck\\thefuck\\entrypoints\\fix_command.py", "class_name": null, "function_name": "fix_command", "parameters": ["known_args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_raw_command", "get_corrected_commands", "logs.debug", "logs.debug_time", "pformat", "select_command", "selected_command.run", "settings.init", "sys.exit", "types.Command.from_raw_script", "u'Run with settings: {}'.format"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Fixes previous command. Used when `thefuck` called without arguments.", "source_code": "def fix_command(known_args):\n    \"\"\"Fixes previous command. Used when `thefuck` called without arguments.\"\"\"\n    settings.init(known_args)\n    with logs.debug_time('Total'):\n        logs.debug(u'Run with settings: {}'.format(pformat(settings)))\n        raw_command = _get_raw_command(known_args)\n\n        try:\n            command = types.Command.from_raw_script(raw_command)\n        except EmptyCommand:\n            logs.debug('Empty command, nothing to do')\n            return\n\n        corrected_commands = get_corrected_commands(command)\n        selected_command = select_command(corrected_commands)\n\n        if selected_command:\n            selected_command.run(command)\n        else:\n            sys.exit(1)", "loc": 20}
{"file": "thefuck\\thefuck\\entrypoints\\main.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Parser", "fix_command", "get_installation_version", "logs.version", "logs.warn", "parser.parse", "parser.print_help", "parser.print_usage", "print_alias", "shell.info", "shell_logger", "sys.version.split"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parser = Parser()\n    known_args = parser.parse(sys.argv)\n\n    if known_args.help:\n        parser.print_help()\n    elif known_args.version:\n        logs.version(get_installation_version(),\n                     sys.version.split()[0], shell.info())\n    # It's important to check if an alias is being requested before checking if\n    # `TF_HISTORY` is in `os.environ`, otherwise it might mess with subshells.\n    # Check https://github.com/nvbn/thefuck/issues/921 for reference\n    elif known_args.alias:\n        print_alias(known_args)\n    elif known_args.command or 'TF_HISTORY' in os.environ:\n        fix_command(known_args)\n    elif known_args.shell_logger:\n        try:\n            from .shell_logger import shell_logger  # noqa: E402\n        except ImportError:\n            logs.warn('Shell logger supports only Linux and macOS')\n        else:\n            shell_logger(known_args.shell_logger)\n    else:\n        parser.print_usage()", "loc": 25}
{"file": "thefuck\\thefuck\\entrypoints\\not_configured.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_configure", "_is_already_configured", "_is_second_run", "_record_first_run", "logs.already_configured", "logs.configured_successfully", "logs.how_to_configure_alias", "settings.init", "shell.how_to_configure"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Shows useful information about how-to configure alias on a first run and configure automatically on a second. It'll be only visible when user type fuck and when alias isn't configured.", "source_code": "def main():\n    \"\"\"Shows useful information about how-to configure alias on a first run\n    and configure automatically on a second.\n\n    It'll be only visible when user type fuck and when alias isn't configured.\n\n    \"\"\"\n    settings.init()\n    configuration_details = shell.how_to_configure()\n    if (\n        configuration_details and\n        configuration_details.can_configure_automatically\n    ):\n        if _is_already_configured(configuration_details):\n            logs.already_configured(configuration_details)\n            return\n        elif _is_second_run():\n            _configure(configuration_details)\n            logs.configured_successfully(configuration_details)\n            return\n        else:\n            _record_first_run()\n\n    logs.how_to_configure_alias(configuration_details)", "loc": 24}
{"file": "thefuck\\thefuck\\output_readers\\shell_logger.py", "class_name": null, "function_name": "get_output", "parameters": ["script"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "'\\n'.join(lines).strip", "_get_last_n", "_get_output_lines", "logs.debug_time", "logs.warn"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Gets command output from shell logger.", "source_code": "def get_output(script):\n    \"\"\"Gets command output from shell logger.\"\"\"\n    with logs.debug_time(u'Read output from external shell logger'):\n        commands = _get_last_n(const.SHELL_LOGGER_LIMIT)\n        for command in commands:\n            if command['command'] == script:\n                lines = _get_output_lines(command['output'])\n                output = '\\n'.join(lines).strip()\n                return output\n            else:\n                logs.warn(\"Output isn't available in shell logger\")\n                return None", "loc": 12}
{"file": "thefuck\\thefuck\\output_readers\\__init__.py", "class_name": null, "function_name": "get_output", "parameters": ["script", "expanded"], "param_types": {}, "return_type": null, "param_doc": {"script": "Console script.", "expanded": "Console script with expanded aliases."}, "return_doc": "", "raises_doc": [], "called_functions": ["read_log.get_output", "rerun.get_output", "shell_logger.get_output", "shell_logger.is_available"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get output of the script.", "source_code": "def get_output(script, expanded):\n    \"\"\"Get output of the script.\n\n    :param script: Console script.\n    :type script: str\n    :param expanded: Console script with expanded aliases.\n    :type expanded: str\n    :rtype: str\n\n    \"\"\"\n    if shell_logger.is_available():\n        return shell_logger.get_output(script)\n    if settings.instant_mode:\n        return read_log.get_output(script)\n    else:\n        return rerun.get_output(script, expanded)", "loc": 16}
{"file": "thefuck\\thefuck\\rules\\adb_unknown_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "get_closest", "replace_argument"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    for idx, arg in enumerate(command.script_parts[1:]):\n        # allowed params to ADB are a/d/e/s/H/P/L where s, H, P and L take additional args\n        # for example 'adb -s 111 logcat' or 'adb -e logcat'\n        if not arg[0] == '-' and not command.script_parts[idx] in ('-s', '-H', '-P', '-L'):\n            adb_cmd = get_closest(arg, _ADB_COMMANDS)\n            return replace_argument(command.script, arg, adb_cmd)", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\apt_get.py", "class_name": null, "function_name": "get_package", "parameters": ["executable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_packages"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_package(executable):\n    try:\n        packages = _get_packages(executable)\n        return packages[0][0]\n    except IndexError:\n        # IndexError is thrown when no matching package is found\n        return None", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\apt_get.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_executable", "get_package", "which"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if 'not found' in command.output or 'not installed' in command.output:\n        executable = _get_executable(command)\n        return not which(executable) and get_package(executable)\n    else:\n        return False", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\apt_get.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_executable", "formatme.format", "get_package", "shell.and_"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    executable = _get_executable(command)\n    name = get_package(executable)\n    formatme = shell.and_('sudo apt-get install {}', '{}')\n    return formatme.format(name, command.script)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\apt_invalid_operation.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_operations", "command.output.split", "command.script.replace", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    invalid_operation = command.output.split()[-1]\n\n    if invalid_operation == 'uninstall':\n        return [command.script.replace('uninstall', 'remove')]\n\n    else:\n        operations = _get_operations(command.script_parts[0])\n        return replace_command(command, invalid_operation, operations)", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\brew_link.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "command_parts.insert"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n    command_parts[1] = 'link'\n    command_parts.insert(2, '--overwrite')\n    command_parts.insert(3, '--dry-run')\n    return ' '.join(command_parts)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\brew_uninstall.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "command_parts.insert"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n    command_parts[1] = 'uninstall'\n    command_parts.insert(2, '--force')\n    return ' '.join(command_parts)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\brew_unknown_command.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_brew_commands", "bool", "get_closest", "re.findall"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    is_proper_command = ('brew' in command.script and\n                         'Unknown command' in command.output)\n\n    if is_proper_command:\n        broken_cmd = re.findall(r'Error: Unknown command: ([a-z]+)',\n                                command.output)[0]\n        return bool(get_closest(broken_cmd, _brew_commands()))\n    return False", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\cargo_no_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken = command.script_parts[1]\n    fix = re.findall(r'Did you mean `([^`]*)`', command.output)[0]\n\n    return replace_argument(command.script, broken, fix)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\cd_correction.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "command.output.lower", "command.script.startswith", "for_app"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Match function copied from cd_mkdir.py", "source_code": "def match(command):\n    \"\"\"Match function copied from cd_mkdir.py\"\"\"\n    return (\n        command.script.startswith('cd ') and any((\n            'no such file or directory' in command.output.lower(),\n            'cd: can\\'t cd to' in command.output.lower(),\n            'does not exist' in command.output.lower()\n        )))", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\cd_mkdir.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "command.output.lower", "command.script.startswith", "for_app"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (\n        command.script.startswith('cd ') and any((\n            'no such file or directory' in command.output.lower(),\n            'cd: can\\'t cd to' in command.output.lower(),\n            'does not exist' in command.output.lower()\n        )))", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\choco_install.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.script.replace", "script_part.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    # Find the argument that is the package name\n    for script_part in command.script_parts:\n        if (\n            script_part not in [\"choco\", \"cinst\", \"install\"]\n            # Need exact match (bc chocolatey is a package)\n            and not script_part.startswith('-')\n            # Leading hyphens are parameters; some packages contain them though\n            and '=' not in script_part and '/' not in script_part\n            # These are certainly parameters\n        ):\n            return command.script.replace(script_part, script_part + \".install\")\n    return []", "loc": 13}
{"file": "thefuck\\thefuck\\rules\\composer_not_command.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.lower", "for_app"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (('did you mean this?' in command.output.lower()\n             or 'did you mean one of these?' in command.output.lower())) or (\n        \"install\" in command.script_parts and \"composer require\" in command.output.lower()\n    )", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\composer_not_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.lower", "new_cmd[0].strip", "re.findall", "replace_argument"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if \"install\" in command.script_parts and \"composer require\" in command.output.lower():\n        broken_cmd, new_cmd = \"install\", \"require\"\n    else:\n        broken_cmd = re.findall(r\"Command \\\"([^']*)\\\" is not defined\", command.output)[0]\n        new_cmd = re.findall(r'Did you mean this\\?[^\\n]*\\n\\s*([^\\n]*)', command.output)\n        if not new_cmd:\n            new_cmd = re.findall(r'Did you mean one of these\\?[^\\n]*\\n\\s*([^\\n]*)', command.output)\n        new_cmd = new_cmd[0].strip()\n    return replace_argument(command.script, broken_cmd, new_cmd)", "loc": 10}
{"file": "thefuck\\thefuck\\rules\\conda_mistype.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    match = re.findall(r\"'conda ([^']*)'\", command.output)\n    broken_cmd = match[0]\n    correct_cmd = match[1]\n    return replace_command(command, broken_cmd, [correct_cmd])", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\cp_create_destination.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.rstrip", "command.output.rstrip().endswith", "command.output.startswith", "for_app"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (\n        \"No such file or directory\" in command.output\n        or command.output.startswith(\"cp: directory\")\n        and command.output.rstrip().endswith(\"does not exist\")\n    )", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\dirty_unzip.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_is_bad_zip", "_zip_file", "for_app"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if '-d' in command.script:\n        return False\n\n    zip_file = _zip_file(command)\n    if zip_file:\n        return _is_bad_zip(zip_file)\n    else:\n        return False", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\docker_image_being_used_by_container.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.strip", "command.output.strip().split", "shell.and_", "shell.and_('docker container rm -f {}', '{}').format"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Prepends docker container rm -f {container ID} to the previous docker image rm {image ID} command", "source_code": "def get_new_command(command):\n    '''\n    Prepends docker container rm -f {container ID} to\n    the previous docker image rm {image ID} command\n    '''\n    container_id = command.output.strip().split(' ')\n    return shell.and_('docker container rm -f {}', '{}').format(container_id[-1], command.script)", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\docker_not_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_parse_commands", "command.output.split", "get_docker_commands", "len", "re.findall", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if 'Usage:' in command.output and len(command.script_parts) > 1:\n        management_subcommands = _parse_commands(command.output.split('\\n'), 'Commands:')\n        return replace_command(command, command.script_parts[2], management_subcommands)\n\n    wrong_command = re.findall(\n        r\"docker: '(\\w+)' is not a docker command.\", command.output)[0]\n    return replace_command(command, wrong_command, get_docker_commands())", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\fab_command_not_found.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' {}'.format", "_get_between", "get_closest", "script.replace"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    not_found_commands = _get_between(\n        command.output, 'Warning: Command(s) not found:',\n        'Available commands:')\n    possible_commands = _get_between(\n        command.output, 'Available commands:')\n\n    script = command.script\n    for not_found in not_found_commands:\n        fix = get_closest(not_found, possible_commands)\n        script = script.replace(' {}'.format(not_found),\n                                ' {}'.format(fix))\n\n    return script", "loc": 14}
{"file": "thefuck\\thefuck\\rules\\fix_file.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if 'EDITOR' not in os.environ:\n        return False\n\n    return _search(command.output)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\fix_file.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_search", "default_settings", "m.group", "m.groupdict", "settings.fixcolcmd.format", "settings.fixlinecmd.format", "shell.and_"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    m = _search(command.output)\n\n    # Note: there does not seem to be a standard for columns, so they are just\n    # ignored by default\n    if settings.fixcolcmd and 'col' in m.groupdict():\n        editor_call = settings.fixcolcmd.format(editor=os.environ['EDITOR'],\n                                                file=m.group('file'),\n                                                line=m.group('line'),\n                                                col=m.group('col'))\n    else:\n        editor_call = settings.fixlinecmd.format(editor=os.environ['EDITOR'],\n                                                 file=m.group('file'),\n                                                 line=m.group('line'))\n\n    return shell.and_(editor_call, command.script)", "loc": 16}
{"file": "thefuck\\thefuck\\rules\\git_branch_0flag.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["branch_name.replace", "command.script.replace", "first_0flag", "shell.and_", "u'git branch -D {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    branch_name = first_0flag(command.script_parts)\n    fixed_flag = branch_name.replace(\"0\", \"-\")\n    fixed_script = command.script.replace(branch_name, fixed_flag)\n    if \"A branch named '\" in command.output and \"' already exists.\" in command.output:\n        delete_branch = u\"git branch -D {}\".format(branch_name)\n        return shell.and_(delete_branch, fixed_script)\n    return fixed_script", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\git_branch_exists.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["branch_name.replace", "re.findall", "shell.and_", "shell.and_(*new_command_template).format"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    branch_name = re.findall(\n        r\"fatal: A branch named '(.+)' already exists.\", command.output)[0]\n    branch_name = branch_name.replace(\"'\", r\"\\'\")\n    new_command_templates = [['git branch -d {0}', 'git branch {0}'],\n                             ['git branch -d {0}', 'git checkout -b {0}'],\n                             ['git branch -D {0}', 'git branch {0}'],\n                             ['git branch -D {0}', 'git checkout -b {0}'],\n                             ['git checkout {0}']]\n    for new_command_template in new_command_templates:\n        yield shell.and_(*new_command_template).format(branch_name)", "loc": 11}
{"file": "thefuck\\thefuck\\rules\\git_checkout.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_branches", "new_commands.append", "re.findall", "replace_argument", "shell.and_", "shell.and_('git branch {}', '{}').format", "utils.get_closest"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    missing_file = re.findall(\n        r\"error: pathspec '([^']*)' \"\n        r\"did not match any file\\(s\\) known to git\", command.output)[0]\n    closest_branch = utils.get_closest(missing_file, get_branches(),\n                                       fallback_to_first=False)\n\n    new_commands = []\n\n    if closest_branch:\n        new_commands.append(replace_argument(command.script, missing_file, closest_branch))\n    if command.script_parts[1] == 'checkout':\n        new_commands.append(replace_argument(command.script, 'checkout', 'checkout -b'))\n\n    if not new_commands:\n        new_commands.append(shell.and_('git branch {}', '{}').format(\n            missing_file, command.script))\n\n    return new_commands", "loc": 19}
{"file": "thefuck\\thefuck\\rules\\git_clone_missing.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "parse.urlparse", "which"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    # We want it to be a URL by itself\n    if len(command.script_parts) != 1:\n        return False\n    # Ensure we got the error we expected\n    if which(command.script_parts[0]) or not (\n        'No such file or directory' in command.output\n        or 'not found' in command.output\n        or 'is not recognised as' in command.output\n    ):\n        return False\n    url = parse.urlparse(command.script, scheme='ssh')\n    # HTTP URLs need a network address\n    if not url.netloc and url.scheme != 'ssh':\n        return False\n    # SSH needs a username and a splitter between the path\n    if url.scheme == 'ssh' and not (\n        '@' in command.script\n        and ':' in command.script\n    ):\n        return False\n    return url.scheme in ['http', 'https', 'ssh']", "loc": 22}
{"file": "thefuck\\thefuck\\rules\\git_diff_no_index.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["arg.startswith", "len"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    files = [arg for arg in command.script_parts[2:]\n             if not arg.startswith('-')]\n    return ('diff' in command.script\n            and '--no-index' not in command.script\n            and len(files) == 2)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\git_fix_stash.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if command.script_parts and len(command.script_parts) > 1:\n        return (command.script_parts[1] == 'stash'\n                and 'usage:' in command.output)\n    else:\n        return False", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\git_fix_stash.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "cmd.insert", "replace_argument", "utils.get_closest"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    stash_cmd = command.script_parts[2]\n    fixed = utils.get_closest(stash_cmd, stash_commands, fallback_to_first=False)\n\n    if fixed is not None:\n        return replace_argument(command.script, stash_cmd, fixed)\n    else:\n        cmd = command.script_parts[:]\n        cmd.insert(2, 'save')\n        return ' '.join(cmd)", "loc": 10}
{"file": "thefuck\\thefuck\\rules\\git_flag_after_filename.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command_parts.index", "match", "match(command).group", "range", "reversed", "u' '.join"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n\n    # find the bad flag\n    bad_flag = match(command).group(1)\n    bad_flag_index = command_parts.index(bad_flag)\n\n    # find the filename\n    for index in reversed(range(bad_flag_index)):\n        if command_parts[index][0] != '-':\n            filename_index = index\n            break\n\n    # swap them\n    command_parts[bad_flag_index], command_parts[filename_index] = \\\n    command_parts[filename_index], command_parts[bad_flag_index]  # noqa: E122\n\n    return u' '.join(command_parts)", "loc": 18}
{"file": "thefuck\\thefuck\\rules\\git_hook_bypass.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["next", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    hooked_command = next(\n        hooked_command\n        for hooked_command in hooked_commands\n        if hooked_command in command.script_parts\n    )\n    return replace_argument(\n        command.script, hooked_command, hooked_command + \" --no-verify\"\n    )", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\git_merge.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    unknown_branch = re.findall(r'merge: (.+) - not something we can merge', command.output)[0]\n    remote_branch = re.findall(r'Did you mean this\\?\\n\\t([^\\n]+)', command.output)[0]\n\n    return replace_argument(command.script, unknown_branch, remote_branch)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\git_not_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_all_matched_commands", "re.findall", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken_cmd = re.findall(r\"git: '([^']*)' is not a git command\",\n                            command.output)[0]\n    matched = get_all_matched_commands(command.output, ['The most similar command', 'Did you mean'])\n    return replace_command(command, broken_cmd, matched)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\git_pull.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.split", "command.output.split('\\n')[-3].strip", "line.replace", "line.replace('<remote>', 'origin').replace", "line.split", "shell.and_"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    line = command.output.split('\\n')[-3].strip()\n    branch = line.split(' ')[-1]\n    set_upstream = line.replace('<remote>', 'origin')\\\n                       .replace('<branch>', branch)\n    return shell.and_(set_upstream, command.script)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\git_push.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'push {}'.format", "_get_upstream_option_index", "command_parts.index", "command_parts.pop", "len", "re.findall", "re.findall('git push (.*)', command.output)[-1].replace", "re.findall('git push (.*)', command.output)[-1].replace(\"'\", \"\\\\'\").strip", "replace_argument"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    # If --set-upstream or -u are passed, remove it and its argument. This is\n    # because the remaining arguments are concatenated onto the command suggested\n    # by git, which includes --set-upstream and its argument\n    command_parts = command.script_parts[:]\n    upstream_option_index = _get_upstream_option_index(command_parts)\n\n    if upstream_option_index is not None:\n        command_parts.pop(upstream_option_index)\n\n        # In case of `git push -u` we don't have next argument:\n        if len(command_parts) > upstream_option_index:\n            command_parts.pop(upstream_option_index)\n    else:\n        # the only non-qualified permitted options are the repository and refspec; git's\n        # suggestion include them, so they won't be lost, but would be duplicated otherwise.\n        push_idx = command_parts.index('push') + 1\n        while len(command_parts) > push_idx and command_parts[len(command_parts) - 1][0] != '-':\n            command_parts.pop(len(command_parts) - 1)\n\n    arguments = re.findall(r'git push (.*)', command.output)[-1].replace(\"'\", r\"\\'\").strip()\n    return replace_argument(\" \".join(command_parts), 'push',\n                            'push {}'.format(arguments))", "loc": 23}
{"file": "thefuck\\thefuck\\rules\\git_rebase_merge_dir.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.split", "command_list.append", "get_close_matches", "rm_cmd.strip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_list = ['git rebase --continue', 'git rebase --abort', 'git rebase --skip']\n    rm_cmd = command.output.split('\\n')[-4]\n    command_list.append(rm_cmd.strip())\n    return get_close_matches(command.script, command_list, 4, 0)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\git_rm_local_modifications.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command_list.append", "command_parts.index", "command_parts.insert", "u' '.join"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n    index = command_parts.index('rm') + 1\n    command_parts.insert(index, '--cached')\n    command_list = [u' '.join(command_parts)]\n    command_parts[index] = '-f'\n    command_list.append(u' '.join(command_parts))\n    return command_list", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\git_rm_recursive.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command_parts.index", "command_parts.insert", "u' '.join"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n    index = command_parts.index('rm') + 1\n    command_parts.insert(index, '-r')\n    return u' '.join(command_parts)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\git_rm_staged.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command_list.append", "command_parts.index", "command_parts.insert", "u' '.join"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    command_parts = command.script_parts[:]\n    index = command_parts.index('rm') + 1\n    command_parts.insert(index, '--cached')\n    command_list = [u' '.join(command_parts)]\n    command_parts[index] = '-f'\n    command_list.append(u' '.join(command_parts))\n    return command_list", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\grep_arguments_order.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "_get_actual_file", "parts.append", "parts.remove"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    actual_file = _get_actual_file(command.script_parts)\n    parts = command.script_parts[::]\n    # Moves file to the end of the script:\n    parts.remove(actual_file)\n    parts.append(actual_file)\n    return ' '.join(parts)", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\grunt_task_not_found.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' {}'.format", "_get_all_tasks", "command.script.replace", "get_closest", "regex.findall", "regex.findall(command.output)[0].split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    misspelled_task = regex.findall(command.output)[0].split(':')[0]\n    tasks = _get_all_tasks()\n    fixed = get_closest(misspelled_task, tasks)\n    return command.script.replace(' {}'.format(misspelled_task),\n                                  ' {}'.format(fixed))", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\hostscli.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["for_app"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    errors = [no_command, no_website]\n    for error in errors:\n        if error in command.output:\n            return True\n    return False", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\hostscli.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if no_website in command.output:\n        return ['hostscli websites']\n\n    misspelled_command = re.findall(\n        r'Error: No such command \".*\"', command.output)[0]\n    commands = ['block', 'unblock', 'websites', 'block_all', 'unblock_all']\n    return replace_command(command, misspelled_command, commands)", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\lein_not_task.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_all_matched_commands", "re.findall", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken_cmd = re.findall(r\"'([^']*)' is not a task\",\n                            command.output)[0]\n    new_cmds = get_all_matched_commands(command.output, 'Did you mean this?')\n    return replace_command(command, broken_cmd, new_cmds)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\ln_s_order.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_destination", "{'-s', '--symbolic'}.intersection"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (command.script_parts[0] == 'ln'\n            and {'-s', '--symbolic'}.intersection(command.script_parts)\n            and 'File exists' in command.output\n            and _get_destination(command.script_parts))", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\ln_s_order.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "_get_destination", "parts.append", "parts.remove"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    destination = _get_destination(command.script_parts)\n    parts = command.script_parts[:]\n    parts.remove(destination)\n    parts.append(destination)\n    return ' '.join(parts)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\long_form_help.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if re.search(help_regex, command.output, re.I) is not None:\n        return True\n\n    if '--help' in command.output:\n        return True\n\n    return False", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\long_form_help.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match_obj.group", "re.search", "replace_argument"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if re.search(help_regex, command.output) is not None:\n        match_obj = re.search(help_regex, command.output, re.I)\n        return match_obj.group(1)\n\n    return replace_argument(command.script, '-h', '--help')", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\man.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "command.output.strip", "command.script.replace", "split_cmd2.insert", "split_cmd3.insert"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if '3' in command.script:\n        return command.script.replace(\"3\", \"2\")\n    if '2' in command.script:\n        return command.script.replace(\"2\", \"3\")\n\n    last_arg = command.script_parts[-1]\n    help_command = last_arg + ' --help'\n\n    # If there are no man pages for last_arg, suggest `last_arg --help` instead.\n    # Otherwise, suggest `--help` after suggesting other man page sections.\n    if command.output.strip() == 'No manual entry for ' + last_arg:\n        return [help_command]\n\n    split_cmd2 = command.script_parts\n    split_cmd3 = split_cmd2[:]\n\n    split_cmd2.insert(1, ' 2 ')\n    split_cmd3.insert(1, ' 3 ')\n\n    return [\n        \"\".join(split_cmd3),\n        \"\".join(split_cmd2),\n        help_command,\n    ]", "loc": 25}
{"file": "thefuck\\thefuck\\rules\\mercurial.py", "class_name": null, "function_name": "extract_possibilities", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["possib[0].split", "re.findall"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_possibilities(command):\n    possib = re.findall(r'\\n\\(did you mean one of ([^\\?]+)\\?\\)', command.output)\n    if possib:\n        return possib[0].split(', ')\n    possib = re.findall(r'\\n    ([^$]+)$', command.output)\n    if possib:\n        return possib[0].split(' ')\n    return possib", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\mercurial.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "extract_possibilities", "get_closest"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    script = command.script_parts[:]\n    possibilities = extract_possibilities(command)\n    script[1] = get_closest(script[1], possibilities)\n    return ' '.join(script)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\mvn_unknown_lifecycle_phase.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_failed_lifecycle", "_getavailable_lifecycles", "available_lifecycles.group", "available_lifecycles.group(1).split", "failed_lifecycle.group", "get_close_matches", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    failed_lifecycle = _get_failed_lifecycle(command)\n    available_lifecycles = _getavailable_lifecycles(command)\n    if available_lifecycles and failed_lifecycle:\n        selected_lifecycle = get_close_matches(\n            failed_lifecycle.group(1), available_lifecycles.group(1).split(\", \"))\n        return replace_command(command, failed_lifecycle.group(1), selected_lifecycle)\n    else:\n        return []", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\no_command.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "get_all_executables", "get_close_matches", "which"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (not which(command.script_parts[0])\n            and ('not found' in command.output\n                 or 'is not recognized as' in command.output)\n            and bool(get_close_matches(command.script_parts[0],\n                                       get_all_executables())))", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\no_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_used_executables", "command.script.replace", "get_all_executables", "get_close_matches", "get_closest"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    old_command = command.script_parts[0]\n\n    # One from history:\n    already_used = get_closest(\n        old_command, _get_used_executables(command),\n        fallback_to_first=False)\n    if already_used:\n        new_cmds = [already_used]\n    else:\n        new_cmds = []\n\n    # Other from all executables:\n    new_cmds += [cmd for cmd in get_close_matches(old_command,\n                                                  get_all_executables())\n                 if cmd not in new_cmds]\n\n    return [command.script.replace(old_command, cmd, 1) for cmd in new_cmds]", "loc": 18}
{"file": "thefuck\\thefuck\\rules\\no_such_file.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    for pattern in patterns:\n        if re.search(pattern, command.output):\n            return True\n\n    return False", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\no_such_file.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["file.rfind", "formatme.format", "re.findall", "shell.and_"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    for pattern in patterns:\n        file = re.findall(pattern, command.output)\n\n        if file:\n            file = file[0]\n            dir = file[0:file.rfind('/')]\n\n            formatme = shell.and_('mkdir -p {}', '{}')\n            return formatme.format(dir, command.script)", "loc": 10}
{"file": "thefuck\\thefuck\\rules\\npm_wrong_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_available_commands", "_get_wrong_command", "get_closest", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    npm_commands = _get_available_commands(command.output)\n    wrong_command = _get_wrong_command(command.script_parts)\n    fixed = get_closest(wrong_command, npm_commands)\n    return replace_argument(command.script, wrong_command, fixed)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\omnienv_no_such_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["COMMON_TYPOS.get", "cache", "matched.extend", "re.findall", "replace_argument", "replace_command", "which"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken = re.findall(r\"env: no such command ['`]([^']*)'\", command.output)[0]\n    matched = [replace_argument(command.script, broken, common_typo)\n               for common_typo in COMMON_TYPOS.get(broken, [])]\n\n    app = command.script_parts[0]\n    app_commands = cache(which(app))(get_app_commands)(app)\n    matched.extend(replace_command(command, broken, app_commands))\n    return matched", "loc": 9}
{"file": "thefuck\\thefuck\\rules\\open.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.strip", "command.script.replace", "command.script.split", "is_arg_url", "output.endswith", "output.startswith", "shell.and_", "u'{} {}'.format"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    output = command.output.strip()\n    if is_arg_url(command):\n        yield command.script.replace('open ', 'open http://')\n    elif output.startswith('The file ') and output.endswith(' does not exist.'):\n        arg = command.script.split(' ', 1)[1]\n        for option in ['touch', 'mkdir']:\n            yield shell.and_(u'{} {}'.format(option, arg), command.script)", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\pacman.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["formatme.format", "get_pkgfile", "shell.and_"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    packages = get_pkgfile(command.script)\n\n    formatme = shell.and_('{} -S {}', '{}')\n    return [formatme.format(pacman, package, command.script)\n            for package in packages]", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\path_from_history.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Path", "Path(path).expanduser", "Path(path).expanduser().exists", "_get_all_absolute_paths_from_history", "_get_destination", "path.endswith", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    destination = _get_destination(command)\n    paths = _get_all_absolute_paths_from_history(command)\n\n    return [replace_argument(command.script, destination, path)\n            for path in paths if path.endswith(destination)\n            and Path(path).expanduser().exists()]", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\pip_install.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'sudo {}'.format", "command.script.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if '--user' not in command.script:  # add --user (attempt 1)\n        return command.script.replace(' install ', ' install --user ')\n\n    return 'sudo {}'.format(command.script.replace(' --user', ''))  # since --user didn't fix things, let's try sudo (attempt 2)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\pip_unknown_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken_cmd = re.findall(r'ERROR: unknown command \"([^\"]+)\"',\n                            command.output)[0]\n    new_cmd = re.findall(r'maybe you meant \"([^\"]+)\"', command.output)[0]\n\n    return replace_argument(command.script, broken_cmd, new_cmd)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\prove_recursively.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_is_recursive", "_isdir", "any", "for_app"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    return (\n        'NOTESTS' in command.output\n        and not any(_is_recursive(part) for part in command.script_parts[1:])\n        and any(_isdir(part) for part in command.script_parts[1:]))", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\react_native_command_unrecognized.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_commands", "re.findall", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    misspelled_command = re.findall(r\"Unrecognized command '(.*)'\",\n                                    command.output)[0]\n    commands = _get_commands()\n    return replace_command(command, misspelled_command, commands)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\rm_dir.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.sub"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    arguments = '-rf'\n    if 'hdfs' in command.script:\n        arguments = '-r'\n    return re.sub('\\\\brm (.*)', 'rm ' + arguments + ' \\\\1', command.script)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\scm_correction.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_actual_scm", "for_app", "wrong_scm_patterns.keys"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    scm = command.script_parts[0]\n    pattern = wrong_scm_patterns[scm]\n\n    return pattern in command.output and _get_actual_scm()", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\sed_unterminated_s.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "e.startswith", "enumerate", "map", "shlex.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    script = shlex.split(command.script)\n\n    for (i, e) in enumerate(script):\n        if e.startswith(('s/', '-es/')) and e[-1] != '/':\n            script[i] += '/'\n\n    return ' '.join(map(shell.quote, script))", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\ssh_known_hosts.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "command.script.startswith", "for_app", "re.findall"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if not command.script:\n        return False\n    if not command.script.startswith(commands):\n        return False\n\n    patterns = (\n        r'WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!',\n        r'WARNING: POSSIBLE DNS SPOOFING DETECTED!',\n        r\"Warning: the \\S+ host key for '([^']+)' differs from the key for the IP address '([^']+)'\",\n    )\n\n    return any(re.findall(pattern, command.output) for pattern in patterns)", "loc": 13}
{"file": "thefuck\\thefuck\\rules\\sudo.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.lower"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if command.script_parts and '&&' not in command.script_parts and command.script_parts[0] == 'sudo':\n        return False\n\n    for pattern in patterns:\n        if pattern in command.output.lower():\n            return True\n    return False", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\sudo.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "command.script.replace", "u'sudo sh -c \"{}\"'.format", "u'sudo {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if '&&' in command.script:\n        return u'sudo sh -c \"{}\"'.format(\" \".join([part for part in command.script_parts if part != \"sudo\"]))\n    elif '>' in command.script:\n        return u'sudo sh -c \"{}\"'.format(command.script.replace('\"', '\\\\\"'))\n    else:\n        return u'sudo {}'.format(command.script)", "loc": 7}
{"file": "thefuck\\thefuck\\rules\\switch_lang.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_matched_layout", "_switch_command", "any", "get_alias"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if 'not found' not in command.output:\n        return False\n    if any(u'ㄱ' <= ch <= u'ㅎ' or u'ㅏ' <= ch <= u'ㅣ' or u'가' <= ch <= u'힣'\n            for ch in command.script):\n        return True\n\n    matched_layout = _get_matched_layout(command)\n    return (matched_layout and\n            _switch_command(command, matched_layout) != get_alias())", "loc": 10}
{"file": "thefuck\\thefuck\\rules\\switch_lang.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_decompose_korean", "_get_matched_layout", "_switch_command", "any"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    if any(u'ㄱ' <= ch <= u'ㅎ' or u'ㅏ' <= ch <= u'ㅣ' or u'가' <= ch <= u'힣'\n            for ch in command.script):\n        command.script = _decompose_korean(command)\n    matched_layout = _get_matched_layout(command)\n    return _switch_command(command, matched_layout)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\systemctl.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cmd.index", "for_app", "len"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    # Catches \"Unknown operation 'service'.\" when executing systemctl with\n    # misordered arguments\n    cmd = command.script_parts\n    return (cmd and 'Unknown operation \\'' in command.output and\n            len(cmd) - cmd.index('systemctl') == 3)", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\tmux.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["c.strip", "cmd.group", "cmd.group(2).split", "re.match", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    cmd = re.match(r\"ambiguous command: (.*), could be: (.*)\",\n                   command.output)\n\n    old_cmd = cmd.group(1)\n    suggestions = [c.strip() for c in cmd.group(2).split(',')]\n\n    return replace_command(command, old_cmd, suggestions)", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\tsuru_not_command.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_all_matched_commands", "re.findall", "replace_command"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken_cmd = re.findall(r'tsuru: \"([^\"]*)\" is not a tsuru command',\n                            command.output)[0]\n    return replace_command(command, broken_cmd,\n                           get_all_matched_commands(command.output))", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\unsudo.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.output.lower"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    if command.script_parts and command.script_parts[0] != 'sudo':\n        return False\n\n    for pattern in patterns:\n        if pattern in command.output.lower():\n            return True\n    return False", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\vagrant_up.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "shell.and_", "u'vagrant up {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    cmds = command.script_parts\n    machine = None\n    if len(cmds) >= 3:\n        machine = cmds[2]\n\n    start_all_instances = shell.and_(u\"vagrant up\", command.script)\n    if machine is None:\n        return start_all_instances\n    else:\n        return [shell.and_(u\"vagrant up {}\".format(machine), command.script),\n                start_all_instances]", "loc": 12}
{"file": "thefuck\\thefuck\\rules\\whois.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "len", "range", "urlparse", "urlparse(url).path.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    url = command.script_parts[1]\n\n    if '/' in command.script:\n        return 'whois ' + urlparse(url).netloc\n    elif '.' in command.script:\n        path = urlparse(url).path.split('.')\n        return ['whois ' + '.'.join(path[n:]) for n in range(1, len(path))]", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\workon_doesnt_exists.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_all_environments", "replace_command", "u'mkvirtualenv {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    misspelled_env = command.script_parts[1]\n    create_new = u'mkvirtualenv {}'.format(misspelled_env)\n\n    available = _get_all_environments()\n    if available:\n        return (replace_command(command, misspelled_env, available)\n                + [create_new])\n    else:\n        return create_new", "loc": 10}
{"file": "thefuck\\thefuck\\rules\\wrong_hyphen_before_subcommand.py", "class_name": null, "function_name": "match", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["first_part.split", "get_all_executables"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(command):\n    first_part = command.script_parts[0]\n    if \"-\" not in first_part or first_part in get_all_executables():\n        return False\n    cmd, _ = first_part.split(\"-\", 1)\n    return cmd in get_all_executables()", "loc": 6}
{"file": "thefuck\\thefuck\\rules\\yarn_alias.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "replace_argument"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    broken = command.script_parts[1]\n    fix = re.findall(r'Did you mean [`\"](?:yarn )?([^`\"]*)[`\"]', command.output)[0]\n\n    return replace_argument(command.script, broken, fix)", "loc": 5}
{"file": "thefuck\\thefuck\\rules\\yarn_command_not_found.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_all_tasks", "regex.findall", "replace_argument", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    misspelled_task = regex.findall(command.output)[0]\n    if misspelled_task in npm_commands:\n        yarn_command = npm_commands[misspelled_task]\n        return replace_argument(command.script, misspelled_task, yarn_command)\n    else:\n        tasks = _get_all_tasks()\n        return replace_command(command, misspelled_task, tasks)", "loc": 8}
{"file": "thefuck\\thefuck\\rules\\yum_invalid_operation.py", "class_name": null, "function_name": "get_new_command", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_operations", "command.script.replace", "replace_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_new_command(command):\n    invalid_operation = command.script_parts[1]\n\n    if invalid_operation == 'uninstall':\n        return [command.script.replace('uninstall', 'remove')]\n\n    return replace_command(command, invalid_operation, _get_operations())", "loc": 7}
{"file": "thefuck\\thefuck\\shells\\fish.py", "class_name": "Fish", "function_name": "app_alias", "parameters": ["self", "alias_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'function {0} -d \"Correct your previous console command\"\\n  set -l fucked_up_command $history[1]\\n  env TF_SHELL=fish TF_ALIAS={0} PYTHONIOENCODING=utf-8 thefuck $fucked_up_command {2} $argv | read -l unfucked_command\\n  if [ \"$unfucked_command\" != \"\" ]\\n    eval $unfucked_command\\n{1}  end\\nend'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def app_alias(self, alias_name):\n    if settings.alter_history:\n        alter_history = ('    builtin history delete --exact'\n                         ' --case-sensitive -- $fucked_up_command\\n'\n                         '    builtin history merge\\n')\n    else:\n        alter_history = ''\n    # It is VERY important to have the variables declared WITHIN the alias\n    return ('function {0} -d \"Correct your previous console command\"\\n'\n            '  set -l fucked_up_command $history[1]\\n'\n            '  env TF_SHELL=fish TF_ALIAS={0} PYTHONIOENCODING=utf-8'\n            ' thefuck $fucked_up_command {2} $argv | read -l unfucked_command\\n'\n            '  if [ \"$unfucked_command\" != \"\" ]\\n'\n            '    eval $unfucked_command\\n{1}'\n            '  end\\n'\n            'end').format(alias_name, alter_history, ARGUMENT_PLACEHOLDER)", "loc": 16}
{"file": "thefuck\\thefuck\\shells\\fish.py", "class_name": "Fish", "function_name": "get_aliases", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_aliases", "_get_functions", "functions.update", "self._get_overridden_aliases"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_aliases(self):\n    overridden = self._get_overridden_aliases()\n    functions = _get_functions(overridden)\n    raw_aliases = _get_aliases(overridden)\n    functions.update(raw_aliases)\n    return functions", "loc": 6}
{"file": "thefuck\\thefuck\\shells\\fish.py", "class_name": "Fish", "function_name": "put_to_history", "parameters": ["self", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["logs.exception", "self._put_to_history", "sys.exc_info"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def put_to_history(self, command):\n    try:\n        return self._put_to_history(command)\n    except IOError:\n        logs.exception(\"Can't update history\", sys.exc_info())", "loc": 5}
{"file": "thefuck\\thefuck\\shells\\generic.py", "class_name": "Generic", "function_name": "split_command", "parameters": ["self", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encoded.replace", "encoded.split", "s.replace", "self.decode_utf8", "self.encode_utf8", "shlex.split"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Split the command using shell-like syntax.", "source_code": "def split_command(self, command):\n    \"\"\"Split the command using shell-like syntax.\"\"\"\n    encoded = self.encode_utf8(command)\n\n    try:\n        splitted = [s.replace(\"??\", \"\\\\ \") for s in shlex.split(encoded.replace('\\\\ ', '??'))]\n    except ValueError:\n        splitted = encoded.split(' ')\n\n    return self.decode_utf8(splitted)", "loc": 10}
{"file": "thefuck\\thefuck\\shells\\generic.py", "class_name": "Generic", "function_name": "quote", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["quote"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a shell-escaped version of the string s.", "source_code": "def quote(self, s):\n    \"\"\"Return a shell-escaped version of the string s.\"\"\"\n\n    if six.PY2:\n        from pipes import quote\n    else:\n        from shlex import quote\n\n    return quote(s)", "loc": 9}
{"file": "thefuck\\thefuck\\shells\\generic.py", "class_name": "Generic", "function_name": "info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_version", "u'Could not determine shell version: {}'.format", "u'{} {}'.format", "u'{} {}'.format(self.friendly_name, version).rstrip", "warn"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def info(self):\n    \"\"\"Returns the name and version of the current shell\"\"\"\n    try:\n        version = self._get_version()\n    except Exception as e:\n        warn(u'Could not determine shell version: {}'.format(e))\n        version = ''\n    return u'{} {}'.format(self.friendly_name, version).rstrip()", "loc": 8}
{"file": "thefuck\\thefuck\\specific\\archlinux.py", "class_name": null, "function_name": "get_pkgfile", "parameters": ["command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.split", "command.startswith", "command.strip", "package.split", "subprocess.check_output", "subprocess.check_output(['pkgfile', '-b', '-v', command], universal_newlines=True, stderr=utils.DEVNULL).splitlines"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Gets the packages that provide the given command using `pkgfile`. If the command is of the form `sudo foo`, searches for the `foo` command instead.", "source_code": "def get_pkgfile(command):\n    \"\"\" Gets the packages that provide the given command using `pkgfile`.\n\n    If the command is of the form `sudo foo`, searches for the `foo` command\n    instead.\n    \"\"\"\n    try:\n        command = command.strip()\n\n        if command.startswith('sudo '):\n            command = command[5:]\n\n        command = command.split(\" \")[0]\n\n        packages = subprocess.check_output(\n            ['pkgfile', '-b', '-v', command],\n            universal_newlines=True, stderr=utils.DEVNULL\n        ).splitlines()\n\n        return [package.split()[0] for package in packages]\n    except subprocess.CalledProcessError as err:\n        if err.returncode == 1 and err.output == \"\":\n            return []\n        else:\n            raise err", "loc": 25}
{"file": "thefuck\\thefuck\\specific\\archlinux.py", "class_name": null, "function_name": "archlinux_env", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["utils.which"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def archlinux_env():\n    if utils.which('yay'):\n        pacman = 'yay'\n    elif utils.which('pikaur'):\n        pacman = 'pikaur'\n    elif utils.which('yaourt'):\n        pacman = 'yaourt'\n    elif utils.which('pacman'):\n        pacman = 'sudo pacman'\n    else:\n        return False, None\n\n    enabled_by_default = utils.which('pkgfile')\n\n    return enabled_by_default, pacman", "loc": 15}
{"file": "thefuck\\thefuck\\specific\\brew.py", "class_name": null, "function_name": "get_brew_path_prefix", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["subprocess.check_output", "subprocess.check_output(['brew', '--prefix'], universal_newlines=True).strip"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "To get brew path", "source_code": "def get_brew_path_prefix():\n    \"\"\"To get brew path\"\"\"\n    try:\n        return subprocess.check_output(['brew', '--prefix'],\n                                       universal_newlines=True).strip()\n    except Exception:\n        return None", "loc": 7}
{"file": "thefuck\\thefuck\\specific\\git.py", "class_name": null, "function_name": "git_support", "parameters": ["fn", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'\\\\b{}\\\\b'.format", "command.update", "fn", "is_app", "re.search", "re.sub", "search.group", "shell.quote", "shell.split_command"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Resolves git aliases and supports testing for both git and hub.", "source_code": "def git_support(fn, command):\n    \"\"\"Resolves git aliases and supports testing for both git and hub.\"\"\"\n    # supports GitHub's `hub` command\n    # which is recommended to be used with `alias git=hub`\n    # but at this point, shell aliases have already been resolved\n    if not is_app(command, 'git', 'hub'):\n        return False\n\n    # perform git aliases expansion\n    if command.output and 'trace: alias expansion:' in command.output:\n        search = re.search(\"trace: alias expansion: ([^ ]*) => ([^\\n]*)\",\n                           command.output)\n        alias = search.group(1)\n\n        # by default git quotes everything, for example:\n        #     'commit' '--amend'\n        # which is surprising and does not allow to easily test for\n        # eg. 'git commit'\n        expansion = ' '.join(shell.quote(part)\n                             for part in shell.split_command(search.group(2)))\n        new_script = re.sub(r\"\\b{}\\b\".format(alias), expansion, command.script)\n\n        command = command.update(script=new_script)\n\n    return fn(command)", "loc": 25}
{"file": "thefuck\\thefuck\\specific\\sudo.py", "class_name": null, "function_name": "sudo_support", "parameters": ["fn", "command"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["command.script.startswith", "command.update", "fn", "isinstance", "u'sudo {}'.format"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Removes sudo before calling fn and adds it after.", "source_code": "def sudo_support(fn, command):\n    \"\"\"Removes sudo before calling fn and adds it after.\"\"\"\n    if not command.script.startswith('sudo '):\n        return fn(command)\n\n    result = fn(command.update(script=command.script[5:]))\n\n    if result and isinstance(result, six.string_types):\n        return u'sudo {}'.format(result)\n    elif isinstance(result, list):\n        return [u'sudo {}'.format(x) for x in result]\n    else:\n        return result", "loc": 13}
{"file": "thefuck\\thefuck\\system\\unix.py", "class_name": null, "function_name": "getch", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["sys.stdin.fileno", "sys.stdin.read", "termios.tcgetattr", "termios.tcsetattr", "tty.setraw"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getch():\n    fd = sys.stdin.fileno()\n    old = termios.tcgetattr(fd)\n    try:\n        tty.setraw(fd)\n        return sys.stdin.read(1)\n    finally:\n        termios.tcsetattr(fd, termios.TCSADRAIN, old)", "loc": 8}
{"file": "thefuck\\thefuck\\system\\unix.py", "class_name": null, "function_name": "get_key", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getch"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_key():\n    ch = getch()\n\n    if ch in const.KEY_MAPPING:\n        return const.KEY_MAPPING[ch]\n    elif ch == '\\x1b':\n        next_ch = getch()\n        if next_ch == '[':\n            last_ch = getch()\n\n            if last_ch == 'A':\n                return const.KEY_UP\n            elif last_ch == 'B':\n                return const.KEY_DOWN\n\n    return ch", "loc": 16}
{"file": "thefuck\\thefuck\\system\\win32.py", "class_name": null, "function_name": "get_key", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["msvcrt.getwch"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_key():\n    ch = msvcrt.getwch()\n    if ch in ('\\x00', '\\xe0'):  # arrow or function key prefix?\n        ch = msvcrt.getwch()  # second call returns the actual key code\n\n    if ch in const.KEY_MAPPING:\n        return const.KEY_MAPPING[ch]\n    if ch == 'H':\n        return const.KEY_UP\n    if ch == 'P':\n        return const.KEY_DOWN\n\n    return ch", "loc": 13}
{"file": "tornado\\demos\\blog\\blog.py", "class_name": "BaseHandler", "function_name": "row_to_obj", "parameters": ["self", "row", "cur"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["tornado.util.ObjectDict", "zip"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Convert a SQL row to an object supporting dict and attribute access.", "source_code": "def row_to_obj(self, row, cur):\n    \"\"\"Convert a SQL row to an object supporting dict and attribute access.\"\"\"\n    obj = tornado.util.ObjectDict()\n    for val, desc in zip(row, cur.description):\n        obj[desc.name] = val\n    return obj", "loc": 6}
{"file": "tornado\\demos\\chat\\chatdemo.py", "class_name": "MessageBuffer", "function_name": "get_messages_since", "parameters": ["self", "cursor"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["results.append", "results.reverse", "reversed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_messages_since(self, cursor):\n    \"\"\"Returns a list of messages newer than the given cursor.\n\n    ``cursor`` should be the ``id`` of the last message received.\n    \"\"\"\n    results = []\n    for msg in reversed(self.cache):\n        if msg[\"id\"] == cursor:\n            break\n        results.append(msg)\n    results.reverse()\n    return results", "loc": 12}
{"file": "tornado\\demos\\chat\\chatdemo.py", "class_name": "MessageBuffer", "function_name": "add_message", "parameters": ["self", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.cache.append", "self.cond.notify_all"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_message(self, message):\n    self.cache.append(message)\n    if len(self.cache) > self.cache_size:\n        self.cache = self.cache[-self.cache_size :]\n    self.cond.notify_all()", "loc": 5}
{"file": "tornado\\demos\\chat\\chatdemo.py", "class_name": "MessageNewHandler", "function_name": "post", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["global_message_buffer.add_message", "self.get_argument", "self.redirect", "self.render_string", "self.write", "str", "tornado.escape.to_unicode", "uuid.uuid4"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post(self):\n    message = {\"id\": str(uuid.uuid4()), \"body\": self.get_argument(\"body\")}\n    # render_string() returns a byte string, which is not supported\n    # in json, so we must convert it to a character string.\n    message[\"html\"] = tornado.escape.to_unicode(\n        self.render_string(\"message.html\", message=message)\n    )\n    if self.get_argument(\"next\", None):\n        self.redirect(self.get_argument(\"next\"))\n    else:\n        self.write(message)\n    global_message_buffer.add_message(message)", "loc": 12}
{"file": "tornado\\demos\\facebook\\facebook.py", "class_name": "BaseHandler", "function_name": "get_current_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_signed_cookie", "tornado.escape.json_decode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_current_user(self):\n    user_json = self.get_signed_cookie(\"fbdemo_user\")\n    if not user_json:\n        return None\n    return tornado.escape.json_decode(user_json)", "loc": 5}
{"file": "tornado\\demos\\file_upload\\file_receiver.py", "class_name": "POSTHandler", "function_name": "post", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "logging.info", "self.request.files.items", "self.write"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post(self):\n    for field_name, files in self.request.files.items():\n        for info in files:\n            filename, content_type = info[\"filename\"], info[\"content_type\"]\n            body = info[\"body\"]\n            logging.info(\n                'POST \"%s\" \"%s\" %d bytes', filename, content_type, len(body)\n            )\n\n    self.write(\"OK\")", "loc": 10}
{"file": "tornado\\demos\\file_upload\\file_receiver.py", "class_name": "PUTHandler", "function_name": "put", "parameters": ["self", "filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["logging.info", "self.request.headers.get", "self.write", "unquote"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def put(self, filename):\n    filename = unquote(filename)\n    mtype = self.request.headers.get(\"Content-Type\")\n    logging.info('PUT \"%s\" \"%s\" %d bytes', filename, mtype, self.bytes_read)\n    self.write(\"OK\")", "loc": 5}
{"file": "tornado\\demos\\google_auth\\main.py", "class_name": "BaseHandler", "function_name": "get_current_user", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.loads", "self.get_signed_cookie"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def get_current_user(self):\n    user_cookie = self.get_signed_cookie(\"googledemo_user\")\n    if user_cookie:\n        return json.loads(user_cookie)\n    return None", "loc": 5}
{"file": "tornado\\demos\\s3server\\s3server.py", "class_name": "BaseRequestHandler", "function_name": "render_xml", "parameters": ["self", "value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "isinstance", "len", "list", "parts.append", "self._render_parts", "self.finish", "self.set_header", "value.keys"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def render_xml(self, value):\n    assert isinstance(value, dict) and len(value) == 1\n    self.set_header(\"Content-Type\", \"application/xml; charset=UTF-8\")\n    name = list(value.keys())[0]\n    parts = []\n    parts.append(\"<\" + name + ' xmlns=\"http://doc.s3.amazonaws.com/2006-03-01\">')\n    self._render_parts(value[name], parts)\n    parts.append(\"</\" + name + \">\")\n    self.finish('<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n' + \"\".join(parts))", "loc": 9}
{"file": "tornado\\demos\\tcpecho\\server.py", "class_name": "EchoServer", "function_name": "handle_stream", "parameters": ["self", "stream", "address"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.endswith", "logger.info", "logger.warning", "print", "stream.read_until", "stream.write"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_stream(self, stream, address):\n    while True:\n        try:\n            data = yield stream.read_until(b\"\\n\")\n            logger.info(\"Received bytes: %s\", data)\n            if not data.endswith(b\"\\n\"):\n                data = data + b\"\\n\"\n            yield stream.write(data)\n        except StreamClosedError:\n            logger.warning(\"Lost client at host %s\", address[0])\n            break\n        except Exception as e:\n            print(e)", "loc": 13}
{"file": "tornado\\demos\\websocket\\chatdemo.py", "class_name": "ChatSocketHandler", "function_name": "send_updates", "parameters": ["cls", "chat"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "logging.error", "logging.info", "waiter.write_message"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def send_updates(cls, chat):\n    logging.info(\"sending message to %d waiters\", len(cls.waiters))\n    for waiter in cls.waiters:\n        try:\n            waiter.write_message(chat)\n        except:\n            logging.error(\"Error sending message\", exc_info=True)", "loc": 7}
{"file": "tornado\\demos\\websocket\\chatdemo.py", "class_name": "ChatSocketHandler", "function_name": "on_message", "parameters": ["self", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ChatSocketHandler.send_updates", "ChatSocketHandler.update_cache", "logging.info", "self.render_string", "str", "tornado.escape.json_decode", "tornado.escape.to_basestring", "uuid.uuid4"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_message(self, message):\n    logging.info(\"got message %r\", message)\n    parsed = tornado.escape.json_decode(message)\n    chat = {\"id\": str(uuid.uuid4()), \"body\": parsed[\"body\"]}\n    chat[\"html\"] = tornado.escape.to_basestring(\n        self.render_string(\"message.html\", message=chat)\n    )\n\n    ChatSocketHandler.update_cache(chat)\n    ChatSocketHandler.send_updates(chat)", "loc": 10}
{"file": "tornado\\demos\\webspider\\webspider.py", "class_name": null, "function_name": "get_links", "parameters": ["html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTMLParser.__init__", "URLSeeker", "dict", "dict(attrs).get", "self.urls.append", "url_seeker.feed"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_links(html):\n    class URLSeeker(HTMLParser):\n        def __init__(self):\n            HTMLParser.__init__(self)\n            self.urls = []\n\n        def handle_starttag(self, tag, attrs):\n            href = dict(attrs).get(\"href\")\n            if href and tag == \"a\":\n                self.urls.append(href)\n\n    url_seeker = URLSeeker()\n    url_seeker.feed(html)\n    return url_seeker.urls", "loc": 14}
{"file": "tornado\\maint\\benchmark\\chunk_benchmark.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Application", "CurlAsyncHTTPClient", "IOLoop.current", "IOLoop.current().start", "IOLoop.current().stop", "SimpleAsyncHTTPClient", "app.listen", "curl_client.fetch", "len", "logging.warning", "parse_command_line", "response.rethrow", "simple_client.fetch"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parse_command_line()\n    app = Application([('/', ChunkHandler)])\n    app.listen(options.port, address='127.0.0.1')\n\n    def callback(response):\n        response.rethrow()\n        assert len(response.body) == (options.num_chunks * options.chunk_size)\n        logging.warning(\"fetch completed in %s seconds\", response.request_time)\n        IOLoop.current().stop()\n\n    logging.warning(\"Starting fetch with curl client\")\n    curl_client = CurlAsyncHTTPClient()\n    curl_client.fetch('http://localhost:%d/' % options.port,\n                      callback=callback)\n    IOLoop.current().start()\n\n    logging.warning(\"Starting fetch with simple client\")\n    simple_client = SimpleAsyncHTTPClient()\n    simple_client.fetch('http://localhost:%d/' % options.port,\n                        callback=callback)\n    IOLoop.current().start()", "loc": 22}
{"file": "tornado\\maint\\benchmark\\chunk_benchmark.py", "class_name": "ChunkHandler", "function_name": "get", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.finish", "self.flush", "self.write", "xrange"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(self):\n    for i in xrange(options.num_chunks):\n        self.write('A' * options.chunk_size)\n        self.flush()\n    self.finish()", "loc": 5}
{"file": "tornado\\maint\\benchmark\\chunk_benchmark.py", "class_name": null, "function_name": "callback", "parameters": ["response"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "IOLoop.current().stop", "len", "logging.warning", "response.rethrow"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def callback(response):\n    response.rethrow()\n    assert len(response.body) == (options.num_chunks * options.chunk_size)\n    logging.warning(\"fetch completed in %s seconds\", response.request_time)\n    IOLoop.current().stop()", "loc": 5}
{"file": "tornado\\maint\\benchmark\\gen_benchmark.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Timer", "parse_command_line", "print", "t.timeit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parse_command_line()\n    t = Timer(e1)\n    results = t.timeit(options.num) / options.num\n    print('engine: %0.3f ms per iteration' % (results * 1000))\n    t = Timer(c1)\n    results = t.timeit(options.num) / options.num\n    print('coroutine: %0.3f ms per iteration' % (results * 1000))", "loc": 8}
{"file": "tornado\\maint\\benchmark\\parsing_benchmark.py", "class_name": null, "function_name": "headers_parse_re", "parameters": ["headers"], "param_types": {"headers": "str"}, "return_type": "HTTPHeaders", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPHeaders", "_CRLF_RE.split", "h.parse_line"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers_parse_re(headers: str) -> HTTPHeaders:\n    h = HTTPHeaders()\n    for line in _CRLF_RE.split(headers):\n        if line:\n            h.parse_line(line)\n    return h", "loc": 6}
{"file": "tornado\\maint\\benchmark\\parsing_benchmark.py", "class_name": null, "function_name": "headers_parse_simple", "parameters": ["headers"], "param_types": {"headers": "str"}, "return_type": "HTTPHeaders", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPHeaders", "h.parse_line", "headers.split", "line.endswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers_parse_simple(headers: str) -> HTTPHeaders:\n    h = HTTPHeaders()\n    for line in headers.split(\"\\n\"):\n        if line.endswith(\"\\r\"):\n            line = line[:-1]\n        if line:\n            h.parse_line(line)\n    return h", "loc": 8}
{"file": "tornado\\maint\\benchmark\\parsing_benchmark.py", "class_name": null, "function_name": "run_headers_split", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["headers_split_re", "headers_split_simple", "print", "timeit.timeit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_headers_split():\n    regex_time = timeit.timeit(lambda: headers_split_re(_TEST_HEADERS), number=100000)\n    print(\"regex\", regex_time)\n\n    simple_time = timeit.timeit(\n        lambda: headers_split_simple(_TEST_HEADERS), number=100000\n    )\n    print(\"str.split\", simple_time)\n\n    print(\"speedup\", regex_time / simple_time)", "loc": 10}
{"file": "tornado\\maint\\benchmark\\parsing_benchmark.py", "class_name": null, "function_name": "run_headers_full", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["headers_parse_re", "headers_parse_simple", "print", "timeit.timeit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_headers_full():\n    regex_time = timeit.timeit(lambda: headers_parse_re(_TEST_HEADERS), number=10000)\n    print(\"regex\", regex_time)\n\n    simple_time = timeit.timeit(\n        lambda: headers_parse_simple(_TEST_HEADERS), number=10000\n    )\n    print(\"str.split\", simple_time)\n\n    print(\"speedup\", regex_time / simple_time)", "loc": 10}
{"file": "tornado\\maint\\benchmark\\parsing_benchmark.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"Unknown benchmark: '{}', supported values are: {}\".format", "', '.join", "Benchmark", "func", "parse_command_line", "print", "range"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parse_command_line()\n\n    try:\n        func = Benchmark(options.benchmark).func\n    except ValueError:\n        known_benchmarks = [benchmark.value for benchmark in Benchmark]\n        print(\n            \"Unknown benchmark: '{}', supported values are: {}\"\n            .format(options.benchmark, \", \".join(known_benchmarks))\n        )\n        return\n\n    for _ in range(options.num_runs):\n        func()", "loc": 15}
{"file": "tornado\\maint\\benchmark\\template_benchmark.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Timer", "parse_command_line", "print", "sys.exit", "t.timeit"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parse_command_line()\n    if options.dump:\n        print(tmpl.code)\n        sys.exit(0)\n    t = Timer(render)\n    results = t.timeit(options.num) / options.num\n    print('%0.3f ms per iteration' % (results * 1000))", "loc": 8}
{"file": "tornado\\maint\\scripts\\custom_fixers\\fix_future_imports.py", "class_name": "FixFutureImports", "function_name": "new_future_import", "parameters": ["self", "old"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Comma", "FromImport", "Name"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def new_future_import(self, old):\n    new = FromImport(\"__future__\",\n                     [Name(\"absolute_import\", prefix=\" \"), Comma(),\n                      Name(\"division\", prefix=\" \"), Comma(),\n                      Name(\"print_function\", prefix=\" \")])\n    if old is not None:\n        new.prefix = old.prefix\n    return new", "loc": 8}
{"file": "tornado\\maint\\scripts\\custom_fixers\\fix_future_imports.py", "class_name": "FixFutureImports", "function_name": "finish_tree", "parameters": ["self", "tree", "filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Newline", "is_docstring", "isinstance", "self.new_future_import", "tree.insert_child"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finish_tree(self, tree, filename):\n    if self.found_future_import:\n        return\n    if not isinstance(tree, pytree.Node):\n        # Empty files (usually __init__.py) show up as a single Leaf\n        # instead of a Node, so leave them alone\n        return\n    first_stmt = tree.children[0]\n    if is_docstring(first_stmt):\n        # Skip a line and add the import after the docstring\n        tree.insert_child(1, Newline())\n        pos = 2\n    elif first_stmt.prefix:\n        # No docstring, but an initial comment (perhaps a #! line).\n        # Transfer the initial comment to a new blank line.\n        newline = Newline()\n        newline.prefix = first_stmt.prefix\n        first_stmt.prefix = \"\"\n        tree.insert_child(0, newline)\n        pos = 1\n    else:\n        # No comments or docstring, just insert at the start\n        pos = 0\n    tree.insert_child(pos, self.new_future_import(None))\n    tree.insert_child(pos + 1, Newline())  # terminates the import stmt", "loc": 25}
{"file": "tornado\\tornado\\auth.py", "class_name": "OAuth2Mixin", "function_name": "authorize_redirect", "parameters": ["self", "redirect_uri", "client_id", "client_secret", "extra_params", "scope", "response_type"], "param_types": {"redirect_uri": "Optional[str]", "client_id": "Optional[str]", "client_secret": "Optional[str]", "extra_params": "Optional[Dict[str, Any]]", "scope": "Optional[List[str]]", "response_type": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "args.update", "cast", "handler.redirect", "url_concat", "warnings.warn"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Redirects the user to obtain OAuth authorization for this service. Some providers require that you register a redirect URL with your application instead of passing one via this method. You", "source_code": "def authorize_redirect(\n    self,\n    redirect_uri: Optional[str] = None,\n    client_id: Optional[str] = None,\n    client_secret: Optional[str] = None,\n    extra_params: Optional[Dict[str, Any]] = None,\n    scope: Optional[List[str]] = None,\n    response_type: str = \"code\",\n) -> None:\n    \"\"\"Redirects the user to obtain OAuth authorization for this service.\n\n    Some providers require that you register a redirect URL with\n    your application instead of passing one via this method. You\n    should call this method to log the user in, and then call\n    ``get_authenticated_user`` in the handler for your\n    redirect URL to complete the authorization process.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument and returned awaitable were removed;\n       this is now an ordinary synchronous function.\n\n    .. deprecated:: 6.4\n       The ``client_secret`` argument (which has never had any effect)\n       is deprecated and will be removed in Tornado 7.0.\n    \"\"\"\n    if client_secret is not None:\n        warnings.warn(\"client_secret argument is deprecated\", DeprecationWarning)\n    handler = cast(RequestHandler, self)\n    args = {\"response_type\": response_type}\n    if redirect_uri is not None:\n        args[\"redirect_uri\"] = redirect_uri\n    if client_id is not None:\n        args[\"client_id\"] = client_id\n    if extra_params:\n        args.update(extra_params)\n    if scope:\n        args[\"scope\"] = \" \".join(scope)\n    url = self._OAUTH_AUTHORIZE_URL  # type: ignore\n    handler.redirect(url_concat(url, args))", "loc": 40}
{"file": "tornado\\tornado\\auth.py", "class_name": null, "function_name": "get_ax_arg", "parameters": ["uri"], "param_types": {"uri": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["handler.get_argument", "handler.request.arguments.keys", "len", "name.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ax_arg(uri: str) -> str:\n    if not ax_ns:\n        return \"\"\n    prefix = \"openid.\" + ax_ns + \".type.\"\n    ax_name = None\n    for name in handler.request.arguments.keys():\n        if handler.get_argument(name) == uri and name.startswith(prefix):\n            part = name[len(prefix) :]\n            ax_name = \"openid.\" + ax_ns + \".value.\" + part\n            break\n    if not ax_name:\n        return \"\"\n    return handler.get_argument(ax_name, \"\")", "loc": 13}
{"file": "tornado\\tornado\\autoreload.py", "class_name": null, "function_name": "start", "parameters": ["check_time"], "param_types": {"check_time": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "gen_log.warning", "ioloop.IOLoop.current", "ioloop.PeriodicCallback", "len", "scheduler.start"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Begins watching source files for changes. .. versionchanged:: 5.0 The ``io_loop`` argument (deprecated since version 4.1) has been removed.", "source_code": "def start(check_time: int = 500) -> None:\n    \"\"\"Begins watching source files for changes.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n    \"\"\"\n    io_loop = ioloop.IOLoop.current()\n    if io_loop in _io_loops:\n        return\n    _io_loops[io_loop] = True\n    if len(_io_loops) > 1:\n        gen_log.warning(\"tornado.autoreload started more than once in the same process\")\n    modify_times: Dict[str, float] = {}\n    callback = functools.partial(_reload_on_update, modify_times)\n    scheduler = ioloop.PeriodicCallback(callback, check_time)\n    scheduler.start()", "loc": 16}
{"file": "tornado\\tornado\\autoreload.py", "class_name": null, "function_name": "wait", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["io_loop.add_callback", "io_loop.start", "ioloop.IOLoop"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Wait for a watched file to change, then restart the process. Intended to be used at the end of scripts like unit test runners, to run the tests again after any source file changes (but see also", "source_code": "def wait() -> None:\n    \"\"\"Wait for a watched file to change, then restart the process.\n\n    Intended to be used at the end of scripts like unit test runners,\n    to run the tests again after any source file changes (but see also\n    the command-line interface in `main`)\n    \"\"\"\n    io_loop = ioloop.IOLoop()\n    io_loop.add_callback(start)\n    io_loop.start()", "loc": 10}
{"file": "tornado\\tornado\\autoreload.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_log.info", "gen_log.warning", "getattr", "isinstance", "loader.get_filename", "optparse.OptionParser", "parser.add_option", "parser.disable_interspersed_args", "parser.parse_args", "pkgutil.get_loader", "print", "runpy.run_module", "runpy.run_path", "sys.exc_info", "sys.exit", "traceback.extract_tb", "wait", "watch"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Command-line wrapper to re-run a script whenever its source changes. Scripts may be specified by filename or module name:: python -m tornado.autoreload -m tornado.test.runtests", "source_code": "def main() -> None:\n    \"\"\"Command-line wrapper to re-run a script whenever its source changes.\n\n    Scripts may be specified by filename or module name::\n\n        python -m tornado.autoreload -m tornado.test.runtests\n        python -m tornado.autoreload tornado/test/runtests.py\n\n    Running a script with this wrapper is similar to calling\n    `tornado.autoreload.wait` at the end of the script, but this wrapper\n    can catch import-time problems like syntax errors that would otherwise\n    prevent the script from reaching its call to `wait`.\n    \"\"\"\n    # Remember that we were launched with autoreload as main.\n    # The main module can be tricky; set the variables both in our globals\n    # (which may be __main__) and the real importable version.\n    #\n    # We use optparse instead of the newer argparse because we want to\n    # mimic the python command-line interface which requires stopping\n    # parsing at the first positional argument. optparse supports\n    # this but as far as I can tell argparse does not.\n    import optparse\n    import tornado.autoreload\n\n    global _autoreload_is_main\n    global _original_argv, _original_spec\n    tornado.autoreload._autoreload_is_main = _autoreload_is_main = True\n    original_argv = sys.argv\n    tornado.autoreload._original_argv = _original_argv = original_argv\n    original_spec = getattr(sys.modules[\"__main__\"], \"__spec__\", None)\n    tornado.autoreload._original_spec = _original_spec = original_spec\n\n    parser = optparse.OptionParser(\n        prog=\"python -m tornado.autoreload\",\n        usage=_USAGE,\n        epilog=\"Either -m or a path must be specified, but not both\",\n    )\n    parser.disable_interspersed_args()\n    parser.add_option(\"-m\", dest=\"module\", metavar=\"module\", help=\"module to run\")\n    parser.add_option(\n        \"--until-success\",\n        action=\"store_true\",\n        help=\"stop reloading after the program exist successfully (status code 0)\",\n    )\n    opts, rest = parser.parse_args()\n    if opts.module is None:\n        if not rest:\n            print(\"Either -m or a path must be specified\", file=sys.stderr)\n            sys.exit(1)\n        path = rest[0]\n        sys.argv = rest[:]\n    else:\n        path = None\n        sys.argv = [sys.argv[0]] + rest\n\n    # SystemExit.code is typed funny: https://github.com/python/typeshed/issues/8513\n    # All we care about is truthiness\n    exit_status: Union[int, str, None] = 1\n    try:\n        import runpy\n\n        if opts.module is not None:\n            runpy.run_module(opts.module, run_name=\"__main__\", alter_sys=True)\n        else:\n            assert path is not None\n            runpy.run_path(path, run_name=\"__main__\")\n    except SystemExit as e:\n        exit_status = e.code\n        gen_log.info(\"Script exited with status %s\", e.code)\n    except Exception as e:\n        gen_log.warning(\"Script exited with uncaught exception\", exc_info=True)\n        # If an exception occurred at import time, the file with the error\n        # never made it into sys.modules and so we won't know to watch it.\n        # Just to make sure we've covered everything, walk the stack trace\n        # from the exception and watch every file.\n        for filename, lineno, name, line in traceback.extract_tb(sys.exc_info()[2]):\n            watch(filename)\n        if isinstance(e, SyntaxError):\n            # SyntaxErrors are special:  their innermost stack frame is fake\n            # so extract_tb won't see it and we have to get the filename\n            # from the exception object.\n            if e.filename is not None:\n                watch(e.filename)\n    else:\n        exit_status = 0\n        gen_log.info(\"Script exited normally\")\n    # restore sys.argv so subsequent executions will include autoreload\n    sys.argv = original_argv\n\n    if opts.module is not None:\n        assert opts.module is not None\n        # runpy did a fake import of the module as __main__, but now it's\n        # no longer in sys.modules.  Figure out where it is and watch it.\n        loader = pkgutil.get_loader(opts.module)\n        if loader is not None and isinstance(loader, importlib.abc.FileLoader):\n            watch(loader.get_filename())\n    if opts.until_success and not exit_status:\n        return\n    wait()", "loc": 99}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "run_on_executor", "parameters": [], "param_types": {}, "return_type": "Callable", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "ValueError", "chain_future", "functools.wraps", "getattr", "getattr(self, executor).submit", "kwargs.get", "len", "run_on_executor_decorator"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Decorator to run a synchronous method asynchronously on an executor.", "source_code": "def run_on_executor(*args: Any, **kwargs: Any) -> Callable:\n    \"\"\"Decorator to run a synchronous method asynchronously on an executor.\n\n    Returns a future.\n\n    The executor to be used is determined by the ``executor``\n    attributes of ``self``. To use a different attribute name, pass a\n    keyword argument to the decorator::\n\n        @run_on_executor(executor='_thread_pool')\n        def foo(self):\n            pass\n\n    This decorator should not be confused with the similarly-named\n    `.IOLoop.run_in_executor`. In general, using ``run_in_executor``\n    when *calling* a blocking method is recommended instead of using\n    this decorator when *defining* a method. If compatibility with older\n    versions of Tornado is required, consider defining an executor\n    and using ``executor.submit()`` at the call site.\n\n    .. versionchanged:: 4.2\n       Added keyword arguments to use alternative attributes.\n\n    .. versionchanged:: 5.0\n       Always uses the current IOLoop instead of ``self.io_loop``.\n\n    .. versionchanged:: 5.1\n       Returns a `.Future` compatible with ``await`` instead of a\n       `concurrent.futures.Future`.\n\n    .. deprecated:: 5.1\n\n       The ``callback`` argument is deprecated and will be removed in\n       6.0. The decorator itself is discouraged in new code but will\n       not be removed in 6.0.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed.\n    \"\"\"\n\n    # Fully type-checking decorators is tricky, and this one is\n    # discouraged anyway so it doesn't have all the generic magic.\n    def run_on_executor_decorator(fn: Callable) -> Callable[..., Future]:\n        executor = kwargs.get(\"executor\", \"executor\")\n\n        @functools.wraps(fn)\n        def wrapper(self: Any, *args: Any, **kwargs: Any) -> Future:\n            async_future = Future()  # type: Future\n            conc_future = getattr(self, executor).submit(fn, self, *args, **kwargs)\n            chain_future(conc_future, async_future)\n            return async_future\n\n        return wrapper\n\n    if args and kwargs:\n        raise ValueError(\"cannot combine positional and keyword args\")\n    if len(args) == 1:\n        return run_on_executor_decorator(args[0])\n    elif len(args) != 0:\n        raise ValueError(\"expected 1 argument, got %d\", len(args))\n    return run_on_executor_decorator", "loc": 62}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "chain_future", "parameters": ["a", "b"], "param_types": {"a": "Union['Future[_T]', 'futures.Future[_T]']", "b": "Union['Future[_T]', 'futures.Future[_T]']"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "IOLoop.current().add_future", "a.exc_info", "a.exception", "a.result", "b.done", "b.set_exception", "b.set_result", "future_add_done_callback", "future_set_exc_info", "hasattr", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Chain two futures together so that when one completes, so does the other. The result (success or failure) of ``a`` will be copied to ``b``, unless ``b`` has already been completed or cancelled by the time ``a`` finishes.", "source_code": "def chain_future(\n    a: Union[\"Future[_T]\", \"futures.Future[_T]\"],\n    b: Union[\"Future[_T]\", \"futures.Future[_T]\"],\n) -> None:\n    \"\"\"Chain two futures together so that when one completes, so does the other.\n\n    The result (success or failure) of ``a`` will be copied to ``b``, unless\n    ``b`` has already been completed or cancelled by the time ``a`` finishes.\n\n    .. versionchanged:: 5.0\n\n       Now accepts both Tornado/asyncio `Future` objects and\n       `concurrent.futures.Future`.\n\n    \"\"\"\n\n    def copy(a: \"Future[_T]\") -> None:\n        if b.done():\n            return\n        if hasattr(a, \"exc_info\") and a.exc_info() is not None:  # type: ignore\n            future_set_exc_info(b, a.exc_info())  # type: ignore\n        else:\n            a_exc = a.exception()\n            if a_exc is not None:\n                b.set_exception(a_exc)\n            else:\n                b.set_result(a.result())\n\n    if isinstance(a, Future):\n        future_add_done_callback(a, copy)\n    else:\n        # concurrent.futures.Future\n        from tornado.ioloop import IOLoop\n\n        IOLoop.current().add_future(a, copy)", "loc": 35}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "future_set_result_unless_cancelled", "parameters": ["future", "value"], "param_types": {"future": "'Union[futures.Future[_T], Future[_T]]'", "value": "_T"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future.cancelled", "future.set_result"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Set the given ``value`` as the `Future`'s result, if not cancelled. Avoids ``asyncio.InvalidStateError`` when calling ``set_result()`` on a cancelled `asyncio.Future`.", "source_code": "def future_set_result_unless_cancelled(\n    future: \"Union[futures.Future[_T], Future[_T]]\", value: _T\n) -> None:\n    \"\"\"Set the given ``value`` as the `Future`'s result, if not cancelled.\n\n    Avoids ``asyncio.InvalidStateError`` when calling ``set_result()`` on\n    a cancelled `asyncio.Future`.\n\n    .. versionadded:: 5.0\n    \"\"\"\n    if not future.cancelled():\n        future.set_result(value)", "loc": 12}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "future_set_exception_unless_cancelled", "parameters": ["future", "exc"], "param_types": {"future": "'Union[futures.Future[_T], Future[_T]]'", "exc": "BaseException"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.error", "future.cancelled", "future.set_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Set the given ``exc`` as the `Future`'s exception. If the Future is already canceled, logs the exception instead. If this logging is not desired, the caller should explicitly check", "source_code": "def future_set_exception_unless_cancelled(\n    future: \"Union[futures.Future[_T], Future[_T]]\", exc: BaseException\n) -> None:\n    \"\"\"Set the given ``exc`` as the `Future`'s exception.\n\n    If the Future is already canceled, logs the exception instead. If\n    this logging is not desired, the caller should explicitly check\n    the state of the Future and call ``Future.set_exception`` instead of\n    this wrapper.\n\n    Avoids ``asyncio.InvalidStateError`` when calling ``set_exception()`` on\n    a cancelled `asyncio.Future`.\n\n    .. versionadded:: 6.0\n\n    \"\"\"\n    if not future.cancelled():\n        future.set_exception(exc)\n    else:\n        app_log.error(\"Exception after Future was cancelled\", exc_info=exc)", "loc": 20}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "future_set_exc_info", "parameters": ["future", "exc_info"], "param_types": {"future": "'Union[futures.Future[_T], Future[_T]]'", "exc_info": "Tuple[Optional[type], Optional[BaseException], Optional[types.TracebackType]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "future_set_exception_unless_cancelled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Set the given ``exc_info`` as the `Future`'s exception. Understands both `asyncio.Future` and the extensions in older versions of Tornado to enable better tracebacks on Python 2.", "source_code": "def future_set_exc_info(\n    future: \"Union[futures.Future[_T], Future[_T]]\",\n    exc_info: Tuple[\n        Optional[type], Optional[BaseException], Optional[types.TracebackType]\n    ],\n) -> None:\n    \"\"\"Set the given ``exc_info`` as the `Future`'s exception.\n\n    Understands both `asyncio.Future` and the extensions in older\n    versions of Tornado to enable better tracebacks on Python 2.\n\n    .. versionadded:: 5.0\n\n    .. versionchanged:: 6.0\n\n       If the future is already cancelled, this function is a no-op.\n       (previously ``asyncio.InvalidStateError`` would be raised)\n\n    \"\"\"\n    if exc_info[1] is None:\n        raise Exception(\"future_set_exc_info called with no exception\")\n    future_set_exception_unless_cancelled(future, exc_info[1])", "loc": 22}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "future_add_done_callback", "parameters": ["future", "callback"], "param_types": {"future": "'Union[futures.Future[_T], Future[_T]]'", "callback": "Callable[..., None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "future.add_done_callback", "future.done"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Arrange to call ``callback`` when ``future`` is complete. ``callback`` is invoked with one argument, the ``future``. If ``future`` is already done, ``callback`` is invoked immediately.", "source_code": "def future_add_done_callback(  # noqa: F811\n    future: \"Union[futures.Future[_T], Future[_T]]\", callback: Callable[..., None]\n) -> None:\n    \"\"\"Arrange to call ``callback`` when ``future`` is complete.\n\n    ``callback`` is invoked with one argument, the ``future``.\n\n    If ``future`` is already done, ``callback`` is invoked immediately.\n    This may differ from the behavior of ``Future.add_done_callback``,\n    which makes no such guarantee.\n\n    .. versionadded:: 5.0\n    \"\"\"\n    if future.done():\n        callback(future)\n    else:\n        future.add_done_callback(callback)", "loc": 17}
{"file": "tornado\\tornado\\concurrent.py", "class_name": "DummyExecutor", "function_name": "submit", "parameters": ["self", "fn"], "param_types": {"fn": "Callable[..., _T]"}, "return_type": "'futures.Future[_T]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fn", "future_set_exc_info", "future_set_result_unless_cancelled", "futures.Future", "sys.exc_info"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def submit(  # type: ignore[override]\n    self, fn: Callable[..., _T], *args: Any, **kwargs: Any\n) -> \"futures.Future[_T]\":\n    future = futures.Future()  # type: futures.Future[_T]\n    try:\n        future_set_result_unless_cancelled(future, fn(*args, **kwargs))\n    except Exception:\n        future_set_exc_info(future, sys.exc_info())\n    return future", "loc": 9}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "run_on_executor_decorator", "parameters": ["fn"], "param_types": {"fn": "Callable"}, "return_type": "Callable[..., Future]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "chain_future", "functools.wraps", "getattr", "getattr(self, executor).submit", "kwargs.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_on_executor_decorator(fn: Callable) -> Callable[..., Future]:\n    executor = kwargs.get(\"executor\", \"executor\")\n\n    @functools.wraps(fn)\n    def wrapper(self: Any, *args: Any, **kwargs: Any) -> Future:\n        async_future = Future()  # type: Future\n        conc_future = getattr(self, executor).submit(fn, self, *args, **kwargs)\n        chain_future(conc_future, async_future)\n        return async_future\n\n    return wrapper", "loc": 11}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "copy", "parameters": ["a"], "param_types": {"a": "'Future[_T]'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["a.exc_info", "a.exception", "a.result", "b.done", "b.set_exception", "b.set_result", "future_set_exc_info", "hasattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def copy(a: \"Future[_T]\") -> None:\n    if b.done():\n        return\n    if hasattr(a, \"exc_info\") and a.exc_info() is not None:  # type: ignore\n        future_set_exc_info(b, a.exc_info())  # type: ignore\n    else:\n        a_exc = a.exception()\n        if a_exc is not None:\n            b.set_exception(a_exc)\n        else:\n            b.set_result(a.result())", "loc": 11}
{"file": "tornado\\tornado\\concurrent.py", "class_name": null, "function_name": "wrapper", "parameters": ["self"], "param_types": {"self": "Any"}, "return_type": "Future", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "chain_future", "functools.wraps", "getattr", "getattr(self, executor).submit"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(self: Any, *args: Any, **kwargs: Any) -> Future:\n    async_future = Future()  # type: Future\n    conc_future = getattr(self, executor).submit(fn, self, *args, **kwargs)\n    chain_future(conc_future, async_future)\n    return async_future", "loc": 5}
{"file": "tornado\\tornado\\curl_httpclient.py", "class_name": "CurlAsyncHTTPClient", "function_name": "initialize", "parameters": ["self", "max_clients", "defaults"], "param_types": {"max_clients": "int", "defaults": "Optional[Dict[str, Any]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["collections.deque", "ioloop.PeriodicCallback", "pycurl.Curl", "pycurl.CurlMulti", "range", "self._curl_create", "self._force_timeout_callback.start", "self._multi.add_handle", "self._multi.remove_handle", "self._multi.setopt", "super", "super().initialize"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(  # type: ignore\n    self, max_clients: int = 10, defaults: Optional[Dict[str, Any]] = None\n) -> None:\n    super().initialize(defaults=defaults)\n    # Typeshed is incomplete for CurlMulti, so just use Any for now.\n    self._multi = pycurl.CurlMulti()  # type: Any\n    self._multi.setopt(pycurl.M_TIMERFUNCTION, self._set_timeout)\n    self._multi.setopt(pycurl.M_SOCKETFUNCTION, self._handle_socket)\n    self._curls = [self._curl_create() for i in range(max_clients)]\n    self._free_list = self._curls[:]\n    self._requests = (\n        collections.deque()\n    )  # type: Deque[Tuple[HTTPRequest, Callable[[HTTPResponse], None], float]]\n    self._fds = {}  # type: Dict[int, int]\n    self._timeout = None  # type: Optional[object]\n\n    # libcurl has bugs that sometimes cause it to not report all\n    # relevant file descriptors and timeouts to TIMERFUNCTION/\n    # SOCKETFUNCTION.  Mitigate the effects of such bugs by\n    # forcing a periodic scan of all active requests.\n    self._force_timeout_callback = ioloop.PeriodicCallback(\n        self._handle_force_timeout, 1000\n    )\n    self._force_timeout_callback.start()\n\n    # Work around a bug in libcurl 7.29.0: Some fields in the curl\n    # multi object are initialized lazily, and its destructor will\n    # segfault if it is destroyed without having been used.  Add\n    # and remove a dummy handle to make sure everything is\n    # initialized.\n    dummy_curl_handle = pycurl.Curl()\n    self._multi.add_handle(dummy_curl_handle)\n    self._multi.remove_handle(dummy_curl_handle)", "loc": 33}
{"file": "tornado\\tornado\\curl_httpclient.py", "class_name": "CurlAsyncHTTPClient", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["curl.close", "self._force_timeout_callback.stop", "self._multi.close", "self.io_loop.remove_timeout", "super", "super().close"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    self._force_timeout_callback.stop()\n    if self._timeout is not None:\n        self.io_loop.remove_timeout(self._timeout)\n    for curl in self._curls:\n        curl.close()\n    self._multi.close()\n    super().close()\n\n    # Set below properties to None to reduce the reference count of current\n    # instance, because those properties hold some methods of current\n    # instance that will case circular reference.\n    self._force_timeout_callback = None  # type: ignore\n    self._multi = None", "loc": 14}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "xhtml_escape", "parameters": ["value"], "param_types": {"value": "Union[str, bytes]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["html.escape", "to_unicode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Escapes a string so it is valid within HTML or XML. Escapes the characters ``<``, ``>``, ``\"``, ``'``, and ``&``. When used in attribute values the escaped strings must be enclosed", "source_code": "def xhtml_escape(value: Union[str, bytes]) -> str:\n    \"\"\"Escapes a string so it is valid within HTML or XML.\n\n    Escapes the characters ``<``, ``>``, ``\"``, ``'``, and ``&``.\n    When used in attribute values the escaped strings must be enclosed\n    in quotes.\n\n    Equivalent to `html.escape` except that this function always returns\n    type `str` while `html.escape` returns `bytes` if its input is `bytes`.\n\n    .. versionchanged:: 3.2\n\n       Added the single quote to the list of escaped characters.\n\n    .. versionchanged:: 6.4\n\n       Now simply wraps `html.escape`. This is equivalent to the old behavior\n       except that single quotes are now escaped as ``&#x27;`` instead of\n       ``&#39;`` and performance may be different.\n    \"\"\"\n    return html.escape(to_unicode(value))", "loc": 21}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "xhtml_unescape", "parameters": ["value"], "param_types": {"value": "Union[str, bytes]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["html.unescape", "to_unicode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Un-escapes an XML-escaped string. Equivalent to `html.unescape` except that this function always returns type `str` while `html.unescape` returns `bytes` if its input is `bytes`.", "source_code": "def xhtml_unescape(value: Union[str, bytes]) -> str:\n    \"\"\"Un-escapes an XML-escaped string.\n\n    Equivalent to `html.unescape` except that this function always returns\n    type `str` while `html.unescape` returns `bytes` if its input is `bytes`.\n\n    .. versionchanged:: 6.4\n\n       Now simply wraps `html.unescape`. This changes behavior for some inputs\n       as required by the HTML 5 specification\n       https://html.spec.whatwg.org/multipage/parsing.html#numeric-character-reference-end-state\n\n       Some invalid inputs such as surrogates now raise an error, and numeric\n       references to certain ISO-8859-1 characters are now handled correctly.\n    \"\"\"\n    return html.unescape(to_unicode(value))", "loc": 16}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "json_encode", "parameters": ["value"], "param_types": {"value": "Any"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "json.dumps(value).replace"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "JSON-encodes the given Python object. Equivalent to `json.dumps` with the additional guarantee that the output will never contain the character sequence ``</`` which can be problematic", "source_code": "def json_encode(value: Any) -> str:\n    \"\"\"JSON-encodes the given Python object.\n\n    Equivalent to `json.dumps` with the additional guarantee that the output\n    will never contain the character sequence ``</`` which can be problematic\n    when JSON is embedded in an HTML ``<script>`` tag.\n    \"\"\"\n    # JSON permits but does not require forward slashes to be escaped.\n    # This is useful when json data is emitted in a <script> tag\n    # in HTML, as it prevents </script> tags from prematurely terminating\n    # the JavaScript.  Some json libraries do this escaping by default,\n    # although python's standard library does not, so we do it here.\n    # http://stackoverflow.com/questions/1580647/json-why-are-forward-slashes-escaped\n    return json.dumps(value).replace(\"</\", \"<\\\\/\")", "loc": 14}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "utf8", "parameters": ["value"], "param_types": {"value": "Union[None, str, bytes]"}, "return_type": "Optional[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "type", "value.encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts a string argument to a byte string. If the argument is already a byte string or None, it is returned unchanged. Otherwise it must be a unicode string and is encoded as utf8.", "source_code": "def utf8(value: Union[None, str, bytes]) -> Optional[bytes]:\n    \"\"\"Converts a string argument to a byte string.\n\n    If the argument is already a byte string or None, it is returned unchanged.\n    Otherwise it must be a unicode string and is encoded as utf8.\n    \"\"\"\n    if isinstance(value, _UTF8_TYPES):\n        return value\n    if not isinstance(value, unicode_type):\n        raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))\n    return value.encode(\"utf-8\")", "loc": 11}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "to_unicode", "parameters": ["value"], "param_types": {"value": "Union[None, str, bytes]"}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "isinstance", "type", "value.decode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts a string argument to a unicode string. If the argument is already a unicode string or None, it is returned unchanged.  Otherwise it must be a byte string and is decoded as utf8.", "source_code": "def to_unicode(value: Union[None, str, bytes]) -> Optional[str]:\n    \"\"\"Converts a string argument to a unicode string.\n\n    If the argument is already a unicode string or None, it is returned\n    unchanged.  Otherwise it must be a byte string and is decoded as utf8.\n    \"\"\"\n    if isinstance(value, _TO_UNICODE_TYPES):\n        return value\n    if not isinstance(value, bytes):\n        raise TypeError(\"Expected bytes, unicode, or None; got %r\" % type(value))\n    return value.decode(\"utf-8\")", "loc": 11}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "recursive_unicode", "parameters": ["obj"], "param_types": {"obj": "Any"}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "list", "obj.items", "recursive_unicode", "to_unicode", "tuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Walks a simple data structure, converting byte strings to unicode. Supports lists, tuples, and dictionaries.", "source_code": "def recursive_unicode(obj: Any) -> Any:\n    \"\"\"Walks a simple data structure, converting byte strings to unicode.\n\n    Supports lists, tuples, and dictionaries.\n    \"\"\"\n    if isinstance(obj, dict):\n        return {recursive_unicode(k): recursive_unicode(v) for (k, v) in obj.items()}\n    elif isinstance(obj, list):\n        return list(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, tuple):\n        return tuple(recursive_unicode(i) for i in obj)\n    elif isinstance(obj, bytes):\n        return to_unicode(obj)\n    else:\n        return obj", "loc": 15}
{"file": "tornado\\tornado\\escape.py", "class_name": null, "function_name": "make_link", "parameters": ["m"], "param_types": {"m": "typing.Match"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callable", "extra_params", "extra_params(href).strip", "len", "m.group", "parts[1][:8].split", "parts[1][:8].split('?')[0].split", "url.rfind", "url[proto_len:].split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_link(m: typing.Match) -> str:\n    url = m.group(1)\n    proto = m.group(2)\n    if require_protocol and not proto:\n        return url  # not protocol, no linkify\n\n    if proto and proto not in permitted_protocols:\n        return url  # bad protocol, no linkify\n\n    href = m.group(1)\n    if not proto:\n        href = \"http://\" + href  # no proto specified, use http\n\n    if callable(extra_params):\n        params = \" \" + extra_params(href).strip()\n    else:\n        params = extra_params\n\n    # clip long urls. max_len is just an approximation\n    max_len = 30\n    if shorten and len(url) > max_len:\n        before_clip = url\n        if proto:\n            proto_len = len(proto) + 1 + len(m.group(3) or \"\")  # +1 for :\n        else:\n            proto_len = 0\n\n        parts = url[proto_len:].split(\"/\")\n        if len(parts) > 1:\n            # Grab the whole host part plus the first bit of the path\n            # The path is usually not that interesting once shortened\n            # (no more slug, etc), so it really just provides a little\n            # extra indication of shortening.\n            url = (\n                url[:proto_len]\n                + parts[0]\n                + \"/\"\n                + parts[1][:8].split(\"?\")[0].split(\".\")[0]\n            )\n\n        if len(url) > max_len * 1.5:  # still too long\n            url = url[:max_len]\n\n        if url != before_clip:\n            amp = url.rfind(\"&\")\n            # avoid splitting html char entities\n            if amp > max_len - 5:\n                url = url[:amp]\n            url += \"...\"\n\n            if len(url) >= len(before_clip):\n                url = before_clip\n            else:\n                # full url is visible on mouse-over (for those who don't\n                # have a status bar, such as Safari by default)\n                params += ' title=\"%s\"' % href\n\n    return f'<a href=\"{href}\"{params}>{url}</a>'", "loc": 58}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "coroutine", "parameters": ["func"], "param_types": {"func": "Union[Callable[..., 'Generator[Any, Any, _T]'], Callable[..., _T]]"}, "return_type": "Callable[..., 'Future[_T]']", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Runner", "_create_future", "_value_from_stopiteration", "contextvars.copy_context", "ctx_run", "functools.wraps", "future.add_done_callback", "future_set_exc_info", "future_set_result_unless_cancelled", "isinstance", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Decorator for asynchronous generators. For compatibility with older versions of Python, coroutines may also \"return\" by raising the special exception `Return(value)", "source_code": "def coroutine(\n    func: Union[Callable[..., \"Generator[Any, Any, _T]\"], Callable[..., _T]],\n) -> Callable[..., \"Future[_T]\"]:\n    \"\"\"Decorator for asynchronous generators.\n\n    For compatibility with older versions of Python, coroutines may\n    also \"return\" by raising the special exception `Return(value)\n    <Return>`.\n\n    Functions with this decorator return a `.Future`.\n\n    .. warning::\n\n       When exceptions occur inside a coroutine, the exception\n       information will be stored in the `.Future` object. You must\n       examine the result of the `.Future` object, or the exception\n       may go unnoticed by your code. This means yielding the function\n       if called from another coroutine, using something like\n       `.IOLoop.run_sync` for top-level calls, or passing the `.Future`\n       to `.IOLoop.add_future`.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       awaitable object instead.\n\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # type: (*Any, **Any) -> Future[_T]\n        # This function is type-annotated with a comment to work around\n        # https://bitbucket.org/pypy/pypy/issues/2868/segfault-with-args-type-annotation-in\n        future = _create_future()\n        if contextvars is not None:\n            ctx_run = contextvars.copy_context().run  # type: Callable\n        else:\n            ctx_run = _fake_ctx_run\n        try:\n            result = ctx_run(func, *args, **kwargs)\n        except (Return, StopIteration) as e:\n            result = _value_from_stopiteration(e)\n        except Exception:\n            future_set_exc_info(future, sys.exc_info())\n            try:\n                return future\n            finally:\n                # Avoid circular references\n                future = None  # type: ignore\n        else:\n            if isinstance(result, Generator):\n                # Inline the first iteration of Runner.run.  This lets us\n                # avoid the cost of creating a Runner when the coroutine\n                # never actually yields, which in turn allows us to\n                # use \"optional\" coroutines in critical path code without\n                # performance penalty for the synchronous case.\n                try:\n                    yielded = ctx_run(next, result)\n                except (StopIteration, Return) as e:\n                    future_set_result_unless_cancelled(\n                        future, _value_from_stopiteration(e)\n                    )\n                except Exception:\n                    future_set_exc_info(future, sys.exc_info())\n                else:\n                    # Provide strong references to Runner objects as long\n                    # as their result future objects also have strong\n                    # references (typically from the parent coroutine's\n                    # Runner). This keeps the coroutine's Runner alive.\n                    # We do this by exploiting the public API\n                    # add_done_callback() instead of putting a private\n                    # attribute on the Future.\n                    # (GitHub issues #1769, #2229).\n                    runner = Runner(ctx_run, result, future, yielded)\n                    future.add_done_callback(lambda _: runner)\n                yielded = None\n                try:\n                    return future\n                finally:\n                    # Subtle memory optimization: if next() raised an exception,\n                    # the future's exc_info contains a traceback which\n                    # includes this stack frame.  This creates a cycle,\n                    # which will be collected at the next full GC but has\n                    # been shown to greatly increase memory usage of\n                    # benchmarks (relative to the refcount-based scheme\n                    # used in the absence of cycles).  We can avoid the\n                    # cycle by clearing the local variable after we return it.\n                    future = None  # type: ignore\n        future_set_result_unless_cancelled(future, result)\n        return future\n\n    wrapper.__wrapped__ = func  # type: ignore\n    wrapper.__tornado_coroutine__ = True  # type: ignore\n    return wrapper", "loc": 94}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "multi_future", "parameters": ["children", "quiet_exceptions"], "param_types": {"children": "Union[Sequence[_Yieldable], Mapping[Any, _Yieldable]]", "quiet_exceptions": "'Union[Type[Exception], Tuple[Type[Exception], ...]]'"}, "return_type": "'Union[Future[List], Future[Dict]]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_create_future", "all", "app_log.error", "children.keys", "children.values", "dict", "f.result", "future.done", "future_add_done_callback", "future_set_exc_info", "future_set_result_unless_cancelled", "is_future", "isinstance", "list", "listening.add", "map", "result_list.append", "set", "sys.exc_info", "unfinished_children.remove", "zip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Wait for multiple asynchronous futures in parallel. Since Tornado 6.0, this function is exactly the same as `multi`. .. versionadded:: 4.0", "source_code": "def multi_future(\n    children: Union[Sequence[_Yieldable], Mapping[Any, _Yieldable]],\n    quiet_exceptions: \"Union[Type[Exception], Tuple[Type[Exception], ...]]\" = (),\n) -> \"Union[Future[List], Future[Dict]]\":\n    \"\"\"Wait for multiple asynchronous futures in parallel.\n\n    Since Tornado 6.0, this function is exactly the same as `multi`.\n\n    .. versionadded:: 4.0\n\n    .. versionchanged:: 4.2\n       If multiple ``Futures`` fail, any exceptions after the first (which is\n       raised) will be logged. Added the ``quiet_exceptions``\n       argument to suppress this logging for selected exception types.\n\n    .. deprecated:: 4.3\n       Use `multi` instead.\n    \"\"\"\n    if isinstance(children, dict):\n        keys = list(children.keys())  # type: Optional[List]\n        children_seq = children.values()  # type: Iterable\n    else:\n        keys = None\n        children_seq = children\n    children_futs = list(map(convert_yielded, children_seq))\n    assert all(is_future(i) or isinstance(i, _NullFuture) for i in children_futs)\n    unfinished_children = set(children_futs)\n\n    future = _create_future()\n    if not children_futs:\n        future_set_result_unless_cancelled(future, {} if keys is not None else [])\n\n    def callback(fut: Future) -> None:\n        unfinished_children.remove(fut)\n        if not unfinished_children:\n            result_list = []\n            for f in children_futs:\n                try:\n                    result_list.append(f.result())\n                except Exception as e:\n                    if future.done():\n                        if not isinstance(e, quiet_exceptions):\n                            app_log.error(\n                                \"Multiple exceptions in yield list\", exc_info=True\n                            )\n                    else:\n                        future_set_exc_info(future, sys.exc_info())\n            if not future.done():\n                if keys is not None:\n                    future_set_result_unless_cancelled(\n                        future, dict(zip(keys, result_list))\n                    )\n                else:\n                    future_set_result_unless_cancelled(future, result_list)\n\n    listening = set()  # type: Set[Future]\n    for f in children_futs:\n        if f not in listening:\n            listening.add(f)\n            future_add_done_callback(f, callback)\n    return future", "loc": 61}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "maybe_future", "parameters": ["x"], "param_types": {"x": "Any"}, "return_type": "Future", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_create_future", "fut.set_result", "is_future"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts ``x`` into a `.Future`. If ``x`` is already a `.Future`, it is simply returned; otherwise it is wrapped in a new `.Future`.  This is suitable for use as", "source_code": "def maybe_future(x: Any) -> Future:\n    \"\"\"Converts ``x`` into a `.Future`.\n\n    If ``x`` is already a `.Future`, it is simply returned; otherwise\n    it is wrapped in a new `.Future`.  This is suitable for use as\n    ``result = yield gen.maybe_future(f())`` when you don't know whether\n    ``f()`` returns a `.Future` or not.\n\n    .. deprecated:: 4.3\n       This function only handles ``Futures``, not other yieldable objects.\n       Instead of `maybe_future`, check for the non-future result types\n       you expect (often just ``None``), and ``yield`` anything unknown.\n    \"\"\"\n    if is_future(x):\n        return x\n    else:\n        fut = _create_future()\n        fut.set_result(x)\n        return fut", "loc": 19}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "with_timeout", "parameters": ["timeout", "future", "quiet_exceptions"], "param_types": {"timeout": "Union[float, datetime.timedelta]", "future": "_Yieldable", "quiet_exceptions": "'Union[Type[Exception], Tuple[Type[Exception], ...]]'"}, "return_type": "Future", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "TimeoutError", "_create_future", "app_log.error", "chain_future", "convert_yielded", "future.result", "future_add_done_callback", "io_loop.add_future", "io_loop.add_timeout", "io_loop.remove_timeout", "isinstance", "result.done", "result.set_exception"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Wraps a `.Future` (or other yieldable object) in a timeout.", "source_code": "def with_timeout(\n    timeout: Union[float, datetime.timedelta],\n    future: _Yieldable,\n    quiet_exceptions: \"Union[Type[Exception], Tuple[Type[Exception], ...]]\" = (),\n) -> Future:\n    \"\"\"Wraps a `.Future` (or other yieldable object) in a timeout.\n\n    Raises `tornado.util.TimeoutError` if the input future does not\n    complete before ``timeout``, which may be specified in any form\n    allowed by `.IOLoop.add_timeout` (i.e. a `datetime.timedelta` or\n    an absolute time relative to `.IOLoop.time`)\n\n    If the wrapped `.Future` fails after it has timed out, the exception\n    will be logged unless it is either of a type contained in\n    ``quiet_exceptions`` (which may be an exception type or a sequence of\n    types), or an ``asyncio.CancelledError``.\n\n    The wrapped `.Future` is not canceled when the timeout expires,\n    permitting it to be reused. `asyncio.wait_for` is similar to this\n    function but it does cancel the wrapped `.Future` on timeout.\n\n    .. versionadded:: 4.0\n\n    .. versionchanged:: 4.1\n       Added the ``quiet_exceptions`` argument and the logging of unhandled\n       exceptions.\n\n    .. versionchanged:: 4.4\n       Added support for yieldable objects other than `.Future`.\n\n    .. versionchanged:: 6.0.3\n       ``asyncio.CancelledError`` is now always considered \"quiet\".\n\n    .. versionchanged:: 6.2\n       ``tornado.util.TimeoutError`` is now an alias to ``asyncio.TimeoutError``.\n\n    \"\"\"\n    # It's tempting to optimize this by cancelling the input future on timeout\n    # instead of creating a new one, but A) we can't know if we are the only\n    # one waiting on the input future, so cancelling it might disrupt other\n    # callers and B) concurrent futures can only be cancelled while they are\n    # in the queue, so cancellation cannot reliably bound our waiting time.\n    future_converted = convert_yielded(future)\n    result = _create_future()\n    chain_future(future_converted, result)\n    io_loop = IOLoop.current()\n\n    def error_callback(future: Future) -> None:\n        try:\n            future.result()\n        except asyncio.CancelledError:\n            pass\n        except Exception as e:\n            if not isinstance(e, quiet_exceptions):\n                app_log.error(\n                    \"Exception in Future %r after timeout\", future, exc_info=True\n                )\n\n    def timeout_callback() -> None:\n        if not result.done():\n            result.set_exception(TimeoutError(\"Timeout\"))\n        # In case the wrapped future goes on to fail, log it.\n        future_add_done_callback(future_converted, error_callback)\n\n    timeout_handle = io_loop.add_timeout(timeout, timeout_callback)\n    if isinstance(future_converted, Future):\n        # We know this future will resolve on the IOLoop, so we don't\n        # need the extra thread-safety of IOLoop.add_future (and we also\n        # don't care about StackContext here.\n        future_add_done_callback(\n            future_converted, lambda future: io_loop.remove_timeout(timeout_handle)\n        )\n    else:\n        # concurrent.futures.Futures may resolve on any thread, so we\n        # need to route them back to the IOLoop.\n        io_loop.add_future(\n            future_converted, lambda future: io_loop.remove_timeout(timeout_handle)\n        )\n    return result", "loc": 79}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "sleep", "parameters": ["duration"], "param_types": {"duration": "float"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "IOLoop.current().call_later", "_create_future", "future_set_result_unless_cancelled"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a `.Future` that resolves after the given number of seconds. When used with ``yield`` in a coroutine, this is a non-blocking analogue to `time.sleep` (which should not be used in coroutines", "source_code": "def sleep(duration: float) -> \"Future[None]\":\n    \"\"\"Return a `.Future` that resolves after the given number of seconds.\n\n    When used with ``yield`` in a coroutine, this is a non-blocking\n    analogue to `time.sleep` (which should not be used in coroutines\n    because it is blocking)::\n\n        yield gen.sleep(0.5)\n\n    Note that calling this function on its own does nothing; you must\n    wait on the `.Future` it returns (usually by yielding it).\n\n    .. versionadded:: 4.1\n    \"\"\"\n    f = _create_future()\n    IOLoop.current().call_later(\n        duration, lambda: future_set_result_unless_cancelled(f, None)\n    )\n    return f", "loc": 19}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "convert_yielded", "parameters": ["yielded"], "param_types": {"yielded": "_Yieldable"}, "return_type": "Future", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BadYieldError", "_wrap_awaitable", "is_future", "isawaitable", "isinstance", "multi", "typing.cast"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert a yielded object into a `.Future`. The default implementation accepts lists, dictionaries, and Futures. This has the side effect of starting any coroutines that", "source_code": "def convert_yielded(yielded: _Yieldable) -> Future:\n    \"\"\"Convert a yielded object into a `.Future`.\n\n    The default implementation accepts lists, dictionaries, and\n    Futures. This has the side effect of starting any coroutines that\n    did not start themselves, similar to `asyncio.ensure_future`.\n\n    If the `~functools.singledispatch` library is available, this function\n    may be extended to support additional types. For example::\n\n        @convert_yielded.register(asyncio.Future)\n        def _(asyncio_future):\n            return tornado.platform.asyncio.to_tornado_future(asyncio_future)\n\n    .. versionadded:: 4.1\n\n    \"\"\"\n    if yielded is None or yielded is moment:\n        return moment\n    elif yielded is _null_future:\n        return _null_future\n    elif isinstance(yielded, (list, dict)):\n        return multi(yielded)  # type: ignore\n    elif is_future(yielded):\n        return typing.cast(Future, yielded)\n    elif isawaitable(yielded):\n        return _wrap_awaitable(yielded)  # type: ignore\n    else:\n        raise BadYieldError(f\"yielded unknown object {yielded!r}\")", "loc": 29}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Runner", "_create_future", "_value_from_stopiteration", "contextvars.copy_context", "ctx_run", "functools.wraps", "future.add_done_callback", "future_set_exc_info", "future_set_result_unless_cancelled", "isinstance", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    # type: (*Any, **Any) -> Future[_T]\n    # This function is type-annotated with a comment to work around\n    # https://bitbucket.org/pypy/pypy/issues/2868/segfault-with-args-type-annotation-in\n    future = _create_future()\n    if contextvars is not None:\n        ctx_run = contextvars.copy_context().run  # type: Callable\n    else:\n        ctx_run = _fake_ctx_run\n    try:\n        result = ctx_run(func, *args, **kwargs)\n    except (Return, StopIteration) as e:\n        result = _value_from_stopiteration(e)\n    except Exception:\n        future_set_exc_info(future, sys.exc_info())\n        try:\n            return future\n        finally:\n            # Avoid circular references\n            future = None  # type: ignore\n    else:\n        if isinstance(result, Generator):\n            # Inline the first iteration of Runner.run.  This lets us\n            # avoid the cost of creating a Runner when the coroutine\n            # never actually yields, which in turn allows us to\n            # use \"optional\" coroutines in critical path code without\n            # performance penalty for the synchronous case.\n            try:\n                yielded = ctx_run(next, result)\n            except (StopIteration, Return) as e:\n                future_set_result_unless_cancelled(\n                    future, _value_from_stopiteration(e)\n                )\n            except Exception:\n                future_set_exc_info(future, sys.exc_info())\n            else:\n                # Provide strong references to Runner objects as long\n                # as their result future objects also have strong\n                # references (typically from the parent coroutine's\n                # Runner). This keeps the coroutine's Runner alive.\n                # We do this by exploiting the public API\n                # add_done_callback() instead of putting a private\n                # attribute on the Future.\n                # (GitHub issues #1769, #2229).\n                runner = Runner(ctx_run, result, future, yielded)\n                future.add_done_callback(lambda _: runner)\n            yielded = None\n            try:\n                return future\n            finally:\n                # Subtle memory optimization: if next() raised an exception,\n                # the future's exc_info contains a traceback which\n                # includes this stack frame.  This creates a cycle,\n                # which will be collected at the next full GC but has\n                # been shown to greatly increase memory usage of\n                # benchmarks (relative to the refcount-based scheme\n                # used in the absence of cycles).  We can avoid the\n                # cycle by clearing the local variable after we return it.\n                future = None  # type: ignore\n    future_set_result_unless_cancelled(future, result)\n    return future", "loc": 61}
{"file": "tornado\\tornado\\gen.py", "class_name": "WaitIterator", "function_name": "done", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def done(self) -> bool:\n    \"\"\"Returns True if this iterator has no more results.\"\"\"\n    if self._finished or self._unfinished:\n        return False\n    # Clear the 'current' values when iteration is done.\n    self.current_index = self.current_future = None\n    return True", "loc": 7}
{"file": "tornado\\tornado\\gen.py", "class_name": "WaitIterator", "function_name": "next", "parameters": ["self"], "param_types": {}, "return_type": "Future", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "self._finished.popleft", "self._return_result"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def next(self) -> Future:\n    \"\"\"Returns a `.Future` that will yield the next available result.\n\n    Note that this `.Future` will not be the same object as any of\n    the inputs.\n    \"\"\"\n    self._running_future = Future()\n\n    if self._finished:\n        return self._return_result(self._finished.popleft())\n\n    return self._running_future", "loc": 12}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "callback", "parameters": ["fut"], "param_types": {"fut": "Future"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.error", "dict", "f.result", "future.done", "future_set_exc_info", "future_set_result_unless_cancelled", "isinstance", "result_list.append", "sys.exc_info", "unfinished_children.remove", "zip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def callback(fut: Future) -> None:\n    unfinished_children.remove(fut)\n    if not unfinished_children:\n        result_list = []\n        for f in children_futs:\n            try:\n                result_list.append(f.result())\n            except Exception as e:\n                if future.done():\n                    if not isinstance(e, quiet_exceptions):\n                        app_log.error(\n                            \"Multiple exceptions in yield list\", exc_info=True\n                        )\n                else:\n                    future_set_exc_info(future, sys.exc_info())\n        if not future.done():\n            if keys is not None:\n                future_set_result_unless_cancelled(\n                    future, dict(zip(keys, result_list))\n                )\n            else:\n                future_set_result_unless_cancelled(future, result_list)", "loc": 22}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "error_callback", "parameters": ["future"], "param_types": {"future": "Future"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.error", "future.result", "isinstance"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def error_callback(future: Future) -> None:\n    try:\n        future.result()\n    except asyncio.CancelledError:\n        pass\n    except Exception as e:\n        if not isinstance(e, quiet_exceptions):\n            app_log.error(\n                \"Exception in Future %r after timeout\", future, exc_info=True\n            )", "loc": 10}
{"file": "tornado\\tornado\\gen.py", "class_name": null, "function_name": "timeout_callback", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TimeoutError", "future_add_done_callback", "result.done", "result.set_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def timeout_callback() -> None:\n    if not result.done():\n        result.set_exception(TimeoutError(\"Timeout\"))\n    # In case the wrapped future goes on to fail, log it.\n    future_add_done_callback(future_converted, error_callback)", "loc": 5}
{"file": "tornado\\tornado\\gen.py", "class_name": "Runner", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "_value_from_stopiteration", "future.done", "future.result", "future_set_exc_info", "future_set_result_unless_cancelled", "self.gen.send", "self.gen.throw", "self.handle_yield", "sys.exc_info"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Starts or resumes the generator, running until it reaches a yield point that is not ready.", "source_code": "def run(self) -> None:\n    \"\"\"Starts or resumes the generator, running until it reaches a\n    yield point that is not ready.\n    \"\"\"\n    if self.running or self.finished:\n        return\n    try:\n        self.running = True\n        while True:\n            future = self.future\n            if future is None:\n                raise Exception(\"No pending future\")\n            if not future.done():\n                return\n            self.future = None\n            try:\n                try:\n                    value = future.result()\n                except Exception as e:\n                    # Save the exception for later. It's important that\n                    # gen.throw() not be called inside this try/except block\n                    # because that makes sys.exc_info behave unexpectedly.\n                    exc: Optional[Exception] = e\n                else:\n                    exc = None\n                finally:\n                    future = None\n\n                if exc is not None:\n                    try:\n                        yielded = self.gen.throw(exc)\n                    finally:\n                        # Break up a circular reference for faster GC on\n                        # CPython.\n                        del exc\n                else:\n                    yielded = self.gen.send(value)\n\n            except (StopIteration, Return) as e:\n                self.finished = True\n                self.future = _null_future\n                future_set_result_unless_cancelled(\n                    self.result_future, _value_from_stopiteration(e)\n                )\n                self.result_future = None  # type: ignore\n                return\n            except Exception:\n                self.finished = True\n                self.future = _null_future\n                future_set_exc_info(self.result_future, sys.exc_info())\n                self.result_future = None  # type: ignore\n                return\n            if not self.handle_yield(yielded):\n                return\n            yielded = None\n    finally:\n        self.running = False", "loc": 57}
{"file": "tornado\\tornado\\gen.py", "class_name": "Runner", "function_name": "handle_yield", "parameters": ["self", "yielded"], "param_types": {"yielded": "_Yieldable"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "Future", "convert_yielded", "future_set_exc_info", "self.ctx_run", "self.future.done", "self.io_loop.add_callback", "self.io_loop.add_future", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_yield(self, yielded: _Yieldable) -> bool:\n    try:\n        self.future = convert_yielded(yielded)\n    except BadYieldError:\n        self.future = Future()\n        future_set_exc_info(self.future, sys.exc_info())\n\n    if self.future is moment:\n        self.io_loop.add_callback(self.ctx_run, self.run)\n        return False\n    elif self.future is None:\n        raise Exception(\"no pending future\")\n    elif not self.future.done():\n\n        def inner(f: Any) -> None:\n            # Break a reference cycle to speed GC.\n            f = None  # noqa: F841\n            self.ctx_run(self.run)\n\n        self.io_loop.add_future(self.future, inner)\n        return False\n    return True", "loc": 22}
{"file": "tornado\\tornado\\gen.py", "class_name": "Runner", "function_name": "handle_exception", "parameters": ["self", "typ", "value", "tb"], "param_types": {"typ": "Type[Exception]", "value": "Exception", "tb": "types.TracebackType"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "future_set_exc_info", "self.ctx_run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_exception(\n    self, typ: Type[Exception], value: Exception, tb: types.TracebackType\n) -> bool:\n    if not self.running and not self.finished:\n        self.future = Future()\n        future_set_exc_info(self.future, (typ, value, tb))\n        self.ctx_run(self.run)\n        return True\n    else:\n        return False", "loc": 10}
{"file": "tornado\\tornado\\http1connection.py", "class_name": null, "function_name": "parse_int", "parameters": ["s"], "param_types": {"s": "str"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DIGITS.fullmatch", "ValueError", "int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse a non-negative integer from a string.", "source_code": "def parse_int(s: str) -> int:\n    \"\"\"Parse a non-negative integer from a string.\"\"\"\n    if DIGITS.fullmatch(s) is None:\n        raise ValueError(\"not an integer: %r\" % s)\n    return int(s)", "loc": 5}
{"file": "tornado\\tornado\\http1connection.py", "class_name": null, "function_name": "parse_hex_int", "parameters": ["s"], "param_types": {"s": "str"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HEXDIGITS.fullmatch", "ValueError", "int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse a non-negative hexadecimal integer from a string.", "source_code": "def parse_hex_int(s: str) -> int:\n    \"\"\"Parse a non-negative hexadecimal integer from a string.\"\"\"\n    if HEXDIGITS.fullmatch(s) is None:\n        raise ValueError(\"not a hexadecimal integer: %r\" % s)\n    return int(s, 16)", "loc": 5}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1Connection", "function_name": "read_response", "parameters": ["self", "delegate"], "param_types": {"delegate": "httputil.HTTPMessageDelegate"}, "return_type": "Awaitable[bool]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_GzipMessageDelegate", "self._read_message"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Read a single HTTP response. Typical client-mode usage is to write a request using `write_headers`, `write`, and `finish`, and then call ``read_response``. :arg delegate: a `.HTTPMessageDelegate`", "source_code": "def read_response(self, delegate: httputil.HTTPMessageDelegate) -> Awaitable[bool]:\n    \"\"\"Read a single HTTP response.\n\n    Typical client-mode usage is to write a request using `write_headers`,\n    `write`, and `finish`, and then call ``read_response``.\n\n    :arg delegate: a `.HTTPMessageDelegate`\n\n    Returns a `.Future` that resolves to a bool after the full response has\n    been read. The result is true if the stream is still open.\n    \"\"\"\n    if self.params.decompress:\n        delegate = _GzipMessageDelegate(delegate, self.params.chunk_size)\n    return self._read_message(delegate)", "loc": 14}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1Connection", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_result_unless_cancelled", "self._clear_callbacks", "self._finish_future.done", "self.stream.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    if self.stream is not None:\n        self.stream.close()\n    self._clear_callbacks()\n    if not self._finish_future.done():\n        future_set_result_unless_cancelled(self._finish_future, None)", "loc": 6}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1Connection", "function_name": "detach", "parameters": ["self"], "param_types": {}, "return_type": "iostream.IOStream", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_result_unless_cancelled", "self._clear_callbacks", "self._finish_future.done"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Take control of the underlying stream.", "source_code": "def detach(self) -> iostream.IOStream:\n    \"\"\"Take control of the underlying stream.\n\n    Returns the underlying `.IOStream` object and stops all further\n    HTTP processing.  May only be called during\n    `.HTTPMessageDelegate.headers_received`.  Intended for implementing\n    protocols like websockets that tunnel over an HTTP handshake.\n    \"\"\"\n    self._clear_callbacks()\n    stream = self.stream\n    self.stream = None  # type: ignore\n    if not self._finish_future.done():\n        future_set_result_unless_cancelled(self._finish_future, None)\n    return stream", "loc": 14}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1Connection", "function_name": "write_headers", "parameters": ["self", "start_line", "headers", "chunk"], "param_types": {"start_line": "Union[httputil.RequestStartLine, httputil.ResponseStartLine]", "headers": "httputil.HTTPHeaders", "chunk": "Optional[bytes]"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CR_OR_LF_RE.search", "Future", "ValueError", "b'\\r\\n'.join", "cast", "future.exception", "future.set_exception", "future_add_done_callback", "headers.get_all", "iostream.StreamClosedError", "isinstance", "line.encode", "lines.append", "lines.extend", "native_str", "parse_int", "self._format_chunk", "self._request_headers.get", "self._request_headers.get('Connection', '').lower", "self.stream.closed", "self.stream.write", "utf8"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Implements `.HTTPConnection.write_headers`.", "source_code": "def write_headers(\n    self,\n    start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n    headers: httputil.HTTPHeaders,\n    chunk: Optional[bytes] = None,\n) -> \"Future[None]\":\n    \"\"\"Implements `.HTTPConnection.write_headers`.\"\"\"\n    lines = []\n    if self.is_client:\n        assert isinstance(start_line, httputil.RequestStartLine)\n        self._request_start_line = start_line\n        lines.append(utf8(f\"{start_line[0]} {start_line[1]} HTTP/1.1\"))\n        # Client requests with a non-empty body must have either a\n        # Content-Length or a Transfer-Encoding. If Content-Length is not\n        # present we'll add our Transfer-Encoding below.\n        self._chunking_output = (\n            start_line.method in (\"POST\", \"PUT\", \"PATCH\")\n            and \"Content-Length\" not in headers\n        )\n    else:\n        assert isinstance(start_line, httputil.ResponseStartLine)\n        assert self._request_start_line is not None\n        assert self._request_headers is not None\n        self._response_start_line = start_line\n        lines.append(utf8(\"HTTP/1.1 %d %s\" % (start_line[1], start_line[2])))\n        self._chunking_output = (\n            # TODO: should this use\n            # self._request_start_line.version or\n            # start_line.version?\n            self._request_start_line.version == \"HTTP/1.1\"\n            # Omit payload header field for HEAD request.\n            and self._request_start_line.method != \"HEAD\"\n            # 1xx, 204 and 304 responses have no body (not even a zero-length\n            # body), and so should not have either Content-Length or\n            # Transfer-Encoding headers.\n            and start_line.code not in (204, 304)\n            and (start_line.code < 100 or start_line.code >= 200)\n            # No need to chunk the output if a Content-Length is specified.\n            and \"Content-Length\" not in headers\n        )\n        # If connection to a 1.1 client will be closed, inform client\n        if (\n            self._request_start_line.version == \"HTTP/1.1\"\n            and self._disconnect_on_finish\n        ):\n            headers[\"Connection\"] = \"close\"\n        # If a 1.0 client asked for keep-alive, add the header.\n        if (\n            self._request_start_line.version == \"HTTP/1.0\"\n            and self._request_headers.get(\"Connection\", \"\").lower() == \"keep-alive\"\n        ):\n            headers[\"Connection\"] = \"Keep-Alive\"\n    if self._chunking_output:\n        headers[\"Transfer-Encoding\"] = \"chunked\"\n    if not self.is_client and (\n        self._request_start_line.method == \"HEAD\"\n        or cast(httputil.ResponseStartLine, start_line).code == 304\n    ):\n        self._expected_content_remaining = 0\n    elif \"Content-Length\" in headers:\n        self._expected_content_remaining = parse_int(headers[\"Content-Length\"])\n    else:\n        self._expected_content_remaining = None\n    # TODO: headers are supposed to be of type str, but we still have some\n    # cases that let bytes slip through. Remove these native_str calls when those\n    # are fixed.\n    header_lines = (\n        native_str(n) + \": \" + native_str(v) for n, v in headers.get_all()\n    )\n    lines.extend(line.encode(\"latin1\") for line in header_lines)\n    for line in lines:\n        if CR_OR_LF_RE.search(line):\n            raise ValueError(\"Illegal characters (CR or LF) in header: %r\" % line)\n    future = None\n    if self.stream.closed():\n        future = self._write_future = Future()\n        future.set_exception(iostream.StreamClosedError())\n        future.exception()\n    else:\n        future = self._write_future = Future()\n        data = b\"\\r\\n\".join(lines) + b\"\\r\\n\\r\\n\"\n        if chunk:\n            data += self._format_chunk(chunk)\n        self._pending_write = self.stream.write(data)\n        future_add_done_callback(self._pending_write, self._on_write_complete)\n    return future", "loc": 86}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1Connection", "function_name": "write", "parameters": ["self", "chunk"], "param_types": {"chunk": "bytes"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "future_add_done_callback", "iostream.StreamClosedError", "self._format_chunk", "self._write_future.exception", "self._write_future.set_exception", "self.stream.closed", "self.stream.write"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Implements `.HTTPConnection.write`. For backwards compatibility it is allowed but deprecated to skip `write_headers` and instead call `write()` with a", "source_code": "def write(self, chunk: bytes) -> \"Future[None]\":\n    \"\"\"Implements `.HTTPConnection.write`.\n\n    For backwards compatibility it is allowed but deprecated to\n    skip `write_headers` and instead call `write()` with a\n    pre-encoded header block.\n    \"\"\"\n    future = None\n    if self.stream.closed():\n        future = self._write_future = Future()\n        self._write_future.set_exception(iostream.StreamClosedError())\n        self._write_future.exception()\n    else:\n        future = self._write_future = Future()\n        self._pending_write = self.stream.write(self._format_chunk(chunk))\n        future_add_done_callback(self._pending_write, self._on_write_complete)\n    return future", "loc": 17}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "_GzipMessageDelegate", "function_name": "headers_received", "parameters": ["self", "start_line", "headers"], "param_types": {"start_line": "Union[httputil.RequestStartLine, httputil.ResponseStartLine]", "headers": "httputil.HTTPHeaders"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GzipDecompressor", "headers.add", "headers.get", "headers.get('Content-Encoding', '').lower", "self._delegate.headers_received"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers_received(\n    self,\n    start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n    headers: httputil.HTTPHeaders,\n) -> Optional[Awaitable[None]]:\n    if headers.get(\"Content-Encoding\", \"\").lower() == \"gzip\":\n        self._decompressor = GzipDecompressor()\n        # Downstream delegates will only see uncompressed data,\n        # so rename the content-encoding header.\n        # (but note that curl_httpclient doesn't do this).\n        headers.add(\"X-Consumed-Content-Encoding\", headers[\"Content-Encoding\"])\n        del headers[\"Content-Encoding\"]\n    return self._delegate.headers_received(start_line, headers)", "loc": 13}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "_GzipMessageDelegate", "function_name": "finish", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self._decompressor.flush", "self._delegate.finish"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finish(self) -> None:\n    if self._decompressor is not None:\n        tail = self._decompressor.flush()\n        if tail:\n            # The tail should always be empty: decompress returned\n            # all that it can in data_received and the only\n            # purpose of the flush call is to detect errors such\n            # as truncated input. If we did legitimately get a new\n            # chunk at this point we'd need to change the\n            # interface to make finish() a coroutine.\n            raise ValueError(\n                \"decompressor.flush returned data; possible truncated input\"\n            )\n    return self._delegate.finish()", "loc": 14}
{"file": "tornado\\tornado\\http1connection.py", "class_name": "HTTP1ServerConnection", "function_name": "start_serving", "parameters": ["self", "delegate"], "param_types": {"delegate": "httputil.HTTPServerConnectionDelegate"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.result", "gen.convert_yielded", "isinstance", "self._server_request_loop", "self.stream.io_loop.add_future"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Starts serving requests on this connection. :arg delegate: a `.HTTPServerConnectionDelegate`", "source_code": "def start_serving(self, delegate: httputil.HTTPServerConnectionDelegate) -> None:\n    \"\"\"Starts serving requests on this connection.\n\n    :arg delegate: a `.HTTPServerConnectionDelegate`\n    \"\"\"\n    assert isinstance(delegate, httputil.HTTPServerConnectionDelegate)\n    fut = gen.convert_yielded(self._server_request_loop(delegate))\n    self._serving_future = fut\n    # Register the future on the IOLoop so its errors get logged.\n    self.stream.io_loop.add_future(fut, lambda f: f.result())", "loc": 10}
{"file": "tornado\\tornado\\httpclient.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPClient", "client.close", "client.fetch", "define", "native_str", "parse_command_line", "print"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main() -> None:\n    from tornado.options import define, options, parse_command_line\n\n    define(\"print_headers\", type=bool, default=False)\n    define(\"print_body\", type=bool, default=True)\n    define(\"follow_redirects\", type=bool, default=True)\n    define(\"validate_cert\", type=bool, default=True)\n    define(\"proxy_host\", type=str)\n    define(\"proxy_port\", type=int)\n    args = parse_command_line()\n    client = HTTPClient()\n    for arg in args:\n        try:\n            response = client.fetch(\n                arg,\n                follow_redirects=options.follow_redirects,\n                validate_cert=options.validate_cert,\n                proxy_host=options.proxy_host,\n                proxy_port=options.proxy_port,\n            )\n        except HTTPError as e:\n            if e.response is not None:\n                response = e.response\n            else:\n                raise\n        if options.print_headers:\n            print(response.headers)\n        if options.print_body:\n            print(native_str(response.body))\n    client.close()", "loc": 30}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "HTTPClient", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._async_client.close", "self._io_loop.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Closes the HTTPClient, freeing any resources used.", "source_code": "def close(self) -> None:\n    \"\"\"Closes the HTTPClient, freeing any resources used.\"\"\"\n    if not self._closed:\n        self._async_client.close()\n        self._io_loop.close()\n        self._closed = True", "loc": 6}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "HTTPClient", "function_name": "fetch", "parameters": ["self", "request"], "param_types": {"request": "Union['HTTPRequest', str]"}, "return_type": "'HTTPResponse'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "self._io_loop.run_sync"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Executes a request, returning an `HTTPResponse`. The request may be either a string URL or an `HTTPRequest` object. If it is a string, we construct an `HTTPRequest` using any additional", "source_code": "def fetch(\n    self, request: Union[\"HTTPRequest\", str], **kwargs: Any\n) -> \"HTTPResponse\":\n    \"\"\"Executes a request, returning an `HTTPResponse`.\n\n    The request may be either a string URL or an `HTTPRequest` object.\n    If it is a string, we construct an `HTTPRequest` using any additional\n    kwargs: ``HTTPRequest(request, **kwargs)``\n\n    If an error occurs during the fetch, we raise an `HTTPError` unless\n    the ``raise_error`` keyword argument is set to False.\n    \"\"\"\n    response = self._io_loop.run_sync(\n        functools.partial(self._async_client.fetch, request, **kwargs)\n    )\n    return response", "loc": 16}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "AsyncHTTPClient", "function_name": "initialize", "parameters": ["self", "defaults"], "param_types": {"defaults": "Optional[Dict[str, Any]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "dict", "self.defaults.update"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(self, defaults: Optional[Dict[str, Any]] = None) -> None:\n    self.io_loop = IOLoop.current()\n    self.defaults = dict(HTTPRequest._DEFAULTS)\n    if defaults is not None:\n        self.defaults.update(defaults)\n    self._closed = False", "loc": 6}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "AsyncHTTPClient", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "self._instance_cache.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Destroys this HTTP client, freeing any file descriptors used. This method is **not needed in normal use** due to the way that `AsyncHTTPClient` objects are transparently reused.", "source_code": "def close(self) -> None:\n    \"\"\"Destroys this HTTP client, freeing any file descriptors used.\n\n    This method is **not needed in normal use** due to the way\n    that `AsyncHTTPClient` objects are transparently reused.\n    ``close()`` is generally only necessary when either the\n    `.IOLoop` is also being closed, or the ``force_instance=True``\n    argument was used when creating the `AsyncHTTPClient`.\n\n    No other methods may be called on the `AsyncHTTPClient` after\n    ``close()``.\n\n    \"\"\"\n    if self._closed:\n        return\n    self._closed = True\n    if self._instance_cache is not None:\n        cached_val = self._instance_cache.pop(self.io_loop, None)\n        # If there's an object other than self in the instance\n        # cache for our IOLoop, something has gotten mixed up. A\n        # value of None appears to be possible when this is called\n        # from a destructor (HTTPClient.__del__) as the weakref\n        # gets cleared before the destructor runs.\n        if cached_val is not None and cached_val is not self:\n            raise RuntimeError(\"inconsistent AsyncHTTPClient cache\")", "loc": 25}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "AsyncHTTPClient", "function_name": "configure", "parameters": ["cls", "impl"], "param_types": {"impl": "'Union[None, str, Type[Configurable]]'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().configure"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Configures the `AsyncHTTPClient` subclass to use. ``AsyncHTTPClient()`` actually creates an instance of a subclass. This method may be called with either a class object or the", "source_code": "def configure(\n    cls, impl: \"Union[None, str, Type[Configurable]]\", **kwargs: Any\n) -> None:\n    \"\"\"Configures the `AsyncHTTPClient` subclass to use.\n\n    ``AsyncHTTPClient()`` actually creates an instance of a subclass.\n    This method may be called with either a class object or the\n    fully-qualified name of such a class (or ``None`` to use the default,\n    ``SimpleAsyncHTTPClient``)\n\n    If additional keyword arguments are given, they will be passed\n    to the constructor of each subclass instance created.  The\n    keyword argument ``max_clients`` determines the maximum number\n    of simultaneous `~AsyncHTTPClient.fetch()` operations that can\n    execute in parallel on each `.IOLoop`.  Additional arguments\n    may be supported depending on the implementation class in use.\n\n    Example::\n\n       AsyncHTTPClient.configure(\"tornado.curl_httpclient.CurlAsyncHTTPClient\")\n    \"\"\"\n    super().configure(impl, **kwargs)", "loc": 22}
{"file": "tornado\\tornado\\httpclient.py", "class_name": "HTTPResponse", "function_name": "body", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.buffer.getvalue"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def body(self) -> bytes:\n    if self.buffer is None:\n        return b\"\"\n    elif self._body is None:\n        self._body = self.buffer.getvalue()\n\n    return self._body", "loc": 7}
{"file": "tornado\\tornado\\httpclient.py", "class_name": null, "function_name": "handle_response", "parameters": ["response"], "param_types": {"response": "'HTTPResponse'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_exception_unless_cancelled", "future_set_result_unless_cancelled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_response(response: \"HTTPResponse\") -> None:\n    if response.error:\n        if raise_error or not response._error_is_response_code:\n            future_set_exception_unless_cancelled(future, response.error)\n            return\n    future_set_result_unless_cancelled(future, response)", "loc": 6}
{"file": "tornado\\tornado\\httpserver.py", "class_name": "HTTPServer", "function_name": "initialize", "parameters": ["self", "request_callback", "no_keep_alive", "xheaders", "ssl_options", "protocol", "decompress_request", "chunk_size", "max_header_size", "idle_connection_timeout", "body_timeout", "max_body_size", "max_buffer_size", "trusted_downstream"], "param_types": {"request_callback": "Union[httputil.HTTPServerConnectionDelegate, Callable[[httputil.HTTPServerRequest], None]]", "no_keep_alive": "bool", "xheaders": "bool", "ssl_options": "Optional[Union[Dict[str, Any], ssl.SSLContext]]", "protocol": "Optional[str]", "decompress_request": "bool", "chunk_size": "Optional[int]", "max_header_size": "Optional[int]", "idle_connection_timeout": "Optional[float]", "body_timeout": "Optional[float]", "max_body_size": "Optional[int]", "max_buffer_size": "Optional[int]", "trusted_downstream": "Optional[List[str]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTP1ConnectionParameters", "TCPServer.__init__", "set"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(\n    self,\n    request_callback: Union[\n        httputil.HTTPServerConnectionDelegate,\n        Callable[[httputil.HTTPServerRequest], None],\n    ],\n    no_keep_alive: bool = False,\n    xheaders: bool = False,\n    ssl_options: Optional[Union[Dict[str, Any], ssl.SSLContext]] = None,\n    protocol: Optional[str] = None,\n    decompress_request: bool = False,\n    chunk_size: Optional[int] = None,\n    max_header_size: Optional[int] = None,\n    idle_connection_timeout: Optional[float] = None,\n    body_timeout: Optional[float] = None,\n    max_body_size: Optional[int] = None,\n    max_buffer_size: Optional[int] = None,\n    trusted_downstream: Optional[List[str]] = None,\n) -> None:\n    # This method's signature is not extracted with autodoc\n    # because we want its arguments to appear on the class\n    # constructor. When changing this signature, also update the\n    # copy in httpserver.rst.\n    self.request_callback = request_callback\n    self.xheaders = xheaders\n    self.protocol = protocol\n    self.conn_params = HTTP1ConnectionParameters(\n        decompress=decompress_request,\n        chunk_size=chunk_size,\n        max_header_size=max_header_size,\n        header_timeout=idle_connection_timeout or 3600,\n        max_body_size=max_body_size,\n        body_timeout=body_timeout,\n        no_keep_alive=no_keep_alive,\n    )\n    TCPServer.__init__(\n        self,\n        ssl_options=ssl_options,\n        max_buffer_size=max_buffer_size,\n        read_chunk_size=chunk_size,\n    )\n    self._connections = set()  # type: Set[HTTP1ServerConnection]\n    self.trusted_downstream = trusted_downstream", "loc": 43}
{"file": "tornado\\tornado\\httpserver.py", "class_name": "HTTPServer", "function_name": "handle_stream", "parameters": ["self", "stream", "address"], "param_types": {"stream": "iostream.IOStream", "address": "Tuple"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTP1ServerConnection", "_HTTPRequestContext", "conn.start_serving", "self._connections.add"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_stream(self, stream: iostream.IOStream, address: Tuple) -> None:\n    context = _HTTPRequestContext(\n        stream, address, self.protocol, self.trusted_downstream\n    )\n    conn = HTTP1ServerConnection(stream, self.conn_params, context)\n    self._connections.add(conn)\n    conn.start_serving(self)", "loc": 7}
{"file": "tornado\\tornado\\httpserver.py", "class_name": "HTTPServer", "function_name": "start_request", "parameters": ["self", "server_conn", "request_conn"], "param_types": {"server_conn": "object", "request_conn": "httputil.HTTPConnection"}, "return_type": "httputil.HTTPMessageDelegate", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CallableAdapter", "_ProxyAdapter", "isinstance", "self.request_callback.start_request"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def start_request(\n    self, server_conn: object, request_conn: httputil.HTTPConnection\n) -> httputil.HTTPMessageDelegate:\n    if isinstance(self.request_callback, httputil.HTTPServerConnectionDelegate):\n        delegate = self.request_callback.start_request(server_conn, request_conn)\n    else:\n        delegate = _CallableAdapter(self.request_callback, request_conn)\n\n    if self.xheaders:\n        delegate = _ProxyAdapter(delegate, request_conn)\n\n    return delegate", "loc": 12}
{"file": "tornado\\tornado\\httpserver.py", "class_name": "_CallableAdapter", "function_name": "finish", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["b''.join", "self.request._parse_body", "self.request_callback"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finish(self) -> None:\n    assert self.request is not None\n    self.request.body = b\"\".join(self._chunks)\n    self.request._parse_body()\n    self.request_callback(self.request)", "loc": 5}
{"file": "tornado\\tornado\\httpserver.py", "class_name": "_ProxyAdapter", "function_name": "headers_received", "parameters": ["self", "start_line", "headers"], "param_types": {"start_line": "Union[httputil.RequestStartLine, httputil.ResponseStartLine]", "headers": "httputil.HTTPHeaders"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.connection.context._apply_xheaders", "self.delegate.headers_received"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers_received(\n    self,\n    start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n    headers: httputil.HTTPHeaders,\n) -> Optional[Awaitable[None]]:\n    # TODO: either make context an official part of the\n    # HTTPConnection interface or figure out some other way to do this.\n    self.connection.context._apply_xheaders(headers)  # type: ignore\n    return self.delegate.headers_received(start_line, headers)", "loc": 9}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "url_concat", "parameters": ["url", "args"], "param_types": {"url": "str", "args": "Union[None, Dict[str, str], List[Tuple[str, str]], Tuple[Tuple[str, str], ...]]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["\"'args' parameter should be dict, list or tuple. Not {0}\".format", "TypeError", "args.items", "isinstance", "parse_qsl", "parsed_query.extend", "type", "urlencode", "urlparse", "urlunparse"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Concatenate url and arguments regardless of whether url has existing query parameters. ``args`` may be either a dictionary or a list of key-value pairs", "source_code": "def url_concat(\n    url: str,\n    args: Union[\n        None, Dict[str, str], List[Tuple[str, str]], Tuple[Tuple[str, str], ...]\n    ],\n) -> str:\n    \"\"\"Concatenate url and arguments regardless of whether\n    url has existing query parameters.\n\n    ``args`` may be either a dictionary or a list of key-value pairs\n    (the latter allows for multiple values with the same key.\n\n    >>> url_concat(\"http://example.com/foo\", dict(c=\"d\"))\n    'http://example.com/foo?c=d'\n    >>> url_concat(\"http://example.com/foo?a=b\", dict(c=\"d\"))\n    'http://example.com/foo?a=b&c=d'\n    >>> url_concat(\"http://example.com/foo?a=b\", [(\"c\", \"d\"), (\"c\", \"d2\")])\n    'http://example.com/foo?a=b&c=d&c=d2'\n    \"\"\"\n    if args is None:\n        return url\n    parsed_url = urlparse(url)\n    if isinstance(args, dict):\n        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)\n        parsed_query.extend(args.items())\n    elif isinstance(args, list) or isinstance(args, tuple):\n        parsed_query = parse_qsl(parsed_url.query, keep_blank_values=True)\n        parsed_query.extend(args)\n    else:\n        err = \"'args' parameter should be dict, list or tuple. Not {0}\".format(\n            type(args)\n        )\n        raise TypeError(err)\n    final_query = urlencode(parsed_query)\n    url = urlunparse(\n        (\n            parsed_url[0],\n            parsed_url[1],\n            parsed_url[2],\n            parsed_url[3],\n            final_query,\n            parsed_url[5],\n        )\n    )\n    return url", "loc": 45}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "parse_body_arguments", "parameters": ["content_type", "body", "arguments", "files", "headers"], "param_types": {"content_type": "str", "body": "bytes", "arguments": "Dict[str, List[bytes]]", "files": "Dict[str, List[HTTPFile]]", "headers": "Optional[HTTPHeaders]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPInputError", "arguments.setdefault", "arguments.setdefault(name, []).extend", "content_type.split", "content_type.startswith", "field.strip", "field.strip().partition", "parse_multipart_form_data", "parse_qs_bytes", "uri_arguments.items", "utf8"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Parses a form request body. Supports ``application/x-www-form-urlencoded`` and ``multipart/form-data``.  The ``content_type`` parameter should be", "source_code": "def parse_body_arguments(\n    content_type: str,\n    body: bytes,\n    arguments: Dict[str, List[bytes]],\n    files: Dict[str, List[HTTPFile]],\n    headers: Optional[HTTPHeaders] = None,\n) -> None:\n    \"\"\"Parses a form request body.\n\n    Supports ``application/x-www-form-urlencoded`` and\n    ``multipart/form-data``.  The ``content_type`` parameter should be\n    a string and ``body`` should be a byte string.  The ``arguments``\n    and ``files`` parameters are dictionaries that will be updated\n    with the parsed contents.\n    \"\"\"\n    if content_type.startswith(\"application/x-www-form-urlencoded\"):\n        if headers and \"Content-Encoding\" in headers:\n            raise HTTPInputError(\n                \"Unsupported Content-Encoding: %s\" % headers[\"Content-Encoding\"]\n            )\n        try:\n            # real charset decoding will happen in RequestHandler.decode_argument()\n            uri_arguments = parse_qs_bytes(body, keep_blank_values=True)\n        except Exception as e:\n            raise HTTPInputError(\"Invalid x-www-form-urlencoded body: %s\" % e) from e\n        for name, values in uri_arguments.items():\n            if values:\n                arguments.setdefault(name, []).extend(values)\n    elif content_type.startswith(\"multipart/form-data\"):\n        if headers and \"Content-Encoding\" in headers:\n            raise HTTPInputError(\n                \"Unsupported Content-Encoding: %s\" % headers[\"Content-Encoding\"]\n            )\n        try:\n            fields = content_type.split(\";\")\n            for field in fields:\n                k, sep, v = field.strip().partition(\"=\")\n                if k == \"boundary\" and v:\n                    parse_multipart_form_data(utf8(v), body, arguments, files)\n                    break\n            else:\n                raise HTTPInputError(\"multipart boundary not found\")\n        except Exception as e:\n            raise HTTPInputError(\"Invalid multipart/form-data: %s\" % e) from e", "loc": 44}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "parse_multipart_form_data", "parameters": ["boundary", "data", "arguments", "files"], "param_types": {"boundary": "bytes", "data": "bytes", "arguments": "Dict[str, List[bytes]]", "files": "Dict[str, List[HTTPFile]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPFile", "HTTPHeaders.parse", "HTTPInputError", "_parse_header", "arguments.setdefault", "arguments.setdefault(name, []).append", "boundary.endswith", "boundary.startswith", "data.rfind", "data[:final_boundary_index].split", "disp_params.get", "files.setdefault", "files.setdefault(name, []).append", "headers.get", "part.endswith", "part.find", "part[:eoh].decode"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Parses a ``multipart/form-data`` body. The ``boundary`` and ``data`` parameters are both byte strings. The dictionaries given in the arguments and files parameters", "source_code": "def parse_multipart_form_data(\n    boundary: bytes,\n    data: bytes,\n    arguments: Dict[str, List[bytes]],\n    files: Dict[str, List[HTTPFile]],\n) -> None:\n    \"\"\"Parses a ``multipart/form-data`` body.\n\n    The ``boundary`` and ``data`` parameters are both byte strings.\n    The dictionaries given in the arguments and files parameters\n    will be updated with the contents of the body.\n\n    .. versionchanged:: 5.1\n\n       Now recognizes non-ASCII filenames in RFC 2231/5987\n       (``filename*=``) format.\n    \"\"\"\n    # The standard allows for the boundary to be quoted in the header,\n    # although it's rare (it happens at least for google app engine\n    # xmpp).  I think we're also supposed to handle backslash-escapes\n    # here but I'll save that until we see a client that uses them\n    # in the wild.\n    if boundary.startswith(b'\"') and boundary.endswith(b'\"'):\n        boundary = boundary[1:-1]\n    final_boundary_index = data.rfind(b\"--\" + boundary + b\"--\")\n    if final_boundary_index == -1:\n        raise HTTPInputError(\"Invalid multipart/form-data: no final boundary found\")\n    parts = data[:final_boundary_index].split(b\"--\" + boundary + b\"\\r\\n\")\n    for part in parts:\n        if not part:\n            continue\n        eoh = part.find(b\"\\r\\n\\r\\n\")\n        if eoh == -1:\n            raise HTTPInputError(\"multipart/form-data missing headers\")\n        headers = HTTPHeaders.parse(part[:eoh].decode(\"utf-8\"), _chars_are_bytes=False)\n        disp_header = headers.get(\"Content-Disposition\", \"\")\n        disposition, disp_params = _parse_header(disp_header)\n        if disposition != \"form-data\" or not part.endswith(b\"\\r\\n\"):\n            raise HTTPInputError(\"Invalid multipart/form-data\")\n        value = part[eoh + 4 : -2]\n        if not disp_params.get(\"name\"):\n            raise HTTPInputError(\"multipart/form-data missing name\")\n        name = disp_params[\"name\"]\n        if disp_params.get(\"filename\"):\n            ctype = headers.get(\"Content-Type\", \"application/unknown\")\n            files.setdefault(name, []).append(\n                HTTPFile(\n                    filename=disp_params[\"filename\"], body=value, content_type=ctype\n                )\n            )\n        else:\n            arguments.setdefault(name, []).append(value)", "loc": 52}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "format_timestamp", "parameters": ["ts"], "param_types": {"ts": "Union[int, float, tuple, time.struct_time, datetime.datetime]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "calendar.timegm", "email.utils.formatdate", "isinstance", "ts.utctimetuple"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Formats a timestamp in the format used by HTTP. The argument may be a numeric timestamp as returned by `time.time`, a time tuple as returned by `time.gmtime`, or a `datetime.datetime`", "source_code": "def format_timestamp(\n    ts: Union[int, float, tuple, time.struct_time, datetime.datetime],\n) -> str:\n    \"\"\"Formats a timestamp in the format used by HTTP.\n\n    The argument may be a numeric timestamp as returned by `time.time`,\n    a time tuple as returned by `time.gmtime`, or a `datetime.datetime`\n    object. Naive `datetime.datetime` objects are assumed to represent\n    UTC; aware objects are converted to UTC before formatting.\n\n    >>> format_timestamp(1359312200)\n    'Sun, 27 Jan 2013 18:43:20 GMT'\n    \"\"\"\n    if isinstance(ts, (int, float)):\n        time_num = ts\n    elif isinstance(ts, (tuple, time.struct_time)):\n        time_num = calendar.timegm(ts)\n    elif isinstance(ts, datetime.datetime):\n        time_num = calendar.timegm(ts.utctimetuple())\n    else:\n        raise TypeError(\"unknown timestamp type: %r\" % ts)\n    return email.utils.formatdate(time_num, usegmt=True)", "loc": 22}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "parse_request_start_line", "parameters": ["line"], "param_types": {"line": "str"}, "return_type": "RequestStartLine", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPInputError", "RequestStartLine", "_ABNF.request_line.fullmatch", "match.group", "r.version.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_request_start_line(line: str) -> RequestStartLine:\n    \"\"\"Returns a (method, path, version) tuple for an HTTP 1.x request line.\n\n    The response is a `typing.NamedTuple`.\n\n    >>> parse_request_start_line(\"GET /foo HTTP/1.1\")\n    RequestStartLine(method='GET', path='/foo', version='HTTP/1.1')\n    \"\"\"\n    match = _ABNF.request_line.fullmatch(line)\n    if not match:\n        # https://tools.ietf.org/html/rfc7230#section-3.1.1\n        # invalid request-line SHOULD respond with a 400 (Bad Request)\n        raise HTTPInputError(\"Malformed HTTP request line\")\n    r = RequestStartLine(match.group(1), match.group(2), match.group(3))\n    if not r.version.startswith(\"HTTP/1\"):\n        # HTTP/2 and above doesn't use parse_request_start_line.\n        # This could be folded into the regex but we don't want to deviate\n        # from the ABNF in the RFCs.\n        raise HTTPInputError(\"Unexpected HTTP version %r\" % r.version)\n    return r", "loc": 20}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "parse_response_start_line", "parameters": ["line"], "param_types": {"line": "str"}, "return_type": "ResponseStartLine", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPInputError", "ResponseStartLine", "_ABNF.status_line.fullmatch", "int", "match.group", "r.version.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_response_start_line(line: str) -> ResponseStartLine:\n    \"\"\"Returns a (version, code, reason) tuple for an HTTP 1.x response line.\n\n    The response is a `typing.NamedTuple`.\n\n    >>> parse_response_start_line(\"HTTP/1.1 200 OK\")\n    ResponseStartLine(version='HTTP/1.1', code=200, reason='OK')\n    \"\"\"\n    match = _ABNF.status_line.fullmatch(line)\n    if not match:\n        raise HTTPInputError(\"Error parsing response start line\")\n    r = ResponseStartLine(match.group(1), int(match.group(2)), match.group(3))\n    if not r.version.startswith(\"HTTP/1\"):\n        # HTTP/2 and above doesn't use parse_response_start_line.\n        raise HTTPInputError(\"Unexpected HTTP version %r\" % r.version)\n    return r", "loc": 16}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "encode_username_password", "parameters": ["username", "password"], "param_types": {"username": "Union[str, bytes]", "password": "Union[str, bytes]"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "unicodedata.normalize", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Encodes a username/password pair in the format used by HTTP auth. The return value is a byte string in the form ``username:password``. .. versionadded:: 5.1", "source_code": "def encode_username_password(\n    username: Union[str, bytes], password: Union[str, bytes]\n) -> bytes:\n    \"\"\"Encodes a username/password pair in the format used by HTTP auth.\n\n    The return value is a byte string in the form ``username:password``.\n\n    .. versionadded:: 5.1\n    \"\"\"\n    if isinstance(username, unicode_type):\n        username = unicodedata.normalize(\"NFC\", username)\n    if isinstance(password, unicode_type):\n        password = unicodedata.normalize(\"NFC\", password)\n    return utf8(username) + b\":\" + utf8(password)", "loc": 14}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "split_host_and_port", "parameters": ["netloc"], "param_types": {"netloc": "str"}, "return_type": "Tuple[str, Optional[int]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_netloc_re.match", "int", "match.group"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def split_host_and_port(netloc: str) -> Tuple[str, Optional[int]]:\n    \"\"\"Returns ``(host, port)`` tuple from ``netloc``.\n\n    Returned ``port`` will be ``None`` if not present.\n\n    .. versionadded:: 4.1\n    \"\"\"\n    match = _netloc_re.match(netloc)\n    if match:\n        host = match.group(1)\n        port = int(match.group(2))  # type: Optional[int]\n    else:\n        host = netloc\n        port = None\n    return (host, port)", "loc": 15}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "qs_to_qsl", "parameters": ["qs"], "param_types": {"qs": "Dict[str, List[AnyStr]]"}, "return_type": "Iterable[Tuple[str, AnyStr]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["qs.items"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Generator converting a result of ``parse_qs`` back to name-value pairs. .. versionadded:: 5.0", "source_code": "def qs_to_qsl(qs: Dict[str, List[AnyStr]]) -> Iterable[Tuple[str, AnyStr]]:\n    \"\"\"Generator converting a result of ``parse_qs`` back to name-value pairs.\n\n    .. versionadded:: 5.0\n    \"\"\"\n    for k, vs in qs.items():\n        for v in vs:\n            yield (k, v)", "loc": 8}
{"file": "tornado\\tornado\\httputil.py", "class_name": null, "function_name": "parse_cookie", "parameters": ["cookie"], "param_types": {"cookie": "str"}, "return_type": "Dict[str, str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_unquote_cookie", "chunk.split", "cookie.split", "key.strip", "val.strip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Parse a ``Cookie`` HTTP header into a dict of name/value pairs. This function attempts to mimic browser cookie parsing behavior; it specifically does not follow any of the cookie-related RFCs", "source_code": "def parse_cookie(cookie: str) -> Dict[str, str]:\n    \"\"\"Parse a ``Cookie`` HTTP header into a dict of name/value pairs.\n\n    This function attempts to mimic browser cookie parsing behavior;\n    it specifically does not follow any of the cookie-related RFCs\n    (because browsers don't either).\n\n    The algorithm used is identical to that used by Django version 1.9.10.\n\n    .. versionadded:: 4.4.2\n    \"\"\"\n    cookiedict = {}\n    for chunk in cookie.split(\";\"):\n        if \"=\" in chunk:\n            key, val = chunk.split(\"=\", 1)\n        else:\n            # Assume an empty name per\n            # https://bugzilla.mozilla.org/show_bug.cgi?id=169091\n            key, val = \"\", chunk\n        key, val = key.strip(), val.strip()\n        if key or val:\n            # unquote using Python's algorithm.\n            cookiedict[key] = _unquote_cookie(val)\n    return cookiedict", "loc": 24}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPHeaders", "function_name": "add", "parameters": ["self", "name", "value"], "param_types": {"name": "str", "value": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPInputError", "_ABNF.field_name.fullmatch", "_ABNF.field_value.fullmatch", "_FORBIDDEN_HEADER_CHARS_RE.search", "_normalize_header", "native_str", "self._as_list[norm_name].append", "to_unicode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Adds a new value for the given key.", "source_code": "def add(self, name: str, value: str, *, _chars_are_bytes: bool = True) -> None:\n    \"\"\"Adds a new value for the given key.\"\"\"\n    if not _ABNF.field_name.fullmatch(name):\n        raise HTTPInputError(\"Invalid header name %r\" % name)\n    if _chars_are_bytes:\n        if not _ABNF.field_value.fullmatch(to_unicode(value)):\n            # TODO: the fact we still support bytes here (contrary to type annotations)\n            # and still test for it should probably be changed.\n            raise HTTPInputError(\"Invalid header value %r\" % value)\n    else:\n        if _FORBIDDEN_HEADER_CHARS_RE.search(value):\n            raise HTTPInputError(\"Invalid header value %r\" % value)\n    norm_name = _normalize_header(name)\n    self._last_key = norm_name\n    if norm_name in self:\n        self._dict[norm_name] = (\n            native_str(self[norm_name]) + \",\" + native_str(value)\n        )\n        self._as_list[norm_name].append(value)\n    else:\n        self[norm_name] = value", "loc": 21}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPHeaders", "function_name": "get_all", "parameters": ["self"], "param_types": {}, "return_type": "Iterable[Tuple[str, str]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._as_list.items"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_all(self) -> Iterable[Tuple[str, str]]:\n    \"\"\"Returns an iterable of all (name, value) pairs.\n\n    If a header has multiple values, multiple pairs will be\n    returned with the same name.\n    \"\"\"\n    for name, values in self._as_list.items():\n        for value in values:\n            yield (name, value)", "loc": 9}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPHeaders", "function_name": "parse_line", "parameters": ["self", "line"], "param_types": {"line": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPInputError", "_ABNF.field_value.fullmatch", "_FORBIDDEN_HEADER_CHARS_RE.search", "line.split", "line.strip", "m.start", "re.search", "self.add", "value.strip"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Updates the dictionary with a single header line. >>> h = HTTPHeaders() >>> h.parse_line(\"Content-Type: text/html\")", "source_code": "def parse_line(self, line: str, *, _chars_are_bytes: bool = True) -> None:\n    r\"\"\"Updates the dictionary with a single header line.\n\n    >>> h = HTTPHeaders()\n    >>> h.parse_line(\"Content-Type: text/html\")\n    >>> h.get('content-type')\n    'text/html'\n    >>> h.parse_line(\"Content-Length: 42\\r\\n\")\n    >>> h.get('content-type')\n    'text/html'\n\n    .. versionchanged:: 6.5\n        Now supports lines with or without the trailing CRLF, making it possible\n        to pass lines from AsyncHTTPClient's header_callback directly to this method.\n\n    .. deprecated:: 6.5\n       In Tornado 7.0, certain deprecated features of HTTP will become errors.\n       Specifically, line folding and the use of LF (with CR) as a line separator\n       will be removed.\n    \"\"\"\n    if m := re.search(r\"\\r?\\n$\", line):\n        # RFC 9112 section 2.2: a recipient MAY recognize a single LF as a line\n        # terminator and ignore any preceding CR.\n        # TODO(7.0): Remove this support for LF-only line endings.\n        line = line[: m.start()]\n    if not line:\n        # Empty line, or the final CRLF of a header block.\n        return\n    if line[0] in HTTP_WHITESPACE:\n        # continuation of a multi-line header\n        # TODO(7.0): Remove support for line folding.\n        if self._last_key is None:\n            raise HTTPInputError(\"first header line cannot start with whitespace\")\n        new_part = \" \" + line.strip(HTTP_WHITESPACE)\n        if _chars_are_bytes:\n            if not _ABNF.field_value.fullmatch(new_part[1:]):\n                raise HTTPInputError(\"Invalid header continuation %r\" % new_part)\n        else:\n            if _FORBIDDEN_HEADER_CHARS_RE.search(new_part):\n                raise HTTPInputError(\"Invalid header value %r\" % new_part)\n        self._as_list[self._last_key][-1] += new_part\n        self._dict[self._last_key] += new_part\n    else:\n        try:\n            name, value = line.split(\":\", 1)\n        except ValueError:\n            raise HTTPInputError(\"no colon in header line\")\n        self.add(\n            name, value.strip(HTTP_WHITESPACE), _chars_are_bytes=_chars_are_bytes\n        )", "loc": 50}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPHeaders", "function_name": "parse", "parameters": ["cls", "headers"], "param_types": {"headers": "str"}, "return_type": "'HTTPHeaders'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "h.parse_line", "headers.find"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(cls, headers: str, *, _chars_are_bytes: bool = True) -> \"HTTPHeaders\":\n    \"\"\"Returns a dictionary from HTTP header text.\n\n    >>> h = HTTPHeaders.parse(\"Content-Type: text/html\\\\r\\\\nContent-Length: 42\\\\r\\\\n\")\n    >>> sorted(h.items())\n    [('Content-Length', '42'), ('Content-Type', 'text/html')]\n\n    .. versionchanged:: 5.1\n\n       Raises `HTTPInputError` on malformed headers instead of a\n       mix of `KeyError`, and `ValueError`.\n\n    \"\"\"\n    # _chars_are_bytes is a hack. This method is used in two places, HTTP headers (in which\n    # non-ascii characters are to be interpreted as latin-1) and multipart/form-data (in which\n    # they are to be interpreted as utf-8). For historical reasons, this method handled this by\n    # expecting both callers to decode the headers to strings before parsing them. This wasn't a\n    # problem until we started doing stricter validation of the characters allowed in HTTP\n    # headers (using ABNF rules defined in terms of byte values), which inadvertently started\n    # disallowing non-latin1 characters in multipart/form-data filenames.\n    #\n    # This method should have accepted bytes and a desired encoding, but this change is being\n    # introduced in a patch release that shouldn't change the API. Instead, the _chars_are_bytes\n    # flag decides whether to use HTTP-style ABNF validation (treating the string as bytes\n    # smuggled through the latin1 encoding) or to accept any non-control unicode characters\n    # as required by multipart/form-data. This method will change to accept bytes in a future\n    # release.\n    h = cls()\n\n    start = 0\n    while True:\n        lf = headers.find(\"\\n\", start)\n        if lf == -1:\n            h.parse_line(headers[start:], _chars_are_bytes=_chars_are_bytes)\n            break\n        line = headers[start : lf + 1]\n        start = lf + 1\n        h.parse_line(line, _chars_are_bytes=_chars_are_bytes)\n    return h", "loc": 39}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPServerRequest", "function_name": "request_time", "parameters": ["self"], "param_types": {}, "return_type": "float", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def request_time(self) -> float:\n    \"\"\"Returns the amount of time it took for this request to execute.\"\"\"\n    if self._finish_time is None:\n        return time.time() - self._start_time\n    else:\n        return self._finish_time - self._start_time", "loc": 6}
{"file": "tornado\\tornado\\httputil.py", "class_name": "HTTPServerRequest", "function_name": "get_ssl_certificate", "parameters": ["self", "binary_form"], "param_types": {"binary_form": "bool"}, "return_type": "Union[None, Dict, bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.connection.stream.socket.getpeercert"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ssl_certificate(\n    self, binary_form: bool = False\n) -> Union[None, Dict, bytes]:\n    \"\"\"Returns the client's SSL certificate, if any.\n\n    To use client certificates, the HTTPServer's\n    `ssl.SSLContext.verify_mode` field must be set, e.g.::\n\n        ssl_ctx = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        ssl_ctx.load_cert_chain(\"foo.crt\", \"foo.key\")\n        ssl_ctx.load_verify_locations(\"cacerts.pem\")\n        ssl_ctx.verify_mode = ssl.CERT_REQUIRED\n        server = HTTPServer(app, ssl_options=ssl_ctx)\n\n    By default, the return value is a dictionary (or None, if no\n    client certificate is present).  If ``binary_form`` is true, a\n    DER-encoded form of the certificate is returned instead.  See\n    SSLSocket.getpeercert() in the standard library for more\n    details.\n    http://docs.python.org/library/ssl.html#sslsocket-objects\n    \"\"\"\n    try:\n        if self.connection is None:\n            return None\n        # TODO: add a method to HTTPConnection for this so it can work with HTTP/2\n        return self.connection.stream.socket.getpeercert(  # type: ignore\n            binary_form=binary_form\n        )\n    except SSLError:\n        return None", "loc": 30}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "configure", "parameters": ["cls", "impl"], "param_types": {"impl": "'Union[None, str, Type[Configurable]]'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "import_object", "isinstance", "issubclass", "super", "super().configure"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def configure(\n    cls, impl: \"Union[None, str, Type[Configurable]]\", **kwargs: Any\n) -> None:\n    from tornado.platform.asyncio import BaseAsyncIOLoop\n\n    if isinstance(impl, str):\n        impl = import_object(impl)\n    if isinstance(impl, type) and not issubclass(impl, BaseAsyncIOLoop):\n        raise RuntimeError(\"only AsyncIOLoop is allowed when asyncio is available\")\n    super().configure(impl, **kwargs)", "loc": 10}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "current", "parameters": ["instance"], "param_types": {"instance": "bool"}, "return_type": "Optional['IOLoop']", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AsyncIOMainLoop", "asyncio.get_event_loop", "asyncio.new_event_loop", "asyncio.set_event_loop"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def current(instance: bool = True) -> Optional[\"IOLoop\"]:  # noqa: F811\n    \"\"\"Returns the current thread's `IOLoop`.\n\n    If an `IOLoop` is currently running or has been marked as\n    current by `make_current`, returns that instance.  If there is\n    no current `IOLoop` and ``instance`` is true, creates one.\n\n    .. versionchanged:: 4.1\n       Added ``instance`` argument to control the fallback to\n       `IOLoop.instance()`.\n    .. versionchanged:: 5.0\n       On Python 3, control of the current `IOLoop` is delegated\n       to `asyncio`, with this and other methods as pass-through accessors.\n       The ``instance`` argument now controls whether an `IOLoop`\n       is created automatically when there is none, instead of\n       whether we fall back to `IOLoop.instance()` (which is now\n       an alias for this method). ``instance=False`` is deprecated,\n       since even if we do not create an `IOLoop`, this method\n       may initialize the asyncio loop.\n\n    .. deprecated:: 6.2\n       It is deprecated to call ``IOLoop.current()`` when no `asyncio`\n       event loop is running.\n    \"\"\"\n    try:\n        loop = asyncio.get_event_loop()\n    except RuntimeError:\n        if not instance:\n            return None\n        # Create a new asyncio event loop for this thread.\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n\n    try:\n        return IOLoop._ioloop_for_asyncio[loop]\n    except KeyError:\n        if instance:\n            from tornado.platform.asyncio import AsyncIOMainLoop\n\n            current = AsyncIOMainLoop()  # type: Optional[IOLoop]\n        else:\n            current = None\n    return current", "loc": 43}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "make_current", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._make_current", "warnings.warn"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Makes this the `IOLoop` for the current thread. An `IOLoop` automatically becomes current for its thread when it is started, but it is sometimes useful to call", "source_code": "def make_current(self) -> None:\n    \"\"\"Makes this the `IOLoop` for the current thread.\n\n    An `IOLoop` automatically becomes current for its thread\n    when it is started, but it is sometimes useful to call\n    `make_current` explicitly before starting the `IOLoop`,\n    so that code run at startup time can find the right\n    instance.\n\n    .. versionchanged:: 4.1\n       An `IOLoop` created while there is no current `IOLoop`\n       will automatically become current.\n\n    .. versionchanged:: 5.0\n       This method also sets the current `asyncio` event loop.\n\n    .. deprecated:: 6.2\n       Setting and clearing the current event loop through Tornado is\n       deprecated. Use ``asyncio.set_event_loop`` instead if you need this.\n    \"\"\"\n    warnings.warn(\n        \"make_current is deprecated; start the event loop first\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    self._make_current()", "loc": 26}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "clear_current", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop._clear_current", "warnings.warn"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Clears the `IOLoop` for the current thread. Intended primarily for use by test frameworks in between tests. .. versionchanged:: 5.0", "source_code": "def clear_current() -> None:\n    \"\"\"Clears the `IOLoop` for the current thread.\n\n    Intended primarily for use by test frameworks in between tests.\n\n    .. versionchanged:: 5.0\n       This method also clears the current `asyncio` event loop.\n    .. deprecated:: 6.2\n    \"\"\"\n    warnings.warn(\n        \"clear_current is deprecated\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    IOLoop._clear_current()", "loc": 15}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "run_sync", "parameters": ["self", "func", "timeout"], "param_types": {"func": "Callable", "timeout": "Optional[float]"}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "RuntimeError", "TimeoutError", "TypedDict", "convert_yielded", "func", "fut.set_result", "future_cell['future'].cancel", "future_cell['future'].cancelled", "future_cell['future'].done", "future_cell['future'].result", "future_set_exc_info", "is_future", "self.add_callback", "self.add_future", "self.add_timeout", "self.remove_timeout", "self.start", "self.stop", "self.time", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Starts the `IOLoop`, runs the given function, and stops the loop. The function must return either an awaitable object or ``None``. If the function returns an awaitable object, the", "source_code": "def run_sync(self, func: Callable, timeout: Optional[float] = None) -> Any:\n    \"\"\"Starts the `IOLoop`, runs the given function, and stops the loop.\n\n    The function must return either an awaitable object or\n    ``None``. If the function returns an awaitable object, the\n    `IOLoop` will run until the awaitable is resolved (and\n    `run_sync()` will return the awaitable's result). If it raises\n    an exception, the `IOLoop` will stop and the exception will be\n    re-raised to the caller.\n\n    The keyword-only argument ``timeout`` may be used to set\n    a maximum duration for the function.  If the timeout expires,\n    a `asyncio.TimeoutError` is raised.\n\n    This method is useful to allow asynchronous calls in a\n    ``main()`` function::\n\n        async def main():\n            # do stuff...\n\n        if __name__ == '__main__':\n            IOLoop.current().run_sync(main)\n\n    .. versionchanged:: 4.3\n       Returning a non-``None``, non-awaitable value is now an error.\n\n    .. versionchanged:: 5.0\n       If a timeout occurs, the ``func`` coroutine will be cancelled.\n\n    .. versionchanged:: 6.2\n       ``tornado.util.TimeoutError`` is now an alias to ``asyncio.TimeoutError``.\n    \"\"\"\n    if typing.TYPE_CHECKING:\n        FutureCell = TypedDict(  # noqa: F841\n            \"FutureCell\", {\"future\": Optional[Future], \"timeout_called\": bool}\n        )\n    future_cell = {\"future\": None, \"timeout_called\": False}  # type: FutureCell\n\n    def run() -> None:\n        try:\n            result = func()\n            if result is not None:\n                from tornado.gen import convert_yielded\n\n                result = convert_yielded(result)\n        except Exception:\n            fut = Future()  # type: Future[Any]\n            future_cell[\"future\"] = fut\n            future_set_exc_info(fut, sys.exc_info())\n        else:\n            if is_future(result):\n                future_cell[\"future\"] = result\n            else:\n                fut = Future()\n                future_cell[\"future\"] = fut\n                fut.set_result(result)\n        assert future_cell[\"future\"] is not None\n        self.add_future(future_cell[\"future\"], lambda future: self.stop())\n\n    self.add_callback(run)\n    if timeout is not None:\n\n        def timeout_callback() -> None:\n            # signal that timeout is triggered\n            future_cell[\"timeout_called\"] = True\n            # If we can cancel the future, do so and wait on it. If not,\n            # Just stop the loop and return with the task still pending.\n            # (If we neither cancel nor wait for the task, a warning\n            # will be logged).\n            assert future_cell[\"future\"] is not None\n            if not future_cell[\"future\"].cancel():\n                self.stop()\n\n        timeout_handle = self.add_timeout(self.time() + timeout, timeout_callback)\n    self.start()\n    if timeout is not None:\n        self.remove_timeout(timeout_handle)\n    assert future_cell[\"future\"] is not None\n    if future_cell[\"future\"].cancelled() or not future_cell[\"future\"].done():\n        if future_cell[\"timeout_called\"]:\n            raise TimeoutError(\"Operation timed out after %s seconds\" % timeout)\n        else:\n            # timeout not called; maybe stop() was called explicitly\n            # or some other cancellation\n            raise RuntimeError(\"Event loop stopped before Future completed.\")\n    return future_cell[\"future\"].result()", "loc": 86}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "add_timeout", "parameters": ["self", "deadline", "callback"], "param_types": {"deadline": "Union[float, datetime.timedelta]", "callback": "Callable"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "deadline.total_seconds", "isinstance", "self.call_at", "self.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Runs the ``callback`` at the time ``deadline`` from the I/O loop.", "source_code": "def add_timeout(\n    self,\n    deadline: Union[float, datetime.timedelta],\n    callback: Callable,\n    *args: Any,\n    **kwargs: Any,\n) -> object:\n    \"\"\"Runs the ``callback`` at the time ``deadline`` from the I/O loop.\n\n    Returns an opaque handle that may be passed to\n    `remove_timeout` to cancel.\n\n    ``deadline`` may be a number denoting a time (on the same\n    scale as `IOLoop.time`, normally `time.time`), or a\n    `datetime.timedelta` object for a deadline relative to the\n    current time.  Since Tornado 4.0, `call_later` is a more\n    convenient alternative for the relative case since it does not\n    require a timedelta object.\n\n    Note that it is not safe to call `add_timeout` from other threads.\n    Instead, you must use `add_callback` to transfer control to the\n    `IOLoop`'s thread, and then call `add_timeout` from there.\n\n    Subclasses of IOLoop must implement either `add_timeout` or\n    `call_at`; the default implementations of each will call\n    the other.  `call_at` is usually easier to implement, but\n    subclasses that wish to maintain compatibility with Tornado\n    versions prior to 4.0 must use `add_timeout` instead.\n\n    .. versionchanged:: 4.0\n       Now passes through ``*args`` and ``**kwargs`` to the callback.\n    \"\"\"\n    if isinstance(deadline, numbers.Real):\n        return self.call_at(deadline, callback, *args, **kwargs)\n    elif isinstance(deadline, datetime.timedelta):\n        return self.call_at(\n            self.time() + deadline.total_seconds(), callback, *args, **kwargs\n        )\n    else:\n        raise TypeError(\"Unsupported deadline %r\" % deadline)", "loc": 40}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "call_later", "parameters": ["self", "delay", "callback"], "param_types": {"delay": "float", "callback": "Callable"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.call_at", "self.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Runs the ``callback`` after ``delay`` seconds have passed.", "source_code": "def call_later(\n    self, delay: float, callback: Callable, *args: Any, **kwargs: Any\n) -> object:\n    \"\"\"Runs the ``callback`` after ``delay`` seconds have passed.\n\n    Returns an opaque handle that may be passed to `remove_timeout`\n    to cancel.  Note that unlike the `asyncio` method of the same\n    name, the returned object does not have a ``cancel()`` method.\n\n    See `add_timeout` for comments on thread-safety and subclassing.\n\n    .. versionadded:: 4.0\n    \"\"\"\n    return self.call_at(self.time() + delay, callback, *args, **kwargs)", "loc": 14}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "add_future", "parameters": ["self", "future", "callback"], "param_types": {"future": "'Union[Future[_T], concurrent.futures.Future[_T]]'", "callback": "Callable[['Future[_T]'], None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "future.add_done_callback", "future_add_done_callback", "is_future", "isinstance", "self._run_callback", "self.add_callback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Schedules a callback on the ``IOLoop`` when the given `.Future` is finished. The callback is invoked with one argument, the", "source_code": "def add_future(\n    self,\n    future: \"Union[Future[_T], concurrent.futures.Future[_T]]\",\n    callback: Callable[[\"Future[_T]\"], None],\n) -> None:\n    \"\"\"Schedules a callback on the ``IOLoop`` when the given\n    `.Future` is finished.\n\n    The callback is invoked with one argument, the\n    `.Future`.\n\n    This method only accepts `.Future` objects and not other\n    awaitables (unlike most of Tornado where the two are\n    interchangeable).\n    \"\"\"\n    if isinstance(future, Future):\n        # Note that we specifically do not want the inline behavior of\n        # tornado.concurrent.future_add_done_callback. We always want\n        # this callback scheduled on the next IOLoop iteration (which\n        # asyncio.Future always does).\n        #\n        # Wrap the callback in self._run_callback so we control\n        # the error logging (i.e. it goes to tornado.log.app_log\n        # instead of asyncio's log).\n        future.add_done_callback(\n            lambda f: self._run_callback(functools.partial(callback, f))\n        )\n    else:\n        assert is_future(future)\n        # For concurrent futures, we use self.add_callback, so\n        # it's fine if future_add_done_callback inlines that call.\n        future_add_done_callback(future, lambda f: self.add_callback(callback, f))", "loc": 32}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "run_in_executor", "parameters": ["self", "executor", "func"], "param_types": {"executor": "Optional[concurrent.futures.Executor]", "func": "Callable[..., _T]"}, "return_type": "'Future[_T]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "chain_future", "concurrent.futures.ThreadPoolExecutor", "cpu_count", "executor.submit", "hasattr", "self.add_future"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Runs a function in a ``concurrent.futures.Executor``. If ``executor`` is ``None``, the IO loop's default executor will be used. Use `functools.partial` to pass keyword arguments to ``func``.", "source_code": "def run_in_executor(\n    self,\n    executor: Optional[concurrent.futures.Executor],\n    func: Callable[..., _T],\n    *args: Any,\n) -> \"Future[_T]\":\n    \"\"\"Runs a function in a ``concurrent.futures.Executor``. If\n    ``executor`` is ``None``, the IO loop's default executor will be used.\n\n    Use `functools.partial` to pass keyword arguments to ``func``.\n\n    .. versionadded:: 5.0\n    \"\"\"\n    if executor is None:\n        if not hasattr(self, \"_executor\"):\n            from tornado.process import cpu_count\n\n            self._executor = concurrent.futures.ThreadPoolExecutor(\n                max_workers=(cpu_count() * 5)\n            )  # type: concurrent.futures.Executor\n        executor = self._executor\n    c_future = executor.submit(func, *args)\n    # Concurrent Futures are not usable with await. Wrap this in a\n    # Tornado Future instead, using self.add_future for thread-safety.\n    t_future = Future()  # type: Future[_T]\n    self.add_future(c_future, lambda f: chain_future(f, t_future))\n    return t_future", "loc": 27}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "split_fd", "parameters": ["self", "fd"], "param_types": {"fd": "Union[int, _Selectable]"}, "return_type": "Tuple[int, Union[int, _Selectable]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fd.fileno", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def split_fd(\n    self, fd: Union[int, _Selectable]\n) -> Tuple[int, Union[int, _Selectable]]:\n    # \"\"\"Returns an (fd, obj) pair from an ``fd`` parameter.\n\n    # We accept both raw file descriptors and file-like objects as\n    # input to `add_handler` and related methods.  When a file-like\n    # object is passed, we must retain the object itself so we can\n    # close it correctly when the `IOLoop` shuts down, but the\n    # poller interfaces favor file descriptors (they will accept\n    # file-like objects and call ``fileno()`` for you, but they\n    # always return the descriptor itself).\n\n    # This method is provided for use by `IOLoop` subclasses and should\n    # not generally be used by application code.\n\n    # .. versionadded:: 4.0\n    # \"\"\"\n    if isinstance(fd, int):\n        return fd, fd\n    return fd.fileno(), fd", "loc": 21}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "IOLoop", "function_name": "close_fd", "parameters": ["self", "fd"], "param_types": {"fd": "Union[int, _Selectable]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fd.close", "isinstance", "os.close"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close_fd(self, fd: Union[int, _Selectable]) -> None:\n    # \"\"\"Utility method to close an ``fd``.\n\n    # If ``fd`` is a file-like object, we close it directly; otherwise\n    # we use `os.close`.\n\n    # This method is provided for use by `IOLoop` subclasses (in\n    # implementations of ``IOLoop.close(all_fds=True)`` and should\n    # not generally be used by application code.\n\n    # .. versionadded:: 4.0\n    # \"\"\"\n    try:\n        if isinstance(fd, int):\n            os.close(fd)\n        else:\n            fd.close()\n    except OSError:\n        pass", "loc": 19}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "PeriodicCallback", "function_name": "start", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "self._schedule_next", "self.io_loop.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Starts the timer.", "source_code": "def start(self) -> None:\n    \"\"\"Starts the timer.\"\"\"\n    # Looking up the IOLoop here allows to first instantiate the\n    # PeriodicCallback in another thread, then start it using\n    # IOLoop.add_callback().\n    self.io_loop = IOLoop.current()\n    self._running = True\n    self._next_timeout = self.io_loop.time()\n    self._schedule_next()", "loc": 9}
{"file": "tornado\\tornado\\ioloop.py", "class_name": "PeriodicCallback", "function_name": "stop", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.io_loop.remove_timeout"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Stops the timer.", "source_code": "def stop(self) -> None:\n    \"\"\"Stops the timer.\"\"\"\n    self._running = False\n    if self._timeout is not None:\n        self.io_loop.remove_timeout(self._timeout)\n        self._timeout = None", "loc": 6}
{"file": "tornado\\tornado\\ioloop.py", "class_name": null, "function_name": "run", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "convert_yielded", "func", "fut.set_result", "future_set_exc_info", "is_future", "self.add_future", "self.stop", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run() -> None:\n    try:\n        result = func()\n        if result is not None:\n            from tornado.gen import convert_yielded\n\n            result = convert_yielded(result)\n    except Exception:\n        fut = Future()  # type: Future[Any]\n        future_cell[\"future\"] = fut\n        future_set_exc_info(fut, sys.exc_info())\n    else:\n        if is_future(result):\n            future_cell[\"future\"] = result\n        else:\n            fut = Future()\n            future_cell[\"future\"] = fut\n            fut.set_result(result)\n    assert future_cell[\"future\"] is not None\n    self.add_future(future_cell[\"future\"], lambda future: self.stop())", "loc": 20}
{"file": "tornado\\tornado\\ioloop.py", "class_name": null, "function_name": "timeout_callback", "parameters": [], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_cell['future'].cancel", "self.stop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def timeout_callback() -> None:\n    # signal that timeout is triggered\n    future_cell[\"timeout_called\"] = True\n    # If we can cancel the future, do so and wait on it. If not,\n    # Just stop the loop and return with the task still pending.\n    # (If we neither cancel nor wait for the task, a warning\n    # will be logged).\n    assert future_cell[\"future\"] is not None\n    if not future_cell[\"future\"].cancel():\n        self.stop()", "loc": 10}
{"file": "tornado\\tornado\\iostream.py", "class_name": "_StreamBuffer", "function_name": "append", "parameters": ["self", "data"], "param_types": {"data": "Union[bytes, bytearray, memoryview]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bytearray", "isinstance", "len", "memoryview", "self._buffers.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Append the given piece of data (should be a buffer-compatible object).", "source_code": "def append(self, data: Union[bytes, bytearray, memoryview]) -> None:\n    \"\"\"\n    Append the given piece of data (should be a buffer-compatible object).\n    \"\"\"\n    size = len(data)\n    if size > self._large_buf_threshold:\n        if not isinstance(data, memoryview):\n            data = memoryview(data)\n        self._buffers.append((True, data))\n    elif size > 0:\n        if self._buffers:\n            is_memview, b = self._buffers[-1]\n            new_buf = is_memview or len(b) >= self._large_buf_threshold\n        else:\n            new_buf = True\n        if new_buf:\n            self._buffers.append((False, bytearray(data)))\n        else:\n            b += data  # type: ignore\n\n    self._size += size", "loc": 21}
{"file": "tornado\\tornado\\iostream.py", "class_name": "_StreamBuffer", "function_name": "peek", "parameters": ["self", "size"], "param_types": {"size": "int"}, "return_type": "memoryview", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["memoryview", "typing.cast"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Get a view over at most ``size`` bytes (possibly fewer) at the current buffer position.", "source_code": "def peek(self, size: int) -> memoryview:\n    \"\"\"\n    Get a view over at most ``size`` bytes (possibly fewer) at the\n    current buffer position.\n    \"\"\"\n    assert size > 0\n    try:\n        is_memview, b = self._buffers[0]\n    except IndexError:\n        return memoryview(b\"\")\n\n    pos = self._first_pos\n    if is_memview:\n        return typing.cast(memoryview, b[pos : pos + size])\n    else:\n        return memoryview(b)[pos : pos + size]", "loc": 16}
{"file": "tornado\\tornado\\iostream.py", "class_name": "_StreamBuffer", "function_name": "advance", "parameters": ["self", "size"], "param_types": {"size": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["buffers.popleft", "len", "typing.cast"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Advance the current buffer position by ``size`` bytes.", "source_code": "def advance(self, size: int) -> None:\n    \"\"\"\n    Advance the current buffer position by ``size`` bytes.\n    \"\"\"\n    assert 0 < size <= self._size\n    self._size -= size\n    pos = self._first_pos\n\n    buffers = self._buffers\n    while buffers and size > 0:\n        is_large, b = buffers[0]\n        b_remain = len(b) - size - pos\n        if b_remain <= 0:\n            buffers.popleft()\n            size -= len(b) - pos\n            pos = 0\n        elif is_large:\n            pos += size\n            size = 0\n        else:\n            pos += size\n            del typing.cast(bytearray, b)[:pos]\n            pos = 0\n            size = 0\n\n    assert size == 0\n    self._first_pos = pos", "loc": 27}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "read_until_regex", "parameters": ["self", "regex", "max_bytes"], "param_types": {"regex": "bytes", "max_bytes": "Optional[int]"}, "return_type": "Awaitable[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "future.add_done_callback", "gen_log.info", "re.compile", "self._start_read", "self._try_inline_read", "self.close"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Asynchronously read until we have matched the given regex. The result includes the data that matches the regex and anything that came before it.", "source_code": "def read_until_regex(\n    self, regex: bytes, max_bytes: Optional[int] = None\n) -> Awaitable[bytes]:\n    \"\"\"Asynchronously read until we have matched the given regex.\n\n    The result includes the data that matches the regex and anything\n    that came before it.\n\n    If ``max_bytes`` is not None, the connection will be closed\n    if more than ``max_bytes`` bytes have been read and the regex is\n    not satisfied.\n\n    .. versionchanged:: 4.0\n        Added the ``max_bytes`` argument.  The ``callback`` argument is\n        now optional and a `.Future` will be returned if it is omitted.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n\n    \"\"\"\n    future = self._start_read()\n    self._read_regex = re.compile(regex)\n    self._read_max_bytes = max_bytes\n    try:\n        self._try_inline_read()\n    except UnsatisfiableReadError as e:\n        # Handle this the same way as in _handle_events.\n        gen_log.info(\"Unsatisfiable read, closing connection: %s\" % e)\n        self.close(exc_info=e)\n        return future\n    except:\n        # Ensure that the future doesn't log an error because its\n        # failure was never examined.\n        future.add_done_callback(lambda f: f.exception())\n        raise\n    return future", "loc": 38}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "read_until", "parameters": ["self", "delimiter", "max_bytes"], "param_types": {"delimiter": "bytes", "max_bytes": "Optional[int]"}, "return_type": "Awaitable[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "future.add_done_callback", "gen_log.info", "self._start_read", "self._try_inline_read", "self.close"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Asynchronously read until we have found the given delimiter. The result includes all the data read including the delimiter. If ``max_bytes`` is not None, the connection will be closed", "source_code": "def read_until(\n    self, delimiter: bytes, max_bytes: Optional[int] = None\n) -> Awaitable[bytes]:\n    \"\"\"Asynchronously read until we have found the given delimiter.\n\n    The result includes all the data read including the delimiter.\n\n    If ``max_bytes`` is not None, the connection will be closed\n    if more than ``max_bytes`` bytes have been read and the delimiter\n    is not found.\n\n    .. versionchanged:: 4.0\n        Added the ``max_bytes`` argument.  The ``callback`` argument is\n        now optional and a `.Future` will be returned if it is omitted.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n    \"\"\"\n    future = self._start_read()\n    self._read_delimiter = delimiter\n    self._read_max_bytes = max_bytes\n    try:\n        self._try_inline_read()\n    except UnsatisfiableReadError as e:\n        # Handle this the same way as in _handle_events.\n        gen_log.info(\"Unsatisfiable read, closing connection: %s\" % e)\n        self.close(exc_info=e)\n        return future\n    except:\n        future.add_done_callback(lambda f: f.exception())\n        raise\n    return future", "loc": 34}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "read_bytes", "parameters": ["self", "num_bytes", "partial"], "param_types": {"num_bytes": "int", "partial": "bool"}, "return_type": "Awaitable[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "future.add_done_callback", "isinstance", "self._start_read", "self._try_inline_read"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Asynchronously read a number of bytes. If ``partial`` is true, data is returned as soon as we have any bytes to return (but never more than ``num_bytes``)", "source_code": "def read_bytes(self, num_bytes: int, partial: bool = False) -> Awaitable[bytes]:\n    \"\"\"Asynchronously read a number of bytes.\n\n    If ``partial`` is true, data is returned as soon as we have\n    any bytes to return (but never more than ``num_bytes``)\n\n    .. versionchanged:: 4.0\n        Added the ``partial`` argument.  The callback argument is now\n        optional and a `.Future` will be returned if it is omitted.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` and ``streaming_callback`` arguments have\n       been removed. Use the returned `.Future` (and\n       ``partial=True`` for ``streaming_callback``) instead.\n\n    \"\"\"\n    future = self._start_read()\n    assert isinstance(num_bytes, numbers.Integral)\n    self._read_bytes = num_bytes\n    self._read_partial = partial\n    try:\n        self._try_inline_read()\n    except:\n        future.add_done_callback(lambda f: f.exception())\n        raise\n    return future", "loc": 27}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "read_into", "parameters": ["self", "buf", "partial"], "param_types": {"buf": "bytearray", "partial": "bool"}, "return_type": "Awaitable[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "future.add_done_callback", "len", "memoryview", "self._start_read", "self._try_inline_read"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Asynchronously read a number of bytes. ``buf`` must be a writable buffer into which data will be read. If ``partial`` is true, the callback is run as soon as any bytes", "source_code": "def read_into(self, buf: bytearray, partial: bool = False) -> Awaitable[int]:\n    \"\"\"Asynchronously read a number of bytes.\n\n    ``buf`` must be a writable buffer into which data will be read.\n\n    If ``partial`` is true, the callback is run as soon as any bytes\n    have been read.  Otherwise, it is run when the ``buf`` has been\n    entirely filled with read data.\n\n    .. versionadded:: 5.0\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n\n    \"\"\"\n    future = self._start_read()\n\n    # First copy data already in read buffer\n    available_bytes = self._read_buffer_size\n    n = len(buf)\n    if available_bytes >= n:\n        buf[:] = memoryview(self._read_buffer)[:n]\n        del self._read_buffer[:n]\n        self._after_user_read_buffer = self._read_buffer\n    elif available_bytes > 0:\n        buf[:available_bytes] = memoryview(self._read_buffer)[:]\n\n    # Set up the supplied buffer as our temporary read buffer.\n    # The original (if it had any data remaining) has been\n    # saved for later.\n    self._user_read_buffer = True\n    self._read_buffer = buf\n    self._read_buffer_size = available_bytes\n    self._read_bytes = n\n    self._read_partial = partial\n\n    try:\n        self._try_inline_read()\n    except:\n        future.add_done_callback(lambda f: f.exception())\n        raise\n    return future", "loc": 44}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "read_until_close", "parameters": ["self"], "param_types": {}, "return_type": "Awaitable[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "future.add_done_callback", "self._finish_read", "self._start_read", "self._try_inline_read", "self.closed"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Asynchronously reads all data from the socket until it is closed. This will buffer all available data until ``max_buffer_size`` is reached. If flow control or cancellation are desired, use a", "source_code": "def read_until_close(self) -> Awaitable[bytes]:\n    \"\"\"Asynchronously reads all data from the socket until it is closed.\n\n    This will buffer all available data until ``max_buffer_size``\n    is reached. If flow control or cancellation are desired, use a\n    loop with `read_bytes(partial=True) <.read_bytes>` instead.\n\n    .. versionchanged:: 4.0\n        The callback argument is now optional and a `.Future` will\n        be returned if it is omitted.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` and ``streaming_callback`` arguments have\n       been removed. Use the returned `.Future` (and `read_bytes`\n       with ``partial=True`` for ``streaming_callback``) instead.\n\n    \"\"\"\n    future = self._start_read()\n    if self.closed():\n        self._finish_read(self._read_buffer_size)\n        return future\n    self._read_until_close = True\n    try:\n        self._try_inline_read()\n    except:\n        future.add_done_callback(lambda f: f.exception())\n        raise\n    return future", "loc": 29}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "write", "parameters": ["self", "data"], "param_types": {"data": "Union[bytes, memoryview]"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "StreamBufferFullError", "f.exception", "future.add_done_callback", "isinstance", "len", "memoryview", "memoryview(data).cast", "self._add_io_state", "self._check_closed", "self._handle_write", "self._maybe_add_error_listener", "self._write_buffer.append", "self._write_futures.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Asynchronously write the given data to this stream. This method returns a `.Future` that resolves (with a result of ``None``) when the write has been completed.", "source_code": "def write(self, data: Union[bytes, memoryview]) -> \"Future[None]\":\n    \"\"\"Asynchronously write the given data to this stream.\n\n    This method returns a `.Future` that resolves (with a result\n    of ``None``) when the write has been completed.\n\n    The ``data`` argument may be of type `bytes` or `memoryview`.\n\n    .. versionchanged:: 4.0\n        Now returns a `.Future` if no callback is given.\n\n    .. versionchanged:: 4.5\n        Added support for `memoryview` arguments.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n\n    \"\"\"\n    self._check_closed()\n    if data:\n        if isinstance(data, memoryview):\n            # Make sure that ``len(data) == data.nbytes``\n            data = memoryview(data).cast(\"B\")\n        if (\n            self.max_write_buffer_size is not None\n            and len(self._write_buffer) + len(data) > self.max_write_buffer_size\n        ):\n            raise StreamBufferFullError(\"Reached maximum write buffer size\")\n        self._write_buffer.append(data)\n        self._total_write_index += len(data)\n    future = Future()  # type: Future[None]\n    future.add_done_callback(lambda f: f.exception())\n    self._write_futures.append((self._total_write_index, future))\n    if not self._connecting:\n        self._handle_write()\n        if self._write_buffer:\n            self._add_io_state(self.io_loop.WRITE)\n        self._maybe_add_error_listener()\n    return future", "loc": 41}
{"file": "tornado\\tornado\\iostream.py", "class_name": "BaseIOStream", "function_name": "close", "parameters": ["self", "exc_info"], "param_types": {"exc_info": "Union[None, bool, BaseException, Tuple['Optional[Type[BaseException]]', Optional[BaseException], Optional[TracebackType]]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "isinstance", "self._find_read_pos", "self._finish_read", "self._read_from_buffer", "self._signal_closed", "self.close_fd", "self.closed", "self.fileno", "self.io_loop.remove_handler", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Close this stream. If ``exc_info`` is true, set the ``error`` attribute to the current exception from `sys.exc_info` (or if ``exc_info`` is a tuple,", "source_code": "def close(\n    self,\n    exc_info: Union[\n        None,\n        bool,\n        BaseException,\n        Tuple[\n            \"Optional[Type[BaseException]]\",\n            Optional[BaseException],\n            Optional[TracebackType],\n        ],\n    ] = False,\n) -> None:\n    \"\"\"Close this stream.\n\n    If ``exc_info`` is true, set the ``error`` attribute to the current\n    exception from `sys.exc_info` (or if ``exc_info`` is a tuple,\n    use that instead of `sys.exc_info`).\n    \"\"\"\n    if not self.closed():\n        if exc_info:\n            if isinstance(exc_info, tuple):\n                self.error = exc_info[1]\n            elif isinstance(exc_info, BaseException):\n                self.error = exc_info\n            else:\n                exc_info = sys.exc_info()\n                if any(exc_info):\n                    self.error = exc_info[1]\n        if self._read_until_close:\n            self._read_until_close = False\n            self._finish_read(self._read_buffer_size)\n        elif self._read_future is not None:\n            # resolve reads that are pending and ready to complete\n            try:\n                pos = self._find_read_pos()\n            except UnsatisfiableReadError:\n                pass\n            else:\n                if pos is not None:\n                    self._read_from_buffer(pos)\n        if self._state is not None:\n            self.io_loop.remove_handler(self.fileno())\n            self._state = None\n        self.close_fd()\n        self._closed = True\n    self._signal_closed()", "loc": 47}
{"file": "tornado\\tornado\\iostream.py", "class_name": "IOStream", "function_name": "read_from_fd", "parameters": ["self", "buf"], "param_types": {"buf": "Union[bytearray, memoryview]"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.socket.recv_into"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_from_fd(self, buf: Union[bytearray, memoryview]) -> Optional[int]:\n    try:\n        return self.socket.recv_into(buf, len(buf))\n    except BlockingIOError:\n        return None\n    finally:\n        del buf", "loc": 7}
{"file": "tornado\\tornado\\iostream.py", "class_name": "IOStream", "function_name": "write_to_fd", "parameters": ["self", "data"], "param_types": {"data": "memoryview"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.socket.send"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write_to_fd(self, data: memoryview) -> int:\n    try:\n        return self.socket.send(data)  # type: ignore\n    finally:\n        # Avoid keeping to data, which can be a memoryview.\n        # See https://github.com/tornadoweb/tornado/pull/2008\n        del data", "loc": 7}
{"file": "tornado\\tornado\\iostream.py", "class_name": "IOStream", "function_name": "connect", "parameters": ["self", "address", "server_hostname"], "param_types": {"self": "_IOStreamType", "address": "Any", "server_hostname": "Optional[str]"}, "return_type": "'Future[_IOStreamType]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "gen_log.warning", "self._add_io_state", "self.close", "self.socket.connect", "self.socket.fileno", "typing.cast"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Connects the socket to a remote address without blocking. May only be called if the socket passed to the constructor was not previously connected.  The address parameter is in the", "source_code": "def connect(\n    self: _IOStreamType, address: Any, server_hostname: Optional[str] = None\n) -> \"Future[_IOStreamType]\":\n    \"\"\"Connects the socket to a remote address without blocking.\n\n    May only be called if the socket passed to the constructor was\n    not previously connected.  The address parameter is in the\n    same format as for `socket.connect <socket.socket.connect>` for\n    the type of socket passed to the IOStream constructor,\n    e.g. an ``(ip, port)`` tuple.  Hostnames are accepted here,\n    but will be resolved synchronously and block the IOLoop.\n    If you have a hostname instead of an IP address, the `.TCPClient`\n    class is recommended instead of calling this method directly.\n    `.TCPClient` will do asynchronous DNS resolution and handle\n    both IPv4 and IPv6.\n\n    If ``callback`` is specified, it will be called with no\n    arguments when the connection is completed; if not this method\n    returns a `.Future` (whose result after a successful\n    connection will be the stream itself).\n\n    In SSL mode, the ``server_hostname`` parameter will be used\n    for certificate validation (unless disabled in the\n    ``ssl_options``) and SNI.\n\n    Note that it is safe to call `IOStream.write\n    <BaseIOStream.write>` while the connection is pending, in\n    which case the data will be written as soon as the connection\n    is ready.  Calling `IOStream` read methods before the socket is\n    connected works on some platforms but is non-portable.\n\n    .. versionchanged:: 4.0\n        If no callback is given, returns a `.Future`.\n\n    .. versionchanged:: 4.2\n       SSL certificates are validated by default; pass\n       ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a\n       suitably-configured `ssl.SSLContext` to the\n       `SSLIOStream` constructor to disable.\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n\n    \"\"\"\n    self._connecting = True\n    future = Future()  # type: Future[_IOStreamType]\n    self._connect_future = typing.cast(\"Future[IOStream]\", future)\n    try:\n        self.socket.connect(address)\n    except BlockingIOError:\n        # In non-blocking mode we expect connect() to raise an\n        # exception with EINPROGRESS or EWOULDBLOCK.\n        pass\n    except OSError as e:\n        # On freebsd, other errors such as ECONNREFUSED may be\n        # returned immediately when attempting to connect to\n        # localhost, so handle them the same way as an error\n        # reported later in _handle_connect.\n        if future is None:\n            gen_log.warning(\"Connect error on fd %s: %s\", self.socket.fileno(), e)\n        self.close(exc_info=e)\n        return future\n    self._add_io_state(self.io_loop.WRITE)\n    return future", "loc": 66}
{"file": "tornado\\tornado\\iostream.py", "class_name": "IOStream", "function_name": "start_tls", "parameters": ["self", "server_side", "ssl_options", "server_hostname"], "param_types": {"server_side": "bool", "ssl_options": "Optional[Union[Dict[str, Any], ssl.SSLContext]]", "server_hostname": "Optional[str]"}, "return_type": "Awaitable['SSLIOStream']", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "SSLIOStream", "ValueError", "self.io_loop.remove_handler", "ssl_stream.set_close_callback", "ssl_wrap_socket"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert this `IOStream` to an `SSLIOStream`. This enables protocols that begin in clear-text mode and switch to SSL after some initial negotiation (such as the", "source_code": "def start_tls(\n    self,\n    server_side: bool,\n    ssl_options: Optional[Union[Dict[str, Any], ssl.SSLContext]] = None,\n    server_hostname: Optional[str] = None,\n) -> Awaitable[\"SSLIOStream\"]:\n    \"\"\"Convert this `IOStream` to an `SSLIOStream`.\n\n    This enables protocols that begin in clear-text mode and\n    switch to SSL after some initial negotiation (such as the\n    ``STARTTLS`` extension to SMTP and IMAP).\n\n    This method cannot be used if there are outstanding reads\n    or writes on the stream, or if there is any data in the\n    IOStream's buffer (data in the operating system's socket\n    buffer is allowed).  This means it must generally be used\n    immediately after reading or writing the last clear-text\n    data.  It can also be used immediately after connecting,\n    before any reads or writes.\n\n    The ``ssl_options`` argument may be either an `ssl.SSLContext`\n    object or a dictionary of keyword arguments for the\n    `ssl.SSLContext.wrap_socket` function.  The ``server_hostname`` argument\n    will be used for certificate validation unless disabled\n    in the ``ssl_options``.\n\n    This method returns a `.Future` whose result is the new\n    `SSLIOStream`.  After this method has been called,\n    any other operation on the original stream is undefined.\n\n    If a close callback is defined on this stream, it will be\n    transferred to the new stream.\n\n    .. versionadded:: 4.0\n\n    .. versionchanged:: 4.2\n       SSL certificates are validated by default; pass\n       ``ssl_options=dict(cert_reqs=ssl.CERT_NONE)`` or a\n       suitably-configured `ssl.SSLContext` to disable.\n    \"\"\"\n    if (\n        self._read_future\n        or self._write_futures\n        or self._connect_future\n        or self._closed\n        or self._read_buffer\n        or self._write_buffer\n    ):\n        raise ValueError(\"IOStream is not idle; cannot convert to SSL\")\n    if ssl_options is None:\n        if server_side:\n            ssl_options = _server_ssl_defaults\n        else:\n            ssl_options = _client_ssl_defaults\n\n    socket = self.socket\n    self.io_loop.remove_handler(socket)\n    self.socket = None  # type: ignore\n    socket = ssl_wrap_socket(\n        socket,\n        ssl_options,\n        server_hostname=server_hostname,\n        server_side=server_side,\n        do_handshake_on_connect=False,\n    )\n    orig_close_callback = self._close_callback\n    self._close_callback = None\n\n    future = Future()  # type: Future[SSLIOStream]\n    ssl_stream = SSLIOStream(socket, ssl_options=ssl_options)\n    ssl_stream.set_close_callback(orig_close_callback)\n    ssl_stream._ssl_connect_future = future\n    ssl_stream.max_buffer_size = self.max_buffer_size\n    ssl_stream.read_chunk_size = self.read_chunk_size\n    return future", "loc": 75}
{"file": "tornado\\tornado\\iostream.py", "class_name": "IOStream", "function_name": "set_nodelay", "parameters": ["self", "value"], "param_types": {"value": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._is_connreset", "self.socket.setsockopt"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_nodelay(self, value: bool) -> None:\n    if self.socket is not None and self.socket.family in (\n        socket.AF_INET,\n        socket.AF_INET6,\n    ):\n        try:\n            self.socket.setsockopt(\n                socket.IPPROTO_TCP, socket.TCP_NODELAY, 1 if value else 0\n            )\n        except OSError as e:\n            # Sometimes setsockopt will fail if the socket is closed\n            # at the wrong time.  This can happen with HTTPServer\n            # resetting the value to ``False`` between requests.\n            if e.errno != errno.EINVAL and not self._is_connreset(e):\n                raise", "loc": 15}
{"file": "tornado\\tornado\\iostream.py", "class_name": "SSLIOStream", "function_name": "connect", "parameters": ["self", "address", "server_hostname"], "param_types": {"address": "Tuple", "server_hostname": "Optional[str]"}, "return_type": "'Future[SSLIOStream]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.exception", "fut.add_done_callback", "self.wait_for_handshake", "super", "super().connect"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def connect(\n    self, address: Tuple, server_hostname: Optional[str] = None\n) -> \"Future[SSLIOStream]\":\n    self._server_hostname = server_hostname\n    # Ignore the result of connect(). If it fails,\n    # wait_for_handshake will raise an error too. This is\n    # necessary for the old semantics of the connect callback\n    # (which takes no arguments). In 6.0 this can be refactored to\n    # be a regular coroutine.\n    # TODO: This is trickier than it looks, since if write()\n    # is called with a connect() pending, we want the connect\n    # to resolve before the write. Or do we care about this?\n    # (There's a test for it, but I think in practice users\n    # either wait for the connect before performing a write or\n    # they don't care about the connect Future at all)\n    fut = super().connect(address)\n    fut.add_done_callback(lambda f: f.exception())\n    return self.wait_for_handshake()", "loc": 18}
{"file": "tornado\\tornado\\iostream.py", "class_name": "SSLIOStream", "function_name": "wait_for_handshake", "parameters": ["self"], "param_types": {}, "return_type": "'Future[SSLIOStream]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "RuntimeError", "self._finish_ssl_connect"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Wait for the initial SSL handshake to complete. If a ``callback`` is given, it will be called with no arguments once the handshake is complete; otherwise this", "source_code": "def wait_for_handshake(self) -> \"Future[SSLIOStream]\":\n    \"\"\"Wait for the initial SSL handshake to complete.\n\n    If a ``callback`` is given, it will be called with no\n    arguments once the handshake is complete; otherwise this\n    method returns a `.Future` which will resolve to the\n    stream itself after the handshake is complete.\n\n    Once the handshake is complete, information such as\n    the peer's certificate and NPN/ALPN selections may be\n    accessed on ``self.socket``.\n\n    This method is intended for use on server-side streams\n    or after using `IOStream.start_tls`; it should not be used\n    with `IOStream.connect` (which already waits for the\n    handshake to complete). It may only be called once per stream.\n\n    .. versionadded:: 4.2\n\n    .. versionchanged:: 6.0\n\n       The ``callback`` argument was removed. Use the returned\n       `.Future` instead.\n\n    \"\"\"\n    if self._ssl_connect_future is not None:\n        raise RuntimeError(\"Already waiting\")\n    future = self._ssl_connect_future = Future()\n    if not self._ssl_accepting:\n        self._finish_ssl_connect()\n    return future", "loc": 31}
{"file": "tornado\\tornado\\iostream.py", "class_name": "SSLIOStream", "function_name": "write_to_fd", "parameters": ["self", "data"], "param_types": {"data": "memoryview"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "memoryview", "self.socket.send"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write_to_fd(self, data: memoryview) -> int:\n    # clip buffer size at 1GB since SSL sockets only support upto 2GB\n    # this change in behaviour is transparent, since the function is\n    # already expected to (possibly) write less than the provided buffer\n    if len(data) >> 30:\n        data = memoryview(data)[: 1 << 30]\n    try:\n        return self.socket.send(data)  # type: ignore\n    except ssl.SSLError as e:\n        if e.args[0] == ssl.SSL_ERROR_WANT_WRITE:\n            # In Python 3.5+, SSLSocket.send raises a WANT_WRITE error if\n            # the socket is not writeable; we need to transform this into\n            # an EWOULDBLOCK socket.error or a zero return value,\n            # either of which will be recognized by the caller of this\n            # method. Prior to Python 3.5, an unwriteable socket would\n            # simply return 0 bytes written.\n            return 0\n        raise\n    finally:\n        # Avoid keeping to data, which can be a memoryview.\n        # See https://github.com/tornadoweb/tornado/pull/2008\n        del data", "loc": 22}
{"file": "tornado\\tornado\\iostream.py", "class_name": "SSLIOStream", "function_name": "read_from_fd", "parameters": ["self", "buf"], "param_types": {"buf": "Union[bytearray, memoryview]"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "memoryview", "self.socket.recv_into"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_from_fd(self, buf: Union[bytearray, memoryview]) -> Optional[int]:\n    try:\n        if self._ssl_accepting:\n            # If the handshake hasn't finished yet, there can't be anything\n            # to read (attempting to read may or may not raise an exception\n            # depending on the SSL version)\n            return None\n        # clip buffer size at 1GB since SSL sockets only support upto 2GB\n        # this change in behaviour is transparent, since the function is\n        # already expected to (possibly) read less than the provided buffer\n        if len(buf) >> 30:\n            buf = memoryview(buf)[: 1 << 30]\n        try:\n            return self.socket.recv_into(buf, len(buf))\n        except ssl.SSLError as e:\n            # SSLError is a subclass of socket.error, so this except\n            # block must come first.\n            if e.args[0] == ssl.SSL_ERROR_WANT_READ:\n                return None\n            else:\n                raise\n        except BlockingIOError:\n            return None\n    finally:\n        del buf", "loc": 25}
{"file": "tornado\\tornado\\iostream.py", "class_name": "PipeIOStream", "function_name": "write_to_fd", "parameters": ["self", "data"], "param_types": {"data": "memoryview"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.write"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write_to_fd(self, data: memoryview) -> int:\n    try:\n        return os.write(self.fd, data)  # type: ignore\n    finally:\n        # Avoid keeping to data, which can be a memoryview.\n        # See https://github.com/tornadoweb/tornado/pull/2008\n        del data", "loc": 7}
{"file": "tornado\\tornado\\iostream.py", "class_name": "PipeIOStream", "function_name": "read_from_fd", "parameters": ["self", "buf"], "param_types": {"buf": "Union[bytearray, memoryview]"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["errno_from_exception", "self._fio.readinto", "self.close"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_from_fd(self, buf: Union[bytearray, memoryview]) -> Optional[int]:\n    try:\n        return self._fio.readinto(buf)  # type: ignore\n    except OSError as e:\n        if errno_from_exception(e) == errno.EBADF:\n            # If the writing half of a pipe is closed, select will\n            # report it as readable but reads will fail with EBADF.\n            self.close(exc_info=e)\n            return None\n        else:\n            raise\n    finally:\n        del buf", "loc": 13}
{"file": "tornado\\tornado\\locale.py", "class_name": null, "function_name": "set_default_locale", "parameters": ["code"], "param_types": {"code": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_translations.keys", "frozenset", "list"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Sets the default locale. The default locale is assumed to be the language used for all strings in the system. The translations loaded from disk are mappings from", "source_code": "def set_default_locale(code: str) -> None:\n    \"\"\"Sets the default locale.\n\n    The default locale is assumed to be the language used for all strings\n    in the system. The translations loaded from disk are mappings from\n    the default locale to the destination locale. Consequently, you don't\n    need to create a translation file for the default locale.\n    \"\"\"\n    global _default_locale\n    global _supported_locales\n    _default_locale = code\n    _supported_locales = frozenset(list(_translations.keys()) + [_default_locale])", "loc": 12}
{"file": "tornado\\tornado\\locale.py", "class_name": "Locale", "function_name": "get_closest", "parameters": ["cls"], "param_types": {}, "return_type": "'Locale'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.get", "code.replace", "code.split", "len", "parts[0].lower", "parts[1].upper"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_closest(cls, *locale_codes: str) -> \"Locale\":\n    \"\"\"Returns the closest match for the given locale code.\"\"\"\n    for code in locale_codes:\n        if not code:\n            continue\n        code = code.replace(\"-\", \"_\")\n        parts = code.split(\"_\")\n        if len(parts) > 2:\n            continue\n        elif len(parts) == 2:\n            code = parts[0].lower() + \"_\" + parts[1].upper()\n        if code in _supported_locales:\n            return cls.get(code)\n        if parts[0].lower() in _supported_locales:\n            return cls.get(parts[0].lower())\n    return cls.get(_default_locale)", "loc": 16}
{"file": "tornado\\tornado\\locale.py", "class_name": "Locale", "function_name": "get", "parameters": ["cls", "code"], "param_types": {"code": "str"}, "return_type": "'Locale'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CSVLocale", "GettextLocale", "_translations.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(cls, code: str) -> \"Locale\":\n    \"\"\"Returns the Locale for the given locale code.\n\n    If it is not supported, we raise an exception.\n    \"\"\"\n    if code not in cls._cache:\n        assert code in _supported_locales\n        translations = _translations.get(code, None)\n        if translations is None:\n            locale = CSVLocale(code, {})  # type: Locale\n        elif _use_gettext:\n            locale = GettextLocale(code, translations)\n        else:\n            locale = CSVLocale(code, translations)\n        cls._cache[code] = locale\n    return cls._cache[code]", "loc": 16}
{"file": "tornado\\tornado\\locale.py", "class_name": "Locale", "function_name": "format_day", "parameters": ["self", "date", "gmt_offset", "dow"], "param_types": {"date": "datetime.datetime", "gmt_offset": "int", "dow": "bool"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_", "datetime.timedelta", "local_date.weekday", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Formats the given date as a day of week. Example: \"Monday, January 22\". You can remove the day of week with ``dow=False``.", "source_code": "def format_day(\n    self, date: datetime.datetime, gmt_offset: int = 0, dow: bool = True\n) -> bool:\n    \"\"\"Formats the given date as a day of week.\n\n    Example: \"Monday, January 22\". You can remove the day of week with\n    ``dow=False``.\n    \"\"\"\n    local_date = date - datetime.timedelta(minutes=gmt_offset)\n    _ = self.translate\n    if dow:\n        return _(\"%(weekday)s, %(month_name)s %(day)s\") % {\n            \"month_name\": self._months[local_date.month - 1],\n            \"weekday\": self._weekdays[local_date.weekday()],\n            \"day\": str(local_date.day),\n        }\n    else:\n        return _(\"%(month_name)s %(day)s\") % {\n            \"month_name\": self._months[local_date.month - 1],\n            \"day\": str(local_date.day),\n        }", "loc": 21}
{"file": "tornado\\tornado\\locale.py", "class_name": "Locale", "function_name": "list", "parameters": ["self", "parts"], "param_types": {"parts": "Any"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_", "comma.join", "len", "self.code.startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list(self, parts: Any) -> str:\n    \"\"\"Returns a comma-separated list for the given list of parts.\n\n    The format is, e.g., \"A, B and C\", \"A and B\" or just \"A\" for lists\n    of size 1.\n    \"\"\"\n    _ = self.translate\n    if len(parts) == 0:\n        return \"\"\n    if len(parts) == 1:\n        return parts[0]\n    comma = \" \\u0648 \" if self.code.startswith(\"fa\") else \", \"\n    return _(\"%(commas)s and %(last)s\") % {\n        \"commas\": comma.join(parts[:-1]),\n        \"last\": parts[len(parts) - 1],\n    }", "loc": 16}
{"file": "tornado\\tornado\\locale.py", "class_name": "Locale", "function_name": "friendly_number", "parameters": ["self", "value"], "param_types": {"value": "int"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "parts.append", "reversed", "str"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def friendly_number(self, value: int) -> str:\n    \"\"\"Returns a comma-separated number for the given integer.\"\"\"\n    if self.code not in (\"en\", \"en_US\"):\n        return str(value)\n    s = str(value)\n    parts = []\n    while s:\n        parts.append(s[-3:])\n        s = s[:-3]\n    return \",\".join(reversed(parts))", "loc": 10}
{"file": "tornado\\tornado\\locale.py", "class_name": "CSVLocale", "function_name": "translate", "parameters": ["self", "message", "plural_message", "count"], "param_types": {"message": "str", "plural_message": "Optional[str]", "count": "Optional[int]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["message_dict.get", "self.translations.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def translate(\n    self,\n    message: str,\n    plural_message: Optional[str] = None,\n    count: Optional[int] = None,\n) -> str:\n    if plural_message is not None:\n        assert count is not None\n        if count != 1:\n            message = plural_message\n            message_dict = self.translations.get(\"plural\", {})\n        else:\n            message_dict = self.translations.get(\"singular\", {})\n    else:\n        message_dict = self.translations.get(\"unknown\", {})\n    return message_dict.get(message, message)", "loc": 16}
{"file": "tornado\\tornado\\locale.py", "class_name": "CSVLocale", "function_name": "pgettext", "parameters": ["self", "context", "message", "plural_message", "count"], "param_types": {"context": "str", "message": "str", "plural_message": "Optional[str]", "count": "Optional[int]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_log.warning", "self.translate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pgettext(\n    self,\n    context: str,\n    message: str,\n    plural_message: Optional[str] = None,\n    count: Optional[int] = None,\n) -> str:\n    if self.translations:\n        gen_log.warning(\"pgettext is not supported by CSVLocale\")\n    return self.translate(message, plural_message, count)", "loc": 10}
{"file": "tornado\\tornado\\locale.py", "class_name": "GettextLocale", "function_name": "translate", "parameters": ["self", "message", "plural_message", "count"], "param_types": {"message": "str", "plural_message": "Optional[str]", "count": "Optional[int]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.gettext", "self.ngettext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def translate(\n    self,\n    message: str,\n    plural_message: Optional[str] = None,\n    count: Optional[int] = None,\n) -> str:\n    if plural_message is not None:\n        assert count is not None\n        return self.ngettext(message, plural_message, count)\n    else:\n        return self.gettext(message)", "loc": 11}
{"file": "tornado\\tornado\\locale.py", "class_name": "GettextLocale", "function_name": "pgettext", "parameters": ["self", "context", "message", "plural_message", "count"], "param_types": {"context": "str", "message": "str", "plural_message": "Optional[str]", "count": "Optional[int]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.gettext", "self.ngettext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Allows to set context for translation, accepts plural forms. Usage example:: pgettext(\"law\", \"right\")", "source_code": "def pgettext(\n    self,\n    context: str,\n    message: str,\n    plural_message: Optional[str] = None,\n    count: Optional[int] = None,\n) -> str:\n    \"\"\"Allows to set context for translation, accepts plural forms.\n\n    Usage example::\n\n        pgettext(\"law\", \"right\")\n        pgettext(\"good\", \"right\")\n\n    Plural message example::\n\n        pgettext(\"organization\", \"club\", \"clubs\", len(clubs))\n        pgettext(\"stick\", \"club\", \"clubs\", len(clubs))\n\n    To generate POT file with context, add following options to step 1\n    of `load_gettext_translations` sequence::\n\n        xgettext [basic options] --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3\n\n    .. versionadded:: 4.2\n    \"\"\"\n    if plural_message is not None:\n        assert count is not None\n        msgs_with_ctxt = (\n            f\"{context}{CONTEXT_SEPARATOR}{message}\",\n            f\"{context}{CONTEXT_SEPARATOR}{plural_message}\",\n            count,\n        )\n        result = self.ngettext(*msgs_with_ctxt)\n        if CONTEXT_SEPARATOR in result:\n            # Translation not found\n            result = self.ngettext(message, plural_message, count)\n        return result\n    else:\n        msg_with_ctxt = f\"{context}{CONTEXT_SEPARATOR}{message}\"\n        result = self.gettext(msg_with_ctxt)\n        if CONTEXT_SEPARATOR in result:\n            # Translation not found\n            result = message\n        return result", "loc": 45}
{"file": "tornado\\tornado\\locks.py", "class_name": "Condition", "function_name": "wait", "parameters": ["self", "timeout"], "param_types": {"timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "Awaitable[bool]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "future_set_result_unless_cancelled", "io_loop.add_timeout", "io_loop.remove_timeout", "ioloop.IOLoop.current", "self._garbage_collect", "self._waiters.append", "waiter.add_done_callback", "waiter.done"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Wait for `.notify`.", "source_code": "def wait(\n    self, timeout: Optional[Union[float, datetime.timedelta]] = None\n) -> Awaitable[bool]:\n    \"\"\"Wait for `.notify`.\n\n    Returns a `.Future` that resolves ``True`` if the condition is notified,\n    or ``False`` after a timeout.\n    \"\"\"\n    waiter = Future()  # type: Future[bool]\n    self._waiters.append(waiter)\n    if timeout:\n\n        def on_timeout() -> None:\n            if not waiter.done():\n                future_set_result_unless_cancelled(waiter, False)\n            self._garbage_collect()\n\n        io_loop = ioloop.IOLoop.current()\n        timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n        waiter.add_done_callback(lambda _: io_loop.remove_timeout(timeout_handle))\n    return waiter", "loc": 21}
{"file": "tornado\\tornado\\locks.py", "class_name": "Condition", "function_name": "notify", "parameters": ["self", "n"], "param_types": {"n": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_result_unless_cancelled", "self._waiters.popleft", "waiter.done", "waiters.append"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Wake ``n`` waiters.", "source_code": "def notify(self, n: int = 1) -> None:\n    \"\"\"Wake ``n`` waiters.\"\"\"\n    waiters = []  # Waiters we plan to run right now.\n    while n and self._waiters:\n        waiter = self._waiters.popleft()\n        if not waiter.done():  # Might have timed out.\n            n -= 1\n            waiters.append(waiter)\n\n    for waiter in waiters:\n        future_set_result_unless_cancelled(waiter, True)", "loc": 11}
{"file": "tornado\\tornado\\locks.py", "class_name": "Event", "function_name": "set", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fut.done", "fut.set_result"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Set the internal flag to ``True``. All waiters are awakened. Calling `.wait` once the flag is set will not block.", "source_code": "def set(self) -> None:\n    \"\"\"Set the internal flag to ``True``. All waiters are awakened.\n\n    Calling `.wait` once the flag is set will not block.\n    \"\"\"\n    if not self._value:\n        self._value = True\n\n        for fut in self._waiters:\n            if not fut.done():\n                fut.set_result(None)", "loc": 11}
{"file": "tornado\\tornado\\locks.py", "class_name": "Event", "function_name": "wait", "parameters": ["self", "timeout"], "param_types": {"timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "Awaitable[None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "fut.add_done_callback", "fut.cancel", "fut.done", "fut.set_result", "gen.with_timeout", "self._waiters.add", "self._waiters.remove", "timeout_fut.add_done_callback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Block until the internal flag is true.", "source_code": "def wait(\n    self, timeout: Optional[Union[float, datetime.timedelta]] = None\n) -> Awaitable[None]:\n    \"\"\"Block until the internal flag is true.\n\n    Returns an awaitable, which raises `tornado.util.TimeoutError` after a\n    timeout.\n    \"\"\"\n    fut = Future()  # type: Future[None]\n    if self._value:\n        fut.set_result(None)\n        return fut\n    self._waiters.add(fut)\n    fut.add_done_callback(lambda fut: self._waiters.remove(fut))\n    if timeout is None:\n        return fut\n    else:\n        timeout_fut = gen.with_timeout(timeout, fut)\n        # This is a slightly clumsy workaround for the fact that\n        # gen.with_timeout doesn't cancel its futures. Cancelling\n        # fut will remove it from the waiters list.\n        timeout_fut.add_done_callback(\n            lambda tf: fut.cancel() if not fut.done() else None\n        )\n        return timeout_fut", "loc": 25}
{"file": "tornado\\tornado\\locks.py", "class_name": "Semaphore", "function_name": "release", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_ReleasingContextManager", "self._waiters.popleft", "waiter.done", "waiter.set_result"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Increment the counter and wake one waiter.", "source_code": "def release(self) -> None:\n    \"\"\"Increment the counter and wake one waiter.\"\"\"\n    self._value += 1\n    while self._waiters:\n        waiter = self._waiters.popleft()\n        if not waiter.done():\n            self._value -= 1\n\n            # If the waiter is a coroutine paused at\n            #\n            #     with (yield semaphore.acquire()):\n            #\n            # then the context manager's __exit__ calls release() at the end\n            # of the \"with\" block.\n            waiter.set_result(_ReleasingContextManager(self))\n            break", "loc": 16}
{"file": "tornado\\tornado\\locks.py", "class_name": "Semaphore", "function_name": "acquire", "parameters": ["self", "timeout"], "param_types": {"timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "Awaitable[_ReleasingContextManager]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "_ReleasingContextManager", "gen.TimeoutError", "io_loop.add_timeout", "io_loop.remove_timeout", "ioloop.IOLoop.current", "self._garbage_collect", "self._waiters.append", "waiter.add_done_callback", "waiter.done", "waiter.set_exception", "waiter.set_result"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Decrement the counter. Returns an awaitable. Block if the counter is zero and wait for a `.release`. The awaitable raises `.TimeoutError` after the deadline.", "source_code": "def acquire(\n    self, timeout: Optional[Union[float, datetime.timedelta]] = None\n) -> Awaitable[_ReleasingContextManager]:\n    \"\"\"Decrement the counter. Returns an awaitable.\n\n    Block if the counter is zero and wait for a `.release`. The awaitable\n    raises `.TimeoutError` after the deadline.\n    \"\"\"\n    waiter = Future()  # type: Future[_ReleasingContextManager]\n    if self._value > 0:\n        self._value -= 1\n        waiter.set_result(_ReleasingContextManager(self))\n    else:\n        self._waiters.append(waiter)\n        if timeout:\n\n            def on_timeout() -> None:\n                if not waiter.done():\n                    waiter.set_exception(gen.TimeoutError())\n                self._garbage_collect()\n\n            io_loop = ioloop.IOLoop.current()\n            timeout_handle = io_loop.add_timeout(timeout, on_timeout)\n            waiter.add_done_callback(\n                lambda _: io_loop.remove_timeout(timeout_handle)\n            )\n    return waiter", "loc": 27}
{"file": "tornado\\tornado\\locks.py", "class_name": "BoundedSemaphore", "function_name": "release", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "super", "super().release"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Increment the counter and wake one waiter.", "source_code": "def release(self) -> None:\n    \"\"\"Increment the counter and wake one waiter.\"\"\"\n    if self._value >= self._initial_value:\n        raise ValueError(\"Semaphore released too many times\")\n    super().release()", "loc": 5}
{"file": "tornado\\tornado\\locks.py", "class_name": "Lock", "function_name": "release", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "self._block.release"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Unlock. The first coroutine in line waiting for `acquire` gets the lock. If not locked, raise a `RuntimeError`.", "source_code": "def release(self) -> None:\n    \"\"\"Unlock.\n\n    The first coroutine in line waiting for `acquire` gets the lock.\n\n    If not locked, raise a `RuntimeError`.\n    \"\"\"\n    try:\n        self._block.release()\n    except ValueError:\n        raise RuntimeError(\"release unlocked lock\")", "loc": 11}
{"file": "tornado\\tornado\\log.py", "class_name": null, "function_name": "enable_pretty_logging", "parameters": ["options", "logger"], "param_types": {"options": "Any", "logger": "Optional[logging.Logger]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LogFormatter", "ValueError", "channel.setFormatter", "getattr", "logger.addHandler", "logger.setLevel", "logging.StreamHandler", "logging.getLogger", "logging.handlers.RotatingFileHandler", "logging.handlers.TimedRotatingFileHandler", "options.logging.lower", "options.logging.upper"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Turns on formatted logging output as configured. This is called automatically by `tornado.options.parse_command_line` and `tornado.options.parse_config_file`.", "source_code": "def enable_pretty_logging(\n    options: Any = None, logger: Optional[logging.Logger] = None\n) -> None:\n    \"\"\"Turns on formatted logging output as configured.\n\n    This is called automatically by `tornado.options.parse_command_line`\n    and `tornado.options.parse_config_file`.\n    \"\"\"\n    if options is None:\n        import tornado.options\n\n        options = tornado.options.options\n    if options.logging is None or options.logging.lower() == \"none\":\n        return\n    if logger is None:\n        logger = logging.getLogger()\n    logger.setLevel(getattr(logging, options.logging.upper()))\n    if options.log_file_prefix:\n        rotate_mode = options.log_rotate_mode\n        if rotate_mode == \"size\":\n            channel = logging.handlers.RotatingFileHandler(\n                filename=options.log_file_prefix,\n                maxBytes=options.log_file_max_size,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )  # type: logging.Handler\n        elif rotate_mode == \"time\":\n            channel = logging.handlers.TimedRotatingFileHandler(\n                filename=options.log_file_prefix,\n                when=options.log_rotate_when,\n                interval=options.log_rotate_interval,\n                backupCount=options.log_file_num_backups,\n                encoding=\"utf-8\",\n            )\n        else:\n            error_message = (\n                \"The value of log_rotate_mode option should be \"\n                + '\"size\" or \"time\", not \"%s\".' % rotate_mode\n            )\n            raise ValueError(error_message)\n        channel.setFormatter(LogFormatter(color=False))\n        logger.addHandler(channel)\n\n    if options.log_to_stderr or (options.log_to_stderr is None and not logger.handlers):\n        # Set up color if we are in a tty and curses is installed\n        channel = logging.StreamHandler()\n        channel.setFormatter(LogFormatter())\n        logger.addHandler(channel)", "loc": 48}
{"file": "tornado\\tornado\\log.py", "class_name": null, "function_name": "define_logging_options", "parameters": ["options"], "param_types": {"options": "Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enable_pretty_logging", "options.add_parse_callback", "options.define"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add logging-related flags to ``options``. These options are present automatically on the default options instance; this method is only necessary if you have created your own `.OptionParser`.", "source_code": "def define_logging_options(options: Any = None) -> None:\n    \"\"\"Add logging-related flags to ``options``.\n\n    These options are present automatically on the default options instance;\n    this method is only necessary if you have created your own `.OptionParser`.\n\n    .. versionadded:: 4.2\n        This function existed in prior versions but was broken and undocumented until 4.2.\n    \"\"\"\n    if options is None:\n        # late import to prevent cycle\n        import tornado.options\n\n        options = tornado.options.options\n    options.define(\n        \"logging\",\n        default=\"info\",\n        help=(\n            \"Set the Python log level. If 'none', tornado won't touch the \"\n            \"logging configuration.\"\n        ),\n        metavar=\"debug|info|warning|error|none\",\n    )\n    options.define(\n        \"log_to_stderr\",\n        type=bool,\n        default=None,\n        help=(\n            \"Send log output to stderr (colorized if possible). \"\n            \"By default use stderr if --log_file_prefix is not set and \"\n            \"no other logging is configured.\"\n        ),\n    )\n    options.define(\n        \"log_file_prefix\",\n        type=str,\n        default=None,\n        metavar=\"PATH\",\n        help=(\n            \"Path prefix for log files. \"\n            \"Note that if you are running multiple tornado processes, \"\n            \"log_file_prefix must be different for each of them (e.g. \"\n            \"include the port number)\"\n        ),\n    )\n    options.define(\n        \"log_file_max_size\",\n        type=int,\n        default=100 * 1000 * 1000,\n        help=\"max size of log files before rollover\",\n    )\n    options.define(\n        \"log_file_num_backups\", type=int, default=10, help=\"number of log files to keep\"\n    )\n\n    options.define(\n        \"log_rotate_when\",\n        type=str,\n        default=\"midnight\",\n        help=(\n            \"specify the type of TimedRotatingFileHandler interval \"\n            \"other options:('S', 'M', 'H', 'D', 'W0'-'W6')\"\n        ),\n    )\n    options.define(\n        \"log_rotate_interval\",\n        type=int,\n        default=1,\n        help=\"The interval value of timed rotating\",\n    )\n\n    options.define(\n        \"log_rotate_mode\",\n        type=str,\n        default=\"size\",\n        help=\"The mode of rotating files(time or size)\",\n    )\n\n    options.add_parse_callback(lambda: enable_pretty_logging(options))", "loc": 79}
{"file": "tornado\\tornado\\log.py", "class_name": "LogFormatter", "function_name": "format", "parameters": ["self", "record"], "param_types": {"record": "Any"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "_safe_unicode", "cast", "formatted.replace", "formatted.rstrip", "isinstance", "lines.extend", "record.exc_text.split", "record.getMessage", "self.formatException", "self.formatTime"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format(self, record: Any) -> str:\n    try:\n        message = record.getMessage()\n        assert isinstance(message, basestring_type)  # guaranteed by logging\n        # Encoding notes:  The logging module prefers to work with character\n        # strings, but only enforces that log messages are instances of\n        # basestring.  In python 2, non-ascii bytestrings will make\n        # their way through the logging framework until they blow up with\n        # an unhelpful decoding error (with this formatter it happens\n        # when we attach the prefix, but there are other opportunities for\n        # exceptions further along in the framework).\n        #\n        # If a byte string makes it this far, convert it to unicode to\n        # ensure it will make it out to the logs.  Use repr() as a fallback\n        # to ensure that all byte strings can be converted successfully,\n        # but don't do it by default so we don't add extra quotes to ascii\n        # bytestrings.  This is a bit of a hacky place to do this, but\n        # it's worth it since the encoding errors that would otherwise\n        # result are so useless (and tornado is fond of using utf8-encoded\n        # byte strings wherever possible).\n        record.message = _safe_unicode(message)\n    except Exception as e:\n        record.message = f\"Bad message ({e!r}): {record.__dict__!r}\"\n\n    record.asctime = self.formatTime(record, cast(str, self.datefmt))\n\n    if record.levelno in self._colors:\n        record.color = self._colors[record.levelno]\n        record.end_color = self._normal\n    else:\n        record.color = record.end_color = \"\"\n\n    formatted = self._fmt % record.__dict__\n\n    if record.exc_info:\n        if not record.exc_text:\n            record.exc_text = self.formatException(record.exc_info)\n    if record.exc_text:\n        # exc_text contains multiple lines.  We need to _safe_unicode\n        # each line separately so that non-utf8 bytes don't cause\n        # all the newlines to turn into '\\n'.\n        lines = [formatted.rstrip()]\n        lines.extend(_safe_unicode(ln) for ln in record.exc_text.split(\"\\n\"))\n        formatted = \"\\n\".join(lines)\n    return formatted.replace(\"\\n\", \"\\n    \")", "loc": 45}
{"file": "tornado\\tornado\\netutil.py", "class_name": null, "function_name": "add_accept_handler", "parameters": ["sock", "callback"], "param_types": {"sock": "socket.socket", "callback": "Callable[[socket.socket, Any], None]"}, "return_type": "Callable[[], None]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOLoop.current", "callback", "io_loop.add_handler", "io_loop.remove_handler", "range", "sock.accept"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Adds an `.IOLoop` event handler to accept new connections on ``sock``. When a connection is accepted, ``callback(connection, address)`` will be run (``connection`` is a socket object, and ``address`` is the", "source_code": "def add_accept_handler(\n    sock: socket.socket, callback: Callable[[socket.socket, Any], None]\n) -> Callable[[], None]:\n    \"\"\"Adds an `.IOLoop` event handler to accept new connections on ``sock``.\n\n    When a connection is accepted, ``callback(connection, address)`` will\n    be run (``connection`` is a socket object, and ``address`` is the\n    address of the other end of the connection).  Note that this signature\n    is different from the ``callback(fd, events)`` signature used for\n    `.IOLoop` handlers.\n\n    A callable is returned which, when called, will remove the `.IOLoop`\n    event handler and stop processing further incoming connections.\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been removed.\n\n    .. versionchanged:: 5.0\n       A callable is returned (``None`` was returned before).\n    \"\"\"\n    io_loop = IOLoop.current()\n    removed = [False]\n\n    def accept_handler(fd: socket.socket, events: int) -> None:\n        # More connections may come in while we're handling callbacks;\n        # to prevent starvation of other tasks we must limit the number\n        # of connections we accept at a time.  Ideally we would accept\n        # up to the number of connections that were waiting when we\n        # entered this method, but this information is not available\n        # (and rearranging this method to call accept() as many times\n        # as possible before running any callbacks would have adverse\n        # effects on load balancing in multiprocess configurations).\n        # Instead, we use the (default) listen backlog as a rough\n        # heuristic for the number of connections we can reasonably\n        # accept at once.\n        for i in range(_DEFAULT_BACKLOG):\n            if removed[0]:\n                # The socket was probably closed\n                return\n            try:\n                connection, address = sock.accept()\n            except BlockingIOError:\n                # EWOULDBLOCK indicates we have accepted every\n                # connection that is available.\n                return\n            except ConnectionAbortedError:\n                # ECONNABORTED indicates that there was a connection\n                # but it was closed while still in the accept queue.\n                # (observed on FreeBSD).\n                continue\n            callback(connection, address)\n\n    def remove_handler() -> None:\n        io_loop.remove_handler(sock)\n        removed[0] = True\n\n    io_loop.add_handler(sock, accept_handler, IOLoop.READ)\n    return remove_handler", "loc": 58}
{"file": "tornado\\tornado\\netutil.py", "class_name": null, "function_name": "is_valid_ip", "parameters": ["ip"], "param_types": {"ip": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "socket.getaddrinfo"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_valid_ip(ip: str) -> bool:\n    \"\"\"Returns ``True`` if the given string is a well-formed IP address.\n\n    Supports IPv4 and IPv6.\n    \"\"\"\n    if not ip or \"\\x00\" in ip:\n        # getaddrinfo resolves empty strings to localhost, and truncates\n        # on zero bytes.\n        return False\n    try:\n        res = socket.getaddrinfo(\n            ip, 0, socket.AF_UNSPEC, socket.SOCK_STREAM, 0, socket.AI_NUMERICHOST\n        )\n        return bool(res)\n    except socket.gaierror as e:\n        if e.args[0] == socket.EAI_NONAME:\n            return False\n        raise\n    except UnicodeError:\n        # `socket.getaddrinfo` will raise a UnicodeError from the\n        # `idna` decoder if the input is longer than 63 characters,\n        # even for socket.AI_NUMERICHOST.  See\n        # https://bugs.python.org/issue32958 for discussion\n        return False\n    return True", "loc": 25}
{"file": "tornado\\tornado\\netutil.py", "class_name": null, "function_name": "ssl_options_to_context", "parameters": ["ssl_options", "server_side"], "param_types": {"ssl_options": "Union[Dict[str, Any], ssl.SSLContext]", "server_side": "Optional[bool]"}, "return_type": "ssl.SSLContext", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all", "context.load_cert_chain", "context.load_verify_locations", "context.set_ciphers", "hasattr", "isinstance", "ssl.SSLContext", "ssl_options.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Try to convert an ``ssl_options`` dictionary to an `~ssl.SSLContext` object. The ``ssl_options`` argument may be either an `ssl.SSLContext` object or a dictionary containing", "source_code": "def ssl_options_to_context(\n    ssl_options: Union[Dict[str, Any], ssl.SSLContext],\n    server_side: Optional[bool] = None,\n) -> ssl.SSLContext:\n    \"\"\"Try to convert an ``ssl_options`` dictionary to an\n    `~ssl.SSLContext` object.\n\n    The ``ssl_options`` argument may be either an `ssl.SSLContext` object or a dictionary containing\n    keywords to be passed to ``ssl.SSLContext.wrap_socket``.  This function converts the dict form\n    to its `~ssl.SSLContext` equivalent, and may be used when a component which accepts both forms\n    needs to upgrade to the `~ssl.SSLContext` version to use features like SNI or ALPN.\n\n    .. versionchanged:: 6.2\n\n       Added server_side argument. Omitting this argument will result in a DeprecationWarning on\n       Python 3.10.\n\n    \"\"\"\n    if isinstance(ssl_options, ssl.SSLContext):\n        return ssl_options\n    assert isinstance(ssl_options, dict)\n    assert all(k in _SSL_CONTEXT_KEYWORDS for k in ssl_options), ssl_options\n    # TODO: Now that we have the server_side argument, can we switch to\n    # create_default_context or would that change behavior?\n    default_version = ssl.PROTOCOL_TLS\n    if server_side:\n        default_version = ssl.PROTOCOL_TLS_SERVER\n    elif server_side is not None:\n        default_version = ssl.PROTOCOL_TLS_CLIENT\n    context = ssl.SSLContext(ssl_options.get(\"ssl_version\", default_version))\n    if \"certfile\" in ssl_options:\n        context.load_cert_chain(\n            ssl_options[\"certfile\"], ssl_options.get(\"keyfile\", None)\n        )\n    if \"cert_reqs\" in ssl_options:\n        if ssl_options[\"cert_reqs\"] == ssl.CERT_NONE:\n            # This may have been set automatically by PROTOCOL_TLS_CLIENT but is\n            # incompatible with CERT_NONE so we must manually clear it.\n            context.check_hostname = False\n        context.verify_mode = ssl_options[\"cert_reqs\"]\n    if \"ca_certs\" in ssl_options:\n        context.load_verify_locations(ssl_options[\"ca_certs\"])\n    if \"ciphers\" in ssl_options:\n        context.set_ciphers(ssl_options[\"ciphers\"])\n    if hasattr(ssl, \"OP_NO_COMPRESSION\"):\n        # Disable TLS compression to avoid CRIME and related attacks.\n        # This constant depends on openssl version 1.0.\n        # TODO: Do we need to do this ourselves or can we trust\n        # the defaults?\n        context.options |= ssl.OP_NO_COMPRESSION\n    return context", "loc": 51}
{"file": "tornado\\tornado\\netutil.py", "class_name": null, "function_name": "ssl_wrap_socket", "parameters": ["socket", "ssl_options", "server_hostname", "server_side"], "param_types": {"socket": "socket.socket", "ssl_options": "Union[Dict[str, Any], ssl.SSLContext]", "server_hostname": "Optional[str]", "server_side": "Optional[bool]"}, "return_type": "ssl.SSLSocket", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["context.wrap_socket", "ssl_options_to_context"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ssl_wrap_socket(\n    socket: socket.socket,\n    ssl_options: Union[Dict[str, Any], ssl.SSLContext],\n    server_hostname: Optional[str] = None,\n    server_side: Optional[bool] = None,\n    **kwargs: Any,\n) -> ssl.SSLSocket:\n    \"\"\"Returns an ``ssl.SSLSocket`` wrapping the given socket.\n\n    ``ssl_options`` may be either an `ssl.SSLContext` object or a\n    dictionary (as accepted by `ssl_options_to_context`).  Additional\n    keyword arguments are passed to `ssl.SSLContext.wrap_socket`.\n\n    .. versionchanged:: 6.2\n\n       Added server_side argument. Omitting this argument will\n       result in a DeprecationWarning on Python 3.10.\n    \"\"\"\n    context = ssl_options_to_context(ssl_options, server_side=server_side)\n    if server_side is None:\n        server_side = False\n    assert ssl.HAS_SNI\n    # TODO: add a unittest for hostname validation (python added server-side SNI support in 3.4)\n    # In the meantime it can be manually tested with\n    # python3 -m tornado.httpclient https://sni.velox.ch\n    return context.wrap_socket(\n        socket, server_hostname=server_hostname, server_side=server_side, **kwargs\n    )", "loc": 28}
{"file": "tornado\\tornado\\netutil.py", "class_name": null, "function_name": "accept_handler", "parameters": ["fd", "events"], "param_types": {"fd": "socket.socket", "events": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["callback", "range", "sock.accept"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def accept_handler(fd: socket.socket, events: int) -> None:\n    # More connections may come in while we're handling callbacks;\n    # to prevent starvation of other tasks we must limit the number\n    # of connections we accept at a time.  Ideally we would accept\n    # up to the number of connections that were waiting when we\n    # entered this method, but this information is not available\n    # (and rearranging this method to call accept() as many times\n    # as possible before running any callbacks would have adverse\n    # effects on load balancing in multiprocess configurations).\n    # Instead, we use the (default) listen backlog as a rough\n    # heuristic for the number of connections we can reasonably\n    # accept at once.\n    for i in range(_DEFAULT_BACKLOG):\n        if removed[0]:\n            # The socket was probably closed\n            return\n        try:\n            connection, address = sock.accept()\n        except BlockingIOError:\n            # EWOULDBLOCK indicates we have accepted every\n            # connection that is available.\n            return\n        except ConnectionAbortedError:\n            # ECONNABORTED indicates that there was a connection\n            # but it was closed while still in the accept queue.\n            # (observed on FreeBSD).\n            continue\n        callback(connection, address)", "loc": 28}
{"file": "tornado\\tornado\\netutil.py", "class_name": "ExecutorResolver", "function_name": "initialize", "parameters": ["self", "executor", "close_executor"], "param_types": {"executor": "Optional[concurrent.futures.Executor]", "close_executor": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(\n    self,\n    executor: Optional[concurrent.futures.Executor] = None,\n    close_executor: bool = True,\n) -> None:\n    if executor is not None:\n        self.executor = executor\n        self.close_executor = close_executor\n    else:\n        self.executor = dummy_executor\n        self.close_executor = False", "loc": 11}
{"file": "tornado\\tornado\\netutil.py", "class_name": "OverrideResolver", "function_name": "resolve", "parameters": ["self", "host", "port", "family"], "param_types": {"host": "str", "port": "int", "family": "socket.AddressFamily"}, "return_type": "Awaitable[List[Tuple[int, Any]]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.resolver.resolve"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve(\n    self, host: str, port: int, family: socket.AddressFamily = socket.AF_UNSPEC\n) -> Awaitable[List[Tuple[int, Any]]]:\n    if (host, port, family) in self.mapping:\n        host, port = self.mapping[(host, port, family)]\n    elif (host, port) in self.mapping:\n        host, port = self.mapping[(host, port)]\n    elif host in self.mapping:\n        host = self.mapping[host]\n    return self.resolver.resolve(host, port, family)", "loc": 10}
{"file": "tornado\\tornado\\options.py", "class_name": "OptionParser", "function_name": "items", "parameters": ["self"], "param_types": {}, "return_type": "Iterable[Tuple[str, Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt.value", "self._options.items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "An iterable of (name, value) pairs. .. versionadded:: 3.1", "source_code": "def items(self) -> Iterable[Tuple[str, Any]]:\n    \"\"\"An iterable of (name, value) pairs.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    return [(opt.name, opt.value()) for name, opt in self._options.items()]", "loc": 6}
{"file": "tornado\\tornado\\options.py", "class_name": "OptionParser", "function_name": "group_dict", "parameters": ["self", "group"], "param_types": {"group": "str"}, "return_type": "Dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt.value", "self._options.items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "The names and values of options in a group. Useful for copying options into Application settings:: from tornado.options import define, parse_command_line, options", "source_code": "def group_dict(self, group: str) -> Dict[str, Any]:\n    \"\"\"The names and values of options in a group.\n\n    Useful for copying options into Application settings::\n\n        from tornado.options import define, parse_command_line, options\n\n        define('template_path', group='application')\n        define('static_path', group='application')\n\n        parse_command_line()\n\n        application = Application(\n            handlers, **options.group_dict('application'))\n\n    .. versionadded:: 3.1\n    \"\"\"\n    return {\n        opt.name: opt.value()\n        for name, opt in self._options.items()\n        if not group or group == opt.group_name\n    }", "loc": 22}
{"file": "tornado\\tornado\\options.py", "class_name": "OptionParser", "function_name": "as_dict", "parameters": ["self"], "param_types": {}, "return_type": "Dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["opt.value", "self._options.items"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "The names and values of all options. .. versionadded:: 3.1", "source_code": "def as_dict(self) -> Dict[str, Any]:\n    \"\"\"The names and values of all options.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    return {opt.name: opt.value() for name, opt in self._options.items()}", "loc": 6}
{"file": "tornado\\tornado\\options.py", "class_name": "OptionParser", "function_name": "define", "parameters": ["self", "name", "default", "type", "help", "metavar", "multiple", "group", "callback"], "param_types": {"name": "str", "default": "Any", "type": "Optional[type]", "help": "Optional[str]", "metavar": "Optional[str]", "multiple": "bool", "group": "Optional[str]", "callback": "Optional[Callable[[Any], None]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Error", "_Option", "self._normalize_name", "sys._getframe"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Defines a new command line option. ``type`` can be any of `str`, `int`, `float`, `bool`, `~datetime.datetime`, or `~datetime.timedelta`. If no ``type``", "source_code": "def define(\n    self,\n    name: str,\n    default: Any = None,\n    type: Optional[type] = None,\n    help: Optional[str] = None,\n    metavar: Optional[str] = None,\n    multiple: bool = False,\n    group: Optional[str] = None,\n    callback: Optional[Callable[[Any], None]] = None,\n) -> None:\n    \"\"\"Defines a new command line option.\n\n    ``type`` can be any of `str`, `int`, `float`, `bool`,\n    `~datetime.datetime`, or `~datetime.timedelta`. If no ``type``\n    is given but a ``default`` is, ``type`` is the type of\n    ``default``. Otherwise, ``type`` defaults to `str`.\n\n    If ``multiple`` is True, the option value is a list of ``type``\n    instead of an instance of ``type``.\n\n    ``help`` and ``metavar`` are used to construct the\n    automatically generated command line help string. The help\n    message is formatted like::\n\n       --name=METAVAR      help string\n\n    ``group`` is used to group the defined options in logical\n    groups. By default, command line options are grouped by the\n    file in which they are defined.\n\n    Command line option names must be unique globally.\n\n    If a ``callback`` is given, it will be run with the new value whenever\n    the option is changed.  This can be used to combine command-line\n    and file-based options::\n\n        define(\"config\", type=str, help=\"path to config file\",\n               callback=lambda path: parse_config_file(path, final=False))\n\n    With this definition, options in the file specified by ``--config`` will\n    override options set earlier on the command line, but can be overridden\n    by later flags.\n\n    \"\"\"\n    normalized = self._normalize_name(name)\n    if normalized in self._options:\n        raise Error(\n            \"Option %r already defined in %s\"\n            % (normalized, self._options[normalized].file_name)\n        )\n    frame = sys._getframe(0)\n    if frame is not None:\n        options_file = frame.f_code.co_filename\n\n        # Can be called directly, or through top level define() fn, in which\n        # case, step up above that frame to look for real caller.\n        if (\n            frame.f_back is not None\n            and frame.f_back.f_code.co_filename == options_file\n            and frame.f_back.f_code.co_name == \"define\"\n        ):\n            frame = frame.f_back\n\n        assert frame.f_back is not None\n        file_name = frame.f_back.f_code.co_filename\n    else:\n        file_name = \"<unknown>\"\n    if file_name == options_file:\n        file_name = \"\"\n    if type is None:\n        if not multiple and default is not None:\n            type = default.__class__\n        else:\n            type = str\n    if group:\n        group_name = group  # type: Optional[str]\n    else:\n        group_name = file_name\n    option = _Option(\n        name,\n        file_name=file_name,\n        default=default,\n        type=type,\n        help=help,\n        metavar=metavar,\n        multiple=multiple,\n        group_name=group_name,\n        callback=callback,\n    )\n    self._options[normalized] = option", "loc": 91}
{"file": "tornado\\tornado\\options.py", "class_name": "OptionParser", "function_name": "parse_command_line", "parameters": ["self", "args", "final"], "param_types": {"args": "Optional[List[str]]", "final": "bool"}, "return_type": "List[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Error", "arg.partition", "args[i].lstrip", "args[i].startswith", "len", "option.parse", "range", "self._normalize_name", "self.print_help", "self.run_parse_callbacks"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Parses all options given on the command line (defaults to `sys.argv`). Options look like ``--option=value`` and are parsed according", "source_code": "def parse_command_line(\n    self, args: Optional[List[str]] = None, final: bool = True\n) -> List[str]:\n    \"\"\"Parses all options given on the command line (defaults to\n    `sys.argv`).\n\n    Options look like ``--option=value`` and are parsed according\n    to their ``type``. For boolean options, ``--option`` is\n    equivalent to ``--option=true``\n\n    If the option has ``multiple=True``, comma-separated values\n    are accepted. For multi-value integer options, the syntax\n    ``x:y`` is also accepted and equivalent to ``range(x, y)``.\n\n    Note that ``args[0]`` is ignored since it is the program name\n    in `sys.argv`.\n\n    We return a list of all arguments that are not parsed as options.\n\n    If ``final`` is ``False``, parse callbacks will not be run.\n    This is useful for applications that wish to combine configurations\n    from multiple sources.\n\n    \"\"\"\n    if args is None:\n        args = sys.argv\n    remaining = []  # type: List[str]\n    for i in range(1, len(args)):\n        # All things after the last option are command line arguments\n        if not args[i].startswith(\"-\"):\n            remaining = args[i:]\n            break\n        if args[i] == \"--\":\n            remaining = args[i + 1 :]\n            break\n        arg = args[i].lstrip(\"-\")\n        name, equals, value = arg.partition(\"=\")\n        name = self._normalize_name(name)\n        if name not in self._options:\n            self.print_help()\n            raise Error(\"Unrecognized command line option: %r\" % name)\n        option = self._options[name]\n        if not equals:\n            if option.type == bool:\n                value = \"true\"\n            else:\n                raise Error(\"Option %r requires a value\" % name)\n        option.parse(value)\n\n    if final:\n        self.run_parse_callbacks()\n\n    return remaining", "loc": 53}
{"file": "tornado\\tornado\\options.py", "class_name": "_Option", "function_name": "parse", "parameters": ["self", "value"], "param_types": {"value": "str"}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_parse", "issubclass", "part.partition", "range", "self._value.append", "self._value.extend", "self.callback", "self.value", "value.split", "{datetime.datetime: self._parse_datetime, datetime.timedelta: self._parse_timedelta, bool: self._parse_bool, basestring_type: self._parse_string}.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(self, value: str) -> Any:\n    _parse = {\n        datetime.datetime: self._parse_datetime,\n        datetime.timedelta: self._parse_timedelta,\n        bool: self._parse_bool,\n        basestring_type: self._parse_string,\n    }.get(\n        self.type, self.type\n    )  # type: Callable[[str], Any]\n    if self.multiple:\n        self._value = []\n        for part in value.split(\",\"):\n            if issubclass(self.type, numbers.Integral):\n                # allow ranges of the form X:Y (inclusive at both ends)\n                lo_str, _, hi_str = part.partition(\":\")\n                lo = _parse(lo_str)\n                hi = _parse(hi_str) if hi_str else lo\n                self._value.extend(range(lo, hi + 1))\n            else:\n                self._value.append(_parse(part))\n    else:\n        self._value = _parse(value)\n    if self.callback is not None:\n        self.callback(self._value)\n    return self.value()", "loc": 25}
{"file": "tornado\\tornado\\options.py", "class_name": "_Option", "function_name": "set", "parameters": ["self", "value"], "param_types": {"value": "Any"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Error", "isinstance", "self.callback", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set(self, value: Any) -> None:\n    if self.multiple:\n        if not isinstance(value, list):\n            raise Error(\n                \"Option %r is required to be a list of %s\"\n                % (self.name, self.type.__name__)\n            )\n        for item in value:\n            if item is not None and not isinstance(item, self.type):\n                raise Error(\n                    \"Option %r is required to be a list of %s\"\n                    % (self.name, self.type.__name__)\n                )\n    else:\n        if value is not None and not isinstance(value, self.type):\n            raise Error(\n                \"Option %r is required to be a %s (%s given)\"\n                % (self.name, self.type.__name__, type(value))\n            )\n    self._value = value\n    if self.callback is not None:\n        self.callback(self._value)", "loc": 22}
{"file": "tornado\\tornado\\process.py", "class_name": null, "function_name": "cpu_count", "parameters": [], "param_types": {}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_log.error", "multiprocessing.cpu_count", "os.sysconf"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cpu_count() -> int:\n    \"\"\"Returns the number of processors on this machine.\"\"\"\n    if multiprocessing is None:\n        return 1\n    try:\n        return multiprocessing.cpu_count()\n    except NotImplementedError:\n        pass\n    try:\n        return os.sysconf(\"SC_NPROCESSORS_CONF\")  # type: ignore\n    except (AttributeError, ValueError):\n        pass\n    gen_log.error(\"Could not detect number of processors; assuming 1\")\n    return 1", "loc": 14}
{"file": "tornado\\tornado\\process.py", "class_name": null, "function_name": "fork_processes", "parameters": ["num_processes", "max_restarts"], "param_types": {"num_processes": "Optional[int]", "max_restarts": "Optional[int]"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "RuntimeError", "_reseed_random", "children.pop", "cpu_count", "gen_log.info", "gen_log.warning", "os.WEXITSTATUS", "os.WIFSIGNALED", "os.WTERMSIG", "os.fork", "os.wait", "range", "start_child", "sys.exit"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Starts multiple worker processes. If ``num_processes`` is None or <= 0, we detect the number of cores available on this machine and fork that number of child", "source_code": "def fork_processes(\n    num_processes: Optional[int], max_restarts: Optional[int] = None\n) -> int:\n    \"\"\"Starts multiple worker processes.\n\n    If ``num_processes`` is None or <= 0, we detect the number of cores\n    available on this machine and fork that number of child\n    processes. If ``num_processes`` is given and > 0, we fork that\n    specific number of sub-processes.\n\n    Since we use processes and not threads, there is no shared memory\n    between any server code.\n\n    Note that multiple processes are not compatible with the autoreload\n    module (or the ``autoreload=True`` option to `tornado.web.Application`\n    which defaults to True when ``debug=True``).\n    When using multiple processes, no IOLoops can be created or\n    referenced until after the call to ``fork_processes``.\n\n    In each child process, ``fork_processes`` returns its *task id*, a\n    number between 0 and ``num_processes``.  Processes that exit\n    abnormally (due to a signal or non-zero exit status) are restarted\n    with the same id (up to ``max_restarts`` times).  In the parent\n    process, ``fork_processes`` calls ``sys.exit(0)`` after all child\n    processes have exited normally.\n\n    max_restarts defaults to 100.\n\n    Availability: Unix\n    \"\"\"\n    if sys.platform == \"win32\":\n        # The exact form of this condition matters to mypy; it understands\n        # if but not assert in this context.\n        raise Exception(\"fork not available on windows\")\n    if max_restarts is None:\n        max_restarts = 100\n\n    assert _task_id is None\n    if num_processes is None or num_processes <= 0:\n        num_processes = cpu_count()\n    gen_log.info(\"Starting %d processes\", num_processes)\n    children = {}\n\n    def start_child(i: int) -> Optional[int]:\n        pid = os.fork()\n        if pid == 0:\n            # child process\n            _reseed_random()\n            global _task_id\n            _task_id = i\n            return i\n        else:\n            children[pid] = i\n            return None\n\n    for i in range(num_processes):\n        id = start_child(i)\n        if id is not None:\n            return id\n    num_restarts = 0\n    while children:\n        pid, status = os.wait()\n        if pid not in children:\n            continue\n        id = children.pop(pid)\n        if os.WIFSIGNALED(status):\n            gen_log.warning(\n                \"child %d (pid %d) killed by signal %d, restarting\",\n                id,\n                pid,\n                os.WTERMSIG(status),\n            )\n        elif os.WEXITSTATUS(status) != 0:\n            gen_log.warning(\n                \"child %d (pid %d) exited with status %d, restarting\",\n                id,\n                pid,\n                os.WEXITSTATUS(status),\n            )\n        else:\n            gen_log.info(\"child %d (pid %d) exited normally\", id, pid)\n            continue\n        num_restarts += 1\n        if num_restarts > max_restarts:\n            raise RuntimeError(\"Too many child restarts, giving up\")\n        new_id = start_child(id)\n        if new_id is not None:\n            return new_id\n    # All child processes exited cleanly, so exit the master process\n    # instead of just returning to right after the call to\n    # fork_processes (which will probably just start up another IOLoop\n    # unless the caller checks the return value).\n    sys.exit(0)", "loc": 93}
{"file": "tornado\\tornado\\process.py", "class_name": null, "function_name": "start_child", "parameters": ["i"], "param_types": {"i": "int"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_reseed_random", "os.fork"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def start_child(i: int) -> Optional[int]:\n    pid = os.fork()\n    if pid == 0:\n        # child process\n        _reseed_random()\n        global _task_id\n        _task_id = i\n        return i\n    else:\n        children[pid] = i\n        return None", "loc": 11}
{"file": "tornado\\tornado\\process.py", "class_name": "Subprocess", "function_name": "set_exit_callback", "parameters": ["self", "callback"], "param_types": {"callback": "Callable[[int], None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Subprocess._try_cleanup_process", "Subprocess.initialize"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Runs ``callback`` when this process exits. The callback takes one argument, the return code of the process. This method uses a ``SIGCHLD`` handler, which is a global setting", "source_code": "def set_exit_callback(self, callback: Callable[[int], None]) -> None:\n    \"\"\"Runs ``callback`` when this process exits.\n\n    The callback takes one argument, the return code of the process.\n\n    This method uses a ``SIGCHLD`` handler, which is a global setting\n    and may conflict if you have other libraries trying to handle the\n    same signal.  If you are using more than one ``IOLoop`` it may\n    be necessary to call `Subprocess.initialize` first to designate\n    one ``IOLoop`` to run the signal handlers.\n\n    In many cases a close callback on the stdout or stderr streams\n    can be used as an alternative to an exit callback if the\n    signal handler is causing a problem.\n\n    Availability: Unix\n    \"\"\"\n    self._exit_callback = callback\n    Subprocess.initialize()\n    Subprocess._waiting[self.pid] = self\n    Subprocess._try_cleanup_process(self.pid)", "loc": 21}
{"file": "tornado\\tornado\\process.py", "class_name": "Subprocess", "function_name": "wait_for_exit", "parameters": ["self", "raise_error"], "param_types": {"raise_error": "bool"}, "return_type": "'Future[int]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CalledProcessError", "Future", "future_set_exception_unless_cancelled", "future_set_result_unless_cancelled", "self.set_exit_callback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wait_for_exit(self, raise_error: bool = True) -> \"Future[int]\":\n    \"\"\"Returns a `.Future` which resolves when the process exits.\n\n    Usage::\n\n        ret = yield proc.wait_for_exit()\n\n    This is a coroutine-friendly alternative to `set_exit_callback`\n    (and a replacement for the blocking `subprocess.Popen.wait`).\n\n    By default, raises `subprocess.CalledProcessError` if the process\n    has a non-zero exit status. Use ``wait_for_exit(raise_error=False)``\n    to suppress this behavior and return the exit status without raising.\n\n    .. versionadded:: 4.2\n\n    Availability: Unix\n    \"\"\"\n    future = Future()  # type: Future[int]\n\n    def callback(ret: int) -> None:\n        if ret != 0 and raise_error:\n            # Unfortunately we don't have the original args any more.\n            future_set_exception_unless_cancelled(\n                future, CalledProcessError(ret, \"unknown\")\n            )\n        else:\n            future_set_result_unless_cancelled(future, ret)\n\n    self.set_exit_callback(callback)\n    return future", "loc": 31}
{"file": "tornado\\tornado\\process.py", "class_name": "Subprocess", "function_name": "initialize", "parameters": ["cls"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.get_event_loop", "loop.add_signal_handler"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Initializes the ``SIGCHLD`` handler. The signal handler is run on an `.IOLoop` to avoid locking issues. Note that the `.IOLoop` used for signal handling need not be the", "source_code": "def initialize(cls) -> None:\n    \"\"\"Initializes the ``SIGCHLD`` handler.\n\n    The signal handler is run on an `.IOLoop` to avoid locking issues.\n    Note that the `.IOLoop` used for signal handling need not be the\n    same one used by individual Subprocess objects (as long as the\n    ``IOLoops`` are each running in separate threads).\n\n    .. versionchanged:: 5.0\n       The ``io_loop`` argument (deprecated since version 4.1) has been\n       removed.\n\n    Availability: Unix\n    \"\"\"\n    if cls._initialized:\n        return\n    loop = asyncio.get_event_loop()\n    loop.add_signal_handler(signal.SIGCHLD, cls._cleanup)\n    cls._initialized = True", "loc": 19}
{"file": "tornado\\tornado\\process.py", "class_name": "Subprocess", "function_name": "uninitialize", "parameters": ["cls"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.get_event_loop", "loop.remove_signal_handler"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Removes the ``SIGCHLD`` handler.", "source_code": "def uninitialize(cls) -> None:\n    \"\"\"Removes the ``SIGCHLD`` handler.\"\"\"\n    if not cls._initialized:\n        return\n    loop = asyncio.get_event_loop()\n    loop.remove_signal_handler(signal.SIGCHLD)\n    cls._initialized = False", "loc": 7}
{"file": "tornado\\tornado\\process.py", "class_name": null, "function_name": "callback", "parameters": ["ret"], "param_types": {"ret": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CalledProcessError", "future_set_exception_unless_cancelled", "future_set_result_unless_cancelled"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def callback(ret: int) -> None:\n    if ret != 0 and raise_error:\n        # Unfortunately we don't have the original args any more.\n        future_set_exception_unless_cancelled(\n            future, CalledProcessError(ret, \"unknown\")\n        )\n    else:\n        future_set_result_unless_cancelled(future, ret)", "loc": 8}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "full", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.qsize"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def full(self) -> bool:\n    if self.maxsize == 0:\n        return False\n    else:\n        return self.qsize() >= self.maxsize", "loc": 5}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "put", "parameters": ["self", "item", "timeout"], "param_types": {"item": "_T", "timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "_set_timeout", "future.set_result", "self._putters.append", "self.put_nowait"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Put an item into the queue, perhaps waiting until there is room.", "source_code": "def put(\n    self, item: _T, timeout: Optional[Union[float, datetime.timedelta]] = None\n) -> \"Future[None]\":\n    \"\"\"Put an item into the queue, perhaps waiting until there is room.\n\n    Returns a Future, which raises `tornado.util.TimeoutError` after a\n    timeout.\n\n    ``timeout`` may be a number denoting a time (on the same\n    scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n    `datetime.timedelta` object for a deadline relative to the\n    current time.\n    \"\"\"\n    future = Future()  # type: Future[None]\n    try:\n        self.put_nowait(item)\n    except QueueFull:\n        self._putters.append((item, future))\n        _set_timeout(future, timeout)\n    else:\n        future.set_result(None)\n    return future", "loc": 22}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "put_nowait", "parameters": ["self", "item"], "param_types": {"item": "_T"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_result_unless_cancelled", "self.__put_internal", "self._consume_expired", "self._get", "self._getters.popleft", "self.empty", "self.full"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Put an item into the queue without blocking. If no free slot is immediately available, raise `QueueFull`.", "source_code": "def put_nowait(self, item: _T) -> None:\n    \"\"\"Put an item into the queue without blocking.\n\n    If no free slot is immediately available, raise `QueueFull`.\n    \"\"\"\n    self._consume_expired()\n    if self._getters:\n        assert self.empty(), \"queue non-empty, why are getters waiting?\"\n        getter = self._getters.popleft()\n        self.__put_internal(item)\n        future_set_result_unless_cancelled(getter, self._get())\n    elif self.full():\n        raise QueueFull\n    else:\n        self.__put_internal(item)", "loc": 15}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "get", "parameters": ["self", "timeout"], "param_types": {"timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "Awaitable[_T]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "_set_timeout", "future.set_result", "self._getters.append", "self.get_nowait"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Remove and return an item from the queue.", "source_code": "def get(\n    self, timeout: Optional[Union[float, datetime.timedelta]] = None\n) -> Awaitable[_T]:\n    \"\"\"Remove and return an item from the queue.\n\n    Returns an awaitable which resolves once an item is available, or raises\n    `tornado.util.TimeoutError` after a timeout.\n\n    ``timeout`` may be a number denoting a time (on the same\n    scale as `tornado.ioloop.IOLoop.time`, normally `time.time`), or a\n    `datetime.timedelta` object for a deadline relative to the\n    current time.\n\n    .. note::\n\n       The ``timeout`` argument of this method differs from that\n       of the standard library's `queue.Queue.get`. That method\n       interprets numeric values as relative timeouts; this one\n       interprets them as absolute deadlines and requires\n       ``timedelta`` objects for relative timeouts (consistent\n       with other timeouts in Tornado).\n\n    \"\"\"\n    future = Future()  # type: Future[_T]\n    try:\n        future.set_result(self.get_nowait())\n    except QueueEmpty:\n        self._getters.append(future)\n        _set_timeout(future, timeout)\n    return future", "loc": 30}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "get_nowait", "parameters": ["self"], "param_types": {}, "return_type": "_T", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future_set_result_unless_cancelled", "self.__put_internal", "self._consume_expired", "self._get", "self._putters.popleft", "self.full", "self.qsize"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Remove and return an item from the queue without blocking. Return an item if one is immediately available, else raise `QueueEmpty`.", "source_code": "def get_nowait(self) -> _T:\n    \"\"\"Remove and return an item from the queue without blocking.\n\n    Return an item if one is immediately available, else raise\n    `QueueEmpty`.\n    \"\"\"\n    self._consume_expired()\n    if self._putters:\n        assert self.full(), \"queue not full, why are putters waiting?\"\n        item, putter = self._putters.popleft()\n        self.__put_internal(item)\n        future_set_result_unless_cancelled(putter, None)\n        return self._get()\n    elif self.qsize():\n        return self._get()\n    else:\n        raise QueueEmpty", "loc": 17}
{"file": "tornado\\tornado\\queues.py", "class_name": "Queue", "function_name": "task_done", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self._finished.set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Indicate that a formerly enqueued task is complete. Used by queue consumers. For each `.get` used to fetch a task, a subsequent call to `.task_done` tells the queue that the processing on the task is complete. If a `.join` is blocking, it resumes when all items have been processed; that is, when every `.put` is matched by a `.task_done`.", "source_code": "def task_done(self) -> None:\n    \"\"\"Indicate that a formerly enqueued task is complete.\n\n    Used by queue consumers. For each `.get` used to fetch a task, a\n    subsequent call to `.task_done` tells the queue that the processing\n    on the task is complete.\n\n    If a `.join` is blocking, it resumes when all items have been\n    processed; that is, when every `.put` is matched by a `.task_done`.\n\n    Raises `ValueError` if called more times than `.put`.\n    \"\"\"\n    if self._unfinished_tasks <= 0:\n        raise ValueError(\"task_done() called too many times\")\n    self._unfinished_tasks -= 1\n    if self._unfinished_tasks == 0:\n        self._finished.set()", "loc": 17}
{"file": "tornado\\tornado\\routing.py", "class_name": "RuleRouter", "function_name": "add_rules", "parameters": ["self", "rules"], "param_types": {"rules": "_RuleList"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PathMatches", "Rule", "isinstance", "len", "self.process_rule", "self.rules.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Appends new rules to the router. :arg rules: a list of Rule instances (or tuples of arguments, which are passed to Rule constructor).", "source_code": "def add_rules(self, rules: _RuleList) -> None:\n    \"\"\"Appends new rules to the router.\n\n    :arg rules: a list of Rule instances (or tuples of arguments, which are\n        passed to Rule constructor).\n    \"\"\"\n    for rule in rules:\n        if isinstance(rule, (tuple, list)):\n            assert len(rule) in (2, 3, 4)\n            if isinstance(rule[0], basestring_type):\n                rule = Rule(PathMatches(rule[0]), *rule[1:])\n            else:\n                rule = Rule(*rule)\n\n        self.rules.append(self.process_rule(rule))", "loc": 15}
{"file": "tornado\\tornado\\routing.py", "class_name": "RuleRouter", "function_name": "find_handler", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "Optional[httputil.HTTPMessageDelegate]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["rule.matcher.match", "self.get_target_delegate"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_handler(\n    self, request: httputil.HTTPServerRequest, **kwargs: Any\n) -> Optional[httputil.HTTPMessageDelegate]:\n    for rule in self.rules:\n        target_params = rule.matcher.match(request)\n        if target_params is not None:\n            if rule.target_kwargs:\n                target_params[\"target_kwargs\"] = rule.target_kwargs\n\n            delegate = self.get_target_delegate(\n                rule.target, request, **target_params\n            )\n\n            if delegate is not None:\n                return delegate\n\n    return None", "loc": 17}
{"file": "tornado\\tornado\\routing.py", "class_name": "RuleRouter", "function_name": "get_target_delegate", "parameters": ["self", "target", "request"], "param_types": {"target": "Any", "request": "httputil.HTTPServerRequest"}, "return_type": "Optional[httputil.HTTPMessageDelegate]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_CallableAdapter", "callable", "isinstance", "partial", "target.find_handler", "target.start_request"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_target_delegate(\n    self, target: Any, request: httputil.HTTPServerRequest, **target_params: Any\n) -> Optional[httputil.HTTPMessageDelegate]:\n    \"\"\"Returns an instance of `~.httputil.HTTPMessageDelegate` for a\n    Rule's target. This method is called by `~.find_handler` and can be\n    extended to provide additional target types.\n\n    :arg target: a Rule's target.\n    :arg httputil.HTTPServerRequest request: current request.\n    :arg target_params: additional parameters that can be useful\n        for `~.httputil.HTTPMessageDelegate` creation.\n    \"\"\"\n    if isinstance(target, Router):\n        return target.find_handler(request, **target_params)\n\n    elif isinstance(target, httputil.HTTPServerConnectionDelegate):\n        assert request.connection is not None\n        return target.start_request(request.server_connection, request.connection)\n\n    elif callable(target):\n        assert request.connection is not None\n        return _CallableAdapter(\n            partial(target, **target_params), request.connection\n        )\n\n    return None", "loc": 26}
{"file": "tornado\\tornado\\routing.py", "class_name": "ReversibleRuleRouter", "function_name": "process_rule", "parameters": ["self", "rule"], "param_types": {"rule": "'Rule'"}, "return_type": "'Rule'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.warning", "super", "super().process_rule"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_rule(self, rule: \"Rule\") -> \"Rule\":\n    rule = super().process_rule(rule)\n\n    if rule.name:\n        if rule.name in self.named_rules:\n            app_log.warning(\n                \"Multiple handlers named %s; replacing previous value\", rule.name\n            )\n        self.named_rules[rule.name] = rule\n\n    return rule", "loc": 11}
{"file": "tornado\\tornado\\routing.py", "class_name": "ReversibleRuleRouter", "function_name": "reverse_url", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "rule.target.reverse_url", "self.named_rules[name].matcher.reverse"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reverse_url(self, name: str, *args: Any) -> Optional[str]:\n    if name in self.named_rules:\n        return self.named_rules[name].matcher.reverse(*args)\n\n    for rule in self.rules:\n        if isinstance(rule.target, ReversibleRouter):\n            reversed_url = rule.target.reverse_url(name, *args)\n            if reversed_url is not None:\n                return reversed_url\n\n    return None", "loc": 11}
{"file": "tornado\\tornado\\routing.py", "class_name": "HostMatches", "function_name": "match", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "Optional[Dict[str, Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.host_pattern.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(self, request: httputil.HTTPServerRequest) -> Optional[Dict[str, Any]]:\n    if self.host_pattern.match(request.host_name):\n        return {}\n\n    return None", "loc": 5}
{"file": "tornado\\tornado\\routing.py", "class_name": "DefaultHostMatches", "function_name": "match", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "Optional[Dict[str, Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.host_pattern.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(self, request: httputil.HTTPServerRequest) -> Optional[Dict[str, Any]]:\n    # Look for default host if not behind load balancer (for debugging)\n    if \"X-Real-Ip\" not in request.headers:\n        if self.host_pattern.match(self.application.default_host):\n            return {}\n    return None", "loc": 6}
{"file": "tornado\\tornado\\routing.py", "class_name": "PathMatches", "function_name": "match", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "Optional[Dict[str, Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_unquote_or_none", "dict", "match.groupdict", "match.groupdict().items", "match.groups", "self.regex.match", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match(self, request: httputil.HTTPServerRequest) -> Optional[Dict[str, Any]]:\n    match = self.regex.match(request.path)\n    if match is None:\n        return None\n    if not self.regex.groups:\n        return {}\n\n    path_args = []  # type: List[bytes]\n    path_kwargs = {}  # type: Dict[str, bytes]\n\n    # Pass matched groups to the handler.  Since\n    # match.groups() includes both named and\n    # unnamed groups, we want to use either groups\n    # or groupdict but not both.\n    if self.regex.groupindex:\n        path_kwargs = {\n            str(k): _unquote_or_none(v) for (k, v) in match.groupdict().items()\n        }\n    else:\n        path_args = [_unquote_or_none(s) for s in match.groups()]\n\n    return dict(path_args=path_args, path_kwargs=path_kwargs)", "loc": 22}
{"file": "tornado\\tornado\\routing.py", "class_name": "PathMatches", "function_name": "reverse", "parameters": ["self"], "param_types": {}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "converted_args.append", "isinstance", "len", "str", "tuple", "url_escape", "utf8"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reverse(self, *args: Any) -> Optional[str]:\n    if self._path is None:\n        raise ValueError(\"Cannot reverse url regex \" + self.regex.pattern)\n    assert len(args) == self._group_count, (\n        \"required number of arguments \" \"not found\"\n    )\n    if not len(args):\n        return self._path\n    converted_args = []\n    for a in args:\n        if not isinstance(a, (unicode_type, bytes)):\n            a = str(a)\n        converted_args.append(url_escape(utf8(a), plus=False))\n    return self._path % tuple(converted_args)", "loc": 14}
{"file": "tornado\\tornado\\simple_httpclient.py", "class_name": "SimpleAsyncHTTPClient", "function_name": "initialize", "parameters": ["self", "max_clients", "hostname_mapping", "max_buffer_size", "resolver", "defaults", "max_header_size", "max_body_size"], "param_types": {"max_clients": "int", "hostname_mapping": "Optional[Dict[str, str]]", "max_buffer_size": "int", "resolver": "Optional[Resolver]", "defaults": "Optional[Dict[str, Any]]", "max_header_size": "Optional[int]", "max_body_size": "Optional[int]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OverrideResolver", "Resolver", "TCPClient", "collections.deque", "super", "super().initialize"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(  # type: ignore\n    self,\n    max_clients: int = 10,\n    hostname_mapping: Optional[Dict[str, str]] = None,\n    max_buffer_size: int = 104857600,\n    resolver: Optional[Resolver] = None,\n    defaults: Optional[Dict[str, Any]] = None,\n    max_header_size: Optional[int] = None,\n    max_body_size: Optional[int] = None,\n) -> None:\n    super().initialize(defaults=defaults)\n    self.max_clients = max_clients\n    self.queue = (\n        collections.deque()\n    )  # type: Deque[Tuple[object, HTTPRequest, Callable[[HTTPResponse], None]]]\n    self.active = (\n        {}\n    )  # type: Dict[object, Tuple[HTTPRequest, Callable[[HTTPResponse], None]]]\n    self.waiting = (\n        {}\n    )  # type: Dict[object, Tuple[HTTPRequest, Callable[[HTTPResponse], None], object]]\n    self.max_buffer_size = max_buffer_size\n    self.max_header_size = max_header_size\n    self.max_body_size = max_body_size\n    # TCPClient could create a Resolver for us, but we have to do it\n    # ourselves to support hostname_mapping.\n    if resolver:\n        self.resolver = resolver\n        self.own_resolver = False\n    else:\n        self.resolver = Resolver()\n        self.own_resolver = True\n    if hostname_mapping is not None:\n        self.resolver = OverrideResolver(\n            resolver=self.resolver, mapping=hostname_mapping\n        )\n    self.tcp_client = TCPClient(resolver=self.resolver)", "loc": 37}
{"file": "tornado\\tornado\\simple_httpclient.py", "class_name": "SimpleAsyncHTTPClient", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.resolver.close", "self.tcp_client.close", "super", "super().close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    super().close()\n    if self.own_resolver:\n        self.resolver.close()\n    self.tcp_client.close()", "loc": 5}
{"file": "tornado\\tornado\\simple_httpclient.py", "class_name": "SimpleAsyncHTTPClient", "function_name": "fetch_impl", "parameters": ["self", "request", "callback"], "param_types": {"request": "HTTPRequest", "callback": "Callable[[HTTPResponse], None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "gen_log.debug", "len", "min", "object", "self._process_queue", "self.io_loop.add_timeout", "self.io_loop.time", "self.queue.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fetch_impl(\n    self, request: HTTPRequest, callback: Callable[[HTTPResponse], None]\n) -> None:\n    key = object()\n    self.queue.append((key, request, callback))\n    assert request.connect_timeout is not None\n    assert request.request_timeout is not None\n    timeout_handle = None\n    if len(self.active) >= self.max_clients:\n        timeout = (\n            min(request.connect_timeout, request.request_timeout)\n            or request.connect_timeout\n            or request.request_timeout\n        )  # min but skip zero\n        if timeout:\n            timeout_handle = self.io_loop.add_timeout(\n                self.io_loop.time() + timeout,\n                functools.partial(self._on_timeout, key, \"in request queue\"),\n            )\n    self.waiting[key] = (request, callback, timeout_handle)\n    self._process_queue()\n    if self.queue:\n        gen_log.debug(\n            \"max_clients limit reached, request queued. \"\n            \"%d active, %d queued requests.\" % (len(self.active), len(self.queue))\n        )", "loc": 26}
{"file": "tornado\\tornado\\simple_httpclient.py", "class_name": "_HTTPConnection", "function_name": "on_connection_close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPStreamClosedError", "self._handle_exception", "sys.exc_info"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_connection_close(self) -> None:\n    if self.final_callback is not None:\n        message = \"Connection closed\"\n        if self.stream.error:\n            raise self.stream.error\n        try:\n            raise HTTPStreamClosedError(message)\n        except HTTPStreamClosedError:\n            self._handle_exception(*sys.exc_info())", "loc": 9}
{"file": "tornado\\tornado\\simple_httpclient.py", "class_name": "_HTTPConnection", "function_name": "data_received", "parameters": ["self", "chunk"], "param_types": {"chunk": "bytes"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._should_follow_redirect", "self.chunks.append", "self.request.streaming_callback"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def data_received(self, chunk: bytes) -> Optional[Awaitable[None]]:\n    if self._should_follow_redirect():\n        # We're going to follow a redirect so just discard the body.\n        return None\n    if self.request.streaming_callback is not None:\n        return self.request.streaming_callback(chunk)\n    else:\n        self.chunks.append(chunk)\n        return None", "loc": 9}
{"file": "tornado\\tornado\\tcpclient.py", "class_name": "_Connector", "function_name": "split", "parameters": ["addrinfo"], "param_types": {"addrinfo": "List[Tuple]"}, "return_type": "Tuple[List[Tuple[socket.AddressFamily, Tuple]], List[Tuple[socket.AddressFamily, Tuple]]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["primary.append", "secondary.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Partition the ``addrinfo`` list by address family.", "source_code": "def split(\n    addrinfo: List[Tuple],\n) -> Tuple[\n    List[Tuple[socket.AddressFamily, Tuple]],\n    List[Tuple[socket.AddressFamily, Tuple]],\n]:\n    \"\"\"Partition the ``addrinfo`` list by address family.\n\n    Returns two lists.  The first list contains the first entry from\n    ``addrinfo`` and all others with the same family, and the\n    second list contains all other addresses (normally one list will\n    be AF_INET and the other AF_INET6, although non-standard resolvers\n    may return additional families).\n    \"\"\"\n    primary = []\n    secondary = []\n    primary_af = addrinfo[0][0]\n    for af, addr in addrinfo:\n        if af == primary_af:\n            primary.append((af, addr))\n        else:\n            secondary.append((af, addr))\n    return primary, secondary", "loc": 23}
{"file": "tornado\\tornado\\tcpclient.py", "class_name": "_Connector", "function_name": "start", "parameters": ["self", "timeout", "connect_timeout"], "param_types": {"timeout": "float", "connect_timeout": "Optional[Union[float, datetime.timedelta]]"}, "return_type": "'Future[Tuple[socket.AddressFamily, Any, IOStream]]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["iter", "self.set_connect_timeout", "self.set_timeout", "self.try_connect"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def start(\n    self,\n    timeout: float = _INITIAL_CONNECT_TIMEOUT,\n    connect_timeout: Optional[Union[float, datetime.timedelta]] = None,\n) -> \"Future[Tuple[socket.AddressFamily, Any, IOStream]]\":\n    self.try_connect(iter(self.primary_addrs))\n    self.set_timeout(timeout)\n    if connect_timeout is not None:\n        self.set_connect_timeout(connect_timeout)\n    return self.future", "loc": 10}
{"file": "tornado\\tornado\\tcpclient.py", "class_name": "_Connector", "function_name": "try_connect", "parameters": ["self", "addrs"], "param_types": {"addrs": "Iterator[Tuple[socket.AddressFamily, Tuple]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IOError", "functools.partial", "future_add_done_callback", "next", "self.connect", "self.future.done", "self.future.set_exception", "self.streams.add"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def try_connect(self, addrs: Iterator[Tuple[socket.AddressFamily, Tuple]]) -> None:\n    try:\n        af, addr = next(addrs)\n    except StopIteration:\n        # We've reached the end of our queue, but the other queue\n        # might still be working.  Send a final error on the future\n        # only when both queues are finished.\n        if self.remaining == 0 and not self.future.done():\n            self.future.set_exception(\n                self.last_error or IOError(\"connection failed\")\n            )\n        return\n    stream, future = self.connect(af, addr)\n    self.streams.add(stream)\n    future_add_done_callback(\n        future, functools.partial(self.on_connect_done, addrs, af, addr)\n    )", "loc": 17}
{"file": "tornado\\tornado\\tcpclient.py", "class_name": "_Connector", "function_name": "on_connect_done", "parameters": ["self", "addrs", "af", "addr", "future"], "param_types": {"addrs": "Iterator[Tuple[socket.AddressFamily, Tuple]]", "af": "socket.AddressFamily", "addr": "Tuple", "future": "'Future[IOStream]'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["future.result", "self.clear_timeouts", "self.close_streams", "self.future.done", "self.future.set_result", "self.io_loop.remove_timeout", "self.on_timeout", "self.streams.discard", "self.try_connect", "stream.close"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_connect_done(\n    self,\n    addrs: Iterator[Tuple[socket.AddressFamily, Tuple]],\n    af: socket.AddressFamily,\n    addr: Tuple,\n    future: \"Future[IOStream]\",\n) -> None:\n    self.remaining -= 1\n    try:\n        stream = future.result()\n    except Exception as e:\n        if self.future.done():\n            return\n        # Error: try again (but remember what happened so we have an\n        # error to raise in the end)\n        self.last_error = e\n        self.try_connect(addrs)\n        if self.timeout is not None:\n            # If the first attempt failed, don't wait for the\n            # timeout to try an address from the secondary queue.\n            self.io_loop.remove_timeout(self.timeout)\n            self.on_timeout()\n        return\n    self.clear_timeouts()\n    if self.future.done():\n        # This is a late arrival; just drop it.\n        stream.close()\n    else:\n        self.streams.discard(stream)\n        self.future.set_result((af, addr, stream))\n        self.close_streams()", "loc": 31}
{"file": "tornado\\tornado\\tcpclient.py", "class_name": "_Connector", "function_name": "clear_timeouts", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.io_loop.remove_timeout"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def clear_timeouts(self) -> None:\n    if self.timeout is not None:\n        self.io_loop.remove_timeout(self.timeout)\n    if self.connect_timeout is not None:\n        self.io_loop.remove_timeout(self.connect_timeout)", "loc": 5}
{"file": "tornado\\tornado\\tcpserver.py", "class_name": "TCPServer", "function_name": "listen", "parameters": ["self", "port", "address", "family", "backlog", "flags", "reuse_port"], "param_types": {"port": "int", "address": "Optional[str]", "family": "socket.AddressFamily", "backlog": "int", "flags": "Optional[int]", "reuse_port": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bind_sockets", "self.add_sockets"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Starts accepting connections on the given port. This method may be called more than once to listen on multiple ports. `listen` takes effect immediately; it is not necessary to call", "source_code": "def listen(\n    self,\n    port: int,\n    address: Optional[str] = None,\n    family: socket.AddressFamily = socket.AF_UNSPEC,\n    backlog: int = _DEFAULT_BACKLOG,\n    flags: Optional[int] = None,\n    reuse_port: bool = False,\n) -> None:\n    \"\"\"Starts accepting connections on the given port.\n\n    This method may be called more than once to listen on multiple ports.\n    `listen` takes effect immediately; it is not necessary to call\n    `TCPServer.start` afterwards.  It is, however, necessary to start the\n    event loop if it is not already running.\n\n    All arguments have the same meaning as in\n    `tornado.netutil.bind_sockets`.\n\n    .. versionchanged:: 6.2\n\n       Added ``family``, ``backlog``, ``flags``, and ``reuse_port``\n       arguments to match `tornado.netutil.bind_sockets`.\n    \"\"\"\n    sockets = bind_sockets(\n        port,\n        address=address,\n        family=family,\n        backlog=backlog,\n        flags=flags,\n        reuse_port=reuse_port,\n    )\n    self.add_sockets(sockets)", "loc": 33}
{"file": "tornado\\tornado\\tcpserver.py", "class_name": "TCPServer", "function_name": "add_sockets", "parameters": ["self", "sockets"], "param_types": {"sockets": "Iterable[socket.socket]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_accept_handler", "sock.fileno"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Makes this server start accepting connections on the given sockets. The ``sockets`` parameter is a list of socket objects such as those returned by `~tornado.netutil.bind_sockets`.", "source_code": "def add_sockets(self, sockets: Iterable[socket.socket]) -> None:\n    \"\"\"Makes this server start accepting connections on the given sockets.\n\n    The ``sockets`` parameter is a list of socket objects such as\n    those returned by `~tornado.netutil.bind_sockets`.\n    `add_sockets` is typically used in combination with that\n    method and `tornado.process.fork_processes` to provide greater\n    control over the initialization of a multi-process server.\n    \"\"\"\n    for sock in sockets:\n        self._sockets[sock.fileno()] = sock\n        self._handlers[sock.fileno()] = add_accept_handler(\n            sock, self._handle_connection\n        )", "loc": 14}
{"file": "tornado\\tornado\\tcpserver.py", "class_name": "TCPServer", "function_name": "bind", "parameters": ["self", "port", "address", "family", "backlog", "flags", "reuse_port"], "param_types": {"port": "int", "address": "Optional[str]", "family": "socket.AddressFamily", "backlog": "int", "flags": "Optional[int]", "reuse_port": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bind_sockets", "self._pending_sockets.extend", "self.add_sockets"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Binds this server to the given port on the given address. To start the server, call `start`. If you want to run this server in a single process, you can call `listen` as a shortcut to the sequence of", "source_code": "def bind(\n    self,\n    port: int,\n    address: Optional[str] = None,\n    family: socket.AddressFamily = socket.AF_UNSPEC,\n    backlog: int = _DEFAULT_BACKLOG,\n    flags: Optional[int] = None,\n    reuse_port: bool = False,\n) -> None:\n    \"\"\"Binds this server to the given port on the given address.\n\n    To start the server, call `start`. If you want to run this server in a\n    single process, you can call `listen` as a shortcut to the sequence of\n    `bind` and `start` calls.\n\n    Address may be either an IP address or hostname.  If it's a hostname,\n    the server will listen on all IP addresses associated with the name.\n    Address may be an empty string or None to listen on all available\n    interfaces.  Family may be set to either `socket.AF_INET` or\n    `socket.AF_INET6` to restrict to IPv4 or IPv6 addresses, otherwise both\n    will be used if available.\n\n    The ``backlog`` argument has the same meaning as for `socket.listen\n    <socket.socket.listen>`. The ``reuse_port`` argument has the same\n    meaning as for `.bind_sockets`.\n\n    This method may be called multiple times prior to `start` to listen on\n    multiple ports or interfaces.\n\n    .. versionchanged:: 4.4\n       Added the ``reuse_port`` argument.\n\n    .. versionchanged:: 6.2\n       Added the ``flags`` argument to match `.bind_sockets`.\n\n    .. deprecated:: 6.2\n       Use either ``listen()`` or ``add_sockets()`` instead of ``bind()``\n       and ``start()``.\n    \"\"\"\n    sockets = bind_sockets(\n        port,\n        address=address,\n        family=family,\n        backlog=backlog,\n        flags=flags,\n        reuse_port=reuse_port,\n    )\n    if self._started:\n        self.add_sockets(sockets)\n    else:\n        self._pending_sockets.extend(sockets)", "loc": 51}
{"file": "tornado\\tornado\\tcpserver.py", "class_name": "TCPServer", "function_name": "start", "parameters": ["self", "num_processes", "max_restarts"], "param_types": {"num_processes": "Optional[int]", "max_restarts": "Optional[int]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["process.fork_processes", "self.add_sockets"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Starts this server in the `.IOLoop`. By default, we run the server in this process and do not fork any additional child process.", "source_code": "def start(\n    self, num_processes: Optional[int] = 1, max_restarts: Optional[int] = None\n) -> None:\n    \"\"\"Starts this server in the `.IOLoop`.\n\n    By default, we run the server in this process and do not fork any\n    additional child process.\n\n    If num_processes is ``None`` or <= 0, we detect the number of cores\n    available on this machine and fork that number of child\n    processes. If num_processes is given and > 1, we fork that\n    specific number of sub-processes.\n\n    Since we use processes and not threads, there is no shared memory\n    between any server code.\n\n    Note that multiple processes are not compatible with the autoreload\n    module (or the ``autoreload=True`` option to `tornado.web.Application`\n    which defaults to True when ``debug=True``).\n    When using multiple processes, no IOLoops can be created or\n    referenced until after the call to ``TCPServer.start(n)``.\n\n    Values of ``num_processes`` other than 1 are not supported on Windows.\n\n    The ``max_restarts`` argument is passed to `.fork_processes`.\n\n    .. versionchanged:: 6.0\n\n       Added ``max_restarts`` argument.\n\n    .. deprecated:: 6.2\n       Use either ``listen()`` or ``add_sockets()`` instead of ``bind()``\n       and ``start()``.\n    \"\"\"\n    assert not self._started\n    self._started = True\n    if num_processes != 1:\n        process.fork_processes(num_processes, max_restarts)\n    sockets = self._pending_sockets\n    self._pending_sockets = []\n    self.add_sockets(sockets)", "loc": 41}
{"file": "tornado\\tornado\\tcpserver.py", "class_name": "TCPServer", "function_name": "stop", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._handlers.pop", "self._sockets.items", "sock.close", "sock.fileno"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Stops listening for new connections. Requests currently in progress may still continue after the server is stopped.", "source_code": "def stop(self) -> None:\n    \"\"\"Stops listening for new connections.\n\n    Requests currently in progress may still continue after the\n    server is stopped.\n    \"\"\"\n    if self._stopped:\n        return\n    self._stopped = True\n    for fd, sock in self._sockets.items():\n        assert sock.fileno() == fd\n        # Unregister socket from IOLoop\n        self._handlers.pop(fd)()\n        sock.close()", "loc": 14}
{"file": "tornado\\tornado\\template.py", "class_name": null, "function_name": "filter_whitespace", "parameters": ["mode", "text"], "param_types": {"mode": "str", "text": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "re.sub"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Transform whitespace in ``text`` according to ``mode``. Available modes are: * ``all``: Return all whitespace unmodified.", "source_code": "def filter_whitespace(mode: str, text: str) -> str:\n    \"\"\"Transform whitespace in ``text`` according to ``mode``.\n\n    Available modes are:\n\n    * ``all``: Return all whitespace unmodified.\n    * ``single``: Collapse consecutive whitespace with a single whitespace\n      character, preserving newlines.\n    * ``oneline``: Collapse all runs of whitespace into a single space\n      character, removing all newlines in the process.\n\n    .. versionadded:: 4.3\n    \"\"\"\n    if mode == \"all\":\n        return text\n    elif mode == \"single\":\n        text = re.sub(r\"([\\t ]+)\", \" \", text)\n        text = re.sub(r\"(\\s*\\n\\s*)\", \"\\n\", text)\n        return text\n    elif mode == \"oneline\":\n        return re.sub(r\"(\\s+)\", \" \", text)\n    else:\n        raise Exception(\"invalid whitespace mode %s\" % mode)", "loc": 23}
{"file": "tornado\\tornado\\template.py", "class_name": "Template", "function_name": "generate", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ObjectDict", "exec_in", "execute", "linecache.clearcache", "namespace.update", "self.name.replace", "typing.cast"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Generate this template with the given arguments.", "source_code": "def generate(self, **kwargs: Any) -> bytes:\n    \"\"\"Generate this template with the given arguments.\"\"\"\n    namespace = {\n        \"escape\": escape.xhtml_escape,\n        \"xhtml_escape\": escape.xhtml_escape,\n        \"url_escape\": escape.url_escape,\n        \"json_encode\": escape.json_encode,\n        \"squeeze\": escape.squeeze,\n        \"linkify\": escape.linkify,\n        \"datetime\": datetime,\n        \"_tt_utf8\": escape.utf8,  # for internal use\n        \"_tt_string_types\": (unicode_type, bytes),\n        # __name__ and __loader__ allow the traceback mechanism to find\n        # the generated source code.\n        \"__name__\": self.name.replace(\".\", \"_\"),\n        \"__loader__\": ObjectDict(get_source=lambda name: self.code),\n    }\n    namespace.update(self.namespace)\n    namespace.update(kwargs)\n    exec_in(self.compiled, namespace)\n    execute = typing.cast(Callable[[], bytes], namespace[\"_tt_execute\"])\n    # Clear the traceback module's cache of source data now that\n    # we've generated a new template (mainly for this module's\n    # unittests, where different tests reuse the same name).\n    linecache.clearcache()\n    return execute()", "loc": 26}
{"file": "tornado\\tornado\\template.py", "class_name": "BaseLoader", "function_name": "load", "parameters": ["self", "name", "parent_path"], "param_types": {"name": "str", "parent_path": "Optional[str]"}, "return_type": "Template", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._create_template", "self.resolve_path"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Loads a template.", "source_code": "def load(self, name: str, parent_path: Optional[str] = None) -> Template:\n    \"\"\"Loads a template.\"\"\"\n    name = self.resolve_path(name, parent_path=parent_path)\n    with self.lock:\n        if name not in self.templates:\n            self.templates[name] = self._create_template(name)\n        return self.templates[name]", "loc": 7}
{"file": "tornado\\tornado\\template.py", "class_name": "DictLoader", "function_name": "resolve_path", "parameters": ["self", "name", "parent_path"], "param_types": {"name": "str", "parent_path": "Optional[str]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["name.startswith", "parent_path.startswith", "posixpath.dirname", "posixpath.join", "posixpath.normpath"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve_path(self, name: str, parent_path: Optional[str] = None) -> str:\n    if (\n        parent_path\n        and not parent_path.startswith(\"<\")\n        and not parent_path.startswith(\"/\")\n        and not name.startswith(\"/\")\n    ):\n        file_dir = posixpath.dirname(parent_path)\n        name = posixpath.normpath(posixpath.join(file_dir, name))\n    return name", "loc": 10}
{"file": "tornado\\tornado\\template.py", "class_name": "_Node", "function_name": "find_named_blocks", "parameters": ["self", "loader", "named_blocks"], "param_types": {"loader": "Optional[BaseLoader]", "named_blocks": "Dict[str, '_NamedBlock']"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["child.find_named_blocks", "self.each_child"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_named_blocks(\n    self, loader: Optional[BaseLoader], named_blocks: Dict[str, \"_NamedBlock\"]\n) -> None:\n    for child in self.each_child():\n        child.find_named_blocks(loader, named_blocks)", "loc": 5}
{"file": "tornado\\tornado\\template.py", "class_name": "_File", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.body.generate", "writer.indent", "writer.write_line"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    writer.write_line(\"def _tt_execute():\", self.line)\n    with writer.indent():\n        writer.write_line(\"_tt_buffer = []\", self.line)\n        writer.write_line(\"_tt_append = _tt_buffer.append\", self.line)\n        self.body.generate(writer)\n        writer.write_line(\"return _tt_utf8('').join(_tt_buffer)\", self.line)", "loc": 7}
{"file": "tornado\\tornado\\template.py", "class_name": "_IncludeBlock", "function_name": "find_named_blocks", "parameters": ["self", "loader", "named_blocks"], "param_types": {"loader": "Optional[BaseLoader]", "named_blocks": "Dict[str, _NamedBlock]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["included.file.find_named_blocks", "loader.load"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_named_blocks(\n    self, loader: Optional[BaseLoader], named_blocks: Dict[str, _NamedBlock]\n) -> None:\n    assert loader is not None\n    included = loader.load(self.name, self.template_name)\n    included.file.find_named_blocks(loader, named_blocks)", "loc": 6}
{"file": "tornado\\tornado\\template.py", "class_name": "_IncludeBlock", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["included.file.body.generate", "writer.include", "writer.loader.load"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    assert writer.loader is not None\n    included = writer.loader.load(self.name, self.template_name)\n    with writer.include(included, self.line):\n        included.file.body.generate(writer)", "loc": 5}
{"file": "tornado\\tornado\\template.py", "class_name": "_ApplyBlock", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.body.generate", "writer.indent", "writer.write_line"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    method_name = \"_tt_apply%d\" % writer.apply_counter\n    writer.apply_counter += 1\n    writer.write_line(\"def %s():\" % method_name, self.line)\n    with writer.indent():\n        writer.write_line(\"_tt_buffer = []\", self.line)\n        writer.write_line(\"_tt_append = _tt_buffer.append\", self.line)\n        self.body.generate(writer)\n        writer.write_line(\"return _tt_utf8('').join(_tt_buffer)\", self.line)\n    writer.write_line(\n        f\"_tt_append(_tt_utf8({self.method}({method_name}())))\", self.line\n    )", "loc": 12}
{"file": "tornado\\tornado\\template.py", "class_name": "_ControlBlock", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.body.generate", "writer.indent", "writer.write_line"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    writer.write_line(\"%s:\" % self.statement, self.line)\n    with writer.indent():\n        self.body.generate(writer)\n        # Just in case the body was empty\n        writer.write_line(\"pass\", self.line)", "loc": 6}
{"file": "tornado\\tornado\\template.py", "class_name": "_Expression", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["writer.write_line"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    writer.write_line(\"_tt_tmp = %s\" % self.expression, self.line)\n    writer.write_line(\n        \"if isinstance(_tt_tmp, _tt_string_types):\" \" _tt_tmp = _tt_utf8(_tt_tmp)\",\n        self.line,\n    )\n    writer.write_line(\"else: _tt_tmp = _tt_utf8(str(_tt_tmp))\", self.line)\n    if not self.raw and writer.current_template.autoescape is not None:\n        # In python3 functions like xhtml_escape return unicode,\n        # so we have to convert to utf8 again.\n        writer.write_line(\n            \"_tt_tmp = _tt_utf8(%s(_tt_tmp))\" % writer.current_template.autoescape,\n            self.line,\n        )\n    writer.write_line(\"_tt_append(_tt_tmp)\", self.line)", "loc": 15}
{"file": "tornado\\tornado\\template.py", "class_name": "_Text", "function_name": "generate", "parameters": ["self", "writer"], "param_types": {"writer": "'_CodeWriter'"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["escape.utf8", "filter_whitespace", "writer.write_line"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def generate(self, writer: \"_CodeWriter\") -> None:\n    value = self.value\n\n    # Compress whitespace if requested, with a crude heuristic to avoid\n    # altering preformatted whitespace.\n    if \"<pre>\" not in value:\n        value = filter_whitespace(self.whitespace, value)\n\n    if value:\n        writer.write_line(\"_tt_append(%r)\" % escape.utf8(value), self.line)", "loc": 10}
{"file": "tornado\\tornado\\template.py", "class_name": "_CodeWriter", "function_name": "include", "parameters": ["self", "template", "line"], "param_types": {"template": "Template", "line": "int"}, "return_type": "'ContextManager'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IncludeTemplate", "self.include_stack.append", "self.include_stack.pop"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def include(self, template: Template, line: int) -> \"ContextManager\":\n    self.include_stack.append((self.current_template, line))\n    self.current_template = template\n\n    class IncludeTemplate:\n        def __enter__(_) -> \"_CodeWriter\":\n            return self\n\n        def __exit__(_, *args: Any) -> None:\n            self.current_template = self.include_stack.pop()[0]\n\n    return IncludeTemplate()", "loc": 12}
{"file": "tornado\\tornado\\template.py", "class_name": "_CodeWriter", "function_name": "write_line", "parameters": ["self", "line", "line_number", "indent"], "param_types": {"line": "str", "line_number": "int", "indent": "Optional[int]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "print", "reversed"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write_line(\n    self, line: str, line_number: int, indent: Optional[int] = None\n) -> None:\n    if indent is None:\n        indent = self._indent\n    line_comment = \"  # %s:%d\" % (self.current_template.name, line_number)\n    if self.include_stack:\n        ancestors = [\n            \"%s:%d\" % (tmpl.name, lineno) for (tmpl, lineno) in self.include_stack\n        ]\n        line_comment += \" (via %s)\" % \", \".join(reversed(ancestors))\n    print(\"    \" * indent + line + line_comment, file=self.file)", "loc": 12}
{"file": "tornado\\tornado\\template.py", "class_name": "_TemplateReader", "function_name": "find", "parameters": ["self", "needle", "start", "end"], "param_types": {"needle": "str", "start": "int", "end": "Optional[int]"}, "return_type": "int", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.text.find"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find(self, needle: str, start: int = 0, end: Optional[int] = None) -> int:\n    assert start >= 0, start\n    pos = self.pos\n    start += pos\n    if end is None:\n        index = self.text.find(needle, start)\n    else:\n        end += pos\n        assert end >= start\n        index = self.text.find(needle, start, end)\n    if index != -1:\n        index -= pos\n    return index", "loc": 13}
{"file": "tornado\\tornado\\template.py", "class_name": "_TemplateReader", "function_name": "consume", "parameters": ["self", "count"], "param_types": {"count": "Optional[int]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "self.text.count"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def consume(self, count: Optional[int] = None) -> str:\n    if count is None:\n        count = len(self.text) - self.pos\n    newpos = self.pos + count\n    self.line += self.text.count(\"\\n\", self.pos, newpos)\n    s = self.text[self.pos : newpos]\n    self.pos = newpos\n    return s", "loc": 8}
{"file": "tornado\\tornado\\util.py", "class_name": null, "function_name": "import_object", "parameters": ["name"], "param_types": {"name": "str"}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'.'.join", "ImportError", "__import__", "getattr", "name.count", "name.split"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Imports an object by name. ``import_object('x')`` is equivalent to ``import x``. ``import_object('x.y.z')`` is equivalent to ``from x.y import z``.", "source_code": "def import_object(name: str) -> Any:\n    \"\"\"Imports an object by name.\n\n    ``import_object('x')`` is equivalent to ``import x``.\n    ``import_object('x.y.z')`` is equivalent to ``from x.y import z``.\n\n    >>> import tornado.escape\n    >>> import_object('tornado.escape') is tornado.escape\n    True\n    >>> import_object('tornado.escape.utf8') is tornado.escape.utf8\n    True\n    >>> import_object('tornado') is tornado\n    True\n    >>> import_object('tornado.missing_module')\n    Traceback (most recent call last):\n        ...\n    ImportError: No module named missing_module\n    \"\"\"\n    if name.count(\".\") == 0:\n        return __import__(name)\n\n    parts = name.split(\".\")\n    obj = __import__(\".\".join(parts[:-1]), fromlist=[parts[-1]])\n    try:\n        return getattr(obj, parts[-1])\n    except AttributeError:\n        raise ImportError(\"No module named %s\" % parts[-1])", "loc": 27}
{"file": "tornado\\tornado\\util.py", "class_name": null, "function_name": "exec_in", "parameters": ["code", "glob", "loc"], "param_types": {"code": "Any", "glob": "Dict[str, Any]", "loc": "Optional[Optional[Mapping[str, Any]]]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compile", "exec", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exec_in(\n    code: Any, glob: Dict[str, Any], loc: Optional[Optional[Mapping[str, Any]]] = None\n) -> None:\n    if isinstance(code, str):\n        # exec(string) inherits the caller's future imports; compile\n        # the string first to prevent that.\n        code = compile(code, \"<string>\", \"exec\", dont_inherit=True)\n    exec(code, glob, loc)", "loc": 8}
{"file": "tornado\\tornado\\util.py", "class_name": null, "function_name": "raise_exc_info", "parameters": ["exc_info"], "param_types": {"exc_info": "Tuple[Optional[type], Optional[BaseException], Optional['TracebackType']]"}, "return_type": "typing.NoReturn", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "exc_info[1].with_traceback"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def raise_exc_info(\n    exc_info: Tuple[Optional[type], Optional[BaseException], Optional[\"TracebackType\"]],\n) -> typing.NoReturn:\n    try:\n        if exc_info[1] is not None:\n            raise exc_info[1].with_traceback(exc_info[2])\n        else:\n            raise TypeError(\"raise_exc_info called with no exception\")\n    finally:\n        # Clear the traceback reference from our stack frame to\n        # minimize circular references that slow down GC.\n        exc_info = (None, None, None)", "loc": 12}
{"file": "tornado\\tornado\\util.py", "class_name": null, "function_name": "errno_from_exception", "parameters": ["e"], "param_types": {"e": "BaseException"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Provides the errno from an Exception object. There are cases that the errno attribute was not set so we pull the errno out of the args but if someone instantiates an Exception", "source_code": "def errno_from_exception(e: BaseException) -> Optional[int]:\n    \"\"\"Provides the errno from an Exception object.\n\n    There are cases that the errno attribute was not set so we pull\n    the errno out of the args but if someone instantiates an Exception\n    without any args you will get a tuple error. So this function\n    abstracts all that behavior to give you a safe way to get the\n    errno.\n    \"\"\"\n\n    if hasattr(e, \"errno\"):\n        return e.errno  # type: ignore\n    elif e.args:\n        return e.args[0]\n    else:\n        return None", "loc": 16}
{"file": "tornado\\tornado\\util.py", "class_name": "Configurable", "function_name": "configure", "parameters": ["cls", "impl"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "cls.configurable_base", "import_object", "isinstance", "issubclass", "typing.cast"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the class to use when the base class is instantiated. Keyword arguments will be saved and added to the arguments passed to the constructor.  This can be used to set global defaults for", "source_code": "def configure(cls, impl, **kwargs):\n    # type: (Union[None, str, Type[Configurable]], Any) -> None\n    \"\"\"Sets the class to use when the base class is instantiated.\n\n    Keyword arguments will be saved and added to the arguments passed\n    to the constructor.  This can be used to set global defaults for\n    some parameters.\n    \"\"\"\n    base = cls.configurable_base()\n    if isinstance(impl, str):\n        impl = typing.cast(Type[Configurable], import_object(impl))\n    if impl is not None and not issubclass(impl, cls):\n        raise ValueError(\"Invalid subclass of %s\" % cls)\n    base.__impl_class = impl\n    base.__impl_kwargs = kwargs", "loc": 15}
{"file": "tornado\\tornado\\util.py", "class_name": "Configurable", "function_name": "configured_class", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "base.__dict__.get", "cls.configurable_base", "cls.configurable_default"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def configured_class(cls):\n    # type: () -> Type[Configurable]\n    \"\"\"Returns the currently configured class.\"\"\"\n    base = cls.configurable_base()\n    # Manually mangle the private name to see whether this base\n    # has been configured (and not another base higher in the\n    # hierarchy).\n    if base.__dict__.get(\"_Configurable__impl_class\") is None:\n        base.__impl_class = cls.configurable_default()\n    if base.__impl_class is not None:\n        return base.__impl_class\n    else:\n        # Should be impossible, but mypy wants an explicit check.\n        raise ValueError(\"configured class not found\")", "loc": 14}
{"file": "tornado\\tornado\\util.py", "class_name": "ArgReplacer", "function_name": "get_old_value", "parameters": ["self", "args", "kwargs", "default"], "param_types": {"args": "Sequence[Any]", "kwargs": "Dict[str, Any]", "default": "Any"}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.get", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_old_value(\n    self, args: Sequence[Any], kwargs: Dict[str, Any], default: Any = None\n) -> Any:\n    \"\"\"Returns the old value of the named argument without replacing it.\n\n    Returns ``default`` if the argument is not present.\n    \"\"\"\n    if self.arg_pos is not None and len(args) > self.arg_pos:\n        return args[self.arg_pos]\n    else:\n        return kwargs.get(self.name, default)", "loc": 11}
{"file": "tornado\\tornado\\util.py", "class_name": "ArgReplacer", "function_name": "replace", "parameters": ["self", "new_value", "args", "kwargs"], "param_types": {"new_value": "Any", "args": "Sequence[Any]", "kwargs": "Dict[str, Any]"}, "return_type": "Tuple[Any, Sequence[Any], Dict[str, Any]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.get", "len", "list"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Replace the named argument in ``args, kwargs`` with ``new_value``.", "source_code": "def replace(\n    self, new_value: Any, args: Sequence[Any], kwargs: Dict[str, Any]\n) -> Tuple[Any, Sequence[Any], Dict[str, Any]]:\n    \"\"\"Replace the named argument in ``args, kwargs`` with ``new_value``.\n\n    Returns ``(old_value, args, kwargs)``.  The returned ``args`` and\n    ``kwargs`` objects may not be the same as the input objects, or\n    the input objects may be mutated.\n\n    If the named argument was not found, ``new_value`` will be added\n    to ``kwargs`` and None will be returned as ``old_value``.\n    \"\"\"\n    if self.arg_pos is not None and len(args) > self.arg_pos:\n        # The arg to replace is passed positionally\n        old_value = args[self.arg_pos]\n        args = list(args)  # *args is normally a tuple\n        args[self.arg_pos] = new_value\n    else:\n        # The arg to replace is either omitted or passed by keyword.\n        old_value = kwargs.get(self.name)\n        kwargs[self.name] = new_value\n    return old_value, args, kwargs", "loc": 22}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "stream_request_body", "parameters": ["cls"], "param_types": {"cls": "Type[_RequestHandlerType]"}, "return_type": "Type[_RequestHandlerType]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "issubclass"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Apply to `RequestHandler` subclasses to enable streaming body support. This decorator implies the following changes: * `.HTTPServerRequest.body` is undefined, and body arguments will not", "source_code": "def stream_request_body(cls: Type[_RequestHandlerType]) -> Type[_RequestHandlerType]:\n    \"\"\"Apply to `RequestHandler` subclasses to enable streaming body support.\n\n    This decorator implies the following changes:\n\n    * `.HTTPServerRequest.body` is undefined, and body arguments will not\n      be included in `RequestHandler.get_argument`.\n    * `RequestHandler.prepare` is called when the request headers have been\n      read instead of after the entire body has been read.\n    * The subclass must define a method ``data_received(self, data):``, which\n      will be called zero or more times as data is available.  Note that\n      if the request has an empty body, ``data_received`` may not be called.\n    * ``prepare`` and ``data_received`` may return Futures (such as via\n      ``@gen.coroutine``, in which case the next method will not be called\n      until those futures have completed.\n    * The regular HTTP method (``post``, ``put``, etc) will be called after\n      the entire body has been read.\n\n    See the `file receiver demo <https://github.com/tornadoweb/tornado/tree/stable/demos/file_upload/>`_\n    for example usage.\n    \"\"\"  # noqa: E501\n    if not issubclass(cls, RequestHandler):\n        raise TypeError(\"expected subclass of RequestHandler, got %r\", cls)\n    cls._stream_request_body = True\n    return cls", "loc": 25}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "removeslash", "parameters": ["method"], "param_types": {"method": "Callable[..., Optional[Awaitable[None]]]"}, "return_type": "Callable[..., Optional[Awaitable[None]]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPError", "functools.wraps", "method", "self.redirect", "self.request.path.endswith", "self.request.path.rstrip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Use this decorator to remove trailing slashes from the request path. For example, a request to ``/foo/`` would redirect to ``/foo`` with this decorator. Your request handler mapping should use a regular expression", "source_code": "def removeslash(\n    method: Callable[..., Optional[Awaitable[None]]],\n) -> Callable[..., Optional[Awaitable[None]]]:\n    \"\"\"Use this decorator to remove trailing slashes from the request path.\n\n    For example, a request to ``/foo/`` would redirect to ``/foo`` with this\n    decorator. Your request handler mapping should use a regular expression\n    like ``r'/foo/*'`` in conjunction with using the decorator.\n    \"\"\"\n\n    @functools.wraps(method)\n    def wrapper(  # type: ignore\n        self: RequestHandler, *args, **kwargs\n    ) -> Optional[Awaitable[None]]:\n        if self.request.path.endswith(\"/\"):\n            if self.request.method in (\"GET\", \"HEAD\"):\n                uri = self.request.path.rstrip(\"/\")\n                if uri:  # don't try to redirect '/' to ''\n                    if self.request.query:\n                        uri += \"?\" + self.request.query\n                    self.redirect(uri, permanent=True)\n                    return None\n            else:\n                raise HTTPError(404)\n        return method(self, *args, **kwargs)\n\n    return wrapper", "loc": 27}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "addslash", "parameters": ["method"], "param_types": {"method": "Callable[..., Optional[Awaitable[None]]]"}, "return_type": "Callable[..., Optional[Awaitable[None]]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPError", "functools.wraps", "method", "self.redirect", "self.request.path.endswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Use this decorator to add a missing trailing slash to the request path. For example, a request to ``/foo`` would redirect to ``/foo/`` with this decorator. Your request handler mapping should use a regular expression", "source_code": "def addslash(\n    method: Callable[..., Optional[Awaitable[None]]],\n) -> Callable[..., Optional[Awaitable[None]]]:\n    \"\"\"Use this decorator to add a missing trailing slash to the request path.\n\n    For example, a request to ``/foo`` would redirect to ``/foo/`` with this\n    decorator. Your request handler mapping should use a regular expression\n    like ``r'/foo/?'`` in conjunction with using the decorator.\n    \"\"\"\n\n    @functools.wraps(method)\n    def wrapper(  # type: ignore\n        self: RequestHandler, *args, **kwargs\n    ) -> Optional[Awaitable[None]]:\n        if not self.request.path.endswith(\"/\"):\n            if self.request.method in (\"GET\", \"HEAD\"):\n                uri = self.request.path + \"/\"\n                if self.request.query:\n                    uri += \"?\" + self.request.query\n                self.redirect(uri, permanent=True)\n                return None\n            raise HTTPError(404)\n        return method(self, *args, **kwargs)\n\n    return wrapper", "loc": 25}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "create_signed_value", "parameters": ["secret", "name", "value", "version", "clock", "key_version"], "param_types": {"secret": "_CookieSecretTypes", "name": "str", "value": "Union[str, bytes]", "version": "Optional[int]", "clock": "Optional[Callable[[], float]]", "key_version": "Optional[int]"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "_create_signature_v1", "_create_signature_v2", "b'|'.join", "base64.b64encode", "clock", "format_field", "int", "isinstance", "len", "str", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_signed_value(\n    secret: _CookieSecretTypes,\n    name: str,\n    value: Union[str, bytes],\n    version: Optional[int] = None,\n    clock: Optional[Callable[[], float]] = None,\n    key_version: Optional[int] = None,\n) -> bytes:\n    if version is None:\n        version = DEFAULT_SIGNED_VALUE_VERSION\n    if clock is None:\n        clock = time.time\n\n    timestamp = utf8(str(int(clock())))\n    value = base64.b64encode(utf8(value))\n    if version == 1:\n        assert not isinstance(secret, dict)\n        signature = _create_signature_v1(secret, name, value, timestamp)\n        value = b\"|\".join([value, timestamp, signature])\n        return value\n    elif version == 2:\n        # The v2 format consists of a version number and a series of\n        # length-prefixed fields \"%d:%s\", the last of which is a\n        # signature, all separated by pipes.  All numbers are in\n        # decimal format with no leading zeros.  The signature is an\n        # HMAC-SHA256 of the whole string up to that point, including\n        # the final pipe.\n        #\n        # The fields are:\n        # - format version (i.e. 2; no length prefix)\n        # - key version (integer, default is 0)\n        # - timestamp (integer seconds since epoch)\n        # - name (not encoded; assumed to be ~alphanumeric)\n        # - value (base64-encoded)\n        # - signature (hex-encoded; no length prefix)\n        def format_field(s: Union[str, bytes]) -> bytes:\n            return utf8(\"%d:\" % len(s)) + utf8(s)\n\n        to_sign = b\"|\".join(\n            [\n                b\"2\",\n                format_field(str(key_version or 0)),\n                format_field(timestamp),\n                format_field(name),\n                format_field(value),\n                b\"\",\n            ]\n        )\n\n        if isinstance(secret, dict):\n            assert (\n                key_version is not None\n            ), \"Key version must be set when sign key dict is used\"\n            assert version >= 2, \"Version must be at least 2 for key version support\"\n            secret = secret[key_version]\n\n        signature = _create_signature_v2(secret, to_sign)\n        return to_sign + signature\n    else:\n        raise ValueError(\"Unsupported version %d\" % version)", "loc": 60}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "decode_signed_value", "parameters": ["secret", "name", "value", "max_age_days", "clock", "min_version"], "param_types": {"secret": "_CookieSecretTypes", "name": "str", "value": "Union[None, str, bytes]", "max_age_days": "float", "clock": "Optional[Callable[[], float]]", "min_version": "Optional[int]"}, "return_type": "Optional[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "_decode_signed_value_v1", "_decode_signed_value_v2", "_get_version", "isinstance", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decode_signed_value(\n    secret: _CookieSecretTypes,\n    name: str,\n    value: Union[None, str, bytes],\n    max_age_days: float = 31,\n    clock: Optional[Callable[[], float]] = None,\n    min_version: Optional[int] = None,\n) -> Optional[bytes]:\n    if clock is None:\n        clock = time.time\n    if min_version is None:\n        min_version = DEFAULT_SIGNED_VALUE_MIN_VERSION\n    if min_version > 2:\n        raise ValueError(\"Unsupported min_version %d\" % min_version)\n    if not value:\n        return None\n\n    value = utf8(value)\n    version = _get_version(value)\n\n    if version < min_version:\n        return None\n    if version == 1:\n        assert not isinstance(secret, dict)\n        return _decode_signed_value_v1(secret, name, value, max_age_days, clock)\n    elif version == 2:\n        return _decode_signed_value_v2(secret, name, value, max_age_days, clock)\n    else:\n        return None", "loc": 29}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "get_signature_key_version", "parameters": ["value"], "param_types": {"value": "Union[str, bytes]"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_decode_fields_v2", "_get_version", "utf8"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_signature_key_version(value: Union[str, bytes]) -> Optional[int]:\n    value = utf8(value)\n    version = _get_version(value)\n    if version < 2:\n        return None\n    try:\n        key_version, _, _, _, _ = _decode_fields_v2(value)\n    except ValueError:\n        return None\n\n    return key_version", "loc": 11}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "on_connection_close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_has_stream_request_body", "iostream.StreamClosedError", "self.request._body_future.done", "self.request._body_future.exception", "self.request._body_future.set_exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Called in async handlers if the client closed the connection. Override this to clean up resources associated with long-lived connections.  Note that this method is called only if", "source_code": "def on_connection_close(self) -> None:\n    \"\"\"Called in async handlers if the client closed the connection.\n\n    Override this to clean up resources associated with\n    long-lived connections.  Note that this method is called only if\n    the connection was closed during asynchronous processing; if you\n    need to do cleanup after every request override `on_finish`\n    instead.\n\n    Proxies may keep a connection open for a time (perhaps\n    indefinitely) after the client has gone away, so this method\n    may not be called promptly after the end user closes their\n    connection.\n    \"\"\"\n    if _has_stream_request_body(self.__class__):\n        if not self.request._body_future.done():\n            self.request._body_future.set_exception(iostream.StreamClosedError())\n            self.request._body_future.exception()", "loc": 18}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "add_header", "parameters": ["self", "name", "value"], "param_types": {"name": "str", "value": "_HeaderTypes"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._convert_header_value", "self._headers.add"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Adds the given response header and value. Unlike `set_header`, `add_header` may be called multiple times to return multiple values for the same header.", "source_code": "def add_header(self, name: str, value: _HeaderTypes) -> None:\n    \"\"\"Adds the given response header and value.\n\n    Unlike `set_header`, `add_header` may be called multiple times\n    to return multiple values for the same header.\n    \"\"\"\n    self._headers.add(name, self._convert_header_value(value))", "loc": 7}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "clear_header", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Clears an outgoing header, undoing a previous `set_header` call. Note that this method does not apply to multi-valued headers set by `add_header`.", "source_code": "def clear_header(self, name: str) -> None:\n    \"\"\"Clears an outgoing header, undoing a previous `set_header` call.\n\n    Note that this method does not apply to multi-valued headers\n    set by `add_header`.\n    \"\"\"\n    if name in self._headers:\n        del self._headers[name]", "loc": 8}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_arguments", "parameters": ["self", "name", "strip"], "param_types": {"name": "str", "strip": "bool"}, "return_type": "List[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._get_arguments"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_arguments(self, name: str, strip: bool = True) -> List[str]:\n    \"\"\"Returns a list of the arguments with the given name.\n\n    If the argument is not present, returns an empty list.\n\n    This method searches both the query and body arguments.\n    \"\"\"\n\n    # Make sure `get_arguments` isn't accidentally being called with a\n    # positional argument that's assumed to be a default (like in\n    # `get_argument`.)\n    assert isinstance(strip, bool)\n\n    return self._get_arguments(name, self.request.arguments, strip)", "loc": 14}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "decode_argument", "parameters": ["self", "value", "name"], "param_types": {"value": "bytes", "name": "Optional[str]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Invalid unicode in {}: {!r}'.format", "HTTPError", "_unicode"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Decodes an argument from the request. The argument has been percent-decoded and is now a byte string. By default, this method decodes the argument as utf-8 and returns", "source_code": "def decode_argument(self, value: bytes, name: Optional[str] = None) -> str:\n    \"\"\"Decodes an argument from the request.\n\n    The argument has been percent-decoded and is now a byte string.\n    By default, this method decodes the argument as utf-8 and returns\n    a unicode string, but this may be overridden in subclasses.\n\n    This method is used as a filter for both `get_argument()` and for\n    values extracted from the url and passed to `get()`/`post()`/etc.\n\n    The name of the argument is provided if known, but may be None\n    (e.g. for unnamed groups in the url regex).\n    \"\"\"\n    try:\n        return _unicode(value)\n    except UnicodeDecodeError:\n        raise HTTPError(\n            400, \"Invalid unicode in {}: {!r}\".format(name or \"url\", value[:40])\n        )", "loc": 19}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_cookie", "parameters": ["self", "name", "default"], "param_types": {"name": "str", "default": "Optional[str]"}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cookie(self, name: str, default: Optional[str] = None) -> Optional[str]:\n    \"\"\"Returns the value of the request cookie with the given name.\n\n    If the named cookie is not present, returns ``default``.\n\n    This method only returns cookies that were present in the request.\n    It does not see the outgoing cookies set by `set_cookie` in this\n    handler.\n    \"\"\"\n    if self.request.cookies is not None and name in self.request.cookies:\n        return self.request.cookies[name].value\n    return default", "loc": 12}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "clear_cookie", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TypeError", "datetime.datetime.now", "datetime.timedelta", "self.set_cookie"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Deletes the cookie with the given name. This method accepts the same arguments as `set_cookie`, except for ``expires`` and ``max_age``. Clearing a cookie requires the same", "source_code": "def clear_cookie(self, name: str, **kwargs: Any) -> None:\n    \"\"\"Deletes the cookie with the given name.\n\n    This method accepts the same arguments as `set_cookie`, except for\n    ``expires`` and ``max_age``. Clearing a cookie requires the same\n    ``domain`` and ``path`` arguments as when it was set. In some cases the\n    ``samesite`` and ``secure`` arguments are also required to match. Other\n    arguments are ignored.\n\n    Similar to `set_cookie`, the effect of this method will not be\n    seen until the following request.\n\n    .. versionchanged:: 6.3\n\n       Now accepts all keyword arguments that ``set_cookie`` does.\n       The ``samesite`` and ``secure`` flags have recently become\n       required for clearing ``samesite=\"none\"`` cookies.\n    \"\"\"\n    for excluded_arg in [\"expires\", \"max_age\"]:\n        if excluded_arg in kwargs:\n            raise TypeError(\n                f\"clear_cookie() got an unexpected keyword argument '{excluded_arg}'\"\n            )\n    expires = datetime.datetime.now(datetime.timezone.utc) - datetime.timedelta(\n        days=365\n    )\n    self.set_cookie(name, value=\"\", expires=expires, **kwargs)", "loc": 27}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "clear_all_cookies", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.clear_cookie"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Attempt to delete all the cookies the user sent with this request. See `clear_cookie` for more information on keyword arguments. Due to limitations of the cookie protocol, it is impossible to determine on the", "source_code": "def clear_all_cookies(self, **kwargs: Any) -> None:\n    \"\"\"Attempt to delete all the cookies the user sent with this request.\n\n    See `clear_cookie` for more information on keyword arguments. Due to\n    limitations of the cookie protocol, it is impossible to determine on the\n    server side which values are necessary for the ``domain``, ``path``,\n    ``samesite``, or ``secure`` arguments, this method can only be\n    successful if you consistently use the same values for these arguments\n    when setting cookies.\n\n    Similar to `set_cookie`, the effect of this method will not be seen\n    until the following request.\n\n    .. versionchanged:: 3.2\n\n       Added the ``path`` and ``domain`` parameters.\n\n    .. versionchanged:: 6.3\n\n       Now accepts all keyword arguments that ``set_cookie`` does.\n\n    .. deprecated:: 6.3\n\n       The increasingly complex rules governing cookies have made it\n       impossible for a ``clear_all_cookies`` method to work reliably\n       since all we know about cookies are their names. Applications\n       should generally use ``clear_cookie`` one at a time instead.\n    \"\"\"\n    for name in self.request.cookies:\n        self.clear_cookie(name, **kwargs)", "loc": 30}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "set_signed_cookie", "parameters": ["self", "name", "value", "expires_days", "version"], "param_types": {"name": "str", "value": "Union[str, bytes]", "expires_days": "Optional[float]", "version": "Optional[int]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.create_signed_value", "self.set_cookie"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Signs and timestamps a cookie so it cannot be forged. You must specify the ``cookie_secret`` setting in your Application to use this method. It should be a long, random sequence of bytes", "source_code": "def set_signed_cookie(\n    self,\n    name: str,\n    value: Union[str, bytes],\n    expires_days: Optional[float] = 30,\n    version: Optional[int] = None,\n    **kwargs: Any,\n) -> None:\n    \"\"\"Signs and timestamps a cookie so it cannot be forged.\n\n    You must specify the ``cookie_secret`` setting in your Application\n    to use this method. It should be a long, random sequence of bytes\n    to be used as the HMAC secret for the signature.\n\n    To read a cookie set with this method, use `get_signed_cookie()`.\n\n    Note that the ``expires_days`` parameter sets the lifetime of the\n    cookie in the browser, but is independent of the ``max_age_days``\n    parameter to `get_signed_cookie`.\n    A value of None limits the lifetime to the current browser session.\n\n    Secure cookies may contain arbitrary byte values, not just unicode\n    strings (unlike regular cookies)\n\n    Similar to `set_cookie`, the effect of this method will not be\n    seen until the following request.\n\n    .. versionchanged:: 3.2.1\n\n       Added the ``version`` argument.  Introduced cookie version 2\n       and made it the default.\n\n    .. versionchanged:: 6.3\n\n       Renamed from ``set_secure_cookie`` to ``set_signed_cookie`` to\n       avoid confusion with other uses of \"secure\" in cookie attributes\n       and prefixes. The old name remains as an alias.\n    \"\"\"\n    self.set_cookie(\n        name,\n        self.create_signed_value(name, value, version=version),\n        expires_days=expires_days,\n        **kwargs,\n    )", "loc": 44}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "create_signed_value", "parameters": ["self", "name", "value", "version"], "param_types": {"name": "str", "value": "Union[str, bytes]", "version": "Optional[int]"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "create_signed_value", "isinstance", "self.application.settings.get", "self.require_setting"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Signs and timestamps a string so it cannot be forged. Normally used via set_signed_cookie, but provided as a separate method for non-cookie uses.  To decode a value not stored", "source_code": "def create_signed_value(\n    self, name: str, value: Union[str, bytes], version: Optional[int] = None\n) -> bytes:\n    \"\"\"Signs and timestamps a string so it cannot be forged.\n\n    Normally used via set_signed_cookie, but provided as a separate\n    method for non-cookie uses.  To decode a value not stored\n    as a cookie use the optional value argument to get_signed_cookie.\n\n    .. versionchanged:: 3.2.1\n\n       Added the ``version`` argument.  Introduced cookie version 2\n       and made it the default.\n    \"\"\"\n    self.require_setting(\"cookie_secret\", \"secure cookies\")\n    secret = self.application.settings[\"cookie_secret\"]\n    key_version = None\n    if isinstance(secret, dict):\n        if self.application.settings.get(\"key_version\") is None:\n            raise Exception(\"key_version setting must be used for secret_key dicts\")\n        key_version = self.application.settings[\"key_version\"]\n\n    return create_signed_value(\n        secret, name, value, version=version, key_version=key_version\n    )", "loc": 25}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_signed_cookie", "parameters": ["self", "name", "value", "max_age_days", "min_version"], "param_types": {"name": "str", "value": "Optional[str]", "max_age_days": "float", "min_version": "Optional[int]"}, "return_type": "Optional[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["decode_signed_value", "self.get_cookie", "self.require_setting"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_signed_cookie(\n    self,\n    name: str,\n    value: Optional[str] = None,\n    max_age_days: float = 31,\n    min_version: Optional[int] = None,\n) -> Optional[bytes]:\n    \"\"\"Returns the given signed cookie if it validates, or None.\n\n    The decoded cookie value is returned as a byte string (unlike\n    `get_cookie`).\n\n    Similar to `get_cookie`, this method only returns cookies that\n    were present in the request. It does not see outgoing cookies set by\n    `set_signed_cookie` in this handler.\n\n    .. versionchanged:: 3.2.1\n\n       Added the ``min_version`` argument.  Introduced cookie version 2;\n       both versions 1 and 2 are accepted by default.\n\n     .. versionchanged:: 6.3\n\n       Renamed from ``get_secure_cookie`` to ``get_signed_cookie`` to\n       avoid confusion with other uses of \"secure\" in cookie attributes\n       and prefixes. The old name remains as an alias.\n\n    \"\"\"\n    self.require_setting(\"cookie_secret\", \"secure cookies\")\n    if value is None:\n        value = self.get_cookie(name)\n    return decode_signed_value(\n        self.application.settings[\"cookie_secret\"],\n        name,\n        value,\n        max_age_days=max_age_days,\n        min_version=min_version,\n    )", "loc": 38}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_signed_cookie_key_version", "parameters": ["self", "name", "value"], "param_types": {"name": "str", "value": "Optional[str]"}, "return_type": "Optional[int]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_signature_key_version", "self.get_cookie", "self.require_setting"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_signed_cookie_key_version(\n    self, name: str, value: Optional[str] = None\n) -> Optional[int]:\n    \"\"\"Returns the signing key version of the secure cookie.\n\n    The version is returned as int.\n\n    .. versionchanged:: 6.3\n\n       Renamed from ``get_secure_cookie_key_version`` to\n       ``set_signed_cookie_key_version`` to avoid confusion with other\n       uses of \"secure\" in cookie attributes and prefixes. The old name\n       remains as an alias.\n\n    \"\"\"\n    self.require_setting(\"cookie_secret\", \"secure cookies\")\n    if value is None:\n        value = self.get_cookie(name)\n    if value is None:\n        return None\n    return get_signature_key_version(value)", "loc": 21}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "redirect", "parameters": ["self", "url", "permanent", "status"], "param_types": {"url": "str", "permanent": "bool", "status": "Optional[int]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "isinstance", "self.finish", "self.set_header", "self.set_status", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sends a redirect to the given (optionally relative) URL. If the ``status`` argument is specified, that value is used as the HTTP status code; otherwise either 301 (permanent) or 302", "source_code": "def redirect(\n    self, url: str, permanent: bool = False, status: Optional[int] = None\n) -> None:\n    \"\"\"Sends a redirect to the given (optionally relative) URL.\n\n    If the ``status`` argument is specified, that value is used as the\n    HTTP status code; otherwise either 301 (permanent) or 302\n    (temporary) is chosen based on the ``permanent`` argument.\n    The default is 302 (temporary).\n    \"\"\"\n    if self._headers_written:\n        raise Exception(\"Cannot redirect after headers have been written\")\n    if status is None:\n        status = 301 if permanent else 302\n    else:\n        assert isinstance(status, int) and 300 <= status <= 399\n    self.set_status(status)\n    self.set_header(\"Location\", utf8(url))\n    self.finish()", "loc": 19}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "write", "parameters": ["self", "chunk"], "param_types": {"chunk": "Union[str, bytes, dict]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "TypeError", "escape.json_encode", "isinstance", "self._write_buffer.append", "self.set_header", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Writes the given chunk to the output buffer. To write the output to the network, use the `flush()` method below. If the given chunk is a dictionary, we write it as JSON and set", "source_code": "def write(self, chunk: Union[str, bytes, dict]) -> None:\n    \"\"\"Writes the given chunk to the output buffer.\n\n    To write the output to the network, use the `flush()` method below.\n\n    If the given chunk is a dictionary, we write it as JSON and set\n    the Content-Type of the response to be ``application/json``.\n    (if you want to send JSON as a different ``Content-Type``, call\n    ``set_header`` *after* calling ``write()``).\n\n    Note that lists are not converted to JSON because of a potential\n    cross-site security vulnerability.  All JSON output should be\n    wrapped in a dictionary.  More details at\n    http://haacked.com/archive/2009/06/25/json-hijacking.aspx/ and\n    https://github.com/facebook/tornado/issues/1009\n    \"\"\"\n    if self._finished:\n        raise RuntimeError(\"Cannot write() after finish()\")\n    if not isinstance(chunk, (bytes, unicode_type, dict)):\n        message = \"write() only accepts bytes, unicode, and dict objects\"\n        if isinstance(chunk, list):\n            message += (\n                \". Lists not accepted for security reasons; see \"\n                + \"http://www.tornadoweb.org/en/stable/web.html#tornado.web.RequestHandler.write\"  # noqa: E501\n            )\n        raise TypeError(message)\n    if isinstance(chunk, dict):\n        chunk = escape.json_encode(chunk)\n        self.set_header(\"Content-Type\", \"application/json; charset=UTF-8\")\n    chunk = utf8(chunk)\n    self._write_buffer.append(chunk)", "loc": 31}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "render", "parameters": ["self", "template_name"], "param_types": {"template_name": "str"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "_unicode", "b''.join", "css_embed.append", "css_files.append", "css_files.extend", "getattr", "getattr(self, '_active_modules', {}).values", "html.index", "html.rindex", "html_bodies.append", "html_heads.append", "isinstance", "js_embed.append", "js_files.append", "js_files.extend", "module.css_files", "module.embedded_css", "module.embedded_javascript", "module.html_body", "module.html_head", "module.javascript_files", "self.finish", "self.render_embed_css", "self.render_embed_js", "self.render_linked_css", "self.render_linked_js", "self.render_string", "utf8"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Renders the template with the given arguments as the response. ``render()`` calls ``finish()``, so no other output methods can be called after it.", "source_code": "def render(self, template_name: str, **kwargs: Any) -> \"Future[None]\":\n    \"\"\"Renders the template with the given arguments as the response.\n\n    ``render()`` calls ``finish()``, so no other output methods can be called\n    after it.\n\n    Returns a `.Future` with the same semantics as the one returned by `finish`.\n    Awaiting this `.Future` is optional.\n\n    .. versionchanged:: 5.1\n\n       Now returns a `.Future` instead of ``None``.\n    \"\"\"\n    if self._finished:\n        raise RuntimeError(\"Cannot render() after finish()\")\n    html = self.render_string(template_name, **kwargs)\n\n    # Insert the additional JS and CSS added by the modules on the page\n    js_embed = []\n    js_files = []\n    css_embed = []\n    css_files = []\n    html_heads = []\n    html_bodies = []\n    for module in getattr(self, \"_active_modules\", {}).values():\n        embed_part = module.embedded_javascript()\n        if embed_part:\n            js_embed.append(utf8(embed_part))\n        file_part = module.javascript_files()\n        if file_part:\n            if isinstance(file_part, (unicode_type, bytes)):\n                js_files.append(_unicode(file_part))\n            else:\n                js_files.extend(file_part)\n        embed_part = module.embedded_css()\n        if embed_part:\n            css_embed.append(utf8(embed_part))\n        file_part = module.css_files()\n        if file_part:\n            if isinstance(file_part, (unicode_type, bytes)):\n                css_files.append(_unicode(file_part))\n            else:\n                css_files.extend(file_part)\n        head_part = module.html_head()\n        if head_part:\n            html_heads.append(utf8(head_part))\n        body_part = module.html_body()\n        if body_part:\n            html_bodies.append(utf8(body_part))\n\n    if js_files:\n        # Maintain order of JavaScript files given by modules\n        js = self.render_linked_js(js_files)\n        sloc = html.rindex(b\"</body>\")\n        html = html[:sloc] + utf8(js) + b\"\\n\" + html[sloc:]\n    if js_embed:\n        js_bytes = self.render_embed_js(js_embed)\n        sloc = html.rindex(b\"</body>\")\n        html = html[:sloc] + js_bytes + b\"\\n\" + html[sloc:]\n    if css_files:\n        css = self.render_linked_css(css_files)\n        hloc = html.index(b\"</head>\")\n        html = html[:hloc] + utf8(css) + b\"\\n\" + html[hloc:]\n    if css_embed:\n        css_bytes = self.render_embed_css(css_embed)\n        hloc = html.index(b\"</head>\")\n        html = html[:hloc] + css_bytes + b\"\\n\" + html[hloc:]\n    if html_heads:\n        hloc = html.index(b\"</head>\")\n        html = html[:hloc] + b\"\".join(html_heads) + b\"\\n\" + html[hloc:]\n    if html_bodies:\n        hloc = html.index(b\"</body>\")\n        html = html[:hloc] + b\"\".join(html_bodies) + b\"\\n\" + html[hloc:]\n    return self.finish(html)", "loc": 74}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "render_linked_js", "parameters": ["self", "js_files"], "param_types": {"js_files": "Iterable[str]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "escape.xhtml_escape", "is_absolute", "paths.append", "self.static_url", "set", "unique_paths.add"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Default method used to render the final js links for the rendered webpage. Override this method in a sub-classed controller to change the output.", "source_code": "def render_linked_js(self, js_files: Iterable[str]) -> str:\n    \"\"\"Default method used to render the final js links for the\n    rendered webpage.\n\n    Override this method in a sub-classed controller to change the output.\n    \"\"\"\n    paths = []\n    unique_paths = set()  # type: Set[str]\n\n    for path in js_files:\n        if not is_absolute(path):\n            path = self.static_url(path)\n        if path not in unique_paths:\n            paths.append(path)\n            unique_paths.add(path)\n\n    return \"\".join(\n        '<script src=\"'\n        + escape.xhtml_escape(p)\n        + '\" type=\"text/javascript\"></script>'\n        for p in paths\n    )", "loc": 22}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "render_linked_css", "parameters": ["self", "css_files"], "param_types": {"css_files": "Iterable[str]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "escape.xhtml_escape", "is_absolute", "paths.append", "self.static_url", "set", "unique_paths.add"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Default method used to render the final css links for the rendered webpage. Override this method in a sub-classed controller to change the output.", "source_code": "def render_linked_css(self, css_files: Iterable[str]) -> str:\n    \"\"\"Default method used to render the final css links for the\n    rendered webpage.\n\n    Override this method in a sub-classed controller to change the output.\n    \"\"\"\n    paths = []\n    unique_paths = set()  # type: Set[str]\n\n    for path in css_files:\n        if not is_absolute(path):\n            path = self.static_url(path)\n        if path not in unique_paths:\n            paths.append(path)\n            unique_paths.add(path)\n\n    return \"\".join(\n        '<link href=\"' + escape.xhtml_escape(p) + '\" '\n        'type=\"text/css\" rel=\"stylesheet\"/>'\n        for p in paths\n    )", "loc": 21}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_template_namespace", "parameters": ["self"], "param_types": {}, "return_type": "Dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "namespace.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_template_namespace(self) -> Dict[str, Any]:\n    \"\"\"Returns a dictionary to be used as the default template namespace.\n\n    May be overridden by subclasses to add or modify values.\n\n    The results of this method will be combined with additional\n    defaults in the `tornado.template` module and keyword arguments\n    to `render` or `render_string`.\n    \"\"\"\n    namespace = dict(\n        handler=self,\n        request=self.request,\n        current_user=self.current_user,\n        locale=self.locale,\n        _=self.locale.translate,\n        pgettext=self.locale.pgettext,\n        static_url=self.static_url,\n        xsrf_form_html=self.xsrf_form_html,\n        reverse_url=self.reverse_url,\n    )\n    namespace.update(self.ui)\n    return namespace", "loc": 22}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "create_template_loader", "parameters": ["self", "template_path"], "param_types": {"template_path": "str"}, "return_type": "template.BaseLoader", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["template.Loader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_template_loader(self, template_path: str) -> template.BaseLoader:\n    \"\"\"Returns a new template loader for the given path.\n\n    May be overridden by subclasses.  By default returns a\n    directory-based loader on the given path, using the\n    ``autoescape`` and ``template_whitespace`` application\n    settings.  If a ``template_loader`` application setting is\n    supplied, uses that instead.\n    \"\"\"\n    settings = self.application.settings\n    if \"template_loader\" in settings:\n        return settings[\"template_loader\"]\n    kwargs = {}\n    if \"autoescape\" in settings:\n        # autoescape=None means \"no escaping\", so we have to be sure\n        # to only pass this kwarg if the user asked for it.\n        kwargs[\"autoescape\"] = settings[\"autoescape\"]\n    if \"template_whitespace\" in settings:\n        kwargs[\"whitespace\"] = settings[\"template_whitespace\"]\n    return template.Loader(template_path, **kwargs)", "loc": 20}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "finish", "parameters": ["self", "chunk"], "param_types": {"chunk": "Optional[Union[str, bytes, dict]]"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RuntimeError", "len", "self._break_cycles", "self._clear_representation_headers", "self._log", "self.check_etag_header", "self.flush", "self.on_finish", "self.request.connection.finish", "self.request.connection.set_close_callback", "self.set_etag_header", "self.set_header", "self.set_status", "self.write", "sum"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Finishes this response, ending the HTTP request. Passing a ``chunk`` to ``finish()`` is equivalent to passing that chunk to ``write()`` and then calling ``finish()`` with no arguments.", "source_code": "def finish(self, chunk: Optional[Union[str, bytes, dict]] = None) -> \"Future[None]\":\n    \"\"\"Finishes this response, ending the HTTP request.\n\n    Passing a ``chunk`` to ``finish()`` is equivalent to passing that\n    chunk to ``write()`` and then calling ``finish()`` with no arguments.\n\n    Returns a `.Future` which may optionally be awaited to track the sending\n    of the response to the client. This `.Future` resolves when all the response\n    data has been sent, and raises an error if the connection is closed before all\n    data can be sent.\n\n    .. versionchanged:: 5.1\n\n       Now returns a `.Future` instead of ``None``.\n    \"\"\"\n    if self._finished:\n        raise RuntimeError(\"finish() called twice\")\n\n    if chunk is not None:\n        self.write(chunk)\n\n    # Automatically support ETags and add the Content-Length header if\n    # we have not flushed any content yet.\n    if not self._headers_written:\n        if (\n            self._status_code == 200\n            and self.request.method in (\"GET\", \"HEAD\")\n            and \"Etag\" not in self._headers\n        ):\n            self.set_etag_header()\n            if self.check_etag_header():\n                self._write_buffer = []\n                self.set_status(304)\n        if self._status_code in (204, 304) or (100 <= self._status_code < 200):\n            assert not self._write_buffer, (\n                \"Cannot send body with %s\" % self._status_code\n            )\n            self._clear_representation_headers()\n        elif \"Content-Length\" not in self._headers:\n            content_length = sum(len(part) for part in self._write_buffer)\n            self.set_header(\"Content-Length\", content_length)\n\n    assert self.request.connection is not None\n    # Now that the request is finished, clear the callback we\n    # set on the HTTPConnection (which would otherwise prevent the\n    # garbage collection of the RequestHandler when there\n    # are keepalive connections)\n    self.request.connection.set_close_callback(None)  # type: ignore\n\n    future = self.flush(include_footers=True)\n    self.request.connection.finish()\n    self._log()\n    self._finished = True\n    self.on_finish()\n    self._break_cycles()\n    return future", "loc": 56}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "send_error", "parameters": ["self", "status_code"], "param_types": {"status_code": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.error", "gen_log.error", "isinstance", "kwargs.get", "self.clear", "self.finish", "self.set_status", "self.write_error"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Sends the given HTTP error code to the browser. If `flush()` has already been called, it is not possible to send an error, so this method will simply terminate the response.", "source_code": "def send_error(self, status_code: int = 500, **kwargs: Any) -> None:\n    \"\"\"Sends the given HTTP error code to the browser.\n\n    If `flush()` has already been called, it is not possible to send\n    an error, so this method will simply terminate the response.\n    If output has been written but not yet flushed, it will be discarded\n    and replaced with the error page.\n\n    Override `write_error()` to customize the error page that is returned.\n    Additional keyword arguments are passed through to `write_error`.\n    \"\"\"\n    if self._headers_written:\n        gen_log.error(\"Cannot send error response after headers written\")\n        if not self._finished:\n            # If we get an error between writing headers and finishing,\n            # we are unlikely to be able to finish due to a\n            # Content-Length mismatch. Try anyway to release the\n            # socket.\n            try:\n                self.finish()\n            except Exception:\n                gen_log.error(\"Failed to flush partial response\", exc_info=True)\n        return\n    self.clear()\n\n    reason = kwargs.get(\"reason\")\n    if \"exc_info\" in kwargs:\n        exception = kwargs[\"exc_info\"][1]\n        if isinstance(exception, HTTPError) and exception.reason:\n            reason = exception.reason\n    self.set_status(status_code, reason=reason)\n    try:\n        self.write_error(status_code, **kwargs)\n    except Exception:\n        app_log.error(\"Uncaught exception in write_error\", exc_info=True)\n    if not self._finished:\n        self.finish()", "loc": 37}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "write_error", "parameters": ["self", "status_code"], "param_types": {"status_code": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.finish", "self.set_header", "self.settings.get", "self.write", "traceback.format_exception"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Override to implement custom error pages. ``write_error`` may call `write`, `render`, `set_header`, etc to produce output as usual.", "source_code": "def write_error(self, status_code: int, **kwargs: Any) -> None:\n    \"\"\"Override to implement custom error pages.\n\n    ``write_error`` may call `write`, `render`, `set_header`, etc\n    to produce output as usual.\n\n    If this error was caused by an uncaught exception (including\n    HTTPError), an ``exc_info`` triple will be available as\n    ``kwargs[\"exc_info\"]``.  Note that this exception may not be\n    the \"current\" exception for purposes of methods like\n    ``sys.exc_info()`` or ``traceback.format_exc``.\n    \"\"\"\n    if self.settings.get(\"serve_traceback\") and \"exc_info\" in kwargs:\n        # in debug mode, try to send a traceback\n        self.set_header(\"Content-Type\", \"text/plain\")\n        for line in traceback.format_exception(*kwargs[\"exc_info\"]):\n            self.write(line)\n        self.finish()\n    else:\n        self.finish(\n            \"<html><title>%(code)d: %(message)s</title>\"\n            \"<body>%(code)d: %(message)s</body></html>\"\n            % {\"code\": status_code, \"message\": self._reason}\n        )", "loc": 24}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "locale", "parameters": ["self"], "param_types": {}, "return_type": "tornado.locale.Locale", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self.get_browser_locale", "self.get_user_locale"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "The locale for the current session. Determined by either `get_user_locale`, which you can override to set the locale based on, e.g., a user preference stored in a", "source_code": "def locale(self) -> tornado.locale.Locale:\n    \"\"\"The locale for the current session.\n\n    Determined by either `get_user_locale`, which you can override to\n    set the locale based on, e.g., a user preference stored in a\n    database, or `get_browser_locale`, which uses the ``Accept-Language``\n    header.\n\n    .. versionchanged: 4.1\n       Added a property setter.\n    \"\"\"\n    if not hasattr(self, \"_locale\"):\n        loc = self.get_user_locale()\n        if loc is not None:\n            self._locale = loc\n        else:\n            self._locale = self.get_browser_locale()\n            assert self._locale\n    return self._locale", "loc": 19}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "get_browser_locale", "parameters": ["self", "default"], "param_types": {"default": "str"}, "return_type": "tornado.locale.Locale", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "float", "language.strip", "language.strip().split", "len", "locale.get", "locales.append", "locales.sort", "parts[1].strip", "parts[1].strip().startswith", "self.request.headers['Accept-Language'].split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Determines the user's locale from ``Accept-Language`` header. See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4", "source_code": "def get_browser_locale(self, default: str = \"en_US\") -> tornado.locale.Locale:\n    \"\"\"Determines the user's locale from ``Accept-Language`` header.\n\n    See http://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4\n    \"\"\"\n    if \"Accept-Language\" in self.request.headers:\n        languages = self.request.headers[\"Accept-Language\"].split(\",\")\n        locales = []\n        for language in languages:\n            parts = language.strip().split(\";\")\n            if len(parts) > 1 and parts[1].strip().startswith(\"q=\"):\n                try:\n                    score = float(parts[1].strip()[2:])\n                    if score < 0:\n                        raise ValueError()\n                except (ValueError, TypeError):\n                    score = 0.0\n            else:\n                score = 1.0\n            if score > 0:\n                locales.append((parts[0], score))\n        if locales:\n            locales.sort(key=lambda pair: pair[1], reverse=True)\n            codes = [loc[0] for loc in locales]\n            return locale.get(*codes)\n    return locale.get(default)", "loc": 26}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "current_user", "parameters": ["self"], "param_types": {}, "return_type": "Any", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self.get_current_user"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "The authenticated user for this request. This is set in one of two ways: * A subclass may override `get_current_user()`, which will be called", "source_code": "def current_user(self) -> Any:\n    \"\"\"The authenticated user for this request.\n\n    This is set in one of two ways:\n\n    * A subclass may override `get_current_user()`, which will be called\n      automatically the first time ``self.current_user`` is accessed.\n      `get_current_user()` will only be called once per request,\n      and is cached for future access::\n\n          def get_current_user(self):\n              user_cookie = self.get_signed_cookie(\"user\")\n              if user_cookie:\n                  return json.loads(user_cookie)\n              return None\n\n    * It may be set as a normal variable, typically from an overridden\n      `prepare()`::\n\n          @gen.coroutine\n          def prepare(self):\n              user_id_cookie = self.get_signed_cookie(\"user_id\")\n              if user_id_cookie:\n                  self.current_user = yield load_user(user_id_cookie)\n\n    Note that `prepare()` may be a coroutine while `get_current_user()`\n    may not, so the latter form is necessary if loading the user requires\n    asynchronous operations.\n\n    The user object may be any type of the application's choosing.\n    \"\"\"\n    if not hasattr(self, \"_current_user\"):\n        self._current_user = self.get_current_user()\n    return self._current_user", "loc": 34}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "xsrf_token", "parameters": ["self"], "param_types": {}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "_websocket_mask", "b'|'.join", "binascii.b2a_hex", "hasattr", "int", "os.urandom", "self._get_raw_xsrf_token", "self.set_cookie", "self.settings.get", "str", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "The XSRF-prevention token for the current user/session. To prevent cross-site request forgery, we set an '_xsrf' cookie and include the same '_xsrf' value as an argument with all POST", "source_code": "def xsrf_token(self) -> bytes:\n    \"\"\"The XSRF-prevention token for the current user/session.\n\n    To prevent cross-site request forgery, we set an '_xsrf' cookie\n    and include the same '_xsrf' value as an argument with all POST\n    requests. If the two do not match, we reject the form submission\n    as a potential forgery.\n\n    See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n    This property is of type `bytes`, but it contains only ASCII\n    characters. If a character string is required, there is no\n    need to base64-encode it; just decode the byte string as\n    UTF-8.\n\n    .. versionchanged:: 3.2.2\n       The xsrf token will now be have a random mask applied in every\n       request, which makes it safe to include the token in pages\n       that are compressed.  See http://breachattack.com for more\n       information on the issue fixed by this change.  Old (version 1)\n       cookies will be converted to version 2 when this method is called\n       unless the ``xsrf_cookie_version`` `Application` setting is\n       set to 1.\n\n    .. versionchanged:: 4.3\n       The ``xsrf_cookie_kwargs`` `Application` setting may be\n       used to supply additional cookie options (which will be\n       passed directly to `set_cookie`). For example,\n       ``xsrf_cookie_kwargs=dict(httponly=True, secure=True)``\n       will set the ``secure`` and ``httponly`` flags on the\n       ``_xsrf`` cookie.\n    \"\"\"\n    if not hasattr(self, \"_xsrf_token\"):\n        version, token, timestamp = self._get_raw_xsrf_token()\n        output_version = self.settings.get(\"xsrf_cookie_version\", 2)\n        cookie_kwargs = self.settings.get(\"xsrf_cookie_kwargs\", {})\n        if output_version == 1:\n            self._xsrf_token = binascii.b2a_hex(token)\n        elif output_version == 2:\n            mask = os.urandom(4)\n            self._xsrf_token = b\"|\".join(\n                [\n                    b\"2\",\n                    binascii.b2a_hex(mask),\n                    binascii.b2a_hex(_websocket_mask(mask, token)),\n                    utf8(str(int(timestamp))),\n                ]\n            )\n        else:\n            raise ValueError(\"unknown xsrf cookie version %d\", output_version)\n        if version is None:\n            if self.current_user and \"expires_days\" not in cookie_kwargs:\n                cookie_kwargs[\"expires_days\"] = 30\n            cookie_name = self.settings.get(\"xsrf_cookie_name\", \"_xsrf\")\n            self.set_cookie(cookie_name, self._xsrf_token, **cookie_kwargs)\n    return self._xsrf_token", "loc": 56}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "check_xsrf_cookie", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPError", "hmac.compare_digest", "self._decode_xsrf_token", "self._get_raw_xsrf_token", "self.get_argument", "self.request.headers.get", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument. To prevent cross-site request forgery, we set an ``_xsrf`` cookie and include the same value as a non-cookie", "source_code": "def check_xsrf_cookie(self) -> None:\n    \"\"\"Verifies that the ``_xsrf`` cookie matches the ``_xsrf`` argument.\n\n    To prevent cross-site request forgery, we set an ``_xsrf``\n    cookie and include the same value as a non-cookie\n    field with all ``POST`` requests. If the two do not match, we\n    reject the form submission as a potential forgery.\n\n    The ``_xsrf`` value may be set as either a form field named ``_xsrf``\n    or in a custom HTTP header named ``X-XSRFToken`` or ``X-CSRFToken``\n    (the latter is accepted for compatibility with Django).\n\n    See http://en.wikipedia.org/wiki/Cross-site_request_forgery\n\n    .. versionchanged:: 3.2.2\n       Added support for cookie version 2.  Both versions 1 and 2 are\n       supported.\n    \"\"\"\n    # Prior to release 1.1.1, this check was ignored if the HTTP header\n    # ``X-Requested-With: XMLHTTPRequest`` was present.  This exception\n    # has been shown to be insecure and has been removed.  For more\n    # information please see\n    # http://www.djangoproject.com/weblog/2011/feb/08/security/\n    # http://weblog.rubyonrails.org/2011/2/8/csrf-protection-bypass-in-ruby-on-rails\n    input_token = (\n        self.get_argument(\"_xsrf\", None)\n        or self.request.headers.get(\"X-Xsrftoken\")\n        or self.request.headers.get(\"X-Csrftoken\")\n    )\n    if not input_token:\n        raise HTTPError(403, \"'_xsrf' argument missing from POST\")\n    _, token, _ = self._decode_xsrf_token(input_token)\n    _, expected_token, _ = self._get_raw_xsrf_token()\n    if not token:\n        raise HTTPError(403, \"'_xsrf' argument has invalid format\")\n    if not hmac.compare_digest(utf8(token), utf8(expected_token)):\n        raise HTTPError(403, \"XSRF cookie does not match POST argument\")", "loc": 37}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "static_url", "parameters": ["self", "path", "include_host"], "param_types": {"path": "str", "include_host": "Optional[bool]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_url", "getattr", "self.require_setting", "self.settings.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def static_url(\n    self, path: str, include_host: Optional[bool] = None, **kwargs: Any\n) -> str:\n    \"\"\"Returns a static URL for the given relative static file path.\n\n    This method requires you set the ``static_path`` setting in your\n    application (which specifies the root directory of your static\n    files).\n\n    This method returns a versioned url (by default appending\n    ``?v=<signature>``), which allows the static files to be\n    cached indefinitely.  This can be disabled by passing\n    ``include_version=False`` (in the default implementation;\n    other static file implementations are not required to support\n    this, but they may support other options).\n\n    By default this method returns URLs relative to the current\n    host, but if ``include_host`` is true the URL returned will be\n    absolute.  If this handler has an ``include_host`` attribute,\n    that value will be used as the default for all `static_url`\n    calls that do not pass ``include_host`` as a keyword argument.\n\n    \"\"\"\n    self.require_setting(\"static_path\", \"static_url\")\n    get_url = self.settings.get(\n        \"static_handler_class\", StaticFileHandler\n    ).make_static_url\n\n    if include_host is None:\n        include_host = getattr(self, \"include_host\", False)\n\n    if include_host:\n        base = self.request.protocol + \"://\" + self.request.host\n    else:\n        base = \"\"\n\n    return base + get_url(self.settings, path, **kwargs)", "loc": 37}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "require_setting", "parameters": ["self", "name", "feature"], "param_types": {"name": "str", "feature": "str"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "self.application.settings.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def require_setting(self, name: str, feature: str = \"this feature\") -> None:\n    \"\"\"Raises an exception if the given app setting is not defined.\"\"\"\n    if not self.application.settings.get(name):\n        raise Exception(\n            \"You must define the '%s' setting in your \"\n            \"application to use %s\" % (name, feature)\n        )", "loc": 7}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "compute_etag", "parameters": ["self"], "param_types": {}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasher.hexdigest", "hasher.update", "hashlib.sha1"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Computes the etag header to be used for this request. By default uses a hash of the content written so far. May be overridden to provide custom etag implementations,", "source_code": "def compute_etag(self) -> Optional[str]:\n    \"\"\"Computes the etag header to be used for this request.\n\n    By default uses a hash of the content written so far.\n\n    May be overridden to provide custom etag implementations,\n    or may return None to disable tornado's default etag support.\n    \"\"\"\n    hasher = hashlib.sha1()\n    for part in self._write_buffer:\n        hasher.update(part)\n    return '\"%s\"' % hasher.hexdigest()", "loc": 12}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "set_etag_header", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.compute_etag", "self.set_header"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the response's Etag header using ``self.compute_etag()``. Note: no header will be set if ``compute_etag()`` returns ``None``. This method is called automatically when the request is finished.", "source_code": "def set_etag_header(self) -> None:\n    \"\"\"Sets the response's Etag header using ``self.compute_etag()``.\n\n    Note: no header will be set if ``compute_etag()`` returns ``None``.\n\n    This method is called automatically when the request is finished.\n    \"\"\"\n    etag = self.compute_etag()\n    if etag is not None:\n        self.set_header(\"Etag\", etag)", "loc": 10}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "check_etag_header", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "self._headers.get", "self.request.headers.get", "utf8", "val", "x.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Checks the ``Etag`` header against requests's ``If-None-Match``.", "source_code": "def check_etag_header(self) -> bool:\n    \"\"\"Checks the ``Etag`` header against requests's ``If-None-Match``.\n\n    Returns ``True`` if the request's Etag matches and a 304 should be\n    returned. For example::\n\n        self.set_etag_header()\n        if self.check_etag_header():\n            self.set_status(304)\n            return\n\n    This method is called automatically when the request is finished,\n    but may be called earlier for applications that override\n    `compute_etag` and want to do an early check for ``If-None-Match``\n    before completing the request.  The ``Etag`` header should be set\n    (perhaps with `set_etag_header`) before calling this method.\n    \"\"\"\n    computed_etag = utf8(self._headers.get(\"Etag\", \"\"))\n    # Find all weak and strong etag values from If-None-Match header\n    # because RFC 7232 allows multiple etag values in a single header.\n    etags = re.findall(\n        rb'\\*|(?:W/)?\"[^\"]*\"', utf8(self.request.headers.get(\"If-None-Match\", \"\"))\n    )\n    if not computed_etag or not etags:\n        return False\n\n    match = False\n    if etags[0] == b\"*\":\n        match = True\n    else:\n        # Use a weak comparison when comparing entity-tags.\n        def val(x: bytes) -> bytes:\n            return x[2:] if x.startswith(b\"W/\") else x\n\n        for etag in etags:\n            if val(etag) == val(computed_etag):\n                match = True\n                break\n    return match", "loc": 39}
{"file": "tornado\\tornado\\web.py", "class_name": "RequestHandler", "function_name": "log_exception", "parameters": ["self", "typ", "value", "tb"], "param_types": {"typ": "'Optional[Type[BaseException]]'", "value": "Optional[BaseException]", "tb": "Optional[TracebackType]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["app_log.error", "gen_log.warning", "isinstance", "self._request_summary", "value.get_message"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Override to customize logging of uncaught exceptions. By default logs instances of `HTTPError` as warnings without stack traces (on the ``tornado.general`` logger), and all", "source_code": "def log_exception(\n    self,\n    typ: \"Optional[Type[BaseException]]\",\n    value: Optional[BaseException],\n    tb: Optional[TracebackType],\n) -> None:\n    \"\"\"Override to customize logging of uncaught exceptions.\n\n    By default logs instances of `HTTPError` as warnings without\n    stack traces (on the ``tornado.general`` logger), and all\n    other exceptions as errors with stack traces (on the\n    ``tornado.application`` logger).\n\n    .. versionadded:: 3.1\n    \"\"\"\n    if isinstance(value, HTTPError):\n        log_message = value.get_message()\n        if log_message:\n            format = \"%d %s: %s\"\n            args = [value.status_code, self._request_summary(), log_message]\n            gen_log.warning(format, *args)\n    else:\n        app_log.error(\n            \"Uncaught exception %s\\n%r\",\n            self._request_summary(),\n            self.request,\n            exc_info=(typ, value, tb),  # type: ignore\n        )", "loc": 28}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "wrapper", "parameters": ["self"], "param_types": {"self": "RequestHandler"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPError", "functools.wraps", "method", "self.redirect", "self.request.path.endswith", "self.request.path.rstrip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(  # type: ignore\n    self: RequestHandler, *args, **kwargs\n) -> Optional[Awaitable[None]]:\n    if self.request.path.endswith(\"/\"):\n        if self.request.method in (\"GET\", \"HEAD\"):\n            uri = self.request.path.rstrip(\"/\")\n            if uri:  # don't try to redirect '/' to ''\n                if self.request.query:\n                    uri += \"?\" + self.request.query\n                self.redirect(uri, permanent=True)\n                return None\n        else:\n            raise HTTPError(404)\n    return method(self, *args, **kwargs)", "loc": 14}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "wrapper", "parameters": ["self"], "param_types": {"self": "RequestHandler"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPError", "functools.wraps", "method", "self.redirect", "self.request.path.endswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(  # type: ignore\n    self: RequestHandler, *args, **kwargs\n) -> Optional[Awaitable[None]]:\n    if not self.request.path.endswith(\"/\"):\n        if self.request.method in (\"GET\", \"HEAD\"):\n            uri = self.request.path + \"/\"\n            if self.request.query:\n                uri += \"?\" + self.request.query\n            self.redirect(uri, permanent=True)\n            return None\n        raise HTTPError(404)\n    return method(self, *args, **kwargs)", "loc": 12}
{"file": "tornado\\tornado\\web.py", "class_name": "_ApplicationRouter", "function_name": "process_rule", "parameters": ["self", "rule"], "param_types": {"rule": "Rule"}, "return_type": "Rule", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_ApplicationRouter", "isinstance", "super", "super().process_rule"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_rule(self, rule: Rule) -> Rule:\n    rule = super().process_rule(rule)\n\n    if isinstance(rule.target, (list, tuple)):\n        rule.target = _ApplicationRouter(\n            self.application, rule.target  # type: ignore\n        )\n\n    return rule", "loc": 9}
{"file": "tornado\\tornado\\web.py", "class_name": "_ApplicationRouter", "function_name": "get_target_delegate", "parameters": ["self", "target", "request"], "param_types": {"target": "Any", "request": "httputil.HTTPServerRequest"}, "return_type": "Optional[httputil.HTTPMessageDelegate]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isclass", "issubclass", "self.application.get_handler_delegate", "super", "super().get_target_delegate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_target_delegate(\n    self, target: Any, request: httputil.HTTPServerRequest, **target_params: Any\n) -> Optional[httputil.HTTPMessageDelegate]:\n    if isclass(target) and issubclass(target, RequestHandler):\n        return self.application.get_handler_delegate(\n            request, target, **target_params\n        )\n\n    return super().get_target_delegate(target, request, **target_params)", "loc": 9}
{"file": "tornado\\tornado\\web.py", "class_name": "Application", "function_name": "listen", "parameters": ["self", "port", "address"], "param_types": {"port": "int", "address": "Optional[str]"}, "return_type": "HTTPServer", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTTPServer", "server.listen"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Starts an HTTP server for this application on the given port. This is a convenience alias for creating an `.HTTPServer` object and calling its listen method.  Keyword arguments not supported by `HTTPServer.listen <.TCPServer.listen>` are passed to the `.HTTPServer` constructor.  For advanced uses (e.g. multi-process mode), do not use this method; create an `.HTTPServer` and call its `.TCPServer.bind`/`.TCPServer.start` methods directly. Note that after calling this method you still need to call ``IOLoop.current().start()`` (or run within ``asyncio.run``) to start the server.", "source_code": "def listen(\n    self,\n    port: int,\n    address: Optional[str] = None,\n    *,\n    family: socket.AddressFamily = socket.AF_UNSPEC,\n    backlog: int = tornado.netutil._DEFAULT_BACKLOG,\n    flags: Optional[int] = None,\n    reuse_port: bool = False,\n    **kwargs: Any,\n) -> HTTPServer:\n    \"\"\"Starts an HTTP server for this application on the given port.\n\n    This is a convenience alias for creating an `.HTTPServer` object and\n    calling its listen method.  Keyword arguments not supported by\n    `HTTPServer.listen <.TCPServer.listen>` are passed to the `.HTTPServer`\n    constructor.  For advanced uses (e.g. multi-process mode), do not use\n    this method; create an `.HTTPServer` and call its\n    `.TCPServer.bind`/`.TCPServer.start` methods directly.\n\n    Note that after calling this method you still need to call\n    ``IOLoop.current().start()`` (or run within ``asyncio.run``) to start\n    the server.\n\n    Returns the `.HTTPServer` object.\n\n    .. versionchanged:: 4.3\n       Now returns the `.HTTPServer` object.\n\n    .. versionchanged:: 6.2\n       Added support for new keyword arguments in `.TCPServer.listen`,\n       including ``reuse_port``.\n    \"\"\"\n    server = HTTPServer(self, **kwargs)\n    server.listen(\n        port,\n        address=address,\n        family=family,\n        backlog=backlog,\n        flags=flags,\n        reuse_port=reuse_port,\n    )\n    return server", "loc": 43}
{"file": "tornado\\tornado\\web.py", "class_name": "Application", "function_name": "add_handlers", "parameters": ["self", "host_pattern", "host_handlers"], "param_types": {"host_pattern": "str", "host_handlers": "_RuleList"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DefaultHostMatches", "HostMatches", "Rule", "_ApplicationRouter", "self.default_router.rules.insert", "self.wildcard_router.add_rules"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Appends the given handlers to our handler list. Host patterns are processed sequentially in the order they were added. All matching patterns will be considered.", "source_code": "def add_handlers(self, host_pattern: str, host_handlers: _RuleList) -> None:\n    \"\"\"Appends the given handlers to our handler list.\n\n    Host patterns are processed sequentially in the order they were\n    added. All matching patterns will be considered.\n    \"\"\"\n    host_matcher = HostMatches(host_pattern)\n    rule = Rule(host_matcher, _ApplicationRouter(self, host_handlers))\n\n    self.default_router.rules.insert(-1, rule)\n\n    if self.default_host is not None:\n        self.wildcard_router.add_rules(\n            [(DefaultHostMatches(self, host_matcher.host_pattern), host_handlers)]\n        )", "loc": 15}
{"file": "tornado\\tornado\\web.py", "class_name": "Application", "function_name": "find_handler", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "'_HandlerDelegate'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cast", "self.default_router.find_handler", "self.get_handler_delegate", "self.settings.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_handler(\n    self, request: httputil.HTTPServerRequest, **kwargs: Any\n) -> \"_HandlerDelegate\":\n    route = self.default_router.find_handler(request)\n    if route is not None:\n        return cast(\"_HandlerDelegate\", route)\n\n    if self.settings.get(\"default_handler_class\"):\n        return self.get_handler_delegate(\n            request,\n            self.settings[\"default_handler_class\"],\n            self.settings.get(\"default_handler_args\", {}),\n        )\n\n    return self.get_handler_delegate(request, ErrorHandler, {\"status_code\": 404})", "loc": 15}
{"file": "tornado\\tornado\\web.py", "class_name": "Application", "function_name": "reverse_url", "parameters": ["self", "name"], "param_types": {"name": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "self.default_router.reverse_url"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def reverse_url(self, name: str, *args: Any) -> str:\n    \"\"\"Returns a URL path for handler named ``name``\n\n    The handler must be added to the application as a named `URLSpec`.\n\n    Args will be substituted for capturing groups in the `URLSpec` regex.\n    They will be converted to strings if necessary, encoded as utf8,\n    and url-escaped.\n    \"\"\"\n    reversed_url = self.default_router.reverse_url(name, *args)\n    if reversed_url is not None:\n        return reversed_url\n\n    raise KeyError(\"%s not found in named urls\" % name)", "loc": 14}
{"file": "tornado\\tornado\\web.py", "class_name": "Application", "function_name": "log_request", "parameters": ["self", "handler"], "param_types": {"handler": "RequestHandler"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["handler._request_summary", "handler.get_status", "handler.request.request_time", "log_method"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Writes a completed HTTP request to the logs. By default writes to the python root logger.  To change this behavior either subclass Application and override this method,", "source_code": "def log_request(self, handler: RequestHandler) -> None:\n    \"\"\"Writes a completed HTTP request to the logs.\n\n    By default writes to the python root logger.  To change\n    this behavior either subclass Application and override this method,\n    or pass a function in the application settings dictionary as\n    ``log_function``.\n    \"\"\"\n    if \"log_function\" in self.settings:\n        self.settings[\"log_function\"](handler)\n        return\n    if handler.get_status() < 400:\n        log_method = access_log.info\n    elif handler.get_status() < 500:\n        log_method = access_log.warning\n    else:\n        log_method = access_log.error\n    request_time = 1000.0 * handler.request.request_time()\n    log_method(\n        \"%d %s %.2fms\",\n        handler.get_status(),\n        handler._request_summary(),\n        request_time,\n    )", "loc": 24}
{"file": "tornado\\tornado\\web.py", "class_name": "_HandlerDelegate", "function_name": "headers_received", "parameters": ["self", "start_line", "headers"], "param_types": {"start_line": "Union[httputil.RequestStartLine, httputil.ResponseStartLine]", "headers": "httputil.HTTPHeaders"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "self.execute"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def headers_received(\n    self,\n    start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n    headers: httputil.HTTPHeaders,\n) -> Optional[Awaitable[None]]:\n    if self.stream_request_body:\n        self.request._body_future = Future()\n        return self.execute()\n    return None", "loc": 9}
{"file": "tornado\\tornado\\web.py", "class_name": "_HandlerDelegate", "function_name": "data_received", "parameters": ["self", "data"], "param_types": {"data": "bytes"}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.chunks.append", "self.handler.data_received"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def data_received(self, data: bytes) -> Optional[Awaitable[None]]:\n    if self.stream_request_body:\n        return self.handler.data_received(data)\n    else:\n        self.chunks.append(data)\n        return None", "loc": 6}
{"file": "tornado\\tornado\\web.py", "class_name": "_HandlerDelegate", "function_name": "finish", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["b''.join", "future_set_result_unless_cancelled", "self.execute"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def finish(self) -> None:\n    if self.stream_request_body:\n        future_set_result_unless_cancelled(self.request._body_future, None)\n    else:\n        # Note that the body gets parsed in RequestHandler._execute so it can be in\n        # the right exception handler scope.\n        self.request.body = b\"\".join(self.chunks)\n        self.execute()", "loc": 8}
{"file": "tornado\\tornado\\web.py", "class_name": "_HandlerDelegate", "function_name": "on_connection_close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.handler.on_connection_close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_connection_close(self) -> None:\n    if self.stream_request_body:\n        self.handler.on_connection_close()\n    else:\n        self.chunks = None  # type: ignore", "loc": 5}
{"file": "tornado\\tornado\\web.py", "class_name": "_HandlerDelegate", "function_name": "execute", "parameters": ["self"], "param_types": {}, "return_type": "Optional[Awaitable[None]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "RequestHandler._template_loaders.values", "f.result", "fut.add_done_callback", "gen.convert_yielded", "loader.reset", "self.application.settings.get", "self.handler._execute", "self.handler_class", "static_handler_class.reset", "t"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def execute(self) -> Optional[Awaitable[None]]:\n    # If template cache is disabled (usually in the debug mode),\n    # re-compile templates and reload static files on every\n    # request so you don't need to restart to see changes\n    if not self.application.settings.get(\"compiled_template_cache\", True):\n        with RequestHandler._template_loader_lock:\n            for loader in RequestHandler._template_loaders.values():\n                loader.reset()\n    if not self.application.settings.get(\"static_hash_cache\", True):\n        static_handler_class = self.application.settings.get(\n            \"static_handler_class\", StaticFileHandler\n        )\n        static_handler_class.reset()\n\n    self.handler = self.handler_class(\n        self.application, self.request, **self.handler_kwargs\n    )\n    transforms = [t(self.request) for t in self.application.transforms]\n\n    if self.stream_request_body:\n        self.handler._prepared_future = Future()\n    # Note that if an exception escapes handler._execute it will be\n    # trapped in the Future it returns (which we are ignoring here,\n    # leaving it to be logged when the Future is GC'd).\n    # However, that shouldn't happen because _execute has a blanket\n    # except handler, and we cannot easily access the IOLoop here to\n    # call add_future (because of the requirement to remain compatible\n    # with WSGI)\n    fut = gen.convert_yielded(\n        self.handler._execute(transforms, *self.path_args, **self.path_kwargs)\n    )\n    fut.add_done_callback(lambda f: f.result())\n    # If we are streaming the request body, then execute() is finished\n    # when the handler has prepared to receive the body.  If not,\n    # it doesn't matter when execute() finishes (so we return None)\n    return self.handler._prepared_future", "loc": 36}
{"file": "tornado\\tornado\\web.py", "class_name": "HTTPError", "function_name": "log_message", "parameters": ["self"], "param_types": {}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._log_message.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "A backwards compatible way of accessing log_message.", "source_code": "def log_message(self) -> Optional[str]:\n    \"\"\"\n    A backwards compatible way of accessing log_message.\n    \"\"\"\n    if self._log_message and not self.args:\n        return self._log_message.replace(\"%\", \"%%\")\n    return self._log_message", "loc": 7}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "compute_etag", "parameters": ["self"], "param_types": {}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_cached_version"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the ``Etag`` header based on static url version. This allows efficient ``If-None-Match`` checks against cached versions, and sends the correct ``Etag`` for a partial response", "source_code": "def compute_etag(self) -> Optional[str]:\n    \"\"\"Sets the ``Etag`` header based on static url version.\n\n    This allows efficient ``If-None-Match`` checks against cached\n    versions, and sends the correct ``Etag`` for a partial response\n    (i.e. the same ``Etag`` as the full file).\n\n    .. versionadded:: 3.1\n    \"\"\"\n    assert self.absolute_path is not None\n    version_hash = self._get_cached_version(self.absolute_path)\n    if not version_hash:\n        return None\n    return f'\"{version_hash}\"'", "loc": 14}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "set_headers", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.datetime.now", "datetime.timedelta", "self.get_cache_time", "self.get_content_type", "self.set_etag_header", "self.set_extra_headers", "self.set_header", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sets the content and caching headers on the response. .. versionadded:: 3.1", "source_code": "def set_headers(self) -> None:\n    \"\"\"Sets the content and caching headers on the response.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    self.set_header(\"Accept-Ranges\", \"bytes\")\n    self.set_etag_header()\n\n    if self.modified is not None:\n        self.set_header(\"Last-Modified\", self.modified)\n\n    content_type = self.get_content_type()\n    if content_type:\n        self.set_header(\"Content-Type\", content_type)\n\n    cache_time = self.get_cache_time(self.path, self.modified, content_type)\n    if cache_time > 0:\n        self.set_header(\n            \"Expires\",\n            datetime.datetime.now(datetime.timezone.utc)\n            + datetime.timedelta(seconds=cache_time),\n        )\n        self.set_header(\"Cache-Control\", \"max-age=\" + str(cache_time))\n\n    self.set_extra_headers(self.path)", "loc": 25}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "should_return_304", "parameters": ["self"], "param_types": {}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["email.utils.parsedate_to_datetime", "if_since.replace", "self.check_etag_header", "self.request.headers.get"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def should_return_304(self) -> bool:\n    \"\"\"Returns True if the headers indicate that we should return 304.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    # If client sent If-None-Match, use it, ignore If-Modified-Since\n    if self.request.headers.get(\"If-None-Match\"):\n        return self.check_etag_header()\n\n    # Check the If-Modified-Since, and don't send the result if the\n    # content has not been modified\n    ims_value = self.request.headers.get(\"If-Modified-Since\")\n    if ims_value is not None:\n        try:\n            if_since = email.utils.parsedate_to_datetime(ims_value)\n        except Exception:\n            return False\n        if if_since.tzinfo is None:\n            if_since = if_since.replace(tzinfo=datetime.timezone.utc)\n        assert self.modified is not None\n        if if_since >= self.modified:\n            return True\n\n    return False", "loc": 24}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "get_content_version", "parameters": ["cls", "abspath"], "param_types": {"abspath": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.get_content", "hasher.hexdigest", "hasher.update", "hashlib.sha512", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_content_version(cls, abspath: str) -> str:\n    \"\"\"Returns a version string for the resource at the given path.\n\n    This class method may be overridden by subclasses.  The\n    default implementation is a SHA-512 hash of the file's contents.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    data = cls.get_content(abspath)\n    hasher = hashlib.sha512()\n    if isinstance(data, bytes):\n        hasher.update(data)\n    else:\n        for chunk in data:\n            hasher.update(chunk)\n    return hasher.hexdigest()", "loc": 16}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "get_modified_time", "parameters": ["self"], "param_types": {}, "return_type": "Optional[datetime.datetime]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.datetime.fromtimestamp", "int", "self._stat"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_modified_time(self) -> Optional[datetime.datetime]:\n    \"\"\"Returns the time that ``self.absolute_path`` was last modified.\n\n    May be overridden in subclasses.  Should return a `~datetime.datetime`\n    object or None.\n\n    .. versionadded:: 3.1\n\n    .. versionchanged:: 6.4\n       Now returns an aware datetime object instead of a naive one.\n       Subclasses that override this method may return either kind.\n    \"\"\"\n    stat_result = self._stat()\n    # NOTE: Historically, this used stat_result[stat.ST_MTIME],\n    # which truncates the fractional portion of the timestamp. It\n    # was changed from that form to stat_result.st_mtime to\n    # satisfy mypy (which disallows the bracket operator), but the\n    # latter form returns a float instead of an int. For\n    # consistency with the past (and because we have a unit test\n    # that relies on this), we truncate the float here, although\n    # I'm not sure that's the right thing to do.\n    modified = datetime.datetime.fromtimestamp(\n        int(stat_result.st_mtime), datetime.timezone.utc\n    )\n    return modified", "loc": 25}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "get_content_type", "parameters": ["self"], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mimetypes.guess_type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_content_type(self) -> str:\n    \"\"\"Returns the ``Content-Type`` header to be used for this request.\n\n    .. versionadded:: 3.1\n    \"\"\"\n    assert self.absolute_path is not None\n    mime_type, encoding = mimetypes.guess_type(self.absolute_path)\n    # per RFC 6713, use the appropriate type for a gzip compressed file\n    if encoding == \"gzip\":\n        return \"application/gzip\"\n    # As of 2015-07-21 there is no bzip2 encoding defined at\n    # http://www.iana.org/assignments/media-types/media-types.xhtml\n    # So for that (and any other encoding), use octet-stream.\n    elif encoding is not None:\n        return \"application/octet-stream\"\n    elif mime_type is not None:\n        return mime_type\n    # if mime_type not detected, use application/octet-stream\n    else:\n        return \"application/octet-stream\"", "loc": 20}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "make_static_url", "parameters": ["cls", "settings", "path", "include_version"], "param_types": {"settings": "Dict[str, Any]", "path": "str", "include_version": "bool"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.get_version", "settings.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Constructs a versioned url for the given path. This method may be overridden in subclasses (but note that it is a class method rather than an instance method).  Subclasses", "source_code": "def make_static_url(\n    cls, settings: Dict[str, Any], path: str, include_version: bool = True\n) -> str:\n    \"\"\"Constructs a versioned url for the given path.\n\n    This method may be overridden in subclasses (but note that it\n    is a class method rather than an instance method).  Subclasses\n    are only required to implement the signature\n    ``make_static_url(cls, settings, path)``; other keyword\n    arguments may be passed through `~RequestHandler.static_url`\n    but are not standard.\n\n    ``settings`` is the `Application.settings` dictionary.  ``path``\n    is the static path being requested.  The url returned should be\n    relative to the current host.\n\n    ``include_version`` determines whether the generated URL should\n    include the query string containing the version hash of the\n    file corresponding to the given ``path``.\n\n    \"\"\"\n    url = settings.get(\"static_url_prefix\", \"/static/\") + path\n    if not include_version:\n        return url\n\n    version_hash = cls.get_version(settings, path)\n    if not version_hash:\n        return url\n\n    return f\"{url}?v={version_hash}\"", "loc": 30}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "parse_url_path", "parameters": ["self", "url_path"], "param_types": {"url_path": "str"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["url_path.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Converts a static URL path into a filesystem path. ``url_path`` is the path component of the URL with ``static_url_prefix`` removed.  The return value should be", "source_code": "def parse_url_path(self, url_path: str) -> str:\n    \"\"\"Converts a static URL path into a filesystem path.\n\n    ``url_path`` is the path component of the URL with\n    ``static_url_prefix`` removed.  The return value should be\n    filesystem path relative to ``static_path``.\n\n    This is the inverse of `make_static_url`.\n    \"\"\"\n    if os.path.sep != \"/\":\n        url_path = url_path.replace(\"/\", os.path.sep)\n    return url_path", "loc": 12}
{"file": "tornado\\tornado\\web.py", "class_name": "StaticFileHandler", "function_name": "get_version", "parameters": ["cls", "settings", "path"], "param_types": {"settings": "Dict[str, Any]", "path": "str"}, "return_type": "Optional[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls._get_cached_version", "cls.get_absolute_path"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Generate the version string to be used in static URLs. ``settings`` is the `Application.settings` dictionary and ``path`` is the relative location of the requested asset on the filesystem.", "source_code": "def get_version(cls, settings: Dict[str, Any], path: str) -> Optional[str]:\n    \"\"\"Generate the version string to be used in static URLs.\n\n    ``settings`` is the `Application.settings` dictionary and ``path``\n    is the relative location of the requested asset on the filesystem.\n    The returned value should be a string, or ``None`` if no version\n    could be determined.\n\n    .. versionchanged:: 3.1\n       This method was previously recommended for subclasses to override;\n       `get_content_version` is now preferred as it allows the base\n       class to handle caching of the result.\n    \"\"\"\n    abs_path = cls.get_absolute_path(settings[\"static_path\"], path)\n    return cls._get_cached_version(abs_path)", "loc": 15}
{"file": "tornado\\tornado\\web.py", "class_name": "GZipContentEncoding", "function_name": "transform_first_chunk", "parameters": ["self", "status_code", "headers", "chunk", "finishing"], "param_types": {"status_code": "int", "headers": "httputil.HTTPHeaders", "chunk": "bytes", "finishing": "bool"}, "return_type": "Tuple[int, httputil.HTTPHeaders, bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BytesIO", "_unicode", "_unicode(headers.get('Content-Type', '')).split", "gzip.GzipFile", "headers.get", "len", "self._compressible_type", "self.transform_chunk", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def transform_first_chunk(\n    self,\n    status_code: int,\n    headers: httputil.HTTPHeaders,\n    chunk: bytes,\n    finishing: bool,\n) -> Tuple[int, httputil.HTTPHeaders, bytes]:\n    # TODO: can/should this type be inherited from the superclass?\n    if \"Vary\" in headers:\n        headers[\"Vary\"] += \", Accept-Encoding\"\n    else:\n        headers[\"Vary\"] = \"Accept-Encoding\"\n    if self._gzipping:\n        ctype = _unicode(headers.get(\"Content-Type\", \"\")).split(\";\")[0]\n        self._gzipping = (\n            self._compressible_type(ctype)\n            and (not finishing or len(chunk) >= self.MIN_LENGTH)\n            and (\"Content-Encoding\" not in headers)\n        )\n    if self._gzipping:\n        headers[\"Content-Encoding\"] = \"gzip\"\n        self._gzip_value = BytesIO()\n        self._gzip_file = gzip.GzipFile(\n            mode=\"w\", fileobj=self._gzip_value, compresslevel=self.GZIP_LEVEL\n        )\n        chunk = self.transform_chunk(chunk, finishing)\n        if \"Content-Length\" in headers:\n            # The original content length is no longer correct.\n            # If this is the last (and only) chunk, we can set the new\n            # content-length; otherwise we remove it and fall back to\n            # chunked encoding.\n            if finishing:\n                headers[\"Content-Length\"] = str(len(chunk))\n            else:\n                del headers[\"Content-Length\"]\n    return status_code, headers, chunk", "loc": 36}
{"file": "tornado\\tornado\\web.py", "class_name": "GZipContentEncoding", "function_name": "transform_chunk", "parameters": ["self", "chunk", "finishing"], "param_types": {"chunk": "bytes", "finishing": "bool"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._gzip_file.close", "self._gzip_file.flush", "self._gzip_file.write", "self._gzip_value.getvalue", "self._gzip_value.seek", "self._gzip_value.truncate"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def transform_chunk(self, chunk: bytes, finishing: bool) -> bytes:\n    if self._gzipping:\n        self._gzip_file.write(chunk)\n        if finishing:\n            self._gzip_file.close()\n        else:\n            self._gzip_file.flush()\n        chunk = self._gzip_value.getvalue()\n        self._gzip_value.truncate(0)\n        self._gzip_value.seek(0)\n    return chunk", "loc": 11}
{"file": "tornado\\tornado\\web.py", "class_name": "TemplateModule", "function_name": "render", "parameters": ["self", "path"], "param_types": {"path": "str"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self._resource_list.append", "self.render_string"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def render(self, path: str, **kwargs: Any) -> bytes:\n    def set_resources(**kwargs) -> str:  # type: ignore\n        if path not in self._resource_dict:\n            self._resource_list.append(kwargs)\n            self._resource_dict[path] = kwargs\n        else:\n            if self._resource_dict[path] != kwargs:\n                raise ValueError(\n                    \"set_resources called with different \"\n                    \"resources for the same template\"\n                )\n        return \"\"\n\n    return self.render_string(path, set_resources=set_resources, **kwargs)", "loc": 14}
{"file": "tornado\\tornado\\web.py", "class_name": "TemplateModule", "function_name": "javascript_files", "parameters": ["self"], "param_types": {}, "return_type": "Iterable[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "result.append", "result.extend", "self._get_resources"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def javascript_files(self) -> Iterable[str]:\n    result = []\n    for f in self._get_resources(\"javascript_files\"):\n        if isinstance(f, (unicode_type, bytes)):\n            result.append(f)\n        else:\n            result.extend(f)\n    return result", "loc": 8}
{"file": "tornado\\tornado\\web.py", "class_name": "TemplateModule", "function_name": "css_files", "parameters": ["self"], "param_types": {}, "return_type": "Iterable[str]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "result.append", "result.extend", "self._get_resources"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def css_files(self) -> Iterable[str]:\n    result = []\n    for f in self._get_resources(\"css_files\"):\n        if isinstance(f, (unicode_type, bytes)):\n            result.append(f)\n        else:\n            result.extend(f)\n    return result", "loc": 8}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "render", "parameters": [], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_unicode", "hasattr", "module", "self._active_modules[name].render"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def render(*args, **kwargs) -> str:  # type: ignore\n    if not hasattr(self, \"_active_modules\"):\n        self._active_modules = {}  # type: Dict[str, UIModule]\n    if name not in self._active_modules:\n        self._active_modules[name] = module(self)\n    rendered = self._active_modules[name].render(*args, **kwargs)\n    return _unicode(rendered)", "loc": 7}
{"file": "tornado\\tornado\\web.py", "class_name": null, "function_name": "set_resources", "parameters": [], "param_types": {}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self._resource_list.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_resources(**kwargs) -> str:  # type: ignore\n    if path not in self._resource_dict:\n        self._resource_list.append(kwargs)\n        self._resource_dict[path] = kwargs\n    else:\n        if self._resource_dict[path] != kwargs:\n            raise ValueError(\n                \"set_resources called with different \"\n                \"resources for the same template\"\n            )\n    return \"\"", "loc": 11}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "write_message", "parameters": ["self", "message", "binary"], "param_types": {"message": "Union[bytes, str, Dict[str, Any]]", "binary": "bool"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketClosedError", "isinstance", "self.ws_connection.is_closing", "self.ws_connection.write_message", "tornado.escape.json_encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sends the given message to the client of this Web Socket. The message may be either a string or a dict (which will be encoded as json).  If the ``binary`` argument is false, the message will be sent as utf8; in binary mode any byte string is allowed. If the connection is already closed, raises `WebSocketClosedError`.", "source_code": "def write_message(\n    self, message: Union[bytes, str, Dict[str, Any]], binary: bool = False\n) -> \"Future[None]\":\n    \"\"\"Sends the given message to the client of this Web Socket.\n\n    The message may be either a string or a dict (which will be\n    encoded as json).  If the ``binary`` argument is false, the\n    message will be sent as utf8; in binary mode any byte string\n    is allowed.\n\n    If the connection is already closed, raises `WebSocketClosedError`.\n    Returns a `.Future` which can be used for flow control.\n\n    .. versionchanged:: 3.2\n       `WebSocketClosedError` was added (previously a closed connection\n       would raise an `AttributeError`)\n\n    .. versionchanged:: 4.3\n       Returns a `.Future` which can be used for flow control.\n\n    .. versionchanged:: 5.0\n       Consistently raises `WebSocketClosedError`. Previously could\n       sometimes raise `.StreamClosedError`.\n    \"\"\"\n    if self.ws_connection is None or self.ws_connection.is_closing():\n        raise WebSocketClosedError()\n    if isinstance(message, dict):\n        message = tornado.escape.json_encode(message)\n    return self.ws_connection.write_message(message, binary=binary)", "loc": 29}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "ping", "parameters": ["self", "data"], "param_types": {"data": "Union[str, bytes]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketClosedError", "self.ws_connection.is_closing", "self.ws_connection.write_ping", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Send ping frame to the remote end. The data argument allows a small amount of data (up to 125 bytes) to be sent as a part of the ping message. Note that not", "source_code": "def ping(self, data: Union[str, bytes] = b\"\") -> None:\n    \"\"\"Send ping frame to the remote end.\n\n    The data argument allows a small amount of data (up to 125\n    bytes) to be sent as a part of the ping message. Note that not\n    all websocket implementations expose this data to\n    applications.\n\n    Consider using the ``websocket_ping_interval`` application\n    setting instead of sending pings manually.\n\n    .. versionchanged:: 5.1\n\n       The data argument is now optional.\n\n    \"\"\"\n    data = utf8(data)\n    if self.ws_connection is None or self.ws_connection.is_closing():\n        raise WebSocketClosedError()\n    self.ws_connection.write_ping(data)", "loc": 20}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "close", "parameters": ["self", "code", "reason"], "param_types": {"code": "Optional[int]", "reason": "Optional[str]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.ws_connection.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Closes this Web Socket. Once the close handshake is successful the socket will be closed. ``code`` may be a numeric status code, taken from the values", "source_code": "def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n    \"\"\"Closes this Web Socket.\n\n    Once the close handshake is successful the socket will be closed.\n\n    ``code`` may be a numeric status code, taken from the values\n    defined in `RFC 6455 section 7.4.1\n    <https://tools.ietf.org/html/rfc6455#section-7.4.1>`_.\n    ``reason`` may be a textual message about why the connection is\n    closing.  These values are made available to the client, but are\n    not otherwise interpreted by the websocket protocol.\n\n    .. versionchanged:: 4.0\n\n       Added the ``code`` and ``reason`` arguments.\n    \"\"\"\n    if self.ws_connection:\n        self.ws_connection.close(code, reason)\n        self.ws_connection = None", "loc": 19}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "check_origin", "parameters": ["self", "origin"], "param_types": {"origin": "str"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["origin.lower", "self.request.headers.get", "urlparse"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Override to enable support for allowing alternate origins. The ``origin`` argument is the value of the ``Origin`` HTTP header, the url responsible for initiating this request.  This", "source_code": "def check_origin(self, origin: str) -> bool:\n    \"\"\"Override to enable support for allowing alternate origins.\n\n    The ``origin`` argument is the value of the ``Origin`` HTTP\n    header, the url responsible for initiating this request.  This\n    method is not called for clients that do not send this header;\n    such requests are always allowed (because all browsers that\n    implement WebSockets support this header, and non-browser\n    clients do not have the same cross-site security concerns).\n\n    Should return ``True`` to accept the request or ``False`` to\n    reject it. By default, rejects all requests with an origin on\n    a host other than this one.\n\n    This is a security protection against cross site scripting attacks on\n    browsers, since WebSockets are allowed to bypass the usual same-origin\n    policies and don't use CORS headers.\n\n    .. warning::\n\n       This is an important security measure; don't disable it\n       without understanding the security implications. In\n       particular, if your authentication is cookie-based, you\n       must either restrict the origins allowed by\n       ``check_origin()`` or implement your own XSRF-like\n       protection for websocket connections. See `these\n       <https://www.christian-schneider.net/CrossSiteWebSocketHijacking.html>`_\n       `articles\n       <https://devcenter.heroku.com/articles/websocket-security>`_\n       for more.\n\n    To accept all cross-origin traffic (which was the default prior to\n    Tornado 4.0), simply override this method to always return ``True``::\n\n        def check_origin(self, origin):\n            return True\n\n    To allow connections from any subdomain of your site, you might\n    do something like::\n\n        def check_origin(self, origin):\n            parsed_origin = urllib.parse.urlparse(origin)\n            return parsed_origin.netloc.endswith(\".mydomain.com\")\n\n    .. versionadded:: 4.0\n\n    \"\"\"\n    parsed_origin = urlparse(origin)\n    origin = parsed_origin.netloc\n    origin = origin.lower()\n\n    host = self.request.headers.get(\"Host\")\n\n    # Check to see that origin matches host directly, including ports\n    return origin == host", "loc": 55}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "on_connection_close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._break_cycles", "self.on_close", "self.ws_connection.on_connection_close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_connection_close(self) -> None:\n    if self.ws_connection:\n        self.ws_connection.on_connection_close()\n        self.ws_connection = None\n    if not self._on_close_called:\n        self._on_close_called = True\n        self.on_close()\n        self._break_cycles()", "loc": 8}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketHandler", "function_name": "get_websocket_protocol", "parameters": ["self"], "param_types": {}, "return_type": "Optional['WebSocketProtocol']", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketProtocol13", "_WebSocketParams", "self.get_compression_options", "self.request.headers.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_websocket_protocol(self) -> Optional[\"WebSocketProtocol\"]:\n    websocket_version = self.request.headers.get(\"Sec-WebSocket-Version\")\n    if websocket_version in (\"7\", \"8\", \"13\"):\n        params = _WebSocketParams(\n            ping_interval=self.ping_interval,\n            ping_timeout=self.ping_timeout,\n            max_message_size=self.max_message_size,\n            compression_options=self.get_compression_options(),\n        )\n        return WebSocketProtocol13(self, False, params)\n    return None", "loc": 11}
{"file": "tornado\\tornado\\websocket.py", "class_name": "_PerMessageDeflateCompressor", "function_name": "compress", "parameters": ["self", "data"], "param_types": {"data": "bytes"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compressor.compress", "compressor.flush", "data.endswith", "self._create_compressor"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compress(self, data: bytes) -> bytes:\n    compressor = self._compressor or self._create_compressor()\n    data = compressor.compress(data) + compressor.flush(zlib.Z_SYNC_FLUSH)\n    assert data.endswith(b\"\\x00\\x00\\xff\\xff\")\n    return data[:-4]", "loc": 5}
{"file": "tornado\\tornado\\websocket.py", "class_name": "_PerMessageDeflateDecompressor", "function_name": "decompress", "parameters": ["self", "data"], "param_types": {"data": "bytes"}, "return_type": "bytes", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_DecompressTooLargeError", "decompressor.decompress", "self._create_decompressor"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decompress(self, data: bytes) -> bytes:\n    decompressor = self._decompressor or self._create_decompressor()\n    result = decompressor.decompress(\n        data + b\"\\x00\\x00\\xff\\xff\", self._max_message_size\n    )\n    if decompressor.unconsumed_tail:\n        raise _DecompressTooLargeError()\n    return result", "loc": 8}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "compute_accept_value", "parameters": ["key"], "param_types": {"key": "Union[str, bytes]"}, "return_type": "str", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["base64.b64encode", "hashlib.sha1", "native_str", "sha1.digest", "sha1.update", "utf8"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Computes the value for the Sec-WebSocket-Accept header, given the value for Sec-WebSocket-Key.", "source_code": "def compute_accept_value(key: Union[str, bytes]) -> str:\n    \"\"\"Computes the value for the Sec-WebSocket-Accept header,\n    given the value for Sec-WebSocket-Key.\n    \"\"\"\n    sha1 = hashlib.sha1()\n    sha1.update(utf8(key))\n    sha1.update(b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\")  # Magic value\n    return native_str(base64.b64encode(sha1.digest()))", "loc": 8}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "write_message", "parameters": ["self", "message", "binary"], "param_types": {"message": "Union[str, bytes, Dict[str, Any]]", "binary": "bool"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketClosedError", "asyncio.ensure_future", "isinstance", "len", "self._compressor.compress", "self._write_frame", "tornado.escape.json_encode", "tornado.escape.utf8", "wrapper"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Sends the given message to the client of this Web Socket.", "source_code": "def write_message(\n    self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False\n) -> \"Future[None]\":\n    \"\"\"Sends the given message to the client of this Web Socket.\"\"\"\n    if binary:\n        opcode = 0x2\n    else:\n        opcode = 0x1\n    if isinstance(message, dict):\n        message = tornado.escape.json_encode(message)\n    message = tornado.escape.utf8(message)\n    assert isinstance(message, bytes)\n    self._message_bytes_out += len(message)\n    flags = 0\n    if self._compressor:\n        message = self._compressor.compress(message)\n        flags |= self.RSV1\n    # For historical reasons, write methods in Tornado operate in a semi-synchronous\n    # mode in which awaiting the Future they return is optional (But errors can\n    # still be raised). This requires us to go through an awkward dance here\n    # to transform the errors that may be returned while presenting the same\n    # semi-synchronous interface.\n    try:\n        fut = self._write_frame(True, opcode, message, flags=flags)\n    except StreamClosedError:\n        raise WebSocketClosedError()\n\n    async def wrapper() -> None:\n        try:\n            await fut\n        except StreamClosedError:\n            raise WebSocketClosedError()\n\n    return asyncio.ensure_future(wrapper())", "loc": 34}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "close", "parameters": ["self", "code", "reason"], "param_types": {"code": "Optional[int]", "reason": "Optional[str]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._abort", "self._ping_coroutine.cancel", "self._write_frame", "self.stream.close", "self.stream.closed", "self.stream.io_loop.add_timeout", "self.stream.io_loop.remove_timeout", "self.stream.io_loop.time", "struct.pack", "utf8"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Closes the WebSocket connection.", "source_code": "def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n    \"\"\"Closes the WebSocket connection.\"\"\"\n    if not self.server_terminated:\n        if not self.stream.closed():\n            if code is None and reason is not None:\n                code = 1000  # \"normal closure\" status code\n            if code is None:\n                close_data = b\"\"\n            else:\n                close_data = struct.pack(\">H\", code)\n            if reason is not None:\n                close_data += utf8(reason)\n            try:\n                self._write_frame(True, 0x8, close_data)\n            except StreamClosedError:\n                self._abort()\n        self.server_terminated = True\n    if self.client_terminated:\n        if self._waiting is not None:\n            self.stream.io_loop.remove_timeout(self._waiting)\n            self._waiting = None\n        self.stream.close()\n    elif self._waiting is None:\n        # Give the client a few seconds to complete a clean shutdown,\n        # otherwise just close the connection.\n        self._waiting = self.stream.io_loop.add_timeout(\n            self.stream.io_loop.time() + 5, self._abort\n        )\n    if self._ping_coroutine:\n        self._ping_coroutine.cancel()\n        self._ping_coroutine = None", "loc": 31}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "ping_interval", "parameters": ["self"], "param_types": {}, "return_type": "float", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ping_interval(self) -> float:\n    interval = self.params.ping_interval\n    if interval is not None:\n        return interval\n    return 0", "loc": 5}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "ping_timeout", "parameters": ["self"], "param_types": {}, "return_type": "float", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["de_dupe_gen_log"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ping_timeout(self) -> float:\n    timeout = self.params.ping_timeout\n    if timeout is not None:\n        if self.ping_interval and timeout > self.ping_interval:\n            de_dupe_gen_log(\n                # Note: using de_dupe_gen_log to prevent this message from\n                # being duplicated for each connection\n                logging.WARNING,\n                f\"The websocket_ping_timeout ({timeout}) cannot be longer\"\n                f\" than the websocket_ping_interval ({self.ping_interval}).\"\n                f\"\\nSetting websocket_ping_timeout={self.ping_interval}\",\n            )\n            return self.ping_interval\n        return timeout\n    return self.ping_interval", "loc": 15}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketProtocol13", "function_name": "start_pinging", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.create_task", "self.periodic_ping"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Start sending periodic pings to keep the connection alive", "source_code": "def start_pinging(self) -> None:\n    \"\"\"Start sending periodic pings to keep the connection alive\"\"\"\n    if (\n        # prevent multiple ping coroutines being run in parallel\n        not self._ping_coroutine\n        # only run the ping coroutine if a ping interval is configured\n        and self.ping_interval > 0\n    ):\n        self._ping_coroutine = asyncio.create_task(self.periodic_ping())", "loc": 9}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketClientConnection", "function_name": "close", "parameters": ["self", "code", "reason"], "param_types": {"code": "Optional[int]", "reason": "Optional[str]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.protocol.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Closes the websocket connection. ``code`` and ``reason`` are documented under `WebSocketHandler.close`.", "source_code": "def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n    \"\"\"Closes the websocket connection.\n\n    ``code`` and ``reason`` are documented under\n    `WebSocketHandler.close`.\n\n    .. versionadded:: 3.2\n\n    .. versionchanged:: 4.0\n\n       Added the ``code`` and ``reason`` arguments.\n    \"\"\"\n    if self.protocol is not None:\n        self.protocol.close(code, reason)\n        self.protocol = None  # type: ignore", "loc": 15}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketClientConnection", "function_name": "on_connection_close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["StreamClosedError", "self._on_message", "self.connect_future.done", "self.connect_future.set_exception", "self.tcp_client.close", "super", "super().on_connection_close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_connection_close(self) -> None:\n    if not self.connect_future.done():\n        self.connect_future.set_exception(StreamClosedError())\n    self._on_message(None)\n    self.tcp_client.close()\n    super().on_connection_close()", "loc": 6}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketClientConnection", "function_name": "write_message", "parameters": ["self", "message", "binary"], "param_types": {"message": "Union[str, bytes, Dict[str, Any]]", "binary": "bool"}, "return_type": "'Future[None]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketClosedError", "self.protocol.write_message"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sends a message to the WebSocket server. If the stream is closed, raises `WebSocketClosedError`.", "source_code": "def write_message(\n    self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False\n) -> \"Future[None]\":\n    \"\"\"Sends a message to the WebSocket server.\n\n    If the stream is closed, raises `WebSocketClosedError`.\n    Returns a `.Future` which can be used for flow control.\n\n    .. versionchanged:: 5.0\n       Exception raised on a closed stream changed from `.StreamClosedError`\n       to `WebSocketClosedError`.\n    \"\"\"\n    if self.protocol is None:\n        raise WebSocketClosedError(\"Client connection has been closed\")\n    return self.protocol.write_message(message, binary=binary)", "loc": 15}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketClientConnection", "function_name": "read_message", "parameters": ["self", "callback"], "param_types": {"callback": "Optional[Callable[['Future[Union[None, str, bytes]]'], None]]"}, "return_type": "Awaitable[Union[None, str, bytes]]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.ensure_future", "self.io_loop.add_future", "self.read_queue.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Reads a message from the WebSocket server. If on_message_callback was specified at WebSocket initialization, this function will never return messages", "source_code": "def read_message(\n    self,\n    callback: Optional[Callable[[\"Future[Union[None, str, bytes]]\"], None]] = None,\n) -> Awaitable[Union[None, str, bytes]]:\n    \"\"\"Reads a message from the WebSocket server.\n\n    If on_message_callback was specified at WebSocket\n    initialization, this function will never return messages\n\n    Returns a future whose result is the message, or None\n    if the connection is closed.  If a callback argument\n    is given it will be called with the future when it is\n    ready.\n    \"\"\"\n\n    awaitable = self.read_queue.get()\n    if callback is not None:\n        self.io_loop.add_future(asyncio.ensure_future(awaitable), callback)\n    return awaitable", "loc": 19}
{"file": "tornado\\tornado\\websocket.py", "class_name": "WebSocketClientConnection", "function_name": "ping", "parameters": ["self", "data"], "param_types": {"data": "bytes"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["WebSocketClosedError", "self.protocol.write_ping", "utf8"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Send ping frame to the remote end. The data argument allows a small amount of data (up to 125 bytes) to be sent as a part of the ping message. Note that not", "source_code": "def ping(self, data: bytes = b\"\") -> None:\n    \"\"\"Send ping frame to the remote end.\n\n    The data argument allows a small amount of data (up to 125\n    bytes) to be sent as a part of the ping message. Note that not\n    all websocket implementations expose this data to\n    applications.\n\n    Consider using the ``ping_interval`` argument to\n    `websocket_connect` instead of sending pings manually.\n\n    .. versionadded:: 5.1\n\n    \"\"\"\n    data = utf8(data)\n    if self.protocol is None:\n        raise WebSocketClosedError()\n    self.protocol.write_ping(data)", "loc": 18}
{"file": "tornado\\tornado\\wsgi.py", "class_name": "WSGIContainer", "function_name": "environ", "parameters": ["self", "request"], "param_types": {"request": "httputil.HTTPServerRequest"}, "return_type": "Dict[str, Any]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BytesIO", "escape.url_unescape", "escape.utf8", "int", "key.replace", "key.replace('-', '_').upper", "len", "request.headers.items", "request.headers.pop", "request.host.split", "str", "to_wsgi_str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Converts a `tornado.httputil.HTTPServerRequest` to a WSGI environment. .. versionchanged:: 6.3 No longer a static method.", "source_code": "def environ(self, request: httputil.HTTPServerRequest) -> Dict[str, Any]:\n    \"\"\"Converts a `tornado.httputil.HTTPServerRequest` to a WSGI environment.\n\n    .. versionchanged:: 6.3\n       No longer a static method.\n    \"\"\"\n    hostport = request.host.split(\":\")\n    if len(hostport) == 2:\n        host = hostport[0]\n        port = int(hostport[1])\n    else:\n        host = request.host\n        port = 443 if request.protocol == \"https\" else 80\n    environ = {\n        \"REQUEST_METHOD\": request.method,\n        \"SCRIPT_NAME\": \"\",\n        \"PATH_INFO\": to_wsgi_str(\n            escape.url_unescape(request.path, encoding=None, plus=False)\n        ),\n        \"QUERY_STRING\": request.query,\n        \"REMOTE_ADDR\": request.remote_ip,\n        \"SERVER_NAME\": host,\n        \"SERVER_PORT\": str(port),\n        \"SERVER_PROTOCOL\": request.version,\n        \"wsgi.version\": (1, 0),\n        \"wsgi.url_scheme\": request.protocol,\n        \"wsgi.input\": BytesIO(escape.utf8(request.body)),\n        \"wsgi.errors\": sys.stderr,\n        \"wsgi.multithread\": self.executor is not dummy_executor,\n        \"wsgi.multiprocess\": True,\n        \"wsgi.run_once\": False,\n    }\n    if \"Content-Type\" in request.headers:\n        environ[\"CONTENT_TYPE\"] = request.headers.pop(\"Content-Type\")\n    if \"Content-Length\" in request.headers:\n        environ[\"CONTENT_LENGTH\"] = request.headers.pop(\"Content-Length\")\n    for key, value in request.headers.items():\n        environ[\"HTTP_\" + key.replace(\"-\", \"_\").upper()] = value\n    return environ", "loc": 39}
{"file": "tornado\\tornado\\wsgi.py", "class_name": null, "function_name": "next_chunk", "parameters": [], "param_types": {}, "return_type": "Optional[bytes]", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["next"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def next_chunk() -> Optional[bytes]:\n    try:\n        return next(app_response_iter)\n    except StopIteration:\n        # StopIteration is special and is not allowed to pass through\n        # coroutines normally.\n        return None", "loc": 7}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "initialize", "parameters": ["self", "asyncio_loop"], "param_types": {"asyncio_loop": "asyncio.AbstractEventLoop"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AddThreadSelectorEventLoop", "IOLoop._ioloop_for_asyncio.copy", "IOLoop._ioloop_for_asyncio.setdefault", "RuntimeError", "hasattr", "isinstance", "loop.is_closed", "set", "super", "super().initialize"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(  # type: ignore\n    self, asyncio_loop: asyncio.AbstractEventLoop, **kwargs: Any\n) -> None:\n    # asyncio_loop is always the real underlying IOLoop. This is used in\n    # ioloop.py to maintain the asyncio-to-ioloop mappings.\n    self.asyncio_loop = asyncio_loop\n    # selector_loop is an event loop that implements the add_reader family of\n    # methods. Usually the same as asyncio_loop but differs on platforms such\n    # as windows where the default event loop does not implement these methods.\n    self.selector_loop = asyncio_loop\n    if hasattr(asyncio, \"ProactorEventLoop\") and isinstance(\n        asyncio_loop, asyncio.ProactorEventLoop\n    ):\n        # Ignore this line for mypy because the abstract method checker\n        # doesn't understand dynamic proxies.\n        self.selector_loop = AddThreadSelectorEventLoop(asyncio_loop)  # type: ignore\n    # Maps fd to (fileobj, handler function) pair (as in IOLoop.add_handler)\n    self.handlers: Dict[int, Tuple[Union[int, _Selectable], Callable]] = {}\n    # Set of fds listening for reads/writes\n    self.readers: Set[int] = set()\n    self.writers: Set[int] = set()\n    self.closing = False\n    # If an asyncio loop was closed through an asyncio interface\n    # instead of IOLoop.close(), we'd never hear about it and may\n    # have left a dangling reference in our map. In case an\n    # application (or, more likely, a test suite) creates and\n    # destroys a lot of event loops in this way, check here to\n    # ensure that we don't have a lot of dead loops building up in\n    # the map.\n    #\n    # TODO(bdarnell): consider making self.asyncio_loop a weakref\n    # for AsyncIOMainLoop and make _ioloop_for_asyncio a\n    # WeakKeyDictionary.\n    for loop in IOLoop._ioloop_for_asyncio.copy():\n        if loop.is_closed():\n            try:\n                del IOLoop._ioloop_for_asyncio[loop]\n            except KeyError:\n                pass\n\n    # Make sure we don't already have an IOLoop for this asyncio loop\n    existing_loop = IOLoop._ioloop_for_asyncio.setdefault(asyncio_loop, self)\n    if existing_loop is not self:\n        raise RuntimeError(\n            f\"IOLoop {existing_loop} already associated with asyncio loop {asyncio_loop}\"\n        )\n\n    super().initialize(**kwargs)", "loc": 48}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "close", "parameters": ["self", "all_fds"], "param_types": {"all_fds": "bool"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "self.asyncio_loop.close", "self.close_fd", "self.remove_handler", "self.selector_loop.close"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self, all_fds: bool = False) -> None:\n    self.closing = True\n    for fd in list(self.handlers):\n        fileobj, handler_func = self.handlers[fd]\n        self.remove_handler(fd)\n        if all_fds:\n            self.close_fd(fileobj)\n    # Remove the mapping before closing the asyncio loop. If this\n    # happened in the other order, we could race against another\n    # initialize() call which would see the closed asyncio loop,\n    # assume it was closed from the asyncio side, and do this\n    # cleanup for us, leading to a KeyError.\n    del IOLoop._ioloop_for_asyncio[self.asyncio_loop]\n    if self.selector_loop is not self.asyncio_loop:\n        self.selector_loop.close()\n    self.asyncio_loop.close()", "loc": 16}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "add_handler", "parameters": ["self", "fd", "handler", "events"], "param_types": {"fd": "Union[int, _Selectable]", "handler": "Callable[..., None]", "events": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "self.readers.add", "self.selector_loop.add_reader", "self.selector_loop.add_writer", "self.split_fd", "self.writers.add"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_handler(\n    self, fd: Union[int, _Selectable], handler: Callable[..., None], events: int\n) -> None:\n    fd, fileobj = self.split_fd(fd)\n    if fd in self.handlers:\n        raise ValueError(\"fd %s added twice\" % fd)\n    self.handlers[fd] = (fileobj, handler)\n    if events & IOLoop.READ:\n        self.selector_loop.add_reader(fd, self._handle_events, fd, IOLoop.READ)\n        self.readers.add(fd)\n    if events & IOLoop.WRITE:\n        self.selector_loop.add_writer(fd, self._handle_events, fd, IOLoop.WRITE)\n        self.writers.add(fd)", "loc": 13}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "update_handler", "parameters": ["self", "fd", "events"], "param_types": {"fd": "Union[int, _Selectable]", "events": "int"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.readers.add", "self.readers.remove", "self.selector_loop.add_reader", "self.selector_loop.add_writer", "self.selector_loop.remove_reader", "self.selector_loop.remove_writer", "self.split_fd", "self.writers.add", "self.writers.remove"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_handler(self, fd: Union[int, _Selectable], events: int) -> None:\n    fd, fileobj = self.split_fd(fd)\n    if events & IOLoop.READ:\n        if fd not in self.readers:\n            self.selector_loop.add_reader(fd, self._handle_events, fd, IOLoop.READ)\n            self.readers.add(fd)\n    else:\n        if fd in self.readers:\n            self.selector_loop.remove_reader(fd)\n            self.readers.remove(fd)\n    if events & IOLoop.WRITE:\n        if fd not in self.writers:\n            self.selector_loop.add_writer(fd, self._handle_events, fd, IOLoop.WRITE)\n            self.writers.add(fd)\n    else:\n        if fd in self.writers:\n            self.selector_loop.remove_writer(fd)\n            self.writers.remove(fd)", "loc": 18}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "remove_handler", "parameters": ["self", "fd"], "param_types": {"fd": "Union[int, _Selectable]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.readers.remove", "self.selector_loop.remove_reader", "self.selector_loop.remove_writer", "self.split_fd", "self.writers.remove"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_handler(self, fd: Union[int, _Selectable]) -> None:\n    fd, fileobj = self.split_fd(fd)\n    if fd not in self.handlers:\n        return\n    if fd in self.readers:\n        self.selector_loop.remove_reader(fd)\n        self.readers.remove(fd)\n    if fd in self.writers:\n        self.selector_loop.remove_writer(fd)\n        self.writers.remove(fd)\n    del self.handlers[fd]", "loc": 11}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "call_at", "parameters": ["self", "when", "callback"], "param_types": {"when": "float", "callback": "Callable"}, "return_type": "object", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "max", "self.asyncio_loop.call_later", "self.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def call_at(\n    self, when: float, callback: Callable, *args: Any, **kwargs: Any\n) -> object:\n    # asyncio.call_at supports *args but not **kwargs, so bind them here.\n    # We do not synchronize self.time and asyncio_loop.time, so\n    # convert from absolute to relative.\n    return self.asyncio_loop.call_later(\n        max(0, when - self.time()),\n        self._run_callback,\n        functools.partial(callback, *args, **kwargs),\n    )", "loc": 11}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "add_callback", "parameters": ["self", "callback"], "param_types": {"callback": "Callable"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.get_running_loop", "call_soon", "functools.partial"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_callback(self, callback: Callable, *args: Any, **kwargs: Any) -> None:\n    try:\n        if asyncio.get_running_loop() is self.asyncio_loop:\n            call_soon = self.asyncio_loop.call_soon\n        else:\n            call_soon = self.asyncio_loop.call_soon_threadsafe\n    except RuntimeError:\n        call_soon = self.asyncio_loop.call_soon_threadsafe\n\n    try:\n        call_soon(self._run_callback, functools.partial(callback, *args, **kwargs))\n    except RuntimeError:\n        # \"Event loop is closed\". Swallow the exception for\n        # consistency with PollIOLoop (and logical consistency\n        # with the fact that we can't guarantee that an\n        # add_callback that completes without error will\n        # eventually execute).\n        pass\n    except AttributeError:\n        # ProactorEventLoop may raise this instead of RuntimeError\n        # if call_soon_threadsafe races with a call to close().\n        # Swallow it too for consistency.\n        pass", "loc": 23}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "BaseAsyncIOLoop", "function_name": "add_callback_from_signal", "parameters": ["self", "callback"], "param_types": {"callback": "Callable"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "self.asyncio_loop.call_soon_threadsafe", "warnings.warn"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_callback_from_signal(\n    self, callback: Callable, *args: Any, **kwargs: Any\n) -> None:\n    warnings.warn(\"add_callback_from_signal is deprecated\", DeprecationWarning)\n    try:\n        self.asyncio_loop.call_soon_threadsafe(\n            self._run_callback, functools.partial(callback, *args, **kwargs)\n        )\n    except RuntimeError:\n        pass", "loc": 10}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "AsyncIOLoop", "function_name": "initialize", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.new_event_loop", "loop.close", "super", "super().initialize"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def initialize(self, **kwargs: Any) -> None:  # type: ignore\n    self.is_current = False\n    loop = None\n    if \"asyncio_loop\" not in kwargs:\n        kwargs[\"asyncio_loop\"] = loop = asyncio.new_event_loop()\n    try:\n        super().initialize(**kwargs)\n    except Exception:\n        # If initialize() does not succeed (taking ownership of the loop),\n        # we have to close it.\n        if loop is not None:\n            loop.close()\n        raise", "loc": 13}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "SelectorThread", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_selector_loops.discard", "self._select_cond.notify", "self._thread.join", "self._wake_selector", "self._waker_r.close", "self._waker_w.close", "self.remove_reader"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self) -> None:\n    if self._closed:\n        return\n    with self._select_cond:\n        self._closing_selector = True\n        self._select_cond.notify()\n    self._wake_selector()\n    if self._thread is not None:\n        self._thread.join()\n    _selector_loops.discard(self)\n    self.remove_reader(self._waker_r)\n    self._waker_r.close()\n    self._waker_w.close()\n    self._closed = True", "loc": 14}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "SelectorThread", "function_name": "add_reader", "parameters": ["self", "fd", "callback"], "param_types": {"fd": "_FileDescriptorLike", "callback": "Callable[..., None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "self._wake_selector"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_reader(\n    self, fd: _FileDescriptorLike, callback: Callable[..., None], *args: Any\n) -> None:\n    self._readers[fd] = functools.partial(callback, *args)\n    self._wake_selector()", "loc": 5}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "SelectorThread", "function_name": "add_writer", "parameters": ["self", "fd", "callback"], "param_types": {"fd": "_FileDescriptorLike", "callback": "Callable[..., None]"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.partial", "self._wake_selector"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_writer(\n    self, fd: _FileDescriptorLike, callback: Callable[..., None], *args: Any\n) -> None:\n    self._writers[fd] = functools.partial(callback, *args)\n    self._wake_selector()", "loc": 5}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "SelectorThread", "function_name": "remove_reader", "parameters": ["self", "fd"], "param_types": {"fd": "_FileDescriptorLike"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._wake_selector"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_reader(self, fd: _FileDescriptorLike) -> bool:\n    try:\n        del self._readers[fd]\n    except KeyError:\n        return False\n    self._wake_selector()\n    return True", "loc": 7}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "SelectorThread", "function_name": "remove_writer", "parameters": ["self", "fd"], "param_types": {"fd": "_FileDescriptorLike"}, "return_type": "bool", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._wake_selector"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_writer(self, fd: _FileDescriptorLike) -> bool:\n    try:\n        del self._writers[fd]\n    except KeyError:\n        return False\n    self._wake_selector()\n    return True", "loc": 7}
{"file": "tornado\\tornado\\platform\\asyncio.py", "class_name": "AnyThreadEventLoopPolicy", "function_name": "get_event_loop", "parameters": ["self"], "param_types": {}, "return_type": "asyncio.AbstractEventLoop", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.new_event_loop", "self.set_event_loop", "super", "super().get_event_loop"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_event_loop(self) -> asyncio.AbstractEventLoop:\n    try:\n        return super().get_event_loop()\n    except RuntimeError:\n        # \"There is no current event loop in thread %r\"\n        loop = self.new_event_loop()\n        self.set_event_loop(loop)\n        return loop", "loc": 8}
{"file": "tornado\\tornado\\platform\\caresresolver.py", "class_name": "CaresResolver", "function_name": "resolve", "parameters": ["self", "host", "port", "family"], "param_types": {"host": "str", "port": "int", "family": "int"}, "return_type": "'Generator[Any, Any, List[Tuple[int, Any]]]'", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Future", "OSError", "addrinfo.append", "fut.set_result", "is_valid_ip", "pycares.errno.strerror", "self.channel.gethostbyname", "typing.cast"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve(\n    self, host: str, port: int, family: int = 0\n) -> \"Generator[Any, Any, List[Tuple[int, Any]]]\":\n    if is_valid_ip(host):\n        addresses = [host]\n    else:\n        # gethostbyname doesn't take callback as a kwarg\n        fut = Future()  # type: Future[Tuple[Any, Any]]\n        self.channel.gethostbyname(\n            host, family, lambda result, error: fut.set_result((result, error))\n        )\n        result, error = yield fut\n        if error:\n            raise OSError(\n                \"C-Ares returned error %s: %s while resolving %s\"\n                % (error, pycares.errno.strerror(error), host)\n            )\n        addresses = result.addresses\n    addrinfo = []\n    for address in addresses:\n        if \".\" in address:\n            address_family = socket.AF_INET\n        elif \":\" in address:\n            address_family = socket.AF_INET6\n        else:\n            address_family = socket.AF_UNSPEC\n        if family != socket.AF_UNSPEC and family != address_family:\n            raise OSError(\n                \"Requested socket family %d but got %d\" % (family, address_family)\n            )\n        addrinfo.append((typing.cast(int, address_family), (address, port)))\n    return addrinfo", "loc": 32}
{"file": "tornado\\tornado\\platform\\twisted.py", "class_name": null, "function_name": "errback", "parameters": ["failure"], "param_types": {"failure": "failure.Failure"}, "return_type": "None", "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "failure.raiseException", "future_set_exc_info", "sys.exc_info"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def errback(failure: failure.Failure) -> None:\n    try:\n        failure.raiseException()\n        # Should never happen, but just in case\n        raise Exception(\"errback called without error\")\n    except:\n        future_set_exc_info(f, sys.exc_info())", "loc": 7}
{"file": "tqdm\\.meta\\mkdocs.py", "class_name": null, "function_name": "doc2rst", "parameters": ["doc", "arglist", "raw"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "dedent", "doc.replace", "doc.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "arglist  : bool, whether to create argument lists raw  : bool, ignores arglist and indents by 2 spaces", "source_code": "def doc2rst(doc, arglist=True, raw=False):\n    \"\"\"\n    arglist  : bool, whether to create argument lists\n    raw  : bool, ignores arglist and indents by 2 spaces\n    \"\"\"\n    doc = doc.replace('`', '``')\n    if raw:\n        doc = doc.replace('\\n ', '\\n   ')\n    else:\n        doc = dedent(doc)\n        if arglist:\n            doc = '\\n'.join(i if not i or i[0] == ' ' else '* ' + i + '  '\n                            for i in doc.split('\\n'))\n    return doc", "loc": 14}
{"file": "tqdm\\benchmarks\\benchmarks.py", "class_name": "Comparison", "function_name": "run", "parameters": ["self", "cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "self.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, cls):\n    pbar = cls(self.iterable)\n    t0 = self.time()\n    [0 for _ in pbar]  # pylint: disable=pointless-statement\n    t1 = self.time()\n    return t1 - t0", "loc": 6}
{"file": "tqdm\\benchmarks\\benchmarks.py", "class_name": "Comparison", "function_name": "alive_progress", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["alive_bar", "bar", "len", "self.run"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def alive_progress(self):\n    from alive_progress import alive_bar\n\n    class wrapper:\n        def __init__(self, iterable):\n            self.iterable = iterable\n\n        def __iter__(self):\n            iterable = self.iterable\n            with alive_bar(len(iterable)) as bar:\n                for i in iterable:\n                    yield i\n                    bar()\n\n    return self.run(wrapper)", "loc": 15}
{"file": "tqdm\\examples\\async_coroutines.py", "class_name": null, "function_name": "count", "parameters": ["start", "step"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def count(start=0, step=1):\n    i = start\n    while True:\n        new_start = yield i\n        if new_start is None:\n            i += step\n        else:\n            i = new_start", "loc": 8}
{"file": "tqdm\\examples\\coroutine_pipe.py", "class_name": null, "function_name": "autonext", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "next", "wraps"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def autonext(func):\n    @wraps(func)\n    def inner(*args, **kwargs):\n        res = func(*args, **kwargs)\n        next(res)\n        return res\n    return inner", "loc": 7}
{"file": "tqdm\\examples\\coroutine_pipe.py", "class_name": null, "function_name": "tqdm_pipe", "parameters": ["target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["pbar.update", "target.send", "tqdm"], "control_structures": ["While"], "behavior_type": ["logic"], "doc_summary": "Coroutine chain pipe `send()`ing to `target`. This: >>> r = receiver()", "source_code": "def tqdm_pipe(target, **tqdm_kwargs):\n    \"\"\"\n    Coroutine chain pipe `send()`ing to `target`.\n\n    This:\n    >>> r = receiver()\n    >>> p = producer(r)\n    >>> next(r)\n    >>> next(p)\n\n    Becomes:\n    >>> r = receiver()\n    >>> t = tqdm.pipe(r)\n    >>> p = producer(t)\n    >>> next(r)\n    >>> next(p)\n    \"\"\"\n    with tqdm(**tqdm_kwargs) as pbar:\n        while True:\n            obj = (yield)\n            target.send(obj)\n            pbar.update()", "loc": 22}
{"file": "tqdm\\examples\\coroutine_pipe.py", "class_name": null, "function_name": "grep", "parameters": ["pattern", "target"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["target.send"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def grep(pattern, target):\n    while True:\n        line = (yield)\n        if pattern in line:\n            target.send(line)", "loc": 5}
{"file": "tqdm\\examples\\parallel_bars.py", "class_name": null, "function_name": "progresser", "parameters": ["n", "auto_position", "write_safe", "blocking", "progress"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["random", "sleep", "tqdm.write", "trange"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def progresser(n, auto_position=True, write_safe=False, blocking=True, progress=False):\n    interval = random() * 0.002 / (NUM_SUBITERS - n + 2)  # nosec\n    total = 5000\n    text = f\"#{n}, est. {interval * total:<04.2g}s\"\n    for _ in trange(total, desc=text, disable=not progress,\n                    lock_args=None if blocking else (False,),\n                    position=None if auto_position else n):\n        sleep(interval)\n    # NB: may not clear instances with higher `position` upon completion\n    # since this worker may not know about other bars #796\n    if write_safe:  # we think we know about other bars\n        if n == 6:\n            tqdm.write(\"n == 6 completed\")\n    return n + 1", "loc": 14}
{"file": "tqdm\\examples\\redirect_print.py", "class_name": null, "function_name": "std_out_err_redirect_tqdm", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["map"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def std_out_err_redirect_tqdm():\n    orig_out_err = sys.stdout, sys.stderr\n    try:\n        # sys.stdout = sys.stderr = DummyTqdmFile(orig_out_err[0])\n        sys.stdout, sys.stderr = map(DummyTqdmFile, orig_out_err)\n        yield orig_out_err[0]\n    # Relay exceptions\n    except Exception as exc:\n        raise exc\n    # Always restore sys.stdout/err if necessary\n    finally:\n        sys.stdout, sys.stderr = orig_out_err", "loc": 12}
{"file": "tqdm\\examples\\tqdm_wget.py", "class_name": null, "function_name": "my_hook", "parameters": ["t"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["t.update"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Wraps tqdm instance. Don't forget to close() or __exit__() the tqdm instance once you're done with it (easiest using `with` syntax).", "source_code": "def my_hook(t):\n    \"\"\"Wraps tqdm instance.\n\n    Don't forget to close() or __exit__()\n    the tqdm instance once you're done with it (easiest using `with` syntax).\n\n    Example\n    -------\n\n    >>> with tqdm(...) as t:\n    ...     reporthook = my_hook(t)\n    ...     urllib.urlretrieve(..., reporthook=reporthook)\n\n    \"\"\"\n    last_b = [0]\n\n    def update_to(b=1, bsize=1, tsize=None):\n        \"\"\"\n        b  : int, optional\n            Number of blocks transferred so far [default: 1].\n        bsize  : int, optional\n            Size of each block (in tqdm units) [default: 1].\n        tsize  : int, optional\n            Total size (in tqdm units). If [default: None] or -1,\n            remains unchanged.\n        \"\"\"\n        if tsize not in (None, -1):\n            t.total = tsize\n        displayed = t.update((b - last_b[0]) * bsize)\n        last_b[0] = b\n        return displayed\n\n    return update_to", "loc": 33}
{"file": "tqdm\\examples\\tqdm_wget.py", "class_name": null, "function_name": "update_to", "parameters": ["b", "bsize", "tsize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["t.update"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "b  : int, optional Number of blocks transferred so far [default: 1]. bsize  : int, optional", "source_code": "def update_to(b=1, bsize=1, tsize=None):\n    \"\"\"\n    b  : int, optional\n        Number of blocks transferred so far [default: 1].\n    bsize  : int, optional\n        Size of each block (in tqdm units) [default: 1].\n    tsize  : int, optional\n        Total size (in tqdm units). If [default: None] or -1,\n        remains unchanged.\n    \"\"\"\n    if tsize not in (None, -1):\n        t.total = tsize\n    displayed = t.update((b - last_b[0]) * bsize)\n    last_b[0] = b\n    return displayed", "loc": 15}
{"file": "tqdm\\examples\\tqdm_wget.py", "class_name": "TqdmUpTo", "function_name": "update_to", "parameters": ["self", "b", "bsize", "tsize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.update"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "b  : int, optional Number of blocks transferred so far [default: 1]. bsize  : int, optional", "source_code": "def update_to(self, b=1, bsize=1, tsize=None):\n    \"\"\"\n    b  : int, optional\n        Number of blocks transferred so far [default: 1].\n    bsize  : int, optional\n        Size of each block (in tqdm units) [default: 1].\n    tsize  : int, optional\n        Total size (in tqdm units). If [default: None] remains unchanged.\n    \"\"\"\n    if tsize is not None:\n        self.total = tsize\n    return self.update(b * bsize - self.n)  # also sets self.n = b * bsize", "loc": 12}
{"file": "tqdm\\tqdm\\asyncio.py", "class_name": null, "function_name": "tarange", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["range", "tqdm_asyncio"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "A shortcut for `tqdm.asyncio.tqdm(range(*args), **kwargs)`.", "source_code": "def tarange(*args, **kwargs):\n    \"\"\"\n    A shortcut for `tqdm.asyncio.tqdm(range(*args), **kwargs)`.\n    \"\"\"\n    return tqdm_asyncio(range(*args), **kwargs)", "loc": 5}
{"file": "tqdm\\tqdm\\asyncio.py", "class_name": "tqdm_asyncio", "function_name": "as_completed", "parameters": ["cls", "fs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["asyncio.as_completed", "cls", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Wrapper for `asyncio.as_completed`.", "source_code": "def as_completed(cls, fs, *, loop=None, timeout=None, total=None, **tqdm_kwargs):\n    \"\"\"\n    Wrapper for `asyncio.as_completed`.\n    \"\"\"\n    if total is None:\n        total = len(fs)\n    kwargs = {}\n    if version_info[:2] < (3, 10):\n        kwargs['loop'] = loop\n    yield from cls(asyncio.as_completed(fs, timeout=timeout, **kwargs),\n                   total=total, **tqdm_kwargs)", "loc": 11}
{"file": "tqdm\\tqdm\\auto.py", "class_name": null, "function_name": "trange", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["range", "tqdm"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "A shortcut for `tqdm.auto.tqdm(range(*args), **kwargs)`.", "source_code": "def trange(*args, **kwargs):\n    \"\"\"\n    A shortcut for `tqdm.auto.tqdm(range(*args), **kwargs)`.\n    \"\"\"\n    return tqdm(range(*args), **kwargs)", "loc": 5}
{"file": "tqdm\\tqdm\\cli.py", "class_name": null, "function_name": "cast", "parameters": ["val", "typ"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmTypeError", "cast", "eval", "eval(f'\"{val}\"').encode", "float", "int", "len", "log.debug", "re.match", "typ.split", "val.encode"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cast(val, typ):\n    log.debug((val, typ))\n    if \" or \" in typ:\n        for t in typ.split(\" or \"):\n            try:\n                return cast(val, t)\n            except TqdmTypeError:\n                pass\n        raise TqdmTypeError(f\"{val} : {typ}\")\n\n    # sys.stderr.write('\\ndebug | `val:type`: `' + val + ':' + typ + '`.\\n')\n    if typ == 'bool':\n        if (val == 'True') or (val == ''):\n            return True\n        if val == 'False':\n            return False\n        raise TqdmTypeError(val + ' : ' + typ)\n    if typ == 'chr':\n        if len(val) == 1:\n            return val.encode()\n        if re.match(r\"^\\\\\\w+$\", val):\n            return eval(f'\"{val}\"').encode()\n        raise TqdmTypeError(f\"{val} : {typ}\")\n    if typ == 'str':\n        return val\n    if typ == 'int':\n        try:\n            return int(val)\n        except ValueError as exc:\n            raise TqdmTypeError(f\"{val} : {typ}\") from exc\n    if typ == 'float':\n        try:\n            return float(val)\n        except ValueError as exc:\n            raise TqdmTypeError(f\"{val} : {typ}\") from exc\n    raise TqdmTypeError(f\"{val} : {typ}\")", "loc": 36}
{"file": "tqdm\\tqdm\\cli.py", "class_name": null, "function_name": "posix_pipe", "parameters": ["fin", "fout", "delim", "buf_size", "callback", "callback_len"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["buf.count", "buf.split", "callback", "fin.read", "fp_write", "getattr", "len", "tmp.find"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Params ------ fin  : binary file with `read(buf_size : int)` method", "source_code": "def posix_pipe(fin, fout, delim=b'\\\\n', buf_size=256,\n               callback=lambda float: None, callback_len=True):\n    \"\"\"\n    Params\n    ------\n    fin  : binary file with `read(buf_size : int)` method\n    fout  : binary file with `write` (and optionally `flush`) methods.\n    callback  : function(float), e.g.: `tqdm.update`\n    callback_len  : If (default: True) do `callback(len(buffer))`.\n      Otherwise, do `callback(data) for data in buffer.split(delim)`.\n    \"\"\"\n    fp_write = fout.write\n\n    if not delim:\n        while True:\n            tmp = fin.read(buf_size)\n\n            # flush at EOF\n            if not tmp:\n                getattr(fout, 'flush', lambda: None)()\n                return\n\n            fp_write(tmp)\n            callback(len(tmp))\n        # return\n\n    buf = b''\n    len_delim = len(delim)\n    # n = 0\n    while True:\n        tmp = fin.read(buf_size)\n\n        # flush at EOF\n        if not tmp:\n            if buf:\n                fp_write(buf)\n                if callback_len:\n                    # n += 1 + buf.count(delim)\n                    callback(1 + buf.count(delim))\n                else:\n                    for i in buf.split(delim):\n                        callback(i)\n            getattr(fout, 'flush', lambda: None)()\n            return  # n\n\n        while True:\n            i = tmp.find(delim)\n            if i < 0:\n                buf += tmp\n                break\n            fp_write(buf + tmp[:i + len(delim)])\n            # n += 1\n            callback(1 if callback_len else (buf + tmp[:i]))\n            buf = b''\n            tmp = tmp[i + len_delim:]", "loc": 55}
{"file": "tqdm\\tqdm\\cli.py", "class_name": null, "function_name": "cp", "parameters": ["name", "dst"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dst.write_bytes", "fi.read_bytes", "log.info", "resources.files"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "copy resource `name` to `dst`", "source_code": "def cp(name, dst):\n    \"\"\"copy resource `name` to `dst`\"\"\"\n    fi = resources.files('tqdm') / name\n    dst.write_bytes(fi.read_bytes())\n    log.info(\"written:%s\", dst)", "loc": 5}
{"file": "tqdm\\tqdm\\dask.py", "class_name": "TqdmCallback", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display", "getattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Displays in the current cell in Notebooks.", "source_code": "def display(self):\n    \"\"\"Displays in the current cell in Notebooks.\"\"\"\n    container = getattr(self.bar, 'container', None)\n    if container is None:\n        return\n    from .notebook import display\n    display(container)", "loc": 7}
{"file": "tqdm\\tqdm\\gui.py", "class_name": "tqdm_gui", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._instances.remove", "self.display", "self.get_lock", "self.plt.close", "self.plt.ioff"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n\n    self.disable = True\n\n    with self.get_lock():\n        self._instances.remove(self)\n\n    # Restore toolbars\n    self.mpl.rcParams['toolbar'] = self.toolbar\n    # Return to non-interactive mode\n    if not self.wasion:\n        self.plt.ioff()\n    if self.leave:\n        self.display()\n    else:\n        self.plt.close(self.fig)", "loc": 18}
{"file": "tqdm\\tqdm\\gui.py", "class_name": "tqdm_gui", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "ax.figure.canvas.draw", "ax.get_ylim", "ax.set_title", "ax.set_ylim", "d['bar_format'] or '{l_bar}<bar/>{r_bar}'.replace", "getattr", "hspan.set_height", "hspan.set_width", "hspan.set_xy", "line1.set_data", "line2.set_data", "re.split", "self._time", "self.format_meter", "self.plt.pause", "xdata.append", "xdata.popleft", "ydata.append", "ydata.popleft", "zdata.append", "zdata.popleft"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, *_, **__):\n    n = self.n\n    cur_t = self._time()\n    elapsed = cur_t - self.start_t\n    delta_it = n - self.last_print_n\n    delta_t = cur_t - self.last_print_t\n\n    # Inline due to multiple calls\n    total = self.total\n    xdata = self.xdata\n    ydata = self.ydata\n    zdata = self.zdata\n    ax = self.ax\n    line1 = self.line1\n    line2 = self.line2\n    hspan = getattr(self, 'hspan', None)\n    # instantaneous rate\n    y = delta_it / delta_t\n    # overall rate\n    z = n / elapsed\n    # update line data\n    xdata.append(n * 100.0 / total if total else cur_t)\n    ydata.append(y)\n    zdata.append(z)\n\n    # Discard old values\n    # xmin, xmax = ax.get_xlim()\n    # if (not total) and elapsed > xmin * 1.1:\n    if (not total) and elapsed > 66:\n        xdata.popleft()\n        ydata.popleft()\n        zdata.popleft()\n\n    ymin, ymax = ax.get_ylim()\n    if y > ymax or z > ymax:\n        ymax = 1.1 * y\n        ax.set_ylim(ymin, ymax)\n        ax.figure.canvas.draw()\n\n    if total:\n        line1.set_data(xdata, ydata)\n        line2.set_data(xdata, zdata)\n        if hspan:\n            hspan.set_xy((0, ymin))\n            hspan.set_height(ymax - ymin)\n            hspan.set_width(n / total)\n    else:\n        t_ago = [cur_t - i for i in xdata]\n        line1.set_data(t_ago, ydata)\n        line2.set_data(t_ago, zdata)\n\n    d = self.format_dict\n    # remove {bar}\n    d['bar_format'] = (d['bar_format'] or \"{l_bar}<bar/>{r_bar}\").replace(\n        \"{bar}\", \"<bar/>\")\n    msg = self.format_meter(**d)\n    if '<bar/>' in msg:\n        msg = \"\".join(re.split(r'\\|?<bar/>\\|?', msg, maxsplit=1))\n    ax.set_title(msg, fontname=\"DejaVu Sans Mono\", fontsize=11)\n    self.plt.pause(1e-9)", "loc": 60}
{"file": "tqdm\\tqdm\\keras.py", "class_name": "TqdmCallback", "function_name": "bar2callback", "parameters": ["bar", "pop", "delta"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bar.set_postfix", "bar.update", "copy", "delta", "logs.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def bar2callback(bar, pop=None, delta=(lambda logs: 1)):\n    def callback(_, logs=None):\n        n = delta(logs)\n        if logs:\n            if pop:\n                logs = copy(logs)\n                [logs.pop(i, 0) for i in pop]\n            bar.set_postfix(logs, refresh=False)\n        bar.update(n)\n\n    return callback", "loc": 11}
{"file": "tqdm\\tqdm\\keras.py", "class_name": "TqdmCallback", "function_name": "on_train_begin", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["params", "self.epoch_bar.reset"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_train_begin(self, *_, **__):\n    params = self.params.get\n    auto_total = params('epochs', params('nb_epoch', None))\n    if auto_total is not None and auto_total != self.epoch_bar.total:\n        self.epoch_bar.reset(total=auto_total)", "loc": 5}
{"file": "tqdm\\tqdm\\keras.py", "class_name": "TqdmCallback", "function_name": "on_epoch_begin", "parameters": ["self", "epoch"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "hasattr", "logs.get", "params", "self.bar2callback", "self.batch_bar.close", "self.batch_bar.reset", "self.tqdm_class"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def on_epoch_begin(self, epoch, *_, **__):\n    if self.epoch_bar.n < epoch:\n        ebar = self.epoch_bar\n        ebar.n = ebar.last_print_n = ebar.initial = epoch\n    if self.verbose:\n        params = self.params.get\n        total = params('samples', params(\n            'nb_sample', params('steps', None))) or self.batches\n        if self.verbose == 2:\n            if hasattr(self, 'batch_bar'):\n                self.batch_bar.close()\n            self.batch_bar = self.tqdm_class(\n                total=total, unit='batch', leave=True,\n                unit_scale=1 / (params('batch_size', 1) or 1))\n            self.on_batch_end = self.bar2callback(\n                self.batch_bar, pop=['batch', 'size'],\n                delta=lambda logs: logs.get('size', 1))\n        elif self.verbose == 1:\n            self.batch_bar.unit_scale = 1 / (params('batch_size', 1) or 1)\n            self.batch_bar.reset(total=total)\n        else:\n            raise KeyError('Unknown verbosity')", "loc": 22}
{"file": "tqdm\\tqdm\\keras.py", "class_name": "TqdmCallback", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["display", "getattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Displays in the current cell in Notebooks.", "source_code": "def display(self):\n    \"\"\"Displays in the current cell in Notebooks.\"\"\"\n    container = getattr(self.epoch_bar, 'container', None)\n    if container is None:\n        return\n    from .notebook import display\n    display(container)\n    batch_bar = getattr(self, 'batch_bar', None)\n    if batch_bar is not None:\n        display(batch_bar.container)", "loc": 10}
{"file": "tqdm\\tqdm\\keras.py", "class_name": null, "function_name": "callback", "parameters": ["_", "logs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bar.set_postfix", "bar.update", "copy", "delta", "logs.pop"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def callback(_, logs=None):\n    n = delta(logs)\n    if logs:\n        if pop:\n            logs = copy(logs)\n            [logs.pop(i, 0) for i in pop]\n        bar.set_postfix(logs, refresh=False)\n    bar.update(n)", "loc": 8}
{"file": "tqdm\\tqdm\\notebook.py", "class_name": "tqdm_notebook", "function_name": "status_printer", "parameters": ["_", "total", "desc", "ncols"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTML", "IProgress", "ImportError", "TqdmHBox", "int", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Manage the printing of an IPython/Jupyter Notebook progress bar widget.", "source_code": "def status_printer(_, total=None, desc=None, ncols=None):\n    \"\"\"\n    Manage the printing of an IPython/Jupyter Notebook progress bar widget.\n    \"\"\"\n    # Fallback to text bar if there's no total\n    # DEPRECATED: replaced with an 'info' style bar\n    # if not total:\n    #    return super(tqdm_notebook, tqdm_notebook).status_printer(file)\n\n    # fp = file\n\n    # Prepare IPython progress bar\n    if IProgress is None:  # #187 #451 #558 #872\n        raise ImportError(WARN_NOIPYW)\n    if total:\n        pbar = IProgress(min=0, max=total)\n    else:  # No total? Show info style bar with no progress tqdm status\n        pbar = IProgress(min=0, max=1)\n        pbar.value = 1\n        pbar.bar_style = 'info'\n        if ncols is None:\n            pbar.layout.width = \"20px\"\n\n    ltext = HTML()\n    rtext = HTML()\n    if desc:\n        ltext.value = desc\n    container = TqdmHBox(children=[ltext, pbar, rtext])\n    # Prepare layout\n    if ncols is not None:  # use default style of ipywidgets\n        # ncols could be 100, \"100px\", \"100%\"\n        ncols = str(ncols)  # ipywidgets only accepts string\n        try:\n            if int(ncols) > 0:  # isnumeric and positive\n                ncols += 'px'\n        except ValueError:\n            pass\n        pbar.layout.flex = '2'\n        container.layout.width = ncols\n        container.layout.display = 'inline-flex'\n        container.layout.flex_flow = 'row wrap'\n\n    return container", "loc": 43}
{"file": "tqdm\\tqdm\\notebook.py", "class_name": "tqdm_notebook", "function_name": "display", "parameters": ["self", "msg", "pos", "close", "bar_style", "check_delay"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["d['bar_format'] or '{l_bar}<bar/>{r_bar}'.replace", "display", "escape", "map", "msg.replace", "re.split", "self.container.close", "self.format_meter"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, msg=None, pos=None,\n            # additional signals\n            close=False, bar_style=None, check_delay=True):\n    # Note: contrary to native tqdm, msg='' does NOT clear bar\n    # goal is to keep all infos if error happens so user knows\n    # at which iteration the loop failed.\n\n    # Clear previous output (really necessary?)\n    # clear_output(wait=1)\n\n    if not msg and not close:\n        d = self.format_dict\n        # remove {bar}\n        d['bar_format'] = (d['bar_format'] or \"{l_bar}<bar/>{r_bar}\").replace(\n            \"{bar}\", \"<bar/>\")\n        msg = self.format_meter(**d)\n\n    ltext, pbar, rtext = self.container.children\n    pbar.value = self.n\n\n    if msg:\n        msg = msg.replace(' ', u'\\u2007')  # fix html space padding\n        # html escape special characters (like '&')\n        if '<bar/>' in msg:\n            left, right = map(escape, re.split(r'\\|?<bar/>\\|?', msg, maxsplit=1))\n        else:\n            left, right = '', escape(msg)\n\n        # Update description\n        ltext.value = left\n        # never clear the bar (signal: msg='')\n        if right:\n            rtext.value = right\n\n    # Change bar style\n    if bar_style:\n        # Hack-ish way to avoid the danger bar_style being overridden by\n        # success because the bar gets closed after the error...\n        if pbar.bar_style != 'danger' or bar_style != 'success':\n            pbar.bar_style = bar_style\n\n    # Special signal to close the bar\n    if close and pbar.bar_style != 'danger':  # hide only if no error\n        try:\n            self.container.close()\n        except AttributeError:\n            self.container.visible = False\n        self.container.layout.visibility = 'hidden'  # IPYW>=8\n\n    if check_delay and self.delay > 0 and not self.displayed:\n        display(self.container)\n        self.displayed = True", "loc": 52}
{"file": "tqdm\\tqdm\\notebook.py", "class_name": "tqdm_notebook", "function_name": "update", "parameters": ["self", "n"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.disp", "super", "super().update"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update(self, n=1):\n    try:\n        return super().update(n=n)\n    # NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\n    except:  # NOQA\n        # cannot catch KeyboardInterrupt when using manual tqdm\n        # as the interrupt will most likely happen on another statement\n        self.disp(bar_style='danger')\n        raise", "loc": 9}
{"file": "tqdm\\tqdm\\notebook.py", "class_name": "tqdm_notebook", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.disp", "super", "super().close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n    super().close()\n    # Try to detect if there was an error or KeyboardInterrupt\n    # in manual mode: if n < total, things probably got wrong\n    if self.total and self.n < self.total:\n        self.disp(bar_style='danger', check_delay=False)\n    else:\n        if self.leave:\n            self.disp(bar_style='success', check_delay=False)\n        else:\n            self.disp(close=True, check_delay=False)", "loc": 13}
{"file": "tqdm\\tqdm\\notebook.py", "class_name": "tqdm_notebook", "function_name": "reset", "parameters": ["self", "total"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["super", "super().reset"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Resets to 0 iterations for repeated use. Consider combining with `leave=True`. Parameters", "source_code": "def reset(self, total=None):\n    \"\"\"\n    Resets to 0 iterations for repeated use.\n\n    Consider combining with `leave=True`.\n\n    Parameters\n    ----------\n    total  : int or float, optional. Total to use for the new bar.\n    \"\"\"\n    if self.disable:\n        return super().reset(total=total)\n    _, pbar, _ = self.container.children\n    pbar.bar_style = ''\n    if total is not None:\n        pbar.max = total\n        if not self.total and self.ncols is None:  # no longer unknown total\n            pbar.layout.width = None  # reset width\n    return super().reset(total=total)", "loc": 19}
{"file": "tqdm\\tqdm\\rich.py", "class_name": "FractionColumn", "function_name": "render", "parameters": ["self", "task"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Text", "filesize.pick_unit_and_suffix", "int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Calculate common unit for completed and total.", "source_code": "def render(self, task):\n    \"\"\"Calculate common unit for completed and total.\"\"\"\n    completed = int(task.completed)\n    total = int(task.total)\n    if self.unit_scale:\n        unit, suffix = filesize.pick_unit_and_suffix(\n            total,\n            [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n            self.unit_divisor,\n        )\n    else:\n        unit, suffix = filesize.pick_unit_and_suffix(total, [\"\"], 1)\n    precision = 0 if unit == 1 else 1\n    return Text(\n        f\"{completed/unit:,.{precision}f}/{total/unit:,.{precision}f} {suffix}\",\n        style=\"progress.download\")", "loc": 16}
{"file": "tqdm\\tqdm\\rich.py", "class_name": "RateColumn", "function_name": "render", "parameters": ["self", "task"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Text", "filesize.pick_unit_and_suffix"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Show data transfer speed.", "source_code": "def render(self, task):\n    \"\"\"Show data transfer speed.\"\"\"\n    speed = task.speed\n    if speed is None:\n        return Text(f\"? {self.unit}/s\", style=\"progress.data.speed\")\n    if self.unit_scale:\n        unit, suffix = filesize.pick_unit_and_suffix(\n            speed,\n            [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"],\n            self.unit_divisor,\n        )\n    else:\n        unit, suffix = filesize.pick_unit_and_suffix(speed, [\"\"], 1)\n    precision = 0 if unit == 1 else 1\n    return Text(f\"{speed/unit:,.{precision}f} {suffix}{self.unit}/s\",\n                style=\"progress.data.speed\")", "loc": 16}
{"file": "tqdm\\tqdm\\rich.py", "class_name": "tqdm_rich", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._prog.__exit__", "self.display", "super", "super().close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n    self.display()  # print 100%, vis #1306\n    super().close()\n    self._prog.__exit__(None, None, None)", "loc": 6}
{"file": "tqdm\\tqdm\\rich.py", "class_name": "tqdm_rich", "function_name": "reset", "parameters": ["self", "total"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self._prog.reset", "super", "super().reset"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Resets to 0 iterations for repeated use. Parameters ----------", "source_code": "def reset(self, total=None):\n    \"\"\"\n    Resets to 0 iterations for repeated use.\n\n    Parameters\n    ----------\n    total  : int or float, optional. Total to use for the new bar.\n    \"\"\"\n    if hasattr(self, '_prog'):\n        self._prog.reset(total=total)\n    super().reset(total=total)", "loc": 11}
{"file": "tqdm\\tqdm\\std.py", "class_name": null, "function_name": "TRLock", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RLock"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "threading RLock", "source_code": "def TRLock(*args, **kwargs):\n    \"\"\"threading RLock\"\"\"\n    try:\n        from threading import RLock\n        return RLock(*args, **kwargs)\n    except (ImportError, OSError):  # pragma: no cover\n        pass", "loc": 7}
{"file": "tqdm\\tqdm\\std.py", "class_name": "TqdmDefaultWriteLock", "function_name": "create_mp_lock", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RLock", "hasattr"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_mp_lock(cls):\n    if not hasattr(cls, 'mp_lock'):\n        try:\n            from multiprocessing import RLock\n            cls.mp_lock = RLock()\n        except (ImportError, OSError):  # pragma: no cover\n            cls.mp_lock = None", "loc": 7}
{"file": "tqdm\\tqdm\\std.py", "class_name": "Bar", "function_name": "colour", "parameters": ["self", "value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "int", "len", "tuple", "value.upper", "warn"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def colour(self, value):\n    if not value:\n        self._colour = None\n        return\n    try:\n        if value.upper() in self.COLOURS:\n            self._colour = self.COLOURS[value.upper()]\n        elif value[0] == '#' and len(value) == 7:\n            self._colour = self.COLOUR_RGB % tuple(\n                int(i, 16) for i in (value[1:3], value[3:5], value[5:7]))\n        else:\n            raise KeyError\n    except (KeyError, AttributeError):\n        warn(\"Unknown colour (%s); valid choices: [hex (#00ff00), %s]\" % (\n             value, \", \".join(self.COLOURS)),\n             TqdmWarning, stacklevel=2)\n        self._colour = None", "loc": 17}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "format_sizeof", "parameters": ["num", "suffix", "divisor"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["abs"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Formats a number (greater than unity) with SI Order of Magnitude prefixes. Parameters ---------- num  : float Number ( >= 1) to format. suffix  : str, optional Post-postfix [default: '']. divisor  : float, optional Divisor between prefixes [default: 1000].", "source_code": "def format_sizeof(num, suffix='', divisor=1000):\n    \"\"\"\n    Formats a number (greater than unity) with SI Order of Magnitude\n    prefixes.\n\n    Parameters\n    ----------\n    num  : float\n        Number ( >= 1) to format.\n    suffix  : str, optional\n        Post-postfix [default: ''].\n    divisor  : float, optional\n        Divisor between prefixes [default: 1000].\n\n    Returns\n    -------\n    out  : str\n        Number with Order of Magnitude SI unit postfix.\n    \"\"\"\n    for unit in ['', 'k', 'M', 'G', 'T', 'P', 'E', 'Z']:\n        if abs(num) < 999.5:\n            if abs(num) < 99.95:\n                if abs(num) < 9.995:\n                    return f'{num:1.2f}{unit}{suffix}'\n                return f'{num:2.1f}{unit}{suffix}'\n            return f'{num:3.0f}{unit}{suffix}'\n        num /= divisor\n    return f'{num:3.1f}Y{suffix}'", "loc": 28}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "format_interval", "parameters": ["t"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["divmod", "int"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Formats a number of seconds as a clock time, [H:]MM:SS Parameters ---------- t  : int Number of seconds.", "source_code": "def format_interval(t):\n    \"\"\"\n    Formats a number of seconds as a clock time, [H:]MM:SS\n\n    Parameters\n    ----------\n    t  : int\n        Number of seconds.\n\n    Returns\n    -------\n    out  : str\n        [H:]MM:SS\n    \"\"\"\n    mins, s = divmod(int(t), 60)\n    h, m = divmod(mins, 60)\n    return f'{h:d}:{m:02d}:{s:02d}' if h else f'{m:02d}:{s:02d}'", "loc": 17}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "format_num", "parameters": ["n"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f'{n:.3g}'.replace", "f'{n:.3g}'.replace('e+0', 'e+').replace", "len", "str"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Intelligent scientific notation (.3g). Parameters ---------- n  : int or float or Numeric A Number.", "source_code": "def format_num(n):\n    \"\"\"\n    Intelligent scientific notation (.3g).\n\n    Parameters\n    ----------\n    n  : int or float or Numeric\n        A Number.\n\n    Returns\n    -------\n    out  : str\n        Formatted number.\n    \"\"\"\n    f = f'{n:.3g}'.replace('e+0', 'e+').replace('e-0', 'e-')\n    n = str(n)\n    return f if len(f) < len(n) else n", "loc": 17}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "status_printer", "parameters": ["file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["disp_len", "fp.write", "fp_flush", "fp_write", "getattr", "max", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Manage the printing and in-place updating of a line of characters. Note that if the string is longer than a line, then in-place updating may not work (it will print a new line at each refresh).", "source_code": "def status_printer(file):\n    \"\"\"\n    Manage the printing and in-place updating of a line of characters.\n    Note that if the string is longer than a line, then in-place\n    updating may not work (it will print a new line at each refresh).\n    \"\"\"\n    fp = file\n    fp_flush = getattr(fp, 'flush', lambda: None)  # pragma: no cover\n    if fp in (sys.stderr, sys.stdout):\n        getattr(sys.stderr, 'flush', lambda: None)()\n        getattr(sys.stdout, 'flush', lambda: None)()\n\n    def fp_write(s):\n        fp.write(str(s))\n        fp_flush()\n\n    last_len = [0]\n\n    def print_status(s):\n        len_s = disp_len(s)\n        fp_write('\\r' + s + (' ' * max(last_len[0] - len_s, 0)))\n        last_len[0] = len_s\n\n    return print_status", "loc": 24}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "write", "parameters": ["cls", "s", "file", "end", "nolock"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.external_write_mode", "fp.write"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Print a message via tqdm (without overlap with bars).", "source_code": "def write(cls, s, file=None, end=\"\\n\", nolock=False):\n    \"\"\"Print a message via tqdm (without overlap with bars).\"\"\"\n    fp = file if file is not None else sys.stdout\n    with cls.external_write_mode(file=file, nolock=nolock):\n        # Write the message\n        fp.write(s)\n        fp.write(end)", "loc": 7}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "external_write_mode", "parameters": ["cls", "file", "nolock"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all", "cls._lock.release", "cls.get_lock", "cls.get_lock().acquire", "getattr", "hasattr", "inst.clear", "inst.refresh", "inst_cleared.append"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Disable tqdm within context and refresh tqdm when exits. Useful when writing to standard output stream", "source_code": "def external_write_mode(cls, file=None, nolock=False):\n    \"\"\"\n    Disable tqdm within context and refresh tqdm when exits.\n    Useful when writing to standard output stream\n    \"\"\"\n    fp = file if file is not None else sys.stdout\n\n    try:\n        if not nolock:\n            cls.get_lock().acquire()\n        # Clear all bars\n        inst_cleared = []\n        for inst in getattr(cls, '_instances', []):\n            # Clear instance if in the target output file\n            # or if write output + tqdm output are both either\n            # sys.stdout or sys.stderr (because both are mixed in terminal)\n            if hasattr(inst, \"start_t\") and (inst.fp == fp or all(\n                    f in (sys.stdout, sys.stderr) for f in (fp, inst.fp))):\n                inst.clear(nolock=True)\n                inst_cleared.append(inst)\n        yield\n        # Force refresh display of bars we cleared\n        for inst in inst_cleared:\n            inst.refresh(nolock=True)\n    finally:\n        if not nolock:\n            cls._lock.release()", "loc": 27}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "get_lock", "parameters": ["cls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmDefaultWriteLock", "hasattr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get the global lock. Construct it if it does not exist.", "source_code": "def get_lock(cls):\n    \"\"\"Get the global lock. Construct it if it does not exist.\"\"\"\n    if not hasattr(cls, '_lock'):\n        cls._lock = TqdmDefaultWriteLock()\n    return cls._lock", "loc": 5}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "update", "parameters": ["self", "n"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["max", "self._ema_dn", "self._ema_dt", "self._ema_miniters", "self._time", "self.refresh"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Manually update the progress bar, useful for streams such as reading files. E.g.: >>> t = tqdm(total=filesize) # Initialise >>> for current_buffer in stream: ...    ... ...    t.update(len(current_buffer)) >>> t.close() The last line is highly recommended, but possibly not necessary if `t.update()` will be called in such a way that `filesize` will be exactly reached and printed. Parameters ---------- n  : int or float, optional Increment to add to the internal counter of iterations [default: 1]. If using float, consider specifying `{n:.3f}` or similar in `bar_format`, or specifying `unit_scale`.", "source_code": "def update(self, n=1):\n    \"\"\"\n    Manually update the progress bar, useful for streams\n    such as reading files.\n    E.g.:\n    >>> t = tqdm(total=filesize) # Initialise\n    >>> for current_buffer in stream:\n    ...    ...\n    ...    t.update(len(current_buffer))\n    >>> t.close()\n    The last line is highly recommended, but possibly not necessary if\n    `t.update()` will be called in such a way that `filesize` will be\n    exactly reached and printed.\n\n    Parameters\n    ----------\n    n  : int or float, optional\n        Increment to add to the internal counter of iterations\n        [default: 1]. If using float, consider specifying `{n:.3f}`\n        or similar in `bar_format`, or specifying `unit_scale`.\n\n    Returns\n    -------\n    out  : bool or None\n        True if a `display()` was triggered.\n    \"\"\"\n    if self.disable:\n        return\n\n    if n < 0:\n        self.last_print_n += n  # for auto-refresh logic to work\n    self.n += n\n\n    # check counter first to reduce calls to time()\n    if self.n - self.last_print_n >= self.miniters:\n        cur_t = self._time()\n        dt = cur_t - self.last_print_t\n        if dt >= self.mininterval and cur_t >= self.start_t + self.delay:\n            cur_t = self._time()\n            dn = self.n - self.last_print_n  # >= n\n            if self.smoothing and dt and dn:\n                # EMA (not just overall average)\n                self._ema_dn(dn)\n                self._ema_dt(dt)\n            self.refresh(lock_args=self.lock_args)\n            if self.dynamic_miniters:\n                # If no `miniters` was specified, adjust automatically to the\n                # maximum iteration rate seen so far between two prints.\n                # e.g.: After running `tqdm.update(5)`, subsequent\n                # calls to `tqdm.update()` will only cause an update after\n                # at least 5 more iterations.\n                if self.maxinterval and dt >= self.maxinterval:\n                    self.miniters = dn * (self.mininterval or self.maxinterval) / dt\n                elif self.smoothing:\n                    # EMA miniters update\n                    self.miniters = self._ema_miniters(\n                        dn * (self.mininterval / dt if self.mininterval and dt\n                              else 1))\n                else:\n                    # max iters between two prints\n                    self.miniters = max(self.miniters, dn)\n\n            # Store old values for next call\n            self.last_print_n = self.n\n            self.last_print_t = cur_t\n            return True", "loc": 66}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["abs", "fp_write", "getattr", "self._decr_instances", "self.display", "self.fp.write", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Cleanup and (if leave=False) close the progressbar.", "source_code": "def close(self):\n    \"\"\"Cleanup and (if leave=False) close the progressbar.\"\"\"\n    if self.disable:\n        return\n\n    # Prevent multiple closures\n    self.disable = True\n\n    # decrement instance pos and remove from internal set\n    pos = abs(self.pos)\n    self._decr_instances(self)\n\n    if self.last_print_t < self.start_t + self.delay:\n        # haven't ever displayed; nothing to clear\n        return\n\n    # GUI mode\n    if getattr(self, 'sp', None) is None:\n        return\n\n    # annoyingly, _supports_unicode isn't good enough\n    def fp_write(s):\n        self.fp.write(str(s))\n\n    try:\n        fp_write('')\n    except ValueError as e:\n        if 'closed' in str(e):\n            return\n        raise  # pragma: no cover\n\n    leave = pos == 0 if self.leave is None else self.leave\n\n    with self._lock:\n        if leave:\n            # stats for overall rate (no weighted average)\n            self._ema_dt = lambda: None\n            self.display(pos=0)\n            fp_write('\\n')\n        else:\n            # clear previous display\n            if self.display(msg='', pos=pos) and not pos:\n                fp_write('\\r')", "loc": 43}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "clear", "parameters": ["self", "nolock"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["abs", "self._lock.acquire", "self._lock.release", "self.fp.write", "self.moveto", "self.sp"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Clear current bar display.", "source_code": "def clear(self, nolock=False):\n    \"\"\"Clear current bar display.\"\"\"\n    if self.disable:\n        return\n\n    if not nolock:\n        self._lock.acquire()\n    pos = abs(self.pos)\n    if pos < (self.nrows or 20):\n        self.moveto(pos)\n        self.sp('')\n        self.fp.write('\\r')  # place cursor back at the beginning of line\n        self.moveto(-pos)\n    if not nolock:\n        self._lock.release()", "loc": 15}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "refresh", "parameters": ["self", "nolock", "lock_args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._lock.acquire", "self._lock.release", "self.display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Force refresh the display of this bar. Parameters ----------", "source_code": "def refresh(self, nolock=False, lock_args=None):\n    \"\"\"\n    Force refresh the display of this bar.\n\n    Parameters\n    ----------\n    nolock  : bool, optional\n        If `True`, does not lock.\n        If [default: `False`]: calls `acquire()` on internal lock.\n    lock_args  : tuple, optional\n        Passed to internal lock's `acquire()`.\n        If specified, will only `display()` if `acquire()` returns `True`.\n    \"\"\"\n    if self.disable:\n        return\n\n    if not nolock:\n        if lock_args:\n            if not self._lock.acquire(*lock_args):\n                return False\n        else:\n            self._lock.acquire()\n    self.display()\n    if not nolock:\n        self._lock.release()\n    return True", "loc": 26}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "unpause", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Restart tqdm timer from last print time.", "source_code": "def unpause(self):\n    \"\"\"Restart tqdm timer from last print time.\"\"\"\n    if self.disable:\n        return\n    cur_t = self._time()\n    self.start_t += cur_t - self.last_print_t\n    self.last_print_t = cur_t", "loc": 7}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "reset", "parameters": ["self", "total"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["EMA", "self._time", "self.refresh"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Resets to 0 iterations for repeated use. Consider combining with `leave=True`. Parameters", "source_code": "def reset(self, total=None):\n    \"\"\"\n    Resets to 0 iterations for repeated use.\n\n    Consider combining with `leave=True`.\n\n    Parameters\n    ----------\n    total  : int or float, optional. Total to use for the new bar.\n    \"\"\"\n    self.n = 0\n    if total is not None:\n        self.total = total\n    if self.disable:\n        return\n    self.last_print_n = 0\n    self.last_print_t = self.start_t = self._time()\n    self._ema_dn = EMA(self.smoothing)\n    self._ema_dt = EMA(self.smoothing)\n    self._ema_miniters = EMA(self.smoothing)\n    self.refresh()", "loc": 21}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "set_description", "parameters": ["self", "desc", "refresh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.refresh"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Set/modify description of the progress bar. Parameters ----------", "source_code": "def set_description(self, desc=None, refresh=True):\n    \"\"\"\n    Set/modify description of the progress bar.\n\n    Parameters\n    ----------\n    desc  : str, optional\n    refresh  : bool, optional\n        Forces refresh [default: True].\n    \"\"\"\n    self.desc = desc + ': ' if desc else ''\n    if refresh:\n        self.refresh()", "loc": 13}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "set_description_str", "parameters": ["self", "desc", "refresh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.refresh"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Set/modify description without ': ' appended.", "source_code": "def set_description_str(self, desc=None, refresh=True):\n    \"\"\"Set/modify description without ': ' appended.\"\"\"\n    self.desc = desc or ''\n    if refresh:\n        self.refresh()", "loc": 5}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "set_postfix", "parameters": ["self", "ordered_dict", "refresh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "OrderedDict", "isinstance", "kwargs.keys", "postfix.keys", "postfix[key].strip", "self.format_num", "self.refresh", "sorted", "str"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Set/modify postfix (additional stats) with automatic formatting based on datatype. Parameters", "source_code": "def set_postfix(self, ordered_dict=None, refresh=True, **kwargs):\n    \"\"\"\n    Set/modify postfix (additional stats)\n    with automatic formatting based on datatype.\n\n    Parameters\n    ----------\n    ordered_dict  : dict or OrderedDict, optional\n    refresh  : bool, optional\n        Forces refresh [default: True].\n    kwargs  : dict, optional\n    \"\"\"\n    # Sort in alphabetical order to be more deterministic\n    postfix = OrderedDict([] if ordered_dict is None else ordered_dict)\n    for key in sorted(kwargs.keys()):\n        postfix[key] = kwargs[key]\n    # Preprocess stats according to datatype\n    for key in postfix.keys():\n        # Number: limit the length of the string\n        if isinstance(postfix[key], Number):\n            postfix[key] = self.format_num(postfix[key])\n        # Else for any other type, try to get the string conversion\n        elif not isinstance(postfix[key], str):\n            postfix[key] = str(postfix[key])\n        # Else if it's a string, don't need to preprocess anything\n    # Stitch together to get the final postfix\n    self.postfix = ', '.join(key + '=' + postfix[key].strip()\n                             for key in postfix.keys())\n    if refresh:\n        self.refresh()", "loc": 30}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "set_postfix_str", "parameters": ["self", "s", "refresh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.refresh", "str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Postfix without dictionary expansion, similar to prefix handling.", "source_code": "def set_postfix_str(self, s='', refresh=True):\n    \"\"\"\n    Postfix without dictionary expansion, similar to prefix handling.\n    \"\"\"\n    self.postfix = str(s)\n    if refresh:\n        self.refresh()", "loc": 7}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "format_dict", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["defaultdict", "hasattr", "self._ema_dn", "self._ema_dt", "self._time", "self.dynamic_ncols"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Public API for read-only member access.", "source_code": "def format_dict(self):\n    \"\"\"Public API for read-only member access.\"\"\"\n    if self.disable and not hasattr(self, 'unit'):\n        return defaultdict(lambda: None, {\n            'n': self.n, 'total': self.total, 'elapsed': 0, 'unit': 'it'})\n    if self.dynamic_ncols:\n        self.ncols, self.nrows = self.dynamic_ncols(self.fp)\n    return {\n        'n': self.n, 'total': self.total,\n        'elapsed': self._time() - self.start_t if hasattr(self, 'start_t') else 0,\n        'ncols': self.ncols, 'nrows': self.nrows, 'prefix': self.desc,\n        'ascii': self.ascii, 'unit': self.unit, 'unit_scale': self.unit_scale,\n        'rate': self._ema_dn() / self._ema_dt() if self._ema_dt() else None,\n        'bar_format': self.bar_format, 'postfix': self.postfix,\n        'unit_divisor': self.unit_divisor, 'initial': self.initial,\n        'colour': self.colour}", "loc": 16}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "display", "parameters": ["self", "msg", "pos"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmDeprecationWarning", "abs", "getattr", "hasattr", "self.__str__", "self.moveto", "self.sp"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Use `self.sp` to display `msg` in the specified `pos`. Consider overloading this function when inheriting to use e.g.: `self.some_frontend(**self.format_dict)` instead of `self.sp`.", "source_code": "def display(self, msg=None, pos=None):\n    \"\"\"\n    Use `self.sp` to display `msg` in the specified `pos`.\n\n    Consider overloading this function when inheriting to use e.g.:\n    `self.some_frontend(**self.format_dict)` instead of `self.sp`.\n\n    Parameters\n    ----------\n    msg  : str, optional. What to display (default: `repr(self)`).\n    pos  : int, optional. Position to `moveto`\n      (default: `abs(self.pos)`).\n    \"\"\"\n    if pos is None:\n        pos = abs(self.pos)\n\n    nrows = self.nrows or 20\n    if pos >= nrows - 1:\n        if pos >= nrows:\n            return False\n        if msg or msg is None:  # override at `nrows - 1`\n            msg = \" ... (more hidden) ...\"\n\n    if not hasattr(self, \"sp\"):\n        raise TqdmDeprecationWarning(\n            \"Please use `tqdm.gui.tqdm(...)`\"\n            \" instead of `tqdm(..., gui=True)`\\n\",\n            fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\n    if pos:\n        self.moveto(pos)\n    self.sp(self.__str__() if msg is None else msg)\n    if pos:\n        self.moveto(-pos)\n    return True", "loc": 35}
{"file": "tqdm\\tqdm\\std.py", "class_name": "tqdm", "function_name": "wrapattr", "parameters": ["cls", "stream", "method", "total", "bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["CallbackIOWrapper", "cls"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "stream  : file-like object. method  : str, \"read\" or \"write\". The result of `read()` and the first argument of `write()` should have a `len()`.", "source_code": "def wrapattr(cls, stream, method, total=None, bytes=True, **tqdm_kwargs):\n    \"\"\"\n    stream  : file-like object.\n    method  : str, \"read\" or \"write\". The result of `read()` and\n        the first argument of `write()` should have a `len()`.\n\n    >>> with tqdm.wrapattr(file_obj, \"read\", total=file_obj.size) as fobj:\n    ...     while True:\n    ...         chunk = fobj.read(chunk_size)\n    ...         if not chunk:\n    ...             break\n    \"\"\"\n    with cls(total=total, **tqdm_kwargs) as t:\n        if bytes:\n            t.unit = \"B\"\n            t.unit_scale = True\n            t.unit_divisor = 1024\n        yield CallbackIOWrapper(t.update, stream, method)", "loc": 18}
{"file": "tqdm\\tqdm\\std.py", "class_name": null, "function_name": "inner_generator", "parameters": ["df_function"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmDeprecationWarning", "cls", "func", "getattr", "is_builtin_func", "isinstance", "kwargs.get", "len", "t.close", "t.update", "tqdm_kwargs.pop"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner_generator(df_function='apply'):\n    def inner(df, func, *args, **kwargs):\n        \"\"\"\n        Parameters\n        ----------\n        df  : (DataFrame|Series)[GroupBy]\n            Data (may be grouped).\n        func  : function\n            To be applied on the (grouped) data.\n        **kwargs  : optional\n            Transmitted to `df.apply()`.\n        \"\"\"\n\n        # Precompute total iterations\n        total = tqdm_kwargs.pop(\"total\", getattr(df, 'ngroups', None))\n        if total is None:  # not grouped\n            if df_function == 'applymap':\n                total = df.size\n            elif isinstance(df, Series):\n                total = len(df)\n            elif (_Rolling_and_Expanding is None or\n                  not isinstance(df, _Rolling_and_Expanding)):\n                # DataFrame or Panel\n                axis = kwargs.get('axis', 0)\n                if axis == 'index':\n                    axis = 0\n                elif axis == 'columns':\n                    axis = 1\n                # when axis=0, total is shape[axis1]\n                total = df.size // df.shape[axis]\n\n        # Init bar\n        if deprecated_t[0] is not None:\n            t = deprecated_t[0]\n            deprecated_t[0] = None\n        else:\n            t = cls(total=total, **tqdm_kwargs)\n\n        if len(args) > 0:\n            # *args intentionally not supported (see #244, #299)\n            TqdmDeprecationWarning(\n                \"Except func, normal arguments are intentionally\" +\n                \" not supported by\" +\n                \" `(DataFrame|Series|GroupBy).progress_apply`.\" +\n                \" Use keyword arguments instead.\",\n                fp_write=getattr(t.fp, 'write', sys.stderr.write))\n\n        try:  # pandas>=1.3.0\n            from pandas.core.common import is_builtin_func\n        except ImportError:\n            is_builtin_func = df._is_builtin_func\n        try:\n            func = is_builtin_func(func)\n        except TypeError:\n            pass\n\n        # Define bar updating wrapper\n        def wrapper(*args, **kwargs):\n            # update tbar correctly\n            # it seems `pandas apply` calls `func` twice\n            # on the first column/row to decide whether it can\n            # take a fast or slow code path; so stop when t.total==t.n\n            t.update(n=1 if not t.total or t.n < t.total else 0)\n            return func(*args, **kwargs)\n\n        # Apply the provided function (in **kwargs)\n        # on the df using our wrapper (which provides bar updating)\n        try:\n            return getattr(df, df_function)(wrapper, **kwargs)\n        finally:\n            t.close()\n\n    return inner", "loc": 73}
{"file": "tqdm\\tqdm\\std.py", "class_name": null, "function_name": "inner", "parameters": ["df", "func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmDeprecationWarning", "cls", "func", "getattr", "is_builtin_func", "isinstance", "kwargs.get", "len", "t.close", "t.update", "tqdm_kwargs.pop"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Parameters ---------- df  : (DataFrame|Series)[GroupBy]", "source_code": "def inner(df, func, *args, **kwargs):\n    \"\"\"\n    Parameters\n    ----------\n    df  : (DataFrame|Series)[GroupBy]\n        Data (may be grouped).\n    func  : function\n        To be applied on the (grouped) data.\n    **kwargs  : optional\n        Transmitted to `df.apply()`.\n    \"\"\"\n\n    # Precompute total iterations\n    total = tqdm_kwargs.pop(\"total\", getattr(df, 'ngroups', None))\n    if total is None:  # not grouped\n        if df_function == 'applymap':\n            total = df.size\n        elif isinstance(df, Series):\n            total = len(df)\n        elif (_Rolling_and_Expanding is None or\n              not isinstance(df, _Rolling_and_Expanding)):\n            # DataFrame or Panel\n            axis = kwargs.get('axis', 0)\n            if axis == 'index':\n                axis = 0\n            elif axis == 'columns':\n                axis = 1\n            # when axis=0, total is shape[axis1]\n            total = df.size // df.shape[axis]\n\n    # Init bar\n    if deprecated_t[0] is not None:\n        t = deprecated_t[0]\n        deprecated_t[0] = None\n    else:\n        t = cls(total=total, **tqdm_kwargs)\n\n    if len(args) > 0:\n        # *args intentionally not supported (see #244, #299)\n        TqdmDeprecationWarning(\n            \"Except func, normal arguments are intentionally\" +\n            \" not supported by\" +\n            \" `(DataFrame|Series|GroupBy).progress_apply`.\" +\n            \" Use keyword arguments instead.\",\n            fp_write=getattr(t.fp, 'write', sys.stderr.write))\n\n    try:  # pandas>=1.3.0\n        from pandas.core.common import is_builtin_func\n    except ImportError:\n        is_builtin_func = df._is_builtin_func\n    try:\n        func = is_builtin_func(func)\n    except TypeError:\n        pass\n\n    # Define bar updating wrapper\n    def wrapper(*args, **kwargs):\n        # update tbar correctly\n        # it seems `pandas apply` calls `func` twice\n        # on the first column/row to decide whether it can\n        # take a fast or slow code path; so stop when t.total==t.n\n        t.update(n=1 if not t.total or t.n < t.total else 0)\n        return func(*args, **kwargs)\n\n    # Apply the provided function (in **kwargs)\n    # on the df using our wrapper (which provides bar updating)\n    try:\n        return getattr(df, df_function)(wrapper, **kwargs)\n    finally:\n        t.close()", "loc": 70}
{"file": "tqdm\\tqdm\\std.py", "class_name": null, "function_name": "wrapper", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["func", "t.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(*args, **kwargs):\n    # update tbar correctly\n    # it seems `pandas apply` calls `func` twice\n    # on the first column/row to decide whether it can\n    # take a fast or slow code path; so stop when t.total==t.n\n    t.update(n=1 if not t.total or t.n < t.total else 0)\n    return func(*args, **kwargs)", "loc": 7}
{"file": "tqdm\\tqdm\\tk.py", "class_name": "tqdm_tk", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_close", "self._instances.remove", "self._tk_window.after", "self._tk_window.protocol", "self._tk_window.update", "self.get_lock", "warn"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n\n    self.disable = True\n\n    with self.get_lock():\n        self._instances.remove(self)\n\n    def _close():\n        self._tk_window.after('idle', self._tk_window.destroy)\n        if not self._tk_dispatching:\n            self._tk_window.update()\n\n    self._tk_window.protocol(\"WM_DELETE_WINDOW\", _close)\n\n    # if leave is set but we are self-dispatching, the left window is\n    # totally unresponsive unless the user manually dispatches\n    if not self.leave:\n        _close()\n    elif not self._tk_dispatching:\n        if self._warn_leave:\n            warn(\"leave flag ignored if not in tkinter mainloop\",\n                 TqdmWarning, stacklevel=2)\n        _close()", "loc": 25}
{"file": "tqdm\\tqdm\\tk.py", "class_name": "tqdm_tk", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "d['bar_format'] or '{l_bar}<bar/>{r_bar}'.replace", "re.split", "self._tk_n_var.set", "self._tk_text_var.set", "self._tk_window.update", "self.format_meter"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, *_, **__):\n    self._tk_n_var.set(self.n)\n    d = self.format_dict\n    # remove {bar}\n    d['bar_format'] = (d['bar_format'] or \"{l_bar}<bar/>{r_bar}\").replace(\n        \"{bar}\", \"<bar/>\")\n    msg = self.format_meter(**d)\n    if '<bar/>' in msg:\n        msg = \"\".join(re.split(r'\\|?<bar/>\\|?', msg, maxsplit=1))\n    self._tk_text_var.set(msg)\n    if not self._tk_dispatching:\n        self._tk_window.update()", "loc": 12}
{"file": "tqdm\\tqdm\\tk.py", "class_name": "tqdm_tk", "function_name": "set_description_str", "parameters": ["self", "desc", "refresh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._tk_window.update", "self._tk_window.wm_title"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_description_str(self, desc=None, refresh=True):\n    self.desc = desc\n    if not self.disable:\n        self._tk_window.wm_title(desc)\n        if refresh and not self._tk_dispatching:\n            self._tk_window.update()", "loc": 6}
{"file": "tqdm\\tqdm\\tk.py", "class_name": "tqdm_tk", "function_name": "cancel", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._cancel_callback", "self.close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "`cancel_callback()` followed by `close()` when close/cancel buttons clicked.", "source_code": "def cancel(self):\n    \"\"\"\n    `cancel_callback()` followed by `close()`\n    when close/cancel buttons clicked.\n    \"\"\"\n    if self._cancel_callback is not None:\n        self._cancel_callback()\n    self.close()", "loc": 8}
{"file": "tqdm\\tqdm\\tk.py", "class_name": "tqdm_tk", "function_name": "reset", "parameters": ["self", "total"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self._tk_pbar.configure", "super", "super().reset"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Resets to 0 iterations for repeated use. Parameters ----------", "source_code": "def reset(self, total=None):\n    \"\"\"\n    Resets to 0 iterations for repeated use.\n\n    Parameters\n    ----------\n    total  : int or float, optional. Total to use for the new bar.\n    \"\"\"\n    if hasattr(self, '_tk_pbar'):\n        if total is None:\n            self._tk_pbar.configure(maximum=100, mode=\"indeterminate\")\n        else:\n            self._tk_pbar.configure(maximum=total, mode=\"determinate\")\n    super().reset(total=total)", "loc": 14}
{"file": "tqdm\\tqdm\\utils.py", "class_name": null, "function_name": "envwrap", "parameters": ["prefix", "types", "is_method"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["env_overrides.items", "getattr", "k.startswith", "k[i:].lower", "len", "os.environ.items", "part", "signature", "typ", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Override parameter defaults via `os.environ[prefix + param_name]`. Maps UPPER_CASE env vars map to lower_case param names. camelCase isn't supported (because Windows ignores case).", "source_code": "def envwrap(prefix, types=None, is_method=False):\n    \"\"\"\n    Override parameter defaults via `os.environ[prefix + param_name]`.\n    Maps UPPER_CASE env vars map to lower_case param names.\n    camelCase isn't supported (because Windows ignores case).\n\n    Precedence (highest first):\n\n    - call (`foo(a=3)`)\n    - environ (`FOO_A=2`)\n    - signature (`def foo(a=1)`)\n\n    Parameters\n    ----------\n    prefix  : str\n        Env var prefix, e.g. \"FOO_\"\n    types  : dict, optional\n        Fallback mappings `{'param_name': type, ...}` if types cannot be\n        inferred from function signature.\n        Consider using `types=collections.defaultdict(lambda: ast.literal_eval)`.\n    is_method  : bool, optional\n        Whether to use `functools.partialmethod`. If (default: False) use `functools.partial`.\n\n    Examples\n    --------\n    ```\n    $ cat foo.py\n    from tqdm.utils import envwrap\n    @envwrap(\"FOO_\")\n    def test(a=1, b=2, c=3):\n        print(f\"received: a={a}, b={b}, c={c}\")\n\n    $ FOO_A=42 FOO_C=1337 python -c 'import foo; foo.test(c=99)'\n    received: a=42, b=2, c=99\n    ```\n    \"\"\"\n    if types is None:\n        types = {}\n    i = len(prefix)\n    env_overrides = {k[i:].lower(): v for k, v in os.environ.items() if k.startswith(prefix)}\n    part = partialmethod if is_method else partial\n\n    def wrap(func):\n        params = signature(func).parameters\n        # ignore unknown env vars\n        overrides = {k: v for k, v in env_overrides.items() if k in params}\n        # infer overrides' `type`s\n        for k in overrides:\n            param = params[k]\n            if param.annotation is not param.empty:  # typehints\n                for typ in getattr(param.annotation, '__args__', (param.annotation,)):\n                    try:\n                        overrides[k] = typ(overrides[k])\n                    except Exception:\n                        pass\n                    else:\n                        break\n            elif param.default is not None:  # type of default value\n                overrides[k] = type(param.default)(overrides[k])\n            else:\n                try:  # `types` fallback\n                    overrides[k] = types[k](overrides[k])\n                except KeyError:  # keep unconverted (`str`)\n                    pass\n        return part(func, **overrides)\n    return wrap", "loc": 66}
{"file": "tqdm\\tqdm\\utils.py", "class_name": null, "function_name": "disp_len", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RE_ANSI.sub", "_text_width"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def disp_len(data):\n    \"\"\"\n    Returns the real on-screen length of a string which may contain\n    ANSI control codes and wide chars.\n    \"\"\"\n    return _text_width(RE_ANSI.sub('', data))", "loc": 6}
{"file": "tqdm\\tqdm\\utils.py", "class_name": null, "function_name": "disp_trim", "parameters": ["data", "length"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RE_ANSI.search", "bool", "data.endswith", "disp_len", "len"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Trim a string which may contain ANSI control characters.", "source_code": "def disp_trim(data, length):\n    \"\"\"\n    Trim a string which may contain ANSI control characters.\n    \"\"\"\n    if len(data) == disp_len(data):\n        return data[:length]\n\n    ansi_present = bool(RE_ANSI.search(data))\n    while disp_len(data) > length:  # carefully delete one char at a time\n        data = data[:-1]\n    if ansi_present and bool(RE_ANSI.search(data)):\n        # assume ANSI reset is required\n        return data if data.endswith(\"\\033[0m\") else data + \"\\033[0m\"\n    return data", "loc": 14}
{"file": "tqdm\\tqdm\\utils.py", "class_name": null, "function_name": "wrap", "parameters": ["func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["env_overrides.items", "getattr", "part", "signature", "typ", "type"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrap(func):\n    params = signature(func).parameters\n    # ignore unknown env vars\n    overrides = {k: v for k, v in env_overrides.items() if k in params}\n    # infer overrides' `type`s\n    for k in overrides:\n        param = params[k]\n        if param.annotation is not param.empty:  # typehints\n            for typ in getattr(param.annotation, '__args__', (param.annotation,)):\n                try:\n                    overrides[k] = typ(overrides[k])\n                except Exception:\n                    pass\n                else:\n                    break\n        elif param.default is not None:  # type of default value\n            overrides[k] = type(param.default)(overrides[k])\n        else:\n            try:  # `types` fallback\n                overrides[k] = types[k](overrides[k])\n            except KeyError:  # keep unconverted (`str`)\n                pass\n    return part(func, **overrides)", "loc": 23}
{"file": "tqdm\\tqdm\\utils.py", "class_name": "ObjectWrapper", "function_name": "wrapper_getattr", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "object.__getattr__"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Actual `self.getattr` rather than self._wrapped.getattr", "source_code": "def wrapper_getattr(self, name):\n    \"\"\"Actual `self.getattr` rather than self._wrapped.getattr\"\"\"\n    try:\n        return object.__getattr__(self, name)\n    except AttributeError:  # py2\n        return getattr(self, name)", "loc": 6}
{"file": "tqdm\\tqdm\\utils.py", "class_name": "SimpleTextIOWrapper", "function_name": "write", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["s.encode", "self._wrapped.write", "self.wrapper_getattr"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Encode `s` and pass to the wrapped object's `.write()` method.", "source_code": "def write(self, s):\n    \"\"\"\n    Encode `s` and pass to the wrapped object's `.write()` method.\n    \"\"\"\n    return self._wrapped.write(s.encode(self.wrapper_getattr('encoding')))", "loc": 5}
{"file": "tqdm\\tqdm\\utils.py", "class_name": "DisableOnWriteError", "function_name": "disable_on_exception", "parameters": ["tqdm_instance", "func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "func", "proxy", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Quietly set `tqdm_instance.miniters=inf` if `func` raises `errno=5`.", "source_code": "def disable_on_exception(tqdm_instance, func):\n    \"\"\"\n    Quietly set `tqdm_instance.miniters=inf` if `func` raises `errno=5`.\n    \"\"\"\n    tqdm_instance = proxy(tqdm_instance)\n\n    def inner(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except OSError as e:\n            if e.errno != 5:\n                raise\n            try:\n                tqdm_instance.miniters = float('inf')\n            except ReferenceError:\n                pass\n        except ValueError as e:\n            if 'closed' not in str(e):\n                raise\n            try:\n                tqdm_instance.miniters = float('inf')\n            except ReferenceError:\n                pass\n    return inner", "loc": 24}
{"file": "tqdm\\tqdm\\utils.py", "class_name": null, "function_name": "inner", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "func", "str"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner(*args, **kwargs):\n    try:\n        return func(*args, **kwargs)\n    except OSError as e:\n        if e.errno != 5:\n            raise\n        try:\n            tqdm_instance.miniters = float('inf')\n        except ReferenceError:\n            pass\n    except ValueError as e:\n        if 'closed' not in str(e):\n            raise\n        try:\n            tqdm_instance.miniters = float('inf')\n        except ReferenceError:\n            pass", "loc": 17}
{"file": "tqdm\\tqdm\\_monitor.py", "class_name": "TMonitor", "function_name": "exit", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["current_thread", "self.join", "self.report", "self.was_killed.set"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def exit(self):\n    self.was_killed.set()\n    if self is not current_thread():\n        self.join()\n    return self.report()", "loc": 5}
{"file": "tqdm\\tqdm\\_monitor.py", "class_name": "TMonitor", "function_name": "get_instances", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "self.tqdm_cls._instances.copy"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_instances(self):\n    # returns a copy of started `tqdm_cls` instances\n    return [i for i in self.tqdm_cls._instances.copy()\n            # Avoid race by checking that the instance started\n            if hasattr(i, 'start_t')]", "loc": 5}
{"file": "tqdm\\tqdm\\_monitor.py", "class_name": "TMonitor", "function_name": "run", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["instance.refresh", "self._time", "self.get_instances", "self.tqdm_cls.get_lock", "self.was_killed.is_set", "self.was_killed.wait", "warn"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self):\n    cur_t = self._time()\n    while True:\n        # After processing and before sleeping, notify that we woke\n        # Need to be done just before sleeping\n        self.woken = cur_t\n        # Sleep some time...\n        self.was_killed.wait(self.sleep_interval)\n        # Quit if killed\n        if self.was_killed.is_set():\n            return\n        # Then monitor!\n        # Acquire lock (to access _instances)\n        with self.tqdm_cls.get_lock():\n            cur_t = self._time()\n            # Check tqdm instances are waiting too long to print\n            instances = self.get_instances()\n            for instance in instances:\n                # Check event in loop to reduce blocking time on exit\n                if self.was_killed.is_set():\n                    return\n                # Only if mininterval > 1 (else iterations are just slow)\n                # and last refresh exceeded maxinterval\n                if (\n                    instance.miniters > 1\n                    and (cur_t - instance.last_print_t) >= instance.maxinterval\n                ):\n                    # force bypassing miniters on next iteration\n                    # (dynamic_miniters adjusts mininterval automatically)\n                    instance.miniters = 1\n                    # Refresh now! (works only for manual tqdm)\n                    instance.refresh(nolock=True)\n                # Remove accidental long-lived strong reference\n                del instance\n            if instances != self.get_instances():  # pragma: nocover\n                warn(\"Set changed size during iteration\" +\n                     \" (see https://github.com/tqdm/tqdm/issues/481)\",\n                     TqdmSynchronisationWarning, stacklevel=2)\n            # Remove accidental long-lived strong references\n            del instances", "loc": 40}
{"file": "tqdm\\tqdm\\_tqdm_pandas.py", "class_name": null, "function_name": "tqdm_pandas", "parameters": ["tclass"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TqdmDeprecationWarning", "getattr", "getattr(tclass, '__name__', '').startswith", "isinstance", "tclass.pandas", "tqdm_kwargs.get", "type", "type(tclass).pandas"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Registers the given `tqdm` instance with `pandas.core.groupby.DataFrameGroupBy.progress_apply`.", "source_code": "def tqdm_pandas(tclass, **tqdm_kwargs):\n    \"\"\"\n    Registers the given `tqdm` instance with\n    `pandas.core.groupby.DataFrameGroupBy.progress_apply`.\n    \"\"\"\n    from tqdm import TqdmDeprecationWarning\n\n    if isinstance(tclass, type) or (getattr(tclass, '__name__', '').startswith(\n            'tqdm_')):  # delayed adapter case\n        TqdmDeprecationWarning(\n            \"Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm, ...)`.\",\n            fp_write=getattr(tqdm_kwargs.get('file', None), 'write', sys.stderr.write))\n        tclass.pandas(**tqdm_kwargs)\n    else:\n        TqdmDeprecationWarning(\n            \"Please use `tqdm.pandas(...)` instead of `tqdm_pandas(tqdm(...))`.\",\n            fp_write=getattr(tclass.fp, 'write', sys.stderr.write))\n        type(tclass).pandas(deprecated_t=tclass)", "loc": 18}
{"file": "tqdm\\tqdm\\__init__.py", "class_name": null, "function_name": "tqdm_notebook", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_tqdm_notebook", "warn"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "See tqdm.notebook.tqdm for full documentation", "source_code": "def tqdm_notebook(*args, **kwargs):  # pragma: no cover\n    \"\"\"See tqdm.notebook.tqdm for full documentation\"\"\"\n    from warnings import warn\n\n    from .notebook import tqdm as _tqdm_notebook\n    warn(\"This function will be removed in tqdm==5.0.0\\n\"\n         \"Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\",\n         TqdmDeprecationWarning, stacklevel=2)\n    return _tqdm_notebook(*args, **kwargs)", "loc": 9}
{"file": "tqdm\\tqdm\\__init__.py", "class_name": null, "function_name": "tnrange", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_tnrange", "warn"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.", "source_code": "def tnrange(*args, **kwargs):  # pragma: no cover\n    \"\"\"Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.\"\"\"\n    from warnings import warn\n\n    from .notebook import trange as _tnrange\n    warn(\"Please use `tqdm.notebook.trange` instead of `tqdm.tnrange`\",\n         TqdmDeprecationWarning, stacklevel=2)\n    return _tnrange(*args, **kwargs)", "loc": 8}
{"file": "tqdm\\tqdm\\contrib\\concurrent.py", "class_name": null, "function_name": "ensure_lock", "parameters": ["tqdm_class", "lock_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "tqdm_class.get_lock", "tqdm_class.set_lock"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "get (create if necessary) and then restore `tqdm_class`'s lock", "source_code": "def ensure_lock(tqdm_class, lock_name=\"\"):\n    \"\"\"get (create if necessary) and then restore `tqdm_class`'s lock\"\"\"\n    old_lock = getattr(tqdm_class, '_lock', None)  # don't create a new lock\n    lock = old_lock or tqdm_class.get_lock()  # maybe create a new lock\n    lock = getattr(lock, lock_name, lock)  # maybe subtype\n    tqdm_class.set_lock(lock)\n    yield lock\n    if old_lock is None:\n        del tqdm_class._lock\n    else:\n        tqdm_class.set_lock(old_lock)", "loc": 11}
{"file": "tqdm\\tqdm\\contrib\\concurrent.py", "class_name": null, "function_name": "process_map", "parameters": ["fn"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_executor_map", "map", "max", "tqdm_kwargs.copy", "warn"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Equivalent of `list(map(fn, *iterables))` driven by `concurrent.futures.ProcessPoolExecutor`. Parameters", "source_code": "def process_map(fn, *iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `list(map(fn, *iterables))`\n    driven by `concurrent.futures.ProcessPoolExecutor`.\n\n    Parameters\n    ----------\n    tqdm_class  : optional\n        `tqdm` class to use for bars [default: tqdm.auto.tqdm].\n    max_workers  : int, optional\n        Maximum number of workers to spawn; passed to\n        `concurrent.futures.ProcessPoolExecutor.__init__`.\n        [default: min(32, cpu_count() + 4)].\n    chunksize  : int, optional\n        Size of chunks sent to worker processes; passed to\n        `concurrent.futures.ProcessPoolExecutor.map`. [default: 1].\n    lock_name  : str, optional\n        Member of `tqdm_class.get_lock()` to use [default: mp_lock].\n    \"\"\"\n    from concurrent.futures import ProcessPoolExecutor\n    if iterables and \"chunksize\" not in tqdm_kwargs:\n        # default `chunksize=1` has poor performance for large iterables\n        # (most time spent dispatching items to workers).\n        longest_iterable_len = max(map(length_hint, iterables))\n        if longest_iterable_len > 1000:\n            from warnings import warn\n            warn(\"Iterable length %d > 1000 but `chunksize` is not set.\"\n                 \" This may seriously degrade multiprocess performance.\"\n                 \" Set `chunksize=1` or more.\" % longest_iterable_len,\n                 TqdmWarning, stacklevel=2)\n    if \"lock_name\" not in tqdm_kwargs:\n        tqdm_kwargs = tqdm_kwargs.copy()\n        tqdm_kwargs[\"lock_name\"] = \"mp_lock\"\n    return _executor_map(ProcessPoolExecutor, fn, *iterables, **tqdm_kwargs)", "loc": 34}
{"file": "tqdm\\tqdm\\contrib\\discord.py", "class_name": "DiscordIO", "function_name": "message_id", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "res.get", "self.session.post", "self.session.post(f'{self.API}/channels/{self.channel_id}/messages', headers={'Authorization': f'Bot {self.token}', 'User-Agent': self.UA}, json={'content': f'`{self.text}`'}).json", "str", "tqdm_auto.write", "warn"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def message_id(self):\n    if hasattr(self, '_message_id'):\n        return self._message_id\n    try:\n        res = self.session.post(\n            f'{self.API}/channels/{self.channel_id}/messages',\n            headers={'Authorization': f'Bot {self.token}', 'User-Agent': self.UA},\n            json={'content': f\"`{self.text}`\"}).json()\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        if res.get('error_code') == 429:\n            warn(\"Creation rate limit: try increasing `mininterval`.\",\n                 TqdmWarning, stacklevel=2)\n        else:\n            self._message_id = res['id']\n            return self._message_id", "loc": 17}
{"file": "tqdm\\tqdm\\contrib\\discord.py", "class_name": "DiscordIO", "function_name": "write", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["s.replace", "s.replace('\\r', '').strip", "self.submit", "str", "tqdm_auto.write"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Replaces internal `message_id`'s text with `s`.", "source_code": "def write(self, s):\n    \"\"\"Replaces internal `message_id`'s text with `s`.\"\"\"\n    if not s:\n        s = \"...\"\n    s = s.replace('\\r', '').strip()\n    if s == self.text:\n        return  # avoid duplicate message Bot error\n    message_id = self.message_id\n    if message_id is None:\n        return\n    self.text = s\n    try:\n        future = self.submit(\n            self.session.patch,\n            f'{self.API}/channels/{self.channel_id}/messages/{message_id}',\n            headers={'Authorization': f'Bot {self.token}', 'User-Agent': self.UA},\n            json={'content': f\"`{self.text}`\"})\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        return future", "loc": 21}
{"file": "tqdm\\tqdm\\contrib\\discord.py", "class_name": "DiscordIO", "function_name": "delete", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.submit", "str", "tqdm_auto.write"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Deletes internal `message_id`.", "source_code": "def delete(self):\n    \"\"\"Deletes internal `message_id`.\"\"\"\n    try:\n        future = self.submit(\n            self.session.delete,\n            f'{self.API}/channels/{self.channel_id}/messages/{self.message_id}',\n            headers={'Authorization': f'Bot {self.token}', 'User-Agent': self.UA})\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        return future", "loc": 11}
{"file": "tqdm\\tqdm\\contrib\\discord.py", "class_name": "tqdm_discord", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fmt.get", "fmt['bar_format'].replace", "fmt['bar_format'].replace('<bar/>', '{bar:10u}').replace", "self.dio.write", "self.format_meter", "super", "super().display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, **kwargs):\n    super().display(**kwargs)\n    fmt = self.format_dict\n    if fmt.get('bar_format', None):\n        fmt['bar_format'] = fmt['bar_format'].replace(\n            '<bar/>', '{bar:10u}').replace('{bar}', '{bar:10u}')\n    else:\n        fmt['bar_format'] = '{l_bar}{bar:10u}{r_bar}'\n    self.dio.write(self.format_meter(**fmt))", "loc": 9}
{"file": "tqdm\\tqdm\\contrib\\discord.py", "class_name": "tqdm_discord", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.dio.delete", "super", "super().close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n    super().close()\n    if not (self.leave or (self.leave is None and self.pos == 0)):\n        self.dio.delete()", "loc": 6}
{"file": "tqdm\\tqdm\\contrib\\itertools.py", "class_name": null, "function_name": "product", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["itertools.product", "kwargs.pop", "kwargs.setdefault", "list", "map", "t.update", "tqdm_class", "tqdm_kwargs.copy"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "Equivalent of `itertools.product`. Parameters ----------", "source_code": "def product(*iterables, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `itertools.product`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    \"\"\"\n    kwargs = tqdm_kwargs.copy()\n    tqdm_class = kwargs.pop(\"tqdm_class\", tqdm_auto)\n    try:\n        lens = list(map(len, iterables))\n    except TypeError:\n        total = None\n    else:\n        total = 1\n        for i in lens:\n            total *= i\n        kwargs.setdefault(\"total\", total)\n    with tqdm_class(**kwargs) as t:\n        it = itertools.product(*iterables)\n        for i in it:\n            yield i\n            t.update()", "loc": 24}
{"file": "tqdm\\tqdm\\contrib\\logging.py", "class_name": null, "function_name": "logging_redirect_tqdm", "parameters": ["loggers", "tqdm_class"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_TqdmLoggingHandler", "_get_first_found_console_logging_handler", "_is_console_logging_handler", "tqdm_handler.setFormatter", "zip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Context manager redirecting console logging to `tqdm.write()`, leaving other logging handlers (e.g. log files) unaffected. Parameters", "source_code": "def logging_redirect_tqdm(\n    loggers=None,  # type: Optional[List[logging.Logger]],\n    tqdm_class=std_tqdm  # type: Type[std_tqdm]\n):\n    # type: (...) -> Iterator[None]\n    \"\"\"\n    Context manager redirecting console logging to `tqdm.write()`, leaving\n    other logging handlers (e.g. log files) unaffected.\n\n    Parameters\n    ----------\n    loggers  : list, optional\n      Which handlers to redirect (default: [logging.root]).\n    tqdm_class  : optional\n\n    Example\n    -------\n    ```python\n    import logging\n    from tqdm import trange\n    from tqdm.contrib.logging import logging_redirect_tqdm\n\n    LOG = logging.getLogger(__name__)\n\n    if __name__ == '__main__':\n        logging.basicConfig(level=logging.INFO)\n        with logging_redirect_tqdm():\n            for i in trange(9):\n                if i == 4:\n                    LOG.info(\"console logging redirected to `tqdm.write()`\")\n        # logging restored\n    ```\n    \"\"\"\n    if loggers is None:\n        loggers = [logging.root]\n    original_handlers_list = [logger.handlers for logger in loggers]\n    try:\n        for logger in loggers:\n            tqdm_handler = _TqdmLoggingHandler(tqdm_class)\n            orig_handler = _get_first_found_console_logging_handler(logger.handlers)\n            if orig_handler is not None:\n                tqdm_handler.setFormatter(orig_handler.formatter)\n                tqdm_handler.stream = orig_handler.stream\n            logger.handlers = [\n                handler for handler in logger.handlers\n                if not _is_console_logging_handler(handler)] + [tqdm_handler]\n        yield\n    finally:\n        for logger, original_handlers in zip(loggers, original_handlers_list):\n            logger.handlers = original_handlers", "loc": 50}
{"file": "tqdm\\tqdm\\contrib\\logging.py", "class_name": null, "function_name": "tqdm_logging_redirect", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.copy", "logging_redirect_tqdm", "tqdm_class", "tqdm_kwargs.pop"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Convenience shortcut for: ```python with tqdm_class(*args, **tqdm_kwargs) as pbar:", "source_code": "def tqdm_logging_redirect(\n    *args,\n    # loggers=None,  # type: Optional[List[logging.Logger]]\n    # tqdm=None,  # type: Optional[Type[tqdm.tqdm]]\n    **kwargs\n):\n    # type: (...) -> Iterator[None]\n    \"\"\"\n    Convenience shortcut for:\n    ```python\n    with tqdm_class(*args, **tqdm_kwargs) as pbar:\n        with logging_redirect_tqdm(loggers=loggers, tqdm_class=tqdm_class):\n            yield pbar\n    ```\n\n    Parameters\n    ----------\n    tqdm_class  : optional, (default: tqdm.std.tqdm).\n    loggers  : optional, list.\n    **tqdm_kwargs  : passed to `tqdm_class`.\n    \"\"\"\n    tqdm_kwargs = kwargs.copy()\n    loggers = tqdm_kwargs.pop('loggers', None)\n    tqdm_class = tqdm_kwargs.pop('tqdm_class', std_tqdm)\n    with tqdm_class(*args, **tqdm_kwargs) as pbar:\n        with logging_redirect_tqdm(loggers=loggers, tqdm_class=tqdm_class):\n            yield pbar", "loc": 27}
{"file": "tqdm\\tqdm\\contrib\\logging.py", "class_name": "_TqdmLoggingHandler", "function_name": "emit", "parameters": ["self", "record"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.flush", "self.format", "self.handleError", "self.tqdm_class.write"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def emit(self, record):\n    try:\n        msg = self.format(record)\n        self.tqdm_class.write(msg, file=self.stream)\n        self.flush()\n    except (KeyboardInterrupt, SystemExit):\n        raise\n    except:  # noqa pylint: disable=bare-except\n        self.handleError(record)", "loc": 9}
{"file": "tqdm\\tqdm\\contrib\\slack.py", "class_name": "SlackIO", "function_name": "write", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["s.replace", "s.replace('\\r', '').strip", "self.submit", "str", "tqdm_auto.write"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Replaces internal `message`'s text with `s`.", "source_code": "def write(self, s):\n    \"\"\"Replaces internal `message`'s text with `s`.\"\"\"\n    if not s:\n        s = \"...\"\n    s = s.replace('\\r', '').strip()\n    if s == self.text:\n        return  # skip duplicate message\n    message = self.message\n    if message is None:\n        return\n    self.text = s\n    try:\n        future = self.submit(self.client.chat_update, channel=message['channel'],\n                             ts=message['ts'], text='`' + s + '`')\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        return future", "loc": 18}
{"file": "tqdm\\tqdm\\contrib\\slack.py", "class_name": "tqdm_slack", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fmt.get", "fmt['bar_format'].replace", "fmt['bar_format'].replace('<bar/>', '`{bar:10}`').replace", "self.format_meter", "self.sio.write", "super", "super().display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, **kwargs):\n    super().display(**kwargs)\n    fmt = self.format_dict\n    if fmt.get('bar_format', None):\n        fmt['bar_format'] = fmt['bar_format'].replace(\n            '<bar/>', '`{bar:10}`').replace('{bar}', '`{bar:10u}`')\n    else:\n        fmt['bar_format'] = '{l_bar}`{bar:10}`{r_bar}'\n    if fmt['ascii'] is False:\n        fmt['ascii'] = [\":black_square:\", \":small_blue_diamond:\", \":large_blue_diamond:\",\n                        \":large_blue_square:\"]\n        fmt['ncols'] = 336\n    self.sio.write(self.format_meter(**fmt))", "loc": 13}
{"file": "tqdm\\tqdm\\contrib\\telegram.py", "class_name": "TelegramIO", "function_name": "message_id", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "res.get", "self.session.post", "self.session.post(self.API + '%s/sendMessage' % self.token, data={'text': '`' + self.text + '`', 'chat_id': self.chat_id, 'parse_mode': 'MarkdownV2'}).json", "str", "tqdm_auto.write", "warn"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def message_id(self):\n    if hasattr(self, '_message_id'):\n        return self._message_id\n    try:\n        res = self.session.post(\n            self.API + '%s/sendMessage' % self.token,\n            data={'text': '`' + self.text + '`', 'chat_id': self.chat_id,\n                  'parse_mode': 'MarkdownV2'}).json()\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        if res.get('error_code') == 429:\n            warn(\"Creation rate limit: try increasing `mininterval`.\",\n                 TqdmWarning, stacklevel=2)\n        else:\n            self._message_id = res['result']['message_id']\n            return self._message_id", "loc": 17}
{"file": "tqdm\\tqdm\\contrib\\telegram.py", "class_name": "TelegramIO", "function_name": "write", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["s.replace", "s.replace('\\r', '').strip", "self.submit", "str", "tqdm_auto.write"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Replaces internal `message_id`'s text with `s`.", "source_code": "def write(self, s):\n    \"\"\"Replaces internal `message_id`'s text with `s`.\"\"\"\n    if not s:\n        s = \"...\"\n    s = s.replace('\\r', '').strip()\n    if s == self.text:\n        return  # avoid duplicate message Bot error\n    message_id = self.message_id\n    if message_id is None:\n        return\n    self.text = s\n    try:\n        future = self.submit(\n            self.session.post, self.API + '%s/editMessageText' % self.token,\n            data={'text': '`' + s + '`', 'chat_id': self.chat_id,\n                  'message_id': message_id, 'parse_mode': 'MarkdownV2'})\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        return future", "loc": 20}
{"file": "tqdm\\tqdm\\contrib\\telegram.py", "class_name": "TelegramIO", "function_name": "delete", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.submit", "str", "tqdm_auto.write"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Deletes internal `message_id`.", "source_code": "def delete(self):\n    \"\"\"Deletes internal `message_id`.\"\"\"\n    try:\n        future = self.submit(\n            self.session.post, self.API + '%s/deleteMessage' % self.token,\n            data={'chat_id': self.chat_id, 'message_id': self.message_id})\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        return future", "loc": 10}
{"file": "tqdm\\tqdm\\contrib\\telegram.py", "class_name": "tqdm_telegram", "function_name": "display", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fmt.get", "fmt['bar_format'].replace", "fmt['bar_format'].replace('<bar/>', '{bar:10u}').replace", "self.format_meter", "self.tgio.write", "super", "super().display"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def display(self, **kwargs):\n    super().display(**kwargs)\n    fmt = self.format_dict\n    if fmt.get('bar_format', None):\n        fmt['bar_format'] = fmt['bar_format'].replace(\n            '<bar/>', '{bar:10u}').replace('{bar}', '{bar:10u}')\n    else:\n        fmt['bar_format'] = '{l_bar}{bar:10u}{r_bar}'\n    self.tgio.write(self.format_meter(**fmt))", "loc": 9}
{"file": "tqdm\\tqdm\\contrib\\telegram.py", "class_name": "tqdm_telegram", "function_name": "close", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.tgio.delete", "super", "super().close"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def close(self):\n    if self.disable:\n        return\n    super().close()\n    if not (self.leave or (self.leave is None and self.pos == 0)):\n        self.tgio.delete()", "loc": 6}
{"file": "tqdm\\tqdm\\contrib\\utils_worker.py", "class_name": "MonoWorker", "function_name": "submit", "parameters": ["self", "func"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["futures.append", "futures.appendleft", "futures.pop", "futures.popleft", "len", "running.done", "self.pool.submit", "str", "tqdm_auto.write", "waiting.cancel"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "`func(*args, **kwargs)` may replace currently waiting task.", "source_code": "def submit(self, func, *args, **kwargs):\n    \"\"\"`func(*args, **kwargs)` may replace currently waiting task.\"\"\"\n    futures = self.futures\n    if len(futures) == futures.maxlen:\n        running = futures.popleft()\n        if not running.done():\n            if len(futures):  # clear waiting\n                waiting = futures.pop()\n                waiting.cancel()\n            futures.appendleft(running)  # re-insert running\n    try:\n        waiting = self.pool.submit(func, *args, **kwargs)\n    except Exception as e:\n        tqdm_auto.write(str(e))\n    else:\n        futures.append(waiting)\n        return waiting", "loc": 17}
{"file": "tqdm\\tqdm\\contrib\\__init__.py", "class_name": null, "function_name": "tenumerate", "parameters": ["iterable", "start", "total", "tqdm_class"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "isinstance", "np.ndenumerate", "tqdm_class"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Equivalent of `numpy.ndenumerate` or builtin `enumerate`. Parameters ----------", "source_code": "def tenumerate(iterable, start=0, total=None, tqdm_class=tqdm_auto, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of `numpy.ndenumerate` or builtin `enumerate`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    \"\"\"\n    try:\n        import numpy as np\n    except ImportError:\n        pass\n    else:\n        if isinstance(iterable, np.ndarray):\n            return tqdm_class(np.ndenumerate(iterable), total=total or iterable.size,\n                              **tqdm_kwargs)\n    return enumerate(tqdm_class(iterable, total=total, **tqdm_kwargs), start)", "loc": 17}
{"file": "tqdm\\tqdm\\contrib\\__init__.py", "class_name": null, "function_name": "tzip", "parameters": ["iter1"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.pop", "tqdm_class", "tqdm_kwargs.copy", "zip"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Equivalent of builtin `zip`. Parameters ----------", "source_code": "def tzip(iter1, *iter2plus, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of builtin `zip`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    \"\"\"\n    kwargs = tqdm_kwargs.copy()\n    tqdm_class = kwargs.pop(\"tqdm_class\", tqdm_auto)\n    for i in zip(tqdm_class(iter1, **kwargs), *iter2plus):\n        yield i", "loc": 12}
{"file": "tqdm\\tqdm\\contrib\\__init__.py", "class_name": null, "function_name": "tmap", "parameters": ["function"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["function", "tzip"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Equivalent of builtin `map`. Parameters ----------", "source_code": "def tmap(function, *sequences, **tqdm_kwargs):\n    \"\"\"\n    Equivalent of builtin `map`.\n\n    Parameters\n    ----------\n    tqdm_class  : [default: tqdm.auto.tqdm].\n    \"\"\"\n    for i in tzip(*sequences, **tqdm_kwargs):\n        yield function(*i)", "loc": 10}
{"file": "tqdm\\tqdm\\contrib\\__init__.py", "class_name": "DummyTqdmFile", "function_name": "write", "parameters": ["self", "x", "nolock"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["blank.join", "isinstance", "self._buf.append", "tqdm.write", "type", "x.rpartition"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write(self, x, nolock=False):\n    nl = b\"\\n\" if isinstance(x, bytes) else \"\\n\"\n    pre, sep, post = x.rpartition(nl)\n    if sep:\n        blank = type(nl)()\n        tqdm.write(blank.join(self._buf + [pre, sep]),\n                   end=blank, file=self._wrapped, nolock=nolock)\n        self._buf = [post]\n    else:\n        self._buf.append(x)", "loc": 10}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_OpenSCManager", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Exception", "advapi32.OpenSCManagerW"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_OpenSCManager():\n    res = advapi32.OpenSCManagerW(None, None, SC_MANAGER_ALL_ACCESS)\n    if not res:\n        raise Exception('Opening service manager failed - '\n                        'are you running this as administrator?')\n    return res", "loc": 6}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_install_service", "parameters": ["service_name", "cmdline"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "advapi32.CloseServiceHandle", "advapi32.CreateServiceW", "ctypes.FormatError", "win_OpenSCManager"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_install_service(service_name, cmdline):\n    manager = win_OpenSCManager()\n    try:\n        h = advapi32.CreateServiceW(\n            manager, service_name, None,\n            SC_MANAGER_CREATE_SERVICE, SERVICE_WIN32_OWN_PROCESS,\n            SERVICE_AUTO_START, SERVICE_ERROR_NORMAL,\n            cmdline, None, None, None, None, None)\n        if not h:\n            raise OSError('Service creation failed: %s' % ctypes.FormatError())\n\n        advapi32.CloseServiceHandle(h)\n    finally:\n        advapi32.CloseServiceHandle(manager)", "loc": 14}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_uninstall_service", "parameters": ["service_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "advapi32.CloseServiceHandle", "advapi32.DeleteService", "advapi32.OpenServiceW", "ctypes.FormatError", "win_OpenSCManager"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_uninstall_service(service_name):\n    manager = win_OpenSCManager()\n    try:\n        h = advapi32.OpenServiceW(manager, service_name, DELETE)\n        if not h:\n            raise OSError('Could not find service %s: %s' % (\n                service_name, ctypes.FormatError()))\n\n        try:\n            if not advapi32.DeleteService(h):\n                raise OSError('Deletion failed: %s' % ctypes.FormatError())\n        finally:\n            advapi32.CloseServiceHandle(h)\n    finally:\n        advapi32.CloseServiceHandle(manager)", "loc": 15}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_service_handler", "parameters": ["stop_event"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "repr", "str", "traceback.format_exc", "win_service_report_event"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_service_handler(stop_event, *args):\n    try:\n        raise ValueError('Handler called with args ' + repr(args))\n        TODO\n    except Exception as e:\n        tb = traceback.format_exc()\n        msg = str(e) + '\\n' + tb\n        win_service_report_event(service_name, msg, is_error=True)\n        raise", "loc": 9}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_service_set_status", "parameters": ["handle", "status_code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "SERVICE_STATUS", "advapi32.SetServiceStatus", "ctypes.FormatError", "ctypes.byref"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_service_set_status(handle, status_code):\n    svcStatus = SERVICE_STATUS()\n    svcStatus.dwServiceType = SERVICE_WIN32_OWN_PROCESS\n    svcStatus.dwCurrentState = status_code\n    svcStatus.dwControlsAccepted = SERVICE_ACCEPT_STOP\n\n    svcStatus.dwServiceSpecificExitCode = 0\n\n    if not advapi32.SetServiceStatus(handle, ctypes.byref(svcStatus)):\n        raise OSError('SetServiceStatus failed: %r' % ctypes.FormatError())", "loc": 10}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_service_main", "parameters": ["service_name", "real_main", "argc", "argv_raw"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HandlerEx", "OSError", "advapi32.RegisterServiceCtrlHandlerExW", "ctypes.FormatError", "functools.partial", "str", "threading.Event", "traceback.format_exc", "win_service_report_event"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_service_main(service_name, real_main, argc, argv_raw):\n    try:\n        # args = [argv_raw[i].value for i in range(argc)]\n        stop_event = threading.Event()\n        handler = HandlerEx(functools.partial(stop_event, win_service_handler))\n        h = advapi32.RegisterServiceCtrlHandlerExW(service_name, handler, None)\n        if not h:\n            raise OSError('Handler registration failed: %s' %\n                          ctypes.FormatError())\n\n        TODO\n    except Exception as e:\n        tb = traceback.format_exc()\n        msg = str(e) + '\\n' + tb\n        win_service_report_event(service_name, msg, is_error=True)\n        raise", "loc": 16}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": null, "function_name": "win_service_start", "parameters": ["service_name", "real_main"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OSError", "SERVICE_TABLE_ENTRY", "START_CALLBACK", "_ctypes_array", "advapi32.StartServiceCtrlDispatcherW", "ctypes.FormatError", "ctypes.cast", "functools.partial", "str", "traceback.format_exc", "win_service_report_event"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def win_service_start(service_name, real_main):\n    try:\n        cb = START_CALLBACK(\n            functools.partial(win_service_main, service_name, real_main))\n        dispatch_table = _ctypes_array(SERVICE_TABLE_ENTRY, [\n            SERVICE_TABLE_ENTRY(\n                service_name,\n                cb\n            ),\n            SERVICE_TABLE_ENTRY(None, ctypes.cast(None, START_CALLBACK))\n        ])\n\n        if not advapi32.StartServiceCtrlDispatcherW(dispatch_table):\n            raise OSError('ctypes start failed: %s' % ctypes.FormatError())\n    except Exception as e:\n        tb = traceback.format_exc()\n        msg = str(e) + '\\n' + tb\n        win_service_report_event(service_name, msg, is_error=True)\n        raise", "loc": 19}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": "GITBuilder", "function_name": "build", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BuildError", "subprocess.check_output", "super", "super(GITBuilder, self).build"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build(self):\n    try:\n        subprocess.check_output(['git', 'clone', 'git://github.com/%s/%s.git' % (self.user, self.repoName), self.buildPath])\n        subprocess.check_output(['git', 'checkout', self.rev], cwd=self.buildPath)\n    except subprocess.CalledProcessError as e:\n        raise BuildError(e.output)\n\n    super(GITBuilder, self).build()", "loc": 8}
{"file": "youtube-dl\\devscripts\\buildserver.py", "class_name": "BuildHTTPRequestHandler", "function_name": "do_GET", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["builder.build", "builder.close", "builder.start", "compat_str", "compat_str(e).encode", "compat_urlparse.parse_qs", "compat_urlparse.parse_qs(path.query).items", "compat_urlparse.urlparse", "dict", "len", "path.path.strip", "path.path.strip('/').partition", "path.split", "self.end_headers", "self.send_header", "self.send_response", "self.wfile.write"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def do_GET(self):\n    path = compat_urlparse.urlparse(self.path)\n    paramDict = dict([(key, value[0]) for key, value in compat_urlparse.parse_qs(path.query).items()])\n    action, _, path = path.path.strip('/').partition('/')\n    if path:\n        path = path.split('/')\n        if action in self.actionDict:\n            try:\n                builder = self.actionDict[action](path=path, handler=self, **paramDict)\n                builder.start()\n                try:\n                    builder.build()\n                finally:\n                    builder.close()\n            except BuildError as e:\n                self.send_response(e.code)\n                msg = compat_str(e).encode('UTF-8')\n                self.send_header('Content-Type', 'text/plain; charset=UTF-8')\n                self.send_header('Content-Length', len(msg))\n                self.end_headers()\n                self.wfile.write(msg)\n        else:\n            self.send_response(500, 'Unknown build method \"%s\"' % action)\n    else:\n        self.send_response(500, 'Malformed URL')", "loc": 25}
{"file": "youtube-dl\\devscripts\\cli_to_api.py", "class_name": null, "function_name": "cli_to_api", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}'.format", "MethodType", "ParseYTDLResult", "dict", "list", "neq_opt", "parsed_options", "parsed_options(opts).items", "repr", "repr(type(object)).endswith", "super", "super(ParseYTDLResult, self).__init__", "super(YDL, ydl).__init__", "type", "youtube_dl._real_main"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cli_to_api(*opts):\n    YDL = youtube_dl.YoutubeDL\n\n    # to extract the parsed options, break out of YoutubeDL instantiation\n\n    # return options via this Exception\n    class ParseYTDLResult(Exception):\n        def __init__(self, result):\n            super(ParseYTDLResult, self).__init__('result')\n            self.opts = result\n\n    # replacement constructor that raises ParseYTDLResult\n    def ytdl_init(ydl, ydl_opts):\n        super(YDL, ydl).__init__(ydl_opts)\n        raise ParseYTDLResult(ydl_opts)\n\n    # patch in the constructor\n    YDL.__init__ = MethodType(ytdl_init, YDL)\n\n    # core parser\n    def parsed_options(argv):\n        try:\n            youtube_dl._real_main(list(argv))\n        except ParseYTDLResult as result:\n            return result.opts\n\n    # from https://github.com/yt-dlp/yt-dlp/issues/5859#issuecomment-1363938900\n    default = parsed_options([])\n\n    def neq_opt(a, b):\n        if a == b:\n            return False\n        if a is None and repr(type(object)).endswith(\".utils.DateRange'>\"):\n            return '0001-01-01 - 9999-12-31' != '{0}'.format(b)\n        return a != b\n\n    diff = dict((k, v) for k, v in parsed_options(opts).items() if neq_opt(default[k], v))\n    if 'postprocessors' in diff:\n        diff['postprocessors'] = [pp for pp in diff['postprocessors'] if pp not in default['postprocessors']]\n    return diff", "loc": 40}
{"file": "youtube-dl\\devscripts\\cli_to_api.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}: {1}>'.format", "PrettyPrinter", "cli_to_api", "pprint.pprint", "repr", "repr(type(object)).endswith", "super_format", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    from pprint import PrettyPrinter\n\n    pprint = PrettyPrinter()\n    super_format = pprint.format\n\n    def format(object, context, maxlevels, level):\n        if repr(type(object)).endswith(\".utils.DateRange'>\"):\n            return '{0}: {1}>'.format(repr(object)[:-2], object), True, False\n        return super_format(object, context, maxlevels, level)\n\n    pprint.format = format\n\n    pprint.pprint(cli_to_api(*sys.argv))", "loc": 14}
{"file": "youtube-dl\\devscripts\\cli_to_api.py", "class_name": null, "function_name": "parsed_options", "parameters": ["argv"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "youtube_dl._real_main"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parsed_options(argv):\n    try:\n        youtube_dl._real_main(list(argv))\n    except ParseYTDLResult as result:\n        return result.opts", "loc": 5}
{"file": "youtube-dl\\devscripts\\cli_to_api.py", "class_name": null, "function_name": "neq_opt", "parameters": ["a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}'.format", "repr", "repr(type(object)).endswith", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def neq_opt(a, b):\n    if a == b:\n        return False\n    if a is None and repr(type(object)).endswith(\".utils.DateRange'>\"):\n        return '0001-01-01 - 9999-12-31' != '{0}'.format(b)\n    return a != b", "loc": 6}
{"file": "youtube-dl\\devscripts\\create-github-release.py", "class_name": "GitHubReleaser", "function_name": "create_release", "parameters": ["self", "tag_name", "name", "body", "draft", "prerelease"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "json.dumps(data).encode", "sanitized_Request", "self._call"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def create_release(self, tag_name, name=None, body='', draft=False, prerelease=False):\n    data = {\n        'tag_name': tag_name,\n        'target_commitish': 'master',\n        'name': name,\n        'body': body,\n        'draft': draft,\n        'prerelease': prerelease,\n    }\n    req = sanitized_Request(self._API_URL, json.dumps(data).encode('utf-8'))\n    return self._call(req)", "loc": 11}
{"file": "youtube-dl\\devscripts\\fish-completion.py", "class_name": null, "function_name": "build_completion", "parameters": ["opt_parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n'.join", "EXTRA_ARGS.get", "commands.append", "complete_cmd.extend", "option._short_opts[0].strip", "option.get_opt_string", "option.get_opt_string().strip", "read_file", "shell_quote", "template.replace", "write_file"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_completion(opt_parser):\n    commands = []\n\n    for group in opt_parser.option_groups:\n        for option in group.option_list:\n            long_option = option.get_opt_string().strip('-')\n            complete_cmd = ['complete', '--command', 'youtube-dl', '--long-option', long_option]\n            if option._short_opts:\n                complete_cmd += ['--short-option', option._short_opts[0].strip('-')]\n            if option.help != optparse.SUPPRESS_HELP:\n                complete_cmd += ['--description', option.help]\n            complete_cmd.extend(EXTRA_ARGS.get(long_option, []))\n            commands.append(shell_quote(complete_cmd))\n\n    template = read_file(FISH_COMPLETION_TEMPLATE)\n    filled_template = template.replace('{{commands}}', '\\n'.join(commands))\n    write_file(FISH_COMPLETION_FILE, filled_template)", "loc": 17}
{"file": "youtube-dl\\devscripts\\make_contributing.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "optparse.OptionParser", "parser.error", "parser.parse_args", "re.search", "re.search('(?s)#\\\\s*BUGS\\\\s*[^\\\\n]*\\\\s*(.*?)#\\\\s*COPYRIGHT', readme).group", "re.search('(?s)(#\\\\s*DEVELOPER INSTRUCTIONS.*?)#\\\\s*EMBEDDING YOUTUBE-DL', readme).group", "read_file", "write_file"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parser = optparse.OptionParser(usage='%prog INFILE OUTFILE')\n    options, args = parser.parse_args()\n    if len(args) != 2:\n        parser.error('Expected an input and an output filename')\n\n    infile, outfile = args\n\n    readme = read_file(infile)\n\n    bug_text = re.search(\n        r'(?s)#\\s*BUGS\\s*[^\\n]*\\s*(.*?)#\\s*COPYRIGHT', readme).group(1)\n    dev_text = re.search(\n        r'(?s)(#\\s*DEVELOPER INSTRUCTIONS.*?)#\\s*EMBEDDING YOUTUBE-DL',\n        readme).group(1)\n\n    out = bug_text + dev_text\n\n    write_file(outfile, out)", "loc": 19}
{"file": "youtube-dl\\devscripts\\make_issue_template.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "optparse.OptionParser", "parser.error", "parser.parse_args", "read_file", "read_version", "write_file"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parser = optparse.OptionParser(usage='%prog INFILE OUTFILE')\n    options, args = parser.parse_args()\n    if len(args) != 2:\n        parser.error('Expected an input and an output filename')\n\n    infile, outfile = args\n\n    issue_template_tmpl = read_file(infile)\n\n    out = issue_template_tmpl % {'version': read_version()}\n\n    write_file(outfile, out)", "loc": 13}
{"file": "youtube-dl\\devscripts\\make_lazy_extractors.py", "class_name": null, "function_name": "get_base_name", "parameters": ["base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_base_name(base):\n    if base is InfoExtractor:\n        return 'LazyLoadExtractor'\n    elif base is SearchInfoExtractor:\n        return 'LazyLoadSearchExtractor'\n    else:\n        return base.__name__", "loc": 7}
{"file": "youtube-dl\\devscripts\\make_lazy_extractors.py", "class_name": null, "function_name": "build_lazy_ie", "parameters": ["ie", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "get_source", "getattr", "hasattr", "ie._make_valid_url", "ie_template.format", "make_valid_template.format", "map"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_lazy_ie(ie, name):\n    valid_url = getattr(ie, '_VALID_URL', None)\n    s = ie_template.format(\n        name=name,\n        bases=', '.join(map(get_base_name, ie.__bases__)),\n        valid_url=valid_url,\n        module=ie.__module__)\n    if ie.suitable.__func__ is not InfoExtractor.suitable.__func__:\n        s += '\\n' + get_source(ie.suitable)\n    if hasattr(ie, '_make_valid_url'):\n        # search extractors\n        s += make_valid_template.format(valid_url=ie._make_valid_url())\n    return s", "loc": 13}
{"file": "youtube-dl\\devscripts\\make_supportedsites.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "'**{0}**'.format", "': {0}'.format", "gen_ies_md", "getattr", "i.IE_NAME.lower", "ie.working", "len", "optparse.OptionParser", "parser.error", "parser.parse_args", "sorted", "write_file", "youtube_dl.gen_extractors"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parser = optparse.OptionParser(usage='%prog OUTFILE.md')\n    options, args = parser.parse_args()\n    if len(args) != 1:\n        parser.error('Expected an output filename')\n\n    outfile, = args\n\n    def gen_ies_md(ies):\n        for ie in ies:\n            ie_md = '**{0}**'.format(ie.IE_NAME)\n            ie_desc = getattr(ie, 'IE_DESC', None)\n            if ie_desc is False:\n                continue\n            if ie_desc is not None:\n                ie_md += ': {0}'.format(ie.IE_DESC)\n            if not ie.working():\n                ie_md += ' (Currently broken)'\n            yield ie_md\n\n    ies = sorted(youtube_dl.gen_extractors(), key=lambda i: i.IE_NAME.lower())\n    out = '# Supported sites\\n' + ''.join(\n        ' - ' + md + '\\n'\n        for md in gen_ies_md(ies))\n\n    write_file(outfile, out)", "loc": 26}
{"file": "youtube-dl\\devscripts\\make_supportedsites.py", "class_name": null, "function_name": "gen_ies_md", "parameters": ["ies"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'**{0}**'.format", "': {0}'.format", "getattr", "ie.working"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gen_ies_md(ies):\n    for ie in ies:\n        ie_md = '**{0}**'.format(ie.IE_NAME)\n        ie_desc = getattr(ie, 'IE_DESC', None)\n        if ie_desc is False:\n            continue\n        if ie_desc is not None:\n            ie_md += ': {0}'.format(ie.IE_DESC)\n        if not ie.working():\n            ie_md += ' (Currently broken)'\n        yield ie_md", "loc": 11}
{"file": "youtube-dl\\devscripts\\prepare_manpage.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter_options", "len", "optparse.OptionParser", "parser.error", "parser.parse_args", "re.sub", "read_file", "write_file"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    parser = optparse.OptionParser(usage='%prog OUTFILE.md')\n    options, args = parser.parse_args()\n    if len(args) != 1:\n        parser.error('Expected an output filename')\n\n    outfile, = args\n\n    readme = read_file(README_FILE)\n\n    readme = re.sub(r'(?s)^.*?(?=# DESCRIPTION)', '', readme)\n    readme = re.sub(r'\\s+youtube-dl \\[OPTIONS\\] URL \\[URL\\.\\.\\.\\]', '', readme)\n    readme = PREFIX + readme\n\n    readme = filter_options(readme)\n\n    write_file(outfile, readme)", "loc": 17}
{"file": "youtube-dl\\devscripts\\prepare_manpage.py", "class_name": null, "function_name": "filter_options", "parameters": ["readme"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "len", "line.lstrip", "line.lstrip().startswith", "line.startswith", "line[2:].startswith", "option.split", "re.split", "readme.split", "split_option[-1].startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def filter_options(readme):\n    ret = ''\n    in_options = False\n    for line in readme.split('\\n'):\n        if line.startswith('# '):\n            if line[2:].startswith('OPTIONS'):\n                in_options = True\n            else:\n                in_options = False\n\n        if in_options:\n            if line.lstrip().startswith('-'):\n                split = re.split(r'\\s{2,}', line.lstrip())\n                # Description string may start with `-` as well. If there is\n                # only one piece then it's a description bit not an option.\n                if len(split) > 1:\n                    option, description = split\n                    split_option = option.split(' ')\n\n                    if not split_option[-1].startswith('-'):  # metavar\n                        option = ' '.join(split_option[:-1] + ['*%s*' % split_option[-1]])\n\n                    # Pandoc's definition_lists. See http://pandoc.org/README.html\n                    # for more information.\n                    ret += '\\n%s\\n:   %s\\n' % (option, description)\n                    continue\n            ret += line.lstrip() + '\\n'\n        else:\n            ret += line + '\\n'\n\n    return ret", "loc": 31}
{"file": "youtube-dl\\devscripts\\utils.py", "class_name": null, "function_name": "get_filename_args", "parameters": ["has_infile", "default_outfile"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["argparse.ArgumentParser", "compat_kwargs", "parser.add_argument", "parser.parse_args"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_filename_args(has_infile=False, default_outfile=None):\n    parser = argparse.ArgumentParser()\n    if has_infile:\n        parser.add_argument('infile', help='Input file')\n    kwargs = {'nargs': '?', 'default': default_outfile} if default_outfile else {}\n    kwargs['help'] = 'Output file'\n    parser.add_argument('outfile', **compat_kwargs(kwargs))\n\n    opts = parser.parse_args()\n    if has_infile:\n        return opts.infile, opts.outfile\n    return opts.outfile", "loc": 12}
{"file": "youtube-dl\\devscripts\\utils.py", "class_name": null, "function_name": "run_process", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_kwargs", "kwargs.setdefault", "subprocess.run"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_process(*args, **kwargs):\n    kwargs.setdefault('text', True)\n    kwargs.setdefault('check', True)\n    kwargs.setdefault('capture_output', True)\n    if kwargs['text']:\n        kwargs.setdefault('encoding', 'utf-8')\n        kwargs.setdefault('errors', 'replace')\n        kwargs = compat_kwargs(kwargs)\n    return subprocess.run(args, **kwargs)", "loc": 9}
{"file": "youtube-dl\\devscripts\\zsh-completion.py", "class_name": null, "function_name": "build_completion", "parameters": ["opt_parser"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'|'.join", "diropts.extend", "fileopts.extend", "opt.get_opt_string", "read_file", "template.replace", "write_file"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_completion(opt_parser):\n    opts = [opt for group in opt_parser.option_groups\n            for opt in group.option_list]\n    opts_file = [opt for opt in opts if opt.metavar == \"FILE\"]\n    opts_dir = [opt for opt in opts if opt.metavar == \"DIR\"]\n\n    fileopts = []\n    for opt in opts_file:\n        if opt._short_opts:\n            fileopts.extend(opt._short_opts)\n        if opt._long_opts:\n            fileopts.extend(opt._long_opts)\n\n    diropts = []\n    for opt in opts_dir:\n        if opt._short_opts:\n            diropts.extend(opt._short_opts)\n        if opt._long_opts:\n            diropts.extend(opt._long_opts)\n\n    flags = [opt.get_opt_string() for opt in opts]\n\n    template = read_file(ZSH_COMPLETION_TEMPLATE)\n\n    template = template.replace(\"{{fileopts}}\", \"|\".join(fileopts))\n    template = template.replace(\"{{diropts}}\", \"|\".join(diropts))\n    template = template.replace(\"{{flags}}\", \" \".join(flags))\n\n    write_file(ZSH_COMPLETION_FILE, template)", "loc": 29}
{"file": "youtube-dl\\devscripts\\gh-pages\\update-sites.py", "class_name": null, "function_name": "main", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["': {}'.format", "'<b>{}</b>'.format", "'<li>{}</li>'.format", "'\\n'.join", "getattr", "ie.working", "ie_htmls.append", "read_file", "template.replace", "textwrap.indent", "write_file", "youtube_dl.list_extractors"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main():\n    template = read_file('supportedsites.html.in')\n\n    ie_htmls = []\n    for ie in youtube_dl.list_extractors(age_limit=None):\n        ie_html = '<b>{}</b>'.format(ie.IE_NAME)\n        ie_desc = getattr(ie, 'IE_DESC', None)\n        if ie_desc is False:\n            continue\n        elif ie_desc is not None:\n            ie_html += ': {}'.format(ie.IE_DESC)\n        if not ie.working():\n            ie_html += ' (Currently broken)'\n        ie_htmls.append('<li>{}</li>'.format(ie_html))\n\n    template = template.replace('@SITES@', textwrap.indent('\\n'.join(ie_htmls), '\\t'))\n\n    write_file('supportedsites.html', template)", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_ctr_decrypt", "parameters": ["data", "key", "counter"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aes_encrypt", "ceil", "counter.next_value", "float", "int", "key_expansion", "len", "range", "xor"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Decrypt with aes in counter mode @param {int[]} data        cipher @param {int[]} key         16/24/32-Byte cipher key", "source_code": "def aes_ctr_decrypt(data, key, counter):\n    \"\"\"\n    Decrypt with aes in counter mode\n\n    @param {int[]} data        cipher\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {instance} counter  Instance whose next_value function (@returns {int[]}  16-Byte block)\n                               returns the next counter block\n    @returns {int[]}           decrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    decrypted_data = []\n    for i in range(block_count):\n        counter_block = counter.next_value()\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block += [0] * (BLOCK_SIZE_BYTES - len(block))\n\n        cipher_counter_block = aes_encrypt(counter_block, expanded_key)\n        decrypted_data += xor(block, cipher_counter_block)\n    decrypted_data = decrypted_data[:len(data)]\n\n    return decrypted_data", "loc": 24}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_cbc_decrypt", "parameters": ["data", "key", "iv"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aes_decrypt", "ceil", "float", "int", "key_expansion", "len", "range", "xor"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Decrypt with aes in CBC mode @param {int[]} data        cipher @param {int[]} key         16/24/32-Byte cipher key", "source_code": "def aes_cbc_decrypt(data, key, iv):\n    \"\"\"\n    Decrypt with aes in CBC mode\n\n    @param {int[]} data        cipher\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {int[]} iv          16-Byte IV\n    @returns {int[]}           decrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    decrypted_data = []\n    previous_cipher_block = iv\n    for i in range(block_count):\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block += [0] * (BLOCK_SIZE_BYTES - len(block))\n\n        decrypted_block = aes_decrypt(block, expanded_key)\n        decrypted_data += xor(decrypted_block, previous_cipher_block)\n        previous_cipher_block = block\n    decrypted_data = decrypted_data[:len(data)]\n\n    return decrypted_data", "loc": 24}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_cbc_encrypt", "parameters": ["data", "key", "iv"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aes_encrypt", "ceil", "float", "int", "key_expansion", "len", "pkcs7_padding", "range", "xor"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Encrypt with aes in CBC mode. Using PKCS#7 padding @param {int[]} data        cleartext @param {int[]} key         16/24/32-Byte cipher key", "source_code": "def aes_cbc_encrypt(data, key, iv):\n    \"\"\"\n    Encrypt with aes in CBC mode. Using PKCS#7 padding\n\n    @param {int[]} data        cleartext\n    @param {int[]} key         16/24/32-Byte cipher key\n    @param {int[]} iv          16-Byte IV\n    @returns {int[]}           encrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    encrypted_data = []\n    previous_cipher_block = iv\n    for i in range(block_count):\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block = pkcs7_padding(block)\n        mixed_block = xor(block, previous_cipher_block)\n\n        encrypted_block = aes_encrypt(mixed_block, expanded_key)\n        encrypted_data += encrypted_block\n\n        previous_cipher_block = encrypted_block\n\n    return encrypted_data", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_ecb_encrypt", "parameters": ["data", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aes_encrypt", "ceil", "float", "int", "key_expansion", "len", "pkcs7_padding", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Encrypt with aes in ECB mode. Using PKCS#7 padding @param {int[]} data        cleartext @param {int[]} key         16/24/32-Byte cipher key", "source_code": "def aes_ecb_encrypt(data, key):\n    \"\"\"\n    Encrypt with aes in ECB mode. Using PKCS#7 padding\n\n    @param {int[]} data        cleartext\n    @param {int[]} key         16/24/32-Byte cipher key\n    @returns {int[]}           encrypted data\n    \"\"\"\n    expanded_key = key_expansion(key)\n    block_count = int(ceil(float(len(data)) / BLOCK_SIZE_BYTES))\n\n    encrypted_data = []\n    for i in range(block_count):\n        block = data[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES]\n        block = pkcs7_padding(block)\n\n        encrypted_block = aes_encrypt(block, expanded_key)\n        encrypted_data += encrypted_block\n\n    return encrypted_data", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "key_expansion", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["key_schedule_core", "len", "range", "sub_bytes", "xor"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "Generate key schedule @param {int[]} data  16/24/32-Byte cipher key @returns {int[]}     176/208/240-Byte expanded key", "source_code": "def key_expansion(data):\n    \"\"\"\n    Generate key schedule\n\n    @param {int[]} data  16/24/32-Byte cipher key\n    @returns {int[]}     176/208/240-Byte expanded key\n    \"\"\"\n    data = data[:]  # copy\n    rcon_iteration = 1\n    key_size_bytes = len(data)\n    expanded_key_size_bytes = (key_size_bytes // 4 + 7) * BLOCK_SIZE_BYTES\n\n    while len(data) < expanded_key_size_bytes:\n        temp = data[-4:]\n        temp = key_schedule_core(temp, rcon_iteration)\n        rcon_iteration += 1\n        data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        for _ in range(3):\n            temp = data[-4:]\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        if key_size_bytes == 32:\n            temp = data[-4:]\n            temp = sub_bytes(temp)\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n\n        for _ in range(3 if key_size_bytes == 32 else 2 if key_size_bytes == 24 else 0):\n            temp = data[-4:]\n            data += xor(temp, data[-key_size_bytes: 4 - key_size_bytes])\n    data = data[:expanded_key_size_bytes]\n\n    return data", "loc": 33}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_encrypt", "parameters": ["data", "expanded_key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "mix_columns", "range", "shift_rows", "sub_bytes", "xor"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Encrypt one block with aes @param {int[]} data          16-Byte state @param {int[]} expanded_key  176/208/240-Byte expanded key", "source_code": "def aes_encrypt(data, expanded_key):\n    \"\"\"\n    Encrypt one block with aes\n\n    @param {int[]} data          16-Byte state\n    @param {int[]} expanded_key  176/208/240-Byte expanded key\n    @returns {int[]}             16-Byte cipher\n    \"\"\"\n    rounds = len(expanded_key) // BLOCK_SIZE_BYTES - 1\n\n    data = xor(data, expanded_key[:BLOCK_SIZE_BYTES])\n    for i in range(1, rounds + 1):\n        data = sub_bytes(data)\n        data = shift_rows(data)\n        if i != rounds:\n            data = mix_columns(data)\n        data = xor(data, expanded_key[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES])\n\n    return data", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_decrypt", "parameters": ["data", "expanded_key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "mix_columns_inv", "range", "shift_rows_inv", "sub_bytes_inv", "xor"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Decrypt one block with aes @param {int[]} data          16-Byte cipher @param {int[]} expanded_key  176/208/240-Byte expanded key", "source_code": "def aes_decrypt(data, expanded_key):\n    \"\"\"\n    Decrypt one block with aes\n\n    @param {int[]} data          16-Byte cipher\n    @param {int[]} expanded_key  176/208/240-Byte expanded key\n    @returns {int[]}             16-Byte state\n    \"\"\"\n    rounds = len(expanded_key) // BLOCK_SIZE_BYTES - 1\n\n    for i in range(rounds, 0, -1):\n        data = xor(data, expanded_key[i * BLOCK_SIZE_BYTES: (i + 1) * BLOCK_SIZE_BYTES])\n        if i != rounds:\n            data = mix_columns_inv(data)\n        data = shift_rows_inv(data)\n        data = sub_bytes_inv(data)\n    data = xor(data, expanded_key[:BLOCK_SIZE_BYTES])\n\n    return data", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "aes_decrypt_text", "parameters": ["data", "password", "key_size_bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["Counter", "aes_ctr_decrypt", "aes_encrypt", "bytes_to_intlist", "compat_b64decode", "inc", "intlist_to_bytes", "key_expansion", "len", "password.encode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Decrypt text - The first 8 Bytes of decoded 'data' are the 8 high Bytes of the counter - The cipher key is retrieved by encrypting the first 16 Byte of 'password'", "source_code": "def aes_decrypt_text(data, password, key_size_bytes):\n    \"\"\"\n    Decrypt text\n    - The first 8 Bytes of decoded 'data' are the 8 high Bytes of the counter\n    - The cipher key is retrieved by encrypting the first 16 Byte of 'password'\n      with the first 'key_size_bytes' Bytes from 'password' (if necessary filled with 0's)\n    - Mode of operation is 'counter'\n\n    @param {str} data                    Base64 encoded string\n    @param {str,unicode} password        Password (will be encoded with utf-8)\n    @param {int} key_size_bytes          Possible values: 16 for 128-Bit, 24 for 192-Bit or 32 for 256-Bit\n    @returns {str}                       Decrypted data\n    \"\"\"\n    NONCE_LENGTH_BYTES = 8\n\n    data = bytes_to_intlist(compat_b64decode(data))\n    password = bytes_to_intlist(password.encode('utf-8'))\n\n    key = password[:key_size_bytes] + [0] * (key_size_bytes - len(password))\n    key = aes_encrypt(key[:BLOCK_SIZE_BYTES], key_expansion(key)) * (key_size_bytes // BLOCK_SIZE_BYTES)\n\n    nonce = data[:NONCE_LENGTH_BYTES]\n    cipher = data[NONCE_LENGTH_BYTES:]\n\n    class Counter(object):\n        __value = nonce + [0] * (BLOCK_SIZE_BYTES - NONCE_LENGTH_BYTES)\n\n        def next_value(self):\n            temp = self.__value\n            self.__value = inc(self.__value)\n            return temp\n\n    decrypted_data = aes_ctr_decrypt(cipher, key, Counter())\n    plaintext = intlist_to_bytes(decrypted_data)\n\n    return plaintext", "loc": 36}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "key_schedule_core", "parameters": ["data", "rcon_iteration"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["rotate", "sub_bytes"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def key_schedule_core(data, rcon_iteration):\n    data = rotate(data)\n    data = sub_bytes(data)\n    data[0] = data[0] ^ RCON[rcon_iteration]\n\n    return data", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "mix_column", "parameters": ["data", "matrix"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data_mixed.append", "range", "rijndael_mul"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mix_column(data, matrix):\n    data_mixed = []\n    for row in range(4):\n        mixed = 0\n        for column in range(4):\n            # xor is (+) and (-)\n            mixed ^= rijndael_mul(data[column], matrix[row][column])\n        data_mixed.append(mixed)\n    return data_mixed", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "mix_columns", "parameters": ["data", "matrix"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mix_column", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mix_columns(data, matrix=MIX_COLUMN_MATRIX):\n    data_mixed = []\n    for i in range(4):\n        column = data[i * 4: (i + 1) * 4]\n        data_mixed += mix_column(column, matrix)\n    return data_mixed", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "shift_rows", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data_shifted.append", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def shift_rows(data):\n    data_shifted = []\n    for column in range(4):\n        for row in range(4):\n            data_shifted.append(data[((column + row) & 0b11) * 4 + row])\n    return data_shifted", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "shift_rows_inv", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data_shifted.append", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def shift_rows_inv(data):\n    data_shifted = []\n    for column in range(4):\n        for row in range(4):\n            data_shifted.append(data[((column - row) & 0b11) * 4 + row])\n    return data_shifted", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\aes.py", "class_name": null, "function_name": "inc", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "range"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inc(data):\n    data = data[:]  # copy\n    for i in range(len(data) - 1, -1, -1):\n        if data[i] == 255:\n            data[i] = 0\n        else:\n            data[i] = data[i] + 1\n            break\n    return data", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\cache.py", "class_name": "Cache", "function_name": "load", "parameters": ["self", "section", "key", "dtype", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Loading {section}.{key} from cache'.format", "error_to_compat_str", "getattr", "json.load", "kw_min_ver.get", "open", "os.path.getsize", "self._get_cache_fn", "self._report_warning", "self._validate", "self._write_debug"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def load(self, section, key, dtype='json', default=None, **kw_min_ver):\n    assert dtype in ('json',)\n    min_ver = kw_min_ver.get('min_ver')\n\n    if not self.enabled:\n        return default\n\n    cache_fn = self._get_cache_fn(section, key, dtype)\n    try:\n        with open(cache_fn, encoding='utf-8') as cachef:\n            self._write_debug('Loading {section}.{key} from cache'.format(section=section, key=key), only_once=True)\n            return self._validate(json.load(cachef), min_ver)\n    except (ValueError, KeyError):\n        try:\n            file_size = 'size: %d' % os.path.getsize(cache_fn)\n        except (OSError, IOError) as oe:\n            file_size = error_to_compat_str(oe)\n        self._report_warning('Cache retrieval from %s failed (%s)' % (cache_fn, file_size))\n    except Exception as e:\n        if getattr(e, 'errno') == errno.ENOENT:\n            # no cache available\n            return\n        self._report_warning('Cache retrieval from %s failed' % (cache_fn,))\n\n    return default", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_ord", "parameters": ["c"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "ord"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_ord(c):\n    if isinstance(c, int):\n        return c\n    else:\n        return ord(c)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_register_utf8", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lookup", "register"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_register_utf8():\n    if sys.platform == 'win32':\n        # https://github.com/ytdl-org/youtube-dl/issues/820\n        from codecs import register, lookup\n        register(\n            lambda name: lookup('utf-8') if name == 'cp65001' else None)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_etree_fromstring", "parameters": ["text"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_TreeBuilder", "_XML", "_etree.XMLParser", "_etree_iter", "el.text.decode", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_etree_fromstring(text):\n    doc = _XML(text, parser=_etree.XMLParser(target=_TreeBuilder(element_factory=_element_factory)))\n    for el in _etree_iter(doc):\n        if el.text is not None and isinstance(el.text, bytes):\n            el.text = el.text.decode('utf-8')\n    return doc", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_etree_iterfind", "parameters": ["elem", "path", "namespaces"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SyntaxError", "_SelectorContext", "_cache.clear", "_xpath_tokenizer", "len", "select", "selector.append"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_etree_iterfind(elem, path, namespaces=None):\n    # compile selector pattern\n    if path[-1:] == \"/\":\n        path = path + \"*\"  # implicit all (FIXME: keep this?)\n    try:\n        selector = _cache[path]\n    except KeyError:\n        if len(_cache) > 100:\n            _cache.clear()\n        if path[:1] == \"/\":\n            raise SyntaxError(\"cannot use absolute path on element\")\n        tokens = _xpath_tokenizer(path, namespaces)\n        selector = []\n        for token in tokens:\n            if token[0] == \"/\":\n                continue\n            try:\n                selector.append(ops[token[0]](tokens, token))\n            except StopIteration:\n                raise SyntaxError(\"invalid path\")\n        _cache[path] = selector\n    # execute selector pattern\n    result = [elem]\n    context = _SelectorContext(elem)\n    for select in selector:\n        result = select(context, result)\n    return result", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_getenv", "parameters": ["key", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["env.decode", "get_filesystem_encoding", "os.getenv"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_getenv(key, default=None):\n    from .utils import get_filesystem_encoding\n    env = os.getenv(key, default)\n    if env:\n        env = env.decode(get_filesystem_encoding())\n    return env", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_setenv", "parameters": ["key", "value", "env"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encode", "get_filesystem_encoding", "isinstance", "v.encode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_setenv(key, value, env=os.environ):\n    def encode(v):\n        from .utils import get_filesystem_encoding\n        return v.encode(get_filesystem_encoding()) if isinstance(v, compat_str) else v\n    env[encode(key)] = encode(value)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_getpass", "parameters": ["prompt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getpass.getpass", "isinstance", "preferredencoding", "prompt.encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_getpass(prompt, *args, **kwargs):\n    if isinstance(prompt, compat_str):\n        from .utils import preferredencoding\n        prompt = prompt.encode(preferredencoding())\n    return getpass.getpass(prompt, *args, **kwargs)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_socket_create_connection", "parameters": ["address", "timeout", "source_address"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["sock.bind", "sock.close", "sock.connect", "sock.settimeout", "socket.error", "socket.getaddrinfo", "socket.socket"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_socket_create_connection(address, timeout, source_address=None):\n    host, port = address\n    err = None\n    for res in socket.getaddrinfo(host, port, 0, socket.SOCK_STREAM):\n        af, socktype, proto, canonname, sa = res\n        sock = None\n        try:\n            sock = socket.socket(af, socktype, proto)\n            sock.settimeout(timeout)\n            if source_address:\n                sock.bind(source_address)\n            sock.connect(sa)\n            return sock\n        except socket.error as _:\n            err = _\n            if sock is not None:\n                sock.close()\n    if err is not None:\n        raise err\n    else:\n        raise socket.error('getaddrinfo returns an empty list')", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_ctypes_WINFUNCTYPE", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ctypes.WINFUNCTYPE", "real", "str"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_ctypes_WINFUNCTYPE(*args, **kwargs):\n    real = ctypes.WINFUNCTYPE(*args, **kwargs)\n\n    def resf(tpl, *args, **kwargs):\n        funcname, dll = tpl\n        return real((str(funcname), dll), *args, **kwargs)\n\n    return resf", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": "compat_cookies_SimpleCookie", "function_name": "load", "parameters": ["self", "rawdata"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "morsel.get", "re.subn", "self.values", "str", "super", "super(compat_cookies_SimpleCookie, self).load", "sys.platform.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def load(self, rawdata):\n    must_have_value = 0\n    if not isinstance(rawdata, dict):\n        if sys.version_info[:2] != (2, 7) or sys.platform.startswith('java'):\n            # attribute must have value for parsing\n            rawdata, must_have_value = re.subn(\n                r'(?i)(;\\s*)(secure|httponly)(\\s*(?:;|$))', r'\\1\\2=\\2\\3', rawdata)\n        if sys.version_info[0] == 2:\n            if isinstance(rawdata, compat_str):\n                rawdata = str(rawdata)\n    super(compat_cookies_SimpleCookie, self).load(rawdata)\n    if must_have_value > 0:\n        for morsel in self.values():\n            for attr in ('secure', 'httponly'):\n                if morsel.get(attr):\n                    morsel[attr] = True", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_urllib_parse_unquote_to_bytes", "parameters": ["string"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["append", "b''.join", "isinstance", "len", "string.encode", "string.split"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "unquote_to_bytes('abc%20def') -> b'abc def'.", "source_code": "def compat_urllib_parse_unquote_to_bytes(string):\n    \"\"\"unquote_to_bytes('abc%20def') -> b'abc def'.\"\"\"\n    # Note: strings are encoded as UTF-8. This is only an issue if it contains\n    # unescaped non-ASCII characters, which URIs should not.\n    if not string:\n        # Is it a string-like object?\n        string.split\n        return b''\n    if isinstance(string, compat_str):\n        string = string.encode('utf-8')\n    bits = string.split(b'%')\n    if len(bits) == 1:\n        return string\n    res = [bits[0]]\n    append = res.append\n    for item in bits[1:]:\n        try:\n            append(compat_urllib_parse._hextochr[item[:2]])\n            append(item[2:])\n        except KeyError:\n            append(b'%')\n            append(item)\n    return b''.join(res)", "loc": 23}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_parse_qs", "parameters": ["qs", "keep_blank_values", "strict_parsing", "encoding", "errors"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_parse_qsl", "parsed_result[name].append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_parse_qs(qs, keep_blank_values=False, strict_parsing=False,\n                    encoding='utf-8', errors='replace'):\n    parsed_result = {}\n    pairs = _parse_qsl(qs, keep_blank_values, strict_parsing,\n                       encoding=encoding, errors=errors)\n    for name, value in pairs:\n        if name in parsed_result:\n            parsed_result[name].append(value)\n        else:\n            parsed_result[name] = [value]\n    return parsed_result", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_etree_register_namespace", "parameters": ["prefix", "uri"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "_etree._namespace_map.items", "list", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Register a namespace prefix. The registry is global, and any existing mapping for either the given prefix or the namespace URI will be removed.", "source_code": "def compat_etree_register_namespace(prefix, uri):\n    \"\"\"Register a namespace prefix.\n    The registry is global, and any existing mapping for either the\n    given prefix or the namespace URI will be removed.\n    *prefix* is the namespace prefix, *uri* is a namespace uri. Tags and\n    attributes in this namespace will be serialized with prefix if possible.\n    ValueError is raised if prefix is reserved or is invalid.\n    \"\"\"\n    if re.match(r'ns\\d+$', prefix):\n        raise ValueError('Prefix format reserved for internal use')\n    for k, v in list(_etree._namespace_map.items()):\n        if k == uri or v == prefix:\n            del _etree._namespace_map[k]\n    _etree._namespace_map[uri] = prefix", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "select", "parameters": ["context", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["elem.getiterator"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select(context, result):\n    for elem in result:\n        for e in elem.getiterator(tag):\n            if e is not elem:\n                yield e", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "select", "parameters": ["context", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_parent_map"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select(context, result):\n    # FIXME: raise error if .. is applied at toplevel?\n    parent_map = _get_parent_map(context)\n    result_map = {}\n    for elem in result:\n        if elem in parent_map:\n            parent = parent_map[elem]\n            if parent not in result_map:\n                result_map[parent] = None\n                yield parent", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_expanduser", "parameters": ["path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_getenv", "len", "os.getuid", "path.find", "path.startswith", "pwd.getpwnam", "pwd.getpwuid", "userhome.rstrip"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "Expand ~ and ~user constructions.  If user or $HOME is unknown, do nothing.", "source_code": "def compat_expanduser(path):\n    \"\"\"Expand ~ and ~user constructions.  If user or $HOME is unknown,\n    do nothing.\"\"\"\n    if not path.startswith('~'):\n        return path\n    i = path.find('/', 1)\n    if i < 0:\n        i = len(path)\n    if i == 1:\n        if 'HOME' not in os.environ:\n            import pwd\n            userhome = pwd.getpwuid(os.getuid()).pw_dir\n        else:\n            userhome = compat_getenv('HOME')\n    else:\n        import pwd\n        try:\n            pwent = pwd.getpwnam(path[1:i])\n        except KeyError:\n            return path\n        userhome = pwent.pw_dir\n    userhome = userhome.rstrip('/')\n    return (userhome + path[i:]) or '/'", "loc": 23}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_os_makedirs", "parameters": ["name", "mode", "exist_ok"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["os.makedirs"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_os_makedirs(name, mode=0o777, exist_ok=False):\n    try:\n        return os.makedirs(name, mode=mode)\n    except OSError as ose:\n        if not (exist_ok and ose.errno == _errno_EEXIST):\n            raise", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "wrapped_init", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "callable", "init", "kwargs.pop", "self.has_data", "types.MethodType", "x.__dict__.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped_init(self, *args, **kwargs):\n    method = kwargs.pop('method', 'GET')\n    init(self, *args, **kwargs)\n    if any(callable(x.__dict__.get('get_method')) for x in (self.__class__, self) if x != cls):\n        # allow instance or its subclass to override get_method()\n        return\n    if self.has_data() and method == 'GET':\n        method = 'POST'\n    self.get_method = types.MethodType(lambda _: method, self)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "encode_elem", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["e.encode", "encode_dict", "encode_elem", "isinstance", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encode_elem(e):\n    if isinstance(e, dict):\n        e = encode_dict(e)\n    elif isinstance(e, (list, tuple,)):\n        e = type(e)(encode_elem(el) for el in e)\n    elif isinstance(e, compat_str):\n        e = e.encode(encoding, errors)\n    return e", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "itertext", "parameters": ["el"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["el.getiterator"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def itertext(el):\n    for e in el.getiterator():\n        e = e.text\n        if e:\n            yield e", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "select", "parameters": ["context", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "elem.findall", "itertext"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select(context, result):\n    for elem in result:\n        for e in elem.findall(tag):\n            if \"\".join(itertext(e)) == value:\n                yield elem\n                break", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "select", "parameters": ["context", "result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_get_parent_map", "list", "parent.findall"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def select(context, result):\n    parent_map = _get_parent_map(context)\n    for elem in result:\n        try:\n            parent = parent_map[elem]\n            # FIXME: what if the selector is \"*\" ?\n            elems = list(parent.findall(elem.tag))\n            if elems[index] is elem:\n                yield elem\n        except (IndexError, KeyError):\n            pass", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": null, "function_name": "compat_shlex_quote", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.match", "s.replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_shlex_quote(s):\n    if re.match(r'^[-_\\w./]+$', s):\n        return s\n    else:\n        return \"'\" + s.replace(\"'\", \"'\\\"'\\\"'\") + \"'\"", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": "compat_collections_chain_map", "function_name": "pop", "parameters": ["self", "k"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["KeyError", "len", "self.__contains__", "self.__delitem", "self.__getitem__"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pop(self, k, *args):\n    if self.__contains__(k):\n        off = self.__getitem__(k)\n        self.__delitem(k)\n        return off\n    elif len(args) > 0:\n        return args[0]\n    raise KeyError(k)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\compat.py", "class_name": "compat_collections_chain_map", "function_name": "new_child", "parameters": ["self", "m"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.update", "type"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def new_child(self, m=None, **kwargs):\n    m = m or {}\n    m.update(kwargs)\n    # support inheritance !\n    return type(self)(m, *self.maps)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "wraps_op", "parameters": ["op"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["str", "update_wrapper"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wraps_op(op):\n\n    def update_and_rename_wrapper(w):\n        f = update_wrapper(w, op)\n        # fn names are str in both Py 2/3\n        f.__name__ = str('JS_') + f.__name__\n        return f\n\n    return update_and_rename_wrapper", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "update_and_rename_wrapper", "parameters": ["w"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["str", "update_wrapper"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_and_rename_wrapper(w):\n    f = update_wrapper(w, op)\n    # fn names are str in both Py 2/3\n    f.__name__ = str('JS_') + f.__name__\n    return f", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "zeroise", "parameters": ["x", "is_shift_arg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_int", "float", "int", "isinstance"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def zeroise(x, is_shift_arg=False):\n    if isinstance(x, compat_integer_types):\n        return (x % 32) if is_shift_arg else (x & 0xffffffff)\n    try:\n        x = float(x)\n        if is_shift_arg:\n            x = int(x % 32)\n        elif x < 0:\n            x = -compat_int(-x % 0xffffffff)\n        else:\n            x = compat_int(x % 0xffffffff)\n    except (ValueError, TypeError):\n        # also here for int(NaN), including float('inf') % 32\n        x = 0\n    return x", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "wrapped", "parameters": ["a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float_or_none", "isinstance", "op", "wraps_op", "x.strip"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(a, b):\n    if JS_Undefined in (a, b):\n        return _NaN\n    # null, \"\" --> 0\n    a, b = (float_or_none(\n        (x.strip() if isinstance(x, compat_basestring) else x) or 0,\n        default=_NaN) for x in (a, b))\n    if _NaN in (a, b):\n        return _NaN\n    try:\n        return op(a, b)\n    except ZeroDivisionError:\n        return _NaN if not (div and (a or b)) else _Infinity", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "wrapped", "parameters": ["a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "op", "wraps_op"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(a, b):\n    if _NaN in (a, b):\n        return op(_NaN, None)\n    if not isinstance(a, (compat_basestring, compat_numeric_types)):\n        a, b = b, a\n    # strings are === if ==\n    # why 'a' is not 'a': https://stackoverflow.com/a/1504848\n    if isinstance(a, (compat_basestring, compat_numeric_types)):\n        return a == b if op(0, 0) else a != b\n    return op(a, b)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "wrapped", "parameters": ["a", "b"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "isinstance", "op", "wraps_op"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapped(a, b):\n    if JS_Undefined in (a, b):\n        return False\n    if isinstance(a, compat_basestring):\n        b = compat_str(b or 0)\n    elif isinstance(b, compat_basestring):\n        a = compat_str(a or 0)\n    return op(a or 0, b or 0)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "Debugger", "function_name": "write", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'...'.join", "'[debug] JS: {0}{1}\\n'.format", "compat_str", "kwargs.get", "len", "truncate_string", "write_string"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write(*args, **kwargs):\n    level = kwargs.get('level', 100)\n\n    def truncate_string(s, left, right=0):\n        if s is None or len(s) <= left + right:\n            return s\n        return '...'.join((s[:left - 3], s[-right:] if right else ''))\n\n    write_string('[debug] JS: {0}{1}\\n'.format(\n        '  ' * (100 - level),\n        ' '.join(truncate_string(compat_str(x), 50, 50) for x in args)))", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "Debugger", "function_name": "wrap_interpreter", "parameters": ["cls", "f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "cls.write", "f", "isinstance", "repr", "stmt.strip", "wraps"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrap_interpreter(cls, f):\n    if not cls.ENABLED:\n        return f\n\n    @wraps(f)\n    def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n        if cls.ENABLED and stmt.strip():\n            cls.write(stmt, level=allow_recursion)\n        try:\n            ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\n        except Exception as e:\n            if cls.ENABLED:\n                if isinstance(e, ExtractorError):\n                    e = e.orig_msg\n                cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n            raise\n        if cls.ENABLED and stmt.strip():\n            if should_ret or repr(ret) != stmt:\n                cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n        return ret, should_ret\n    return interpret_statement", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "interpret_expression", "parameters": ["self", "expr", "local_vars", "allow_recursion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.Exception", "self.interpret_statement"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def interpret_expression(self, expr, local_vars, allow_recursion):\n    ret, should_return = self.interpret_statement(expr, local_vars, allow_recursion)\n    if should_return:\n        raise self.Exception('Cannot return from an expression', expr)\n    return ret", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "extract_object", "parameters": ["self", "objname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'(?:{n}|\"{n}\"|\\'{n}\\')'.format", "'(?xs)\\n                    {0}\\\\s*\\\\.\\\\s*{1}|{1}\\\\s*=\\\\s*\\\\{{\\\\s*\\n                        (?P<fields>({2}\\\\s*:\\\\s*function\\\\s*\\\\(.*?\\\\)\\\\s*\\\\{{.*?}}(?:,\\\\s*)?)*)\\n                    }}\\\\s*;\\n                '.format", "'F<{0}>'.format", "f.group", "filter", "function_with_repr", "next", "obj_m.group", "re.escape", "re.finditer", "remove_quotes", "self.Exception", "self.build_arglist", "self.build_function"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_object(self, objname, *global_stack):\n    _FUNC_NAME_RE = r'''(?:{n}|\"{n}\"|'{n}')'''.format(n=_NAME_RE)\n    obj = {}\n    fields = next(filter(None, (\n        obj_m.group('fields') for obj_m in re.finditer(\n            r'''(?xs)\n                {0}\\s*\\.\\s*{1}|{1}\\s*=\\s*\\{{\\s*\n                    (?P<fields>({2}\\s*:\\s*function\\s*\\(.*?\\)\\s*\\{{.*?}}(?:,\\s*)?)*)\n                }}\\s*;\n            '''.format(_NAME_RE, re.escape(objname), _FUNC_NAME_RE),\n            self.code))), None)\n    if not fields:\n        raise self.Exception('Could not find object ' + objname)\n    # Currently, it only supports function definitions\n    for f in re.finditer(\n            r'''(?x)\n                (?P<key>%s)\\s*:\\s*function\\s*\\((?P<args>(?:%s|,)*)\\){(?P<code>[^}]+)}\n            ''' % (_FUNC_NAME_RE, _NAME_RE),\n            fields):\n        argnames = self.build_arglist(f.group('args'))\n        name = remove_quotes(f.group('key'))\n        obj[name] = function_with_repr(\n            self.build_function(argnames, f.group('code'), *global_stack), 'F<{0}>'.format(name))\n\n    return obj", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "extract_function_code", "parameters": ["self", "funcname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Could not find JS function \"{funcname}\"'.format", "func_m.group", "locals", "re.escape", "re.search", "self.Exception", "self._separate_at_paren", "self.build_arglist"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "@returns argnames, code", "source_code": "def extract_function_code(self, funcname):\n    \"\"\" @returns argnames, code \"\"\"\n    func_m = re.search(\n        r'''(?xs)\n            (?:\n                function\\s+%(name)s|\n                [{;,]\\s*%(name)s\\s*=\\s*function|\n                (?:var|const|let)\\s+%(name)s\\s*=\\s*function\n            )\\s*\n            \\((?P<args>[^)]*)\\)\\s*\n            (?P<code>{.+})''' % {'name': re.escape(funcname)},\n        self.code)\n    if func_m is None:\n        raise self.Exception('Could not find JS function \"{funcname}\"'.format(**locals()))\n    code, _ = self._separate_at_paren(func_m.group('code'))  # refine the match\n    return self.build_arglist(func_m.group('args')), code", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "extract_function", "parameters": ["self", "funcname"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["function_with_repr", "itertools.chain", "self.extract_function_code", "self.extract_function_from_code"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_function(self, funcname, *global_stack):\n    return function_with_repr(\n        self.extract_function_from_code(*itertools.chain(\n            self.extract_function_code(funcname), global_stack)),\n        'F<%s>' % (funcname,))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "extract_function_from_code", "parameters": ["self", "argnames", "code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mobj.group", "mobj.group('args').split", "mobj.span", "re.search", "self._named_object", "self._separate_at_paren", "self.build_function", "self.extract_function_from_code", "x.strip"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_function_from_code(self, argnames, code, *global_stack):\n    local_vars = {}\n\n    start = None\n    while True:\n        mobj = re.search(r'function\\((?P<args>[^)]*)\\)\\s*{', code[start:])\n        if mobj is None:\n            break\n        start, body_start = ((start or 0) + x for x in mobj.span())\n        body, remaining = self._separate_at_paren(code[body_start - 1:])\n        name = self._named_object(local_vars, self.extract_function_from_code(\n            [x.strip() for x in mobj.group('args').split(',')],\n            body, local_vars, *global_stack))\n        code = code[:start] + name + remaining\n\n    return self.build_function(argnames, code, local_vars, *global_stack)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "build_arglist", "parameters": ["cls", "arg_text"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.Exception", "cls._separate", "valid_arg", "y.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_arglist(cls, arg_text):\n    if not arg_text:\n        return []\n\n    def valid_arg(y):\n        y = y.strip()\n        if not y:\n            raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n        return y\n\n    return [valid_arg(x) for x in cls._separate(arg_text)]", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JSInterpreter", "function_name": "build_function", "parameters": ["self", "argnames", "code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LocalNameSpace", "code.replace", "global_stack[0].update", "list", "self.interpret_statement", "tuple", "zip_longest"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_function(self, argnames, code, *global_stack):\n    global_stack = list(global_stack) or [{}]\n    argnames = tuple(argnames)\n\n    def resf(args, kwargs=None, allow_recursion=100):\n        kwargs = kwargs or {}\n        global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n        global_stack[0].update(kwargs)\n        var_stack = LocalNameSpace(*global_stack)\n        ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n        if should_abort:\n            return ret\n    return resf", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "interpret_statement", "parameters": ["self", "stmt", "local_vars", "allow_recursion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bool", "cls.write", "f", "isinstance", "repr", "stmt.strip", "wraps"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def interpret_statement(self, stmt, local_vars, allow_recursion, *args, **kwargs):\n    if cls.ENABLED and stmt.strip():\n        cls.write(stmt, level=allow_recursion)\n    try:\n        ret, should_ret = f(self, stmt, local_vars, allow_recursion, *args, **kwargs)\n    except Exception as e:\n        if cls.ENABLED:\n            if isinstance(e, ExtractorError):\n                e = e.orig_msg\n            cls.write('=> Raises:', e, '<-|', stmt, level=allow_recursion)\n        raise\n    if cls.ENABLED and stmt.strip():\n        if should_ret or repr(ret) != stmt:\n            cls.write(['->', '=>'][bool(should_ret)], repr(ret), '<-|', stmt, level=allow_recursion)\n    return ret, should_ret", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JS_RegExp", "function_name": "regex_flags", "parameters": ["cls", "expr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def regex_flags(cls, expr):\n    flags = 0\n    if not expr:\n        return flags, expr\n    for idx, ch in enumerate(expr):\n        if ch not in cls.RE_FLAGS:\n            break\n        flags |= cls.RE_FLAGS[ch]\n    return flags, expr[idx + 1:]", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JS_Date", "function_name": "parse", "parameters": ["date_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "kw_is_raw.get", "str_or_none", "unified_timestamp"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse(date_str, **kw_is_raw):\n    is_raw = kw_is_raw.get('is_raw', False)\n\n    t = unified_timestamp(str_or_none(date_str), False)\n    return int(t * 1000) if t is not None else t if is_raw else _NaN", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JS_Date", "function_name": "now", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "kw_is_raw.get", "time.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def now(**kw_is_raw):\n    is_raw = kw_is_raw.get('is_raw', False)\n\n    t = time.time()\n    return int(t * 1000) if t is not None else t if is_raw else _NaN", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": "JS_Date", "function_name": "toString", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["time.strftime", "time.strftime('%a %b %0d %Y %H:%M:%S %Z%z', self._t).rstrip"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def toString(self):\n    try:\n        return time.strftime('%a %b %0d %Y %H:%M:%S %Z%z', self._t).rstrip()\n    except TypeError:\n        return \"Invalid Date\"", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "valid_arg", "parameters": ["y"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.Exception", "y.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def valid_arg(y):\n    y = y.strip()\n    if not y:\n        raise cls.Exception('Missing arg in \"%s\"' % (arg_text, ))\n    return y", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "resf", "parameters": ["args", "kwargs", "allow_recursion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["LocalNameSpace", "code.replace", "global_stack[0].update", "self.interpret_statement", "zip_longest"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resf(args, kwargs=None, allow_recursion=100):\n    kwargs = kwargs or {}\n    global_stack[0].update(zip_longest(argnames, args, fillvalue=JS_Undefined))\n    global_stack[0].update(kwargs)\n    var_stack = LocalNameSpace(*global_stack)\n    ret, should_abort = self.interpret_statement(code.replace('\\n', ' '), var_stack, allow_recursion - 1)\n    if should_abort:\n        return ret", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "assertion", "parameters": ["cndn", "msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{memb} {msg}'.format", "locals", "self.Exception"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "assert, but without risk of getting optimized out", "source_code": "def assertion(cndn, msg):\n    \"\"\" assert, but without risk of getting optimized out \"\"\"\n    if not cndn:\n        memb = member\n        raise self.Exception('{memb} {msg}'.format(**locals()), expr=expr)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "yield_terms", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bodmas[-1].strip", "dm_op.join", "enumerate", "len", "list", "self._separate", "set"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yield_terms(s):\n    skip = False\n    for i, term in enumerate(s[:-1]):\n        if skip:\n            skip = False\n            continue\n        if not (dm_chars & set(term)):\n            yield term\n            continue\n        for dm_op in dm_ops:\n            bodmas = list(self._separate(term, dm_op, skip_delims=skip_delim))\n            if len(bodmas) > 1 and not bodmas[-1].strip():\n                bodmas[-1] = (op if op == '-' else '') + s[i + 1]\n                yield dm_op.join(bodmas)\n                skip = True\n                break\n        else:\n            if term:\n                yield term\n\n    if not skip and s[-1]:\n        yield s[-1]", "loc": 22}
{"file": "youtube-dl\\youtube_dl\\jsinterp.py", "class_name": null, "function_name": "splits", "parameters": ["limit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "where"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def splits(limit=limit):\n    i = 0\n    for j, jj in where():\n        if j == jj == 0:\n            continue\n        if j is None and i >= len(obj):\n            break\n        yield obj[i:j]\n        if jj is None or limit == 1:\n            break\n        limit -= 1\n        i = jj", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\socks.py", "class_name": "sockssocket", "function_name": "recvall", "parameters": ["self", "cnt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0} bytes missing'.format", "EOFError", "len", "self.recv"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def recvall(self, cnt):\n    data = b''\n    while len(data) < cnt:\n        cur = self.recv(cnt - len(data))\n        if not cur:\n            raise EOFError('{0} bytes missing'.format(cnt - len(data)))\n        data += cur\n    return data", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\swfinterp.py", "class_name": "_AVMClass", "function_name": "register_methods", "parameters": ["self", "methods"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "methods.items", "self.method_idxs.update", "self.method_names.update"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def register_methods(self, methods):\n    self.method_names.update(methods.items())\n    self.method_idxs.update(dict(\n        (idx, name)\n        for name, idx in methods.items()))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\swfinterp.py", "class_name": "SWFInterpreter", "function_name": "extract_class", "parameters": ["self", "class_name", "call_cinit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "cinit", "hasattr", "res.register_methods", "self.extract_function"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_class(self, class_name, call_cinit=True):\n    try:\n        res = self._classes_by_name[class_name]\n    except KeyError:\n        raise ExtractorError('Class %r not found' % class_name)\n\n    if call_cinit and hasattr(res, 'cinit_idx'):\n        res.register_methods({'$cinit': res.cinit_idx})\n        res.methods['$cinit'] = self._all_methods[res.cinit_idx]\n        cinit = self.extract_function(res, '$cinit')\n        cinit([])\n\n    return res", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\swfinterp.py", "class_name": null, "function_name": "parse_traits_info", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "range", "read_byte", "u30"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_traits_info():\n    trait_name_idx = u30()\n    kind_full = read_byte()\n    kind = kind_full & 0x0f\n    attrs = kind_full >> 4\n    methods = {}\n    constants = None\n    if kind == 0x00:  # Slot\n        u30()  # Slot id\n        u30()  # type_name_idx\n        vindex = u30()\n        if vindex != 0:\n            read_byte()  # vkind\n    elif kind == 0x06:  # Const\n        u30()  # Slot id\n        u30()  # type_name_idx\n        vindex = u30()\n        vkind = 'any'\n        if vindex != 0:\n            vkind = read_byte()\n        if vkind == 0x03:  # Constant_Int\n            value = self.constant_ints[vindex]\n        elif vkind == 0x04:  # Constant_UInt\n            value = self.constant_uints[vindex]\n        else:\n            return {}, None  # Ignore silently for now\n        constants = {self.multinames[trait_name_idx]: value}\n    elif kind in (0x01, 0x02, 0x03):  # Method / Getter / Setter\n        u30()  # disp_id\n        method_idx = u30()\n        methods[self.multinames[trait_name_idx]] = method_idx\n    elif kind == 0x04:  # Class\n        u30()  # slot_id\n        u30()  # classi\n    elif kind == 0x05:  # Function\n        u30()  # slot_id\n        function_idx = u30()\n        methods[function_idx] = self.multinames[trait_name_idx]\n    else:\n        raise ExtractorError('Unsupported trait kind %d' % kind)\n\n    if attrs & 0x4 != 0:  # Metadata present\n        metadata_count = u30()\n        for _c3 in range(metadata_count):\n            u30()  # metadata index\n\n    return methods, constants", "loc": 47}
{"file": "youtube-dl\\youtube_dl\\update.py", "class_name": null, "function_name": "rsa_verify", "parameters": ["message", "signature", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'%x' % pow(int(signature, 16), key[1], key[0]).encode", "bin", "int", "isinstance", "len", "pow", "sha256", "sha256(message).hexdigest", "sha256(message).hexdigest().encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def rsa_verify(message, signature, key):\n    from hashlib import sha256\n    assert isinstance(message, bytes)\n    byte_size = (len(bin(key[0])) - 2 + 8 - 1) // 8\n    signature = ('%x' % pow(int(signature, 16), key[1], key[0])).encode()\n    signature = (byte_size * 2 - len(signature)) * b'0' + signature\n    asn1 = b'3031300d060960864801650304020105000420'\n    asn1 += sha256(message).hexdigest().encode()\n    if byte_size < len(asn1) // 2 + 11:\n        return False\n    expected = b'0001' + (byte_size - len(asn1) // 2 - 3) * b'ff' + b'00' + asn1\n    return expected == signature", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\update.py", "class_name": null, "function_name": "get_notes", "parameters": ["versions", "fromVersion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["notes.extend", "sorted", "vdata.get", "versions.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_notes(versions, fromVersion):\n    notes = []\n    for v, vdata in sorted(versions.items()):\n        if v > fromVersion:\n            notes.extend(vdata.get('notes', []))\n    return notes", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\update.py", "class_name": null, "function_name": "print_notes", "parameters": ["to_screen", "versions", "fromVersion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_notes", "to_screen"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def print_notes(to_screen, versions, fromVersion=__version__):\n    notes = get_notes(versions, fromVersion)\n    if notes:\n        to_screen('PLEASE NOTE:')\n        for note in notes:\n            to_screen(note)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "preferredencoding", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'TEST'.encode", "locale.getpreferredencoding"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Get preferred encoding.", "source_code": "def preferredencoding():\n    \"\"\"Get preferred encoding.\n\n    Returns the best encoding scheme for the system, based on\n    locale.getpreferredencoding() and some further tweaks.\n    \"\"\"\n    try:\n        pref = locale.getpreferredencoding()\n        'TEST'.encode(pref)\n    except Exception:\n        pref = 'UTF-8'\n\n    return pref", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "write_json_file", "parameters": ["obj", "fn"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["args.update", "compat_contextlib_suppress", "compat_kwargs", "encodeFilename", "get_filesystem_encoding", "json.dump", "os.chmod", "os.path.basename", "os.path.basename(f).decode", "os.path.dirname", "os.path.dirname(f).decode", "os.remove", "os.rename", "os.umask", "os.unlink", "path_basename", "path_dirname", "tempfile.NamedTemporaryFile"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "Encode obj as JSON and write it to fn, atomically if possible", "source_code": "def write_json_file(obj, fn):\n    \"\"\" Encode obj as JSON and write it to fn, atomically if possible \"\"\"\n\n    fn = encodeFilename(fn)\n    if sys.version_info < (3, 0) and sys.platform != 'win32':\n        encoding = get_filesystem_encoding()\n        # os.path.basename returns a bytes object, but NamedTemporaryFile\n        # will fail if the filename contains non-ascii characters unless we\n        # use a unicode object\n        path_basename = lambda f: os.path.basename(f).decode(encoding)\n        # the same for os.path.dirname\n        path_dirname = lambda f: os.path.dirname(f).decode(encoding)\n    else:\n        path_basename = os.path.basename\n        path_dirname = os.path.dirname\n\n    args = {\n        'suffix': '.tmp',\n        'prefix': path_basename(fn) + '.',\n        'dir': path_dirname(fn),\n        'delete': False,\n    }\n\n    # In Python 2.x, json.dump expects a bytestream.\n    # In Python 3.x, it writes to a character stream\n    if sys.version_info < (3, 0):\n        args['mode'] = 'wb'\n    else:\n        args.update({\n            'mode': 'w',\n            'encoding': 'utf-8',\n        })\n\n    tf = tempfile.NamedTemporaryFile(**compat_kwargs(args))\n\n    try:\n        with tf:\n            json.dump(obj, tf)\n        with compat_contextlib_suppress(OSError):\n            if sys.platform == 'win32':\n                # Need to remove existing file on Windows, else os.rename raises\n                # WindowsError or FileExistsError.\n                os.unlink(fn)\n            mask = os.umask(0)\n            os.umask(mask)\n            os.chmod(tf.name, 0o666 & ~mask)\n        os.rename(tf.name, fn)\n    except Exception:\n        with compat_contextlib_suppress(OSError):\n            os.remove(tf.name)\n        raise", "loc": 51}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "xpath_with_ns", "parameters": ["path", "ns_map"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'/'.join", "c.split", "len", "path.split", "replaced.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def xpath_with_ns(path, ns_map):\n    components = [c.split(':') for c in path.split('/')]\n    replaced = []\n    for c in components:\n        if len(c) == 1:\n            replaced.append(c[0])\n        else:\n            ns, tag = c\n            replaced.append('{%s}%s' % (ns_map[ns], tag))\n    return '/'.join(replaced)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "xpath_element", "parameters": ["node", "xpath", "name", "fatal", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "_find_xpath", "compat_xpath", "isinstance", "node.find"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def xpath_element(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    def _find_xpath(xpath):\n        return node.find(compat_xpath(xpath))\n\n    if isinstance(xpath, compat_basestring):\n        n = _find_xpath(xpath)\n    else:\n        for xp in xpath:\n            n = _find_xpath(xp)\n            if n is not None:\n                break\n\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element %s' % name)\n        else:\n            return None\n    return n", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "xpath_text", "parameters": ["node", "xpath", "name", "fatal", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "xpath_element"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def xpath_text(node, xpath, name=None, fatal=False, default=NO_DEFAULT):\n    n = xpath_element(node, xpath, name, fatal=fatal, default=default)\n    if n is None or n == default:\n        return n\n    if n.text is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = xpath if name is None else name\n            raise ExtractorError('Could not find XML element\\'s text %s' % name)\n        else:\n            return None\n    return n.text", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "xpath_attr", "parameters": ["node", "xpath", "key", "name", "fatal", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "find_xpath_attr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def xpath_attr(node, xpath, key, name=None, fatal=False, default=NO_DEFAULT):\n    n = find_xpath_attr(node, xpath, key)\n    if n is None:\n        if default is not NO_DEFAULT:\n            return default\n        elif fatal:\n            name = '%s[@%s]' % (xpath, key) if name is None else name\n            raise ExtractorError('Could not find XML attribute %s' % name)\n        else:\n            return None\n    return n.attrib[key]", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "get_elements_by_class", "parameters": ["class_name", "html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_elements_by_attribute", "re.escape"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return the content of all tags with the specified class in the passed HTML document as a list", "source_code": "def get_elements_by_class(class_name, html):\n    \"\"\"Return the content of all tags with the specified class in the passed HTML document as a list\"\"\"\n    return get_elements_by_attribute(\n        'class', r'[^\\'\"]*\\b%s\\b[^\\'\"]*' % re.escape(class_name),\n        html, escape_value=False)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "get_elements_by_attribute", "parameters": ["attribute", "value", "html", "escape_value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.group", "re.escape", "re.finditer", "res.startswith", "retlist.append", "unescapeHTML"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return the content of the tag with the specified attribute in the passed HTML document", "source_code": "def get_elements_by_attribute(attribute, value, html, escape_value=True):\n    \"\"\"Return the content of the tag with the specified attribute in the passed HTML document\"\"\"\n\n    value = re.escape(value) if escape_value else value\n\n    retlist = []\n    for m in re.finditer(r'''(?xs)\n        <([a-zA-Z0-9:._-]+)\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n         \\s+%s=['\"]?%s['\"]?\n         (?:\\s+[a-zA-Z0-9:._-]+(?:=[a-zA-Z0-9:._-]*|=\"[^\"]*\"|='[^']*'|))*?\n        \\s*>\n        (?P<content>.*?)\n        </\\1>\n    ''' % (re.escape(attribute), value), html):\n        res = m.group('content')\n\n        if res.startswith('\"') or res.startswith(\"'\"):\n            res = res[1:-1]\n\n        retlist.append(unescapeHTML(res))\n\n    return retlist", "loc": 23}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "extract_attributes", "parameters": ["html_element"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HTMLAttributeParser", "compat_contextlib_suppress", "contextlib.closing", "parser.feed"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Given a string for an HTML element such as <el a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz", "source_code": "def extract_attributes(html_element):\n    \"\"\"Given a string for an HTML element such as\n    <el\n         a=\"foo\" B=\"bar\" c=\"&98;az\" d=boz\n         empty= noval entity=\"&amp;\"\n         sq='\"' dq=\"'\"\n    >\n    Decode and return a dictionary of attributes.\n    {\n        'a': 'foo', 'b': 'bar', c: 'baz', d: 'boz',\n        'empty': '', 'noval': None, 'entity': '&',\n        'sq': '\"', 'dq': '\\''\n    }.\n    NB HTMLParser is stricter in Python 2.6 & 3.2 than in later versions,\n    but the cases in the unit test will work for all of 2.6, 2.7, 3.2-3.5.\n    \"\"\"\n    ret = None\n    # Older Python may throw HTMLParseError in case of malformed HTML (and on .close()!)\n    with compat_contextlib_suppress(compat_HTMLParseError):\n        with contextlib.closing(HTMLAttributeParser()) as parser:\n            parser.feed(html_element)\n            ret = parser.attrs\n    return ret or {}", "loc": 23}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "clean_html", "parameters": ["html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["html.replace", "html.strip", "re.sub", "unescapeHTML"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Clean an HTML snippet into a readable string", "source_code": "def clean_html(html):\n    \"\"\"Clean an HTML snippet into a readable string\"\"\"\n\n    if html is None:  # Convenience for sanitizing descriptions etc.\n        return html\n\n    # Newline vs <br />\n    html = html.replace('\\n', ' ')\n    html = re.sub(r'(?u)\\s*<\\s*br\\s*/?\\s*>\\s*', '\\n', html)\n    html = re.sub(r'(?u)<\\s*/\\s*p\\s*>\\s*<\\s*p[^>]*>', '\\n', html)\n    # Strip html tags\n    html = re.sub('<.*?>', '', html)\n    # Replace html entities\n    html = unescapeHTML(html)\n    return html.strip()", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "timeconvert", "parameters": ["timestr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["email.utils.mktime_tz", "email.utils.parsedate_tz"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert RFC 2822 defined time string into system timestamp", "source_code": "def timeconvert(timestr):\n    \"\"\"Convert RFC 2822 defined time string into system timestamp\"\"\"\n    timestamp = None\n    timetuple = email.utils.parsedate_tz(timestr)\n    if timetuple is not None:\n        timestamp = email.utils.mktime_tz(timetuple)\n    return timestamp", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "sanitize_filename", "parameters": ["s", "restricted", "is_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "char.isspace", "len", "m.group", "m.group(0).replace", "map", "ord", "re.sub", "result.lstrip", "result.replace", "result.startswith", "result.strip", "unicodedata.category", "unicodedata.normalize"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "Sanitizes a string so it could be used as part of a filename. If restricted is set, use a stricter subset of allowed characters. Set is_id if this is not an arbitrary string, but an ID that should be kept", "source_code": "def sanitize_filename(s, restricted=False, is_id=False):\n    \"\"\"Sanitizes a string so it could be used as part of a filename.\n    If restricted is set, use a stricter subset of allowed characters.\n    Set is_id if this is not an arbitrary string, but an ID that should be kept\n    if possible.\n    \"\"\"\n    def replace_insane(char):\n        if restricted and char in ACCENT_CHARS:\n            return ACCENT_CHARS[char]\n        if char == '?' or ord(char) < 32 or ord(char) == 127:\n            return ''\n        elif char == '\"':\n            return '' if restricted else '\\''\n        elif char == ':':\n            return '_-' if restricted else ' -'\n        elif char in '\\\\/|*<>':\n            return '_'\n        if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n            return '_'\n        if restricted and ord(char) > 127:\n            return '' if unicodedata.category(char)[0] in 'CM' else '_'\n\n        return char\n\n    # Replace look-alike Unicode glyphs\n    if restricted and not is_id:\n        s = unicodedata.normalize('NFKC', s)\n    # Handle timestamps\n    s = re.sub(r'[0-9]+(?::[0-9]+)+', lambda m: m.group(0).replace(':', '_'), s)\n    result = ''.join(map(replace_insane, s))\n    if not is_id:\n        while '__' in result:\n            result = result.replace('__', '_')\n        result = result.strip('_')\n        # Common case of \"Foreign band name - English song title\"\n        if restricted and result.startswith('-_'):\n            result = result[2:]\n        if result.startswith('-'):\n            result = '_' + result[len('-'):]\n        result = result.lstrip('.')\n        if not result:\n            result = '_'\n    return result", "loc": 43}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "sanitize_url", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.match", "re.sub", "url.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sanitize_url(url):\n    # Prepend protocol-less URLs with `http:` scheme in order to mitigate\n    # the number of unwanted failures due to missing protocol\n    if url.startswith('//'):\n        return 'http:%s' % url\n    # Fix some common typos seen so far\n    COMMON_TYPOS = (\n        # https://github.com/ytdl-org/youtube-dl/issues/15649\n        (r'^httpss://', r'https://'),\n        # https://bx1.be/lives/direct-tv/\n        (r'^rmtp([es]?)://', r'rtmp\\1://'),\n    )\n    for mistake, fixup in COMMON_TYPOS:\n        if re.match(mistake, url):\n            return re.sub(mistake, fixup, url)\n    return url", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "orderedSet", "parameters": ["iterable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["res.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Remove all duplicates from the input iterable", "source_code": "def orderedSet(iterable):\n    \"\"\" Remove all duplicates from the input iterable \"\"\"\n    res = []\n    for el in iterable:\n        if el not in res:\n            res.append(el)\n    return res", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "unescapeHTML", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_htmlentity_transform", "isinstance", "m.group", "re.sub"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unescapeHTML(s):\n    if s is None:\n        return None\n    assert isinstance(s, compat_str)\n\n    return re.sub(\n        r'&([^&;]+;)', lambda m: _htmlentity_transform(m.group(1)), s)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "process_communicate_or_kill", "parameters": ["p"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["p.communicate", "p.kill", "p.wait"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_communicate_or_kill(p, *args, **kwargs):\n    try:\n        return p.communicate(*args, **kwargs)\n    except BaseException:  # Including KeyboardInterrupt\n        p.kill()\n        p.wait()\n        raise", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "get_subprocess_encoding", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["preferredencoding", "sys.getfilesystemencoding", "sys.getwindowsversion"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_subprocess_encoding():\n    if sys.platform == 'win32' and sys.getwindowsversion()[0] >= 5:\n        # For subprocess calls, encode with locale encoding\n        # Refer to http://stackoverflow.com/a/9951851/35070\n        encoding = preferredencoding()\n    else:\n        encoding = sys.getfilesystemencoding()\n    if encoding is None:\n        encoding = 'utf-8'\n    return encoding", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "encodeArgument", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encodeFilename", "isinstance", "s.decode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encodeArgument(s):\n    if not isinstance(s, compat_str):\n        # Legacy code that uses byte strings\n        # Uncomment the following line after fixing all post processors\n        # assert False, 'Internal error: %r should be of type %r, is %r' % (s, compat_str, type(s))\n        s = s.decode('ascii')\n    return encodeFilename(s, True)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "formatSeconds", "parameters": ["secs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def formatSeconds(secs):\n    if secs > 3600:\n        return '%d:%02d:%02d' % (secs // 3600, (secs % 3600) // 60, secs % 60)\n    elif secs > 60:\n        return '%d:%02d' % (secs // 60, secs % 60)\n    else:\n        return '%d' % secs", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "make_HTTPS_handler", "parameters": ["params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["YoutubeDLHTTPSHandler", "compat_contextlib_suppress", "context.set_default_verify_paths", "ctx.set_alpn_protocols", "hasattr", "params.get", "set_alpn_protocols", "ssl.SSLContext", "ssl.create_default_context"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_HTTPS_handler(params, **kwargs):\n\n    # https://www.rfc-editor.org/info/rfc7301\n    ALPN_PROTOCOLS = ['http/1.1']\n\n    def set_alpn_protocols(ctx):\n        # From https://github.com/yt-dlp/yt-dlp/commit/2c6dcb65fb612fc5bc5c61937bf438d3c473d8d0\n        # Thanks @coletdjnz\n        # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n        # https://github.com/python/cpython/issues/85140\n        # https://github.com/yt-dlp/yt-dlp/issues/3878\n        with compat_contextlib_suppress(AttributeError, NotImplementedError):\n            # fails for Python < 2.7.10, not ssl.HAS_ALPN\n            ctx.set_alpn_protocols(ALPN_PROTOCOLS)\n\n    opts_no_check_certificate = params.get('nocheckcertificate', False)\n    if hasattr(ssl, 'create_default_context'):  # Python >= 3.4 or 2.7.9\n        context = ssl.create_default_context(ssl.Purpose.SERVER_AUTH)\n        set_alpn_protocols(context)\n        if opts_no_check_certificate:\n            context.check_hostname = False\n            context.verify_mode = ssl.CERT_NONE\n\n        with compat_contextlib_suppress(TypeError):\n            # Fails with Python 2.7.8 (create_default_context present\n            # but HTTPSHandler has no context=)\n            return YoutubeDLHTTPSHandler(params, context=context, **kwargs)\n\n    if sys.version_info < (3, 2):\n        return YoutubeDLHTTPSHandler(params, **kwargs)\n    else:  # Python3 < 3.4\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1)\n        context.verify_mode = (ssl.CERT_NONE\n                               if opts_no_check_certificate\n                               else ssl.CERT_REQUIRED)\n        context.set_default_verify_paths()\n        set_alpn_protocols(context)\n        return YoutubeDLHTTPSHandler(params, context=context, **kwargs)", "loc": 38}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "bug_reports_message", "parameters": ["before"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["before or ''.rstrip", "before.endswith", "msg[0].title", "ytdl_is_updateable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def bug_reports_message(before=';'):\n    if ytdl_is_updateable():\n        update_cmd = 'type  youtube-dl -U  to update'\n    else:\n        update_cmd = 'see  https://github.com/ytdl-org/youtube-dl/#user-content-installation  on how to update'\n\n    msg = (\n        'please report this issue on https://github.com/ytdl-org/youtube-dl/issues ,'\n        ' using the appropriate issue template.'\n        ' Make sure you are using the latest version; %s.'\n        ' Be sure to call youtube-dl with the --verbose option and include the complete output.'\n    ) % update_cmd\n\n    before = (before or '').rstrip()\n    if not before or before.endswith(('.', '!', '?')):\n        msg = msg[0].title() + msg[1:]\n\n    return (before + ' ' if before else '') + msg", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "handle_youtubedl_headers", "parameters": ["headers"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter_dict", "k.lower"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_youtubedl_headers(headers):\n    filtered_headers = headers\n\n    if 'Youtubedl-no-compression' in filtered_headers:\n        filtered_headers = filter_dict(filtered_headers, cndn=lambda k, _: k.lower() != 'accept-encoding')\n        del filtered_headers['Youtubedl-no-compression']\n\n    return filtered_headers", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "extract_timezone", "parameters": ["date_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TIMEZONE_NAMES.get", "datetime.timedelta", "int", "len", "m.group", "m.group('tz').strip", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_timezone(date_str):\n    m = re.search(\n        r'''(?x)\n            ^.{8,}?                                              # >=8 char non-TZ prefix, if present\n            (?P<tz>Z|                                            # just the UTC Z, or\n                (?:(?<=.\\b\\d{4}|\\b\\d{2}:\\d\\d)|                   # preceded by 4 digits or hh:mm or\n                   (?<!.\\b[a-zA-Z]{3}|[a-zA-Z]{4}|..\\b\\d\\d))     # not preceded by 3 alpha word or >= 4 alpha or 2 digits\n                   [ ]?                                          # optional space\n                (?P<sign>\\+|-)                                   # +/-\n                (?P<hours>[0-9]{2}):?(?P<minutes>[0-9]{2})       # hh[:]mm\n            $)\n        ''', date_str)\n    if not m:\n        m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n        timezone = TIMEZONE_NAMES.get(m and m.group('tz').strip())\n        if timezone is not None:\n            date_str = date_str[:-len(m.group('tz'))]\n        timezone = datetime.timedelta(hours=timezone or 0)\n    else:\n        date_str = date_str[:-len(m.group('tz'))]\n        if not m.group('sign'):\n            timezone = datetime.timedelta()\n        else:\n            sign = 1 if m.group('sign') == '+' else -1\n            timezone = datetime.timedelta(\n                hours=sign * int(m.group('hours')),\n                minutes=sign * int(m.group('minutes')))\n    return timezone, date_str", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_iso8601", "parameters": ["date_str", "delimiter", "timezone"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'%Y-%m-%d{0}%H:%M:%S'.format", "calendar.timegm", "compat_contextlib_suppress", "datetime.datetime.strptime", "dt.timetuple", "extract_timezone", "re.sub"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a UNIX timestamp from the given date", "source_code": "def parse_iso8601(date_str, delimiter='T', timezone=None):\n    \"\"\" Return a UNIX timestamp from the given date \"\"\"\n\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\.[0-9]+', '', date_str)\n\n    if timezone is None:\n        timezone, date_str = extract_timezone(date_str)\n\n    with compat_contextlib_suppress(ValueError):\n        date_format = '%Y-%m-%d{0}%H:%M:%S'.format(delimiter)\n        dt = datetime.datetime.strptime(date_str, date_format) - timezone\n        return calendar.timegm(dt.timetuple())", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "unified_strdate", "parameters": ["date_str", "day_first"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_contextlib_suppress", "compat_str", "date_formats", "date_str.replace", "datetime.datetime", "datetime.datetime(*timetuple[:6]).strftime", "datetime.datetime.strptime", "datetime.datetime.strptime(date_str, expression).strftime", "email.utils.parsedate_tz", "extract_timezone", "re.sub"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a string with the date in the format YYYYMMDD", "source_code": "def unified_strdate(date_str, day_first=True):\n    \"\"\"Return a string with the date in the format YYYYMMDD\"\"\"\n\n    if date_str is None:\n        return None\n    upload_date = None\n    # Replace commas\n    date_str = date_str.replace(',', ' ')\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n    _, date_str = extract_timezone(date_str)\n\n    for expression in date_formats(day_first):\n        with compat_contextlib_suppress(ValueError):\n            upload_date = datetime.datetime.strptime(date_str, expression).strftime('%Y%m%d')\n    if upload_date is None:\n        timetuple = email.utils.parsedate_tz(date_str)\n        if timetuple:\n            with compat_contextlib_suppress(ValueError):\n                upload_date = datetime.datetime(*timetuple[:6]).strftime('%Y%m%d')\n    if upload_date is not None:\n        return compat_str(upload_date)", "loc": 22}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "unified_timestamp", "parameters": ["date_str", "day_first"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["calendar.timegm", "compat_contextlib_suppress", "compat_datetime_timedelta_total_seconds", "date_formats", "datetime.datetime.strptime", "datetime.timedelta", "dt.timetuple", "email.utils.parsedate_tz", "extract_timezone", "len", "m.group", "re.search", "re.sub"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def unified_timestamp(date_str, day_first=True):\n    if date_str is None:\n        return None\n\n    date_str = re.sub(r'\\s+', ' ', re.sub(\n        r'(?i)[,|]|(mon|tues?|wed(nes)?|thu(rs)?|fri|sat(ur)?)(day)?', '', date_str))\n\n    pm_delta = 12 if re.search(r'(?i)PM', date_str) else 0\n    timezone, date_str = extract_timezone(date_str)\n\n    # Remove AM/PM + timezone\n    date_str = re.sub(r'(?i)\\s*(?:AM|PM)(?:\\s+[A-Z]+)?', '', date_str)\n\n    # Remove unrecognized timezones from ISO 8601 alike timestamps\n    m = re.search(r'\\d{1,2}:\\d{1,2}(?:\\.\\d+)?(?P<tz>\\s*[A-Z]+)$', date_str)\n    if m:\n        date_str = date_str[:-len(m.group('tz'))]\n\n    # Python only supports microseconds, so remove nanoseconds\n    m = re.search(r'^([0-9]{4,}-[0-9]{1,2}-[0-9]{1,2}T[0-9]{1,2}:[0-9]{1,2}:[0-9]{1,2}\\.[0-9]{6})[0-9]+$', date_str)\n    if m:\n        date_str = m.group(1)\n\n    for expression in date_formats(day_first):\n        with compat_contextlib_suppress(ValueError):\n            dt = datetime.datetime.strptime(date_str, expression) - timezone + datetime.timedelta(hours=pm_delta)\n            return calendar.timegm(dt.timetuple())\n    timetuple = email.utils.parsedate_tz(date_str)\n    if timetuple:\n        return calendar.timegm(timetuple) + pm_delta * 3600 - compat_datetime_timedelta_total_seconds(timezone)", "loc": 30}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "determine_ext", "parameters": ["url", "default_ext"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["guess.rstrip", "re.match", "url.partition", "url.partition('?')[0].rpartition"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def determine_ext(url, default_ext='unknown_video'):\n    if url is None or '.' not in url:\n        return default_ext\n    guess = url.partition('?')[0].rpartition('.')[2]\n    if re.match(r'^[A-Za-z0-9]+$', guess):\n        return guess\n    # Try extract ext from URLs like http://example.com/foo/bar.mp4/?download\n    elif guess.rstrip('/') in KNOWN_EXTENSIONS:\n        return guess.rstrip('/')\n    else:\n        return default_ext", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "date_from_str", "parameters": ["date_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.date.today", "datetime.datetime.strptime", "datetime.datetime.strptime(date_str, '%Y%m%d').date", "datetime.timedelta", "int", "match.group", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Return a datetime object from a string in the format YYYYMMDD or (now|today)[+-][0-9](day|week|month|year)(s)?", "source_code": "def date_from_str(date_str):\n    \"\"\"\n    Return a datetime object from a string in the format YYYYMMDD or\n    (now|today)[+-][0-9](day|week|month|year)(s)?\"\"\"\n    today = datetime.date.today()\n    if date_str in ('now', 'today'):\n        return today\n    if date_str == 'yesterday':\n        return today - datetime.timedelta(days=1)\n    match = re.match(r'(now|today)(?P<sign>[+-])(?P<time>\\d+)(?P<unit>day|week|month|year)(s)?', date_str)\n    if match is not None:\n        sign = match.group('sign')\n        time = int(match.group('time'))\n        if sign == '-':\n            time = -time\n        unit = match.group('unit')\n        # A bad approximation?\n        if unit == 'month':\n            unit = 'day'\n            time *= 30\n        elif unit == 'year':\n            unit = 'day'\n            time *= 365\n        unit += 's'\n        delta = datetime.timedelta(**{unit: time})\n        return today + delta\n    return datetime.datetime.strptime(date_str, '%Y%m%d').date()", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "hyphenate_date", "parameters": ["date_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'-'.join", "match.groups", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format", "source_code": "def hyphenate_date(date_str):\n    \"\"\"\n    Convert a date in 'YYYYMMDD' format to 'YYYY-MM-DD' format\"\"\"\n    match = re.match(r'^(\\d\\d\\d\\d)(\\d\\d)(\\d\\d)$', date_str)\n    if match is not None:\n        return '-'.join(match.groups())\n    else:\n        return date_str", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "write_string", "parameters": ["s", "out", "encoding"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_windows_write_string", "getattr", "hasattr", "isinstance", "out.buffer.write", "out.flush", "out.write", "preferredencoding", "s.encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def write_string(s, out=None, encoding=None):\n    if out is None:\n        out = sys.stderr\n    assert isinstance(s, compat_str)\n\n    if sys.platform == 'win32' and encoding is None and hasattr(out, 'fileno'):\n        if _windows_write_string(s, out):\n            return\n\n    if ('b' in getattr(out, 'mode', '')\n            or sys.version_info[0] < 3):  # Python 2 lies about mode of sys.stderr\n        byt = s.encode(encoding or preferredencoding(), 'ignore')\n        out.write(byt)\n    elif hasattr(out, 'buffer'):\n        enc = encoding or getattr(out, 'encoding', None) or preferredencoding()\n        byt = s.encode(enc, 'ignore')\n        out.buffer.write(byt)\n    else:\n        out.write(s)\n    out.flush()", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "bytes_to_intlist", "parameters": ["bs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "list", "ord"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def bytes_to_intlist(bs):\n    if not bs:\n        return []\n    if isinstance(bs[0], int):  # Python 3\n        return list(bs)\n    else:\n        return [ord(c) for c in bs]", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "shell_quote", "parameters": ["args"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "_decode_compat_str", "compat_shlex_quote", "get_filesystem_encoding", "quoted_args.append"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def shell_quote(args):\n    quoted_args = []\n    encoding = get_filesystem_encoding()\n    for a in args:\n        # We may get a filename encoded with 'encodeFilename'\n        a = _decode_compat_str(a, encoding)\n        quoted_args.append(compat_shlex_quote(a))\n    return ' '.join(quoted_args)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "smuggle_url", "parameters": ["url", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_urllib_parse_urlencode", "data.update", "json.dumps", "unsmuggle_url"], "control_structures": [], "behavior_type": ["network_io", "serialization"], "doc_summary": "Pass additional data in a URL for internal use.", "source_code": "def smuggle_url(url, data):\n    \"\"\" Pass additional data in a URL for internal use. \"\"\"\n\n    url, idata = unsmuggle_url(url, {})\n    data.update(idata)\n    sdata = compat_urllib_parse_urlencode(\n        {'__youtubedl_smuggle': json.dumps(data)})\n    return url + '#' + sdata", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "unsmuggle_url", "parameters": ["smug_url", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_parse_qs", "json.loads", "smug_url.rpartition"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def unsmuggle_url(smug_url, default=None):\n    if '#__youtubedl_smuggle' not in smug_url:\n        return smug_url, default\n    url, _, sdata = smug_url.rpartition('#')\n    jsond = compat_parse_qs(sdata)['__youtubedl_smuggle'][0]\n    data = json.loads(jsond)\n    return url, data", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "format_bytes", "parameters": ["bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "int", "math.log", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_bytes(bytes):\n    if bytes is None:\n        return 'N/A'\n    if type(bytes) is str:\n        bytes = float(bytes)\n    if bytes == 0.0:\n        exponent = 0\n    else:\n        exponent = int(math.log(bytes, 1024.0))\n    suffix = ['B', 'KiB', 'MiB', 'GiB', 'TiB', 'PiB', 'EiB', 'ZiB', 'YiB'][exponent]\n    converted = float(bytes) / float(1024 ** exponent)\n    return '%.2f%s' % (converted, suffix)", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "lookup_unit_table", "parameters": ["unit_table", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'|'.join", "float", "int", "m.group", "m.group('num').replace", "re.escape", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def lookup_unit_table(unit_table, s):\n    units_re = '|'.join(re.escape(u) for u in unit_table)\n    m = re.match(\n        r'(?P<num>[0-9]+(?:[,.][0-9]*)?)\\s*(?P<unit>%s)\\b' % units_re, s)\n    if not m:\n        return None\n    num_str = m.group('num').replace(',', '.')\n    mult = unit_table[m.group('unit')]\n    return int(float(num_str) * mult)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_filesize", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lookup_unit_table"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_filesize(s):\n    if s is None:\n        return None\n\n    # The lower-case forms are of course incorrect and unofficial,\n    # but we support those too\n    _UNIT_TABLE = {\n        'B': 1,\n        'b': 1,\n        'bytes': 1,\n        'KiB': 1024,\n        'KB': 1000,\n        'kB': 1024,\n        'Kb': 1000,\n        'kb': 1000,\n        'kilobytes': 1000,\n        'kibibytes': 1024,\n        'MiB': 1024 ** 2,\n        'MB': 1000 ** 2,\n        'mB': 1024 ** 2,\n        'Mb': 1000 ** 2,\n        'mb': 1000 ** 2,\n        'megabytes': 1000 ** 2,\n        'mebibytes': 1024 ** 2,\n        'GiB': 1024 ** 3,\n        'GB': 1000 ** 3,\n        'gB': 1024 ** 3,\n        'Gb': 1000 ** 3,\n        'gb': 1000 ** 3,\n        'gigabytes': 1000 ** 3,\n        'gibibytes': 1024 ** 3,\n        'TiB': 1024 ** 4,\n        'TB': 1000 ** 4,\n        'tB': 1024 ** 4,\n        'Tb': 1000 ** 4,\n        'tb': 1000 ** 4,\n        'terabytes': 1000 ** 4,\n        'tebibytes': 1024 ** 4,\n        'PiB': 1024 ** 5,\n        'PB': 1000 ** 5,\n        'pB': 1024 ** 5,\n        'Pb': 1000 ** 5,\n        'pb': 1000 ** 5,\n        'petabytes': 1000 ** 5,\n        'pebibytes': 1024 ** 5,\n        'EiB': 1024 ** 6,\n        'EB': 1000 ** 6,\n        'eB': 1024 ** 6,\n        'Eb': 1000 ** 6,\n        'eb': 1000 ** 6,\n        'exabytes': 1000 ** 6,\n        'exbibytes': 1024 ** 6,\n        'ZiB': 1024 ** 7,\n        'ZB': 1000 ** 7,\n        'zB': 1024 ** 7,\n        'Zb': 1000 ** 7,\n        'zb': 1000 ** 7,\n        'zettabytes': 1000 ** 7,\n        'zebibytes': 1024 ** 7,\n        'YiB': 1024 ** 8,\n        'YB': 1000 ** 8,\n        'yB': 1024 ** 8,\n        'Yb': 1000 ** 8,\n        'yb': 1000 ** 8,\n        'yottabytes': 1000 ** 8,\n        'yobibytes': 1024 ** 8,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)", "loc": 69}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_count", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lookup_unit_table", "re.match", "s.strip", "str_to_int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_count(s):\n    if s is None:\n        return None\n\n    s = s.strip()\n\n    if re.match(r'^[\\d,.]+$', s):\n        return str_to_int(s)\n\n    _UNIT_TABLE = {\n        'k': 1000,\n        'K': 1000,\n        'm': 1000 ** 2,\n        'M': 1000 ** 2,\n        'kk': 1000 ** 2,\n        'KK': 1000 ** 2,\n    }\n\n    return lookup_unit_table(_UNIT_TABLE, s)", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_resolution", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "mobj.group", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_resolution(s):\n    if s is None:\n        return {}\n\n    mobj = re.search(r'\\b(?P<w>\\d+)\\s*[xX×]\\s*(?P<h>\\d+)\\b', s)\n    if mobj:\n        return {\n            'width': int(mobj.group('w')),\n            'height': int(mobj.group('h')),\n        }\n\n    mobj = re.search(r'\\b(\\d+)[pPiI]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1))}\n\n    mobj = re.search(r'\\b([48])[kK]\\b', s)\n    if mobj:\n        return {'height': int(mobj.group(1)) * 540}\n\n    return {}", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_bitrate", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "mobj.group", "re.search", "txt_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_bitrate(s):\n    s = txt_or_none(s)\n    if not s:\n        return None\n    mobj = re.search(r'\\b(\\d+)\\s*kbps', s)\n    if mobj:\n        return int(mobj.group(1))", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "month_by_name", "parameters": ["name", "lang"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["MONTH_NAMES.get", "month_names.index"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return the number of a month by (locale-independently) English name", "source_code": "def month_by_name(name, lang='en'):\n    \"\"\" Return the number of a month by (locale-independently) English name \"\"\"\n\n    month_names = MONTH_NAMES.get(lang, MONTH_NAMES['en'])\n\n    try:\n        return month_names.index(name) + 1\n    except ValueError:\n        return None", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "month_by_abbreviation", "parameters": ["abbrev"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["[s[:3] for s in ENGLISH_MONTH_NAMES].index"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Return the number of a month by (locale-independently) English abbreviations", "source_code": "def month_by_abbreviation(abbrev):\n    \"\"\" Return the number of a month by (locale-independently) English\n        abbreviations \"\"\"\n\n    try:\n        return [s[:3] for s in ENGLISH_MONTH_NAMES].index(abbrev) + 1\n    except ValueError:\n        return None", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "setproctitle", "parameters": ["title"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ctypes.cdll.LoadLibrary", "ctypes.create_string_buffer", "isinstance", "len", "libc.prctl", "sys.platform.startswith", "title.encode"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def setproctitle(title):\n    assert isinstance(title, compat_str)\n\n    # ctypes in Jython is not complete\n    # http://bugs.jython.org/issue2148\n    if sys.platform.startswith('java'):\n        return\n\n    try:\n        libc = ctypes.cdll.LoadLibrary('libc.so.6')\n    except OSError:\n        return\n    except TypeError:\n        # LoadLibrary in Windows Python 2.7.13 only expects\n        # a bytestring, but since unicode_literals turns\n        # every string into a unicode string, it fails.\n        return\n    title_bytes = title.encode('utf-8')\n    buf = ctypes.create_string_buffer(len(title_bytes))\n    buf.value = title_bytes\n    try:\n        libc.prctl(15, buf, 0, 0, 0)\n    except AttributeError:\n        return  # Strange libc, just skip this", "loc": 24}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "remove_quotes", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def remove_quotes(s):\n    if s is None or len(s) < 2:\n        return s\n    for quote in ('\"', \"'\", ):\n        if s[0] == quote and s[-1] == quote:\n            return s[1:-1]\n    return s", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "int_or_none", "parameters": ["v", "scale", "default", "get_attr", "invscale", "base"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "int"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def int_or_none(v, scale=1, default=None, get_attr=None, invscale=1, base=None):\n    if get_attr:\n        if v is not None:\n            v = getattr(v, get_attr, None)\n    if v in (None, ''):\n        return default\n    try:\n        # like int, raise if base is specified and v is not a string\n        return (int(v) if base is None else int(v, base=base)) * invscale // scale\n    except (ValueError, TypeError, OverflowError):\n        return default", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "str_to_int", "parameters": ["int_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "isinstance", "re.sub"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "A more relaxed version of int_or_none", "source_code": "def str_to_int(int_str):\n    \"\"\" A more relaxed version of int_or_none \"\"\"\n    if isinstance(int_str, compat_integer_types):\n        return int_str\n    elif isinstance(int_str, compat_str):\n        int_str = re.sub(r'[,\\.\\+]', '', int_str)\n        return int_or_none(int_str)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "float_or_none", "parameters": ["v", "scale", "invscale", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def float_or_none(v, scale=1, invscale=1, default=None):\n    if v is None:\n        return default\n    try:\n        return float(v) * invscale / scale\n    except (ValueError, TypeError):\n        return default", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "url_or_none", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "re.match", "url.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def url_or_none(url):\n    if not url or not isinstance(url, compat_str):\n        return None\n    url = url.strip()\n    return url if re.match(r'^(?:(?:https?|rt(?:m(?:pt?[es]?|fp)|sp[su]?)|mms|ftps?):)?//', url) else None", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_duration", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "isinstance", "m.groups", "re.match", "s.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_duration(s):\n    if not isinstance(s, compat_basestring):\n        return None\n\n    s = s.strip()\n\n    days, hours, mins, secs, ms = [None] * 5\n    m = re.match(r'(?:(?:(?:(?P<days>[0-9]+):)?(?P<hours>[0-9]+):)?(?P<mins>[0-9]+):)?(?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?Z?$', s)\n    if m:\n        days, hours, mins, secs, ms = m.groups()\n    else:\n        m = re.match(\n            r'''(?ix)(?:P?\n                (?:\n                    [0-9]+\\s*y(?:ears?)?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*m(?:onths?)?\\s*\n                )?\n                (?:\n                    [0-9]+\\s*w(?:eeks?)?\\s*\n                )?\n                (?:\n                    (?P<days>[0-9]+)\\s*d(?:ays?)?\\s*\n                )?\n                T)?\n                (?:\n                    (?P<hours>[0-9]+)\\s*h(?:ours?)?\\s*\n                )?\n                (?:\n                    (?P<mins>[0-9]+)\\s*m(?:in(?:ute)?s?)?\\s*\n                )?\n                (?:\n                    (?P<secs>[0-9]+)(?P<ms>\\.[0-9]+)?\\s*s(?:ec(?:ond)?s?)?\\s*\n                )?Z?$''', s)\n        if m:\n            days, hours, mins, secs, ms = m.groups()\n        else:\n            m = re.match(r'(?i)(?:(?P<hours>[0-9.]+)\\s*(?:hours?)|(?P<mins>[0-9.]+)\\s*(?:mins?\\.?|minutes?)\\s*)Z?$', s)\n            if m:\n                hours, mins = m.groups()\n            else:\n                return None\n\n    duration = 0\n    if secs:\n        duration += float(secs)\n    if mins:\n        duration += float(mins) * 60\n    if hours:\n        duration += float(hours) * 60 * 60\n    if days:\n        duration += float(days) * 24 * 60 * 60\n    if ms:\n        duration += float(ms)\n    return duration", "loc": 56}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "detect_exe_version", "parameters": ["output", "version_re", "unrecognized"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "m.group", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def detect_exe_version(output, version_re=None, unrecognized='present'):\n    assert isinstance(output, compat_str)\n    if version_re is None:\n        version_re = r'version\\s+([-0-9._a-zA-Z]+)'\n    m = re.search(version_re, output)\n    if m:\n        return m.group(1)\n    else:\n        return unrecognized", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "uppercase_escape", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["codecs.getdecoder", "m.group", "re.sub", "unicode_escape"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def uppercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\U[0-9a-fA-F]{8}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "lowercase_escape", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["codecs.getdecoder", "m.group", "re.sub", "unicode_escape"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def lowercase_escape(s):\n    unicode_escape = codecs.getdecoder('unicode_escape')\n    return re.sub(\n        r'\\\\u[0-9a-fA-F]{4}',\n        lambda m: unicode_escape(m.group(0))[0],\n        s)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "read_batch_urls", "parameters": ["batch_fd"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_decode_compat_str", "contextlib.closing", "len", "map", "url.startswith", "url.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_batch_urls(batch_fd):\n    def fixup(url):\n        url = _decode_compat_str(url, 'utf-8', 'replace')\n        BOM_UTF8 = '\\xef\\xbb\\xbf'\n        if url.startswith(BOM_UTF8):\n            url = url[len(BOM_UTF8):]\n        url = url.strip()\n        if url.startswith(('#', ';', ']')):\n            return False\n        return url\n\n    with contextlib.closing(batch_fd) as fd:\n        return [url for url in map(fixup, fd) if url]", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "update_Request", "parameters": ["req", "url", "data", "headers", "query"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "req.get_full_url", "req.get_method", "req.headers.copy", "req_headers.update", "req_type", "update_url_query", "{'HEAD': HEADRequest, 'PUT': PUTRequest}.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def update_Request(req, url=None, data=None, headers={}, query={}):\n    req_headers = req.headers.copy()\n    req_headers.update(headers)\n    req_data = data if data is not None else req.data\n    req_url = update_url_query(url or req.get_full_url(), query)\n    req_type = {'HEAD': HEADRequest, 'PUT': PUTRequest}.get(\n        req.get_method(), compat_urllib_request.Request)\n    new_req = req_type(\n        req_url, data=req_data, headers=req_headers,\n        origin_req_host=req.origin_req_host, unverifiable=req.unverifiable)\n    if hasattr(req, 'timeout'):\n        new_req.timeout = req.timeout\n    return new_req", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "multipart_encode", "parameters": ["data", "boundary"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_multipart_encode_impl", "random.randrange", "str"], "control_structures": ["If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "Encode a dict to RFC 7578-compliant form-data data: A dict where keys and values can be either Unicode or bytes-like", "source_code": "def multipart_encode(data, boundary=None):\n    '''\n    Encode a dict to RFC 7578-compliant form-data\n\n    data:\n        A dict where keys and values can be either Unicode or bytes-like\n        objects.\n    boundary:\n        If specified a Unicode object, it's used as the boundary. Otherwise\n        a random boundary is generated.\n\n    Reference: https://tools.ietf.org/html/rfc7578\n    '''\n    has_specified_boundary = boundary is not None\n\n    while True:\n        if boundary is None:\n            boundary = '---------------' + str(random.randrange(0x0fffffff, 0xffffffff))\n\n        try:\n            out, content_type = _multipart_encode_impl(data, boundary)\n            break\n        except ValueError:\n            if has_specified_boundary:\n                raise\n            boundary = None\n\n    return out, content_type", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "try_call", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f", "isinstance", "kwargs.get"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def try_call(*funcs, **kwargs):\n\n    # parameter defaults\n    expected_type = kwargs.get('expected_type')\n    fargs = kwargs.get('args', [])\n    fkwargs = kwargs.get('kwargs', {})\n\n    for f in funcs:\n        try:\n            val = f(*fargs, **fkwargs)\n        except (AttributeError, KeyError, TypeError, IndexError, ZeroDivisionError):\n            pass\n        else:\n            if expected_type is None or isinstance(val, expected_type):\n                return val", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "try_get", "parameters": ["src", "getter", "expected_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get", "isinstance"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def try_get(src, getter, expected_type=None):\n    if not isinstance(getter, (list, tuple)):\n        getter = [getter]\n    for get in getter:\n        try:\n            v = get(src)\n        except (AttributeError, KeyError, TypeError, IndexError):\n            pass\n        else:\n            if expected_type is None or isinstance(v, expected_type):\n                return v", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "merge_dicts", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["a_dict.items", "can_merge_str", "isinstance", "kwargs.get", "reversed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Merge the `dict`s in `dicts` using the first valid value for each key. Normally valid: not None and not an empty string Keyword-only args:", "source_code": "def merge_dicts(*dicts, **kwargs):\n    \"\"\"\n        Merge the `dict`s in `dicts` using the first valid value for each key.\n        Normally valid: not None and not an empty string\n\n        Keyword-only args:\n        unblank:    allow empty string if False (default True)\n        rev:        merge dicts in reverse order (default False)\n\n        merge_dicts(dct1, dct2, ..., unblank=False, rev=True)\n        matches {**dct1, **dct2, ...}\n\n        However, merge_dicts(dct1, dct2, ..., rev=True) may often be better.\n    \"\"\"\n\n    unblank = kwargs.get('unblank', True)\n    rev = kwargs.get('rev', False)\n\n    if unblank:\n        def can_merge_str(k, v, to_dict):\n            return (isinstance(v, compat_str) and v\n                    and isinstance(to_dict[k], compat_str)\n                    and not to_dict[k])\n    else:\n        can_merge_str = lambda k, v, to_dict: False\n\n    merged = {}\n    for a_dict in reversed(dicts) if rev else dicts:\n        for k, v in a_dict.items():\n            if v is None:\n                continue\n            if (k not in merged) or can_merge_str(k, v, merged):\n                merged[k] = v\n    return merged", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_age_limit", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'|'.join", "int", "int_or_none", "isinstance", "m.group", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_age_limit(s):\n    if not isinstance(s, bool):\n        age = int_or_none(s)\n        if age is not None:\n            return age if 0 <= age <= 21 else None\n    if not isinstance(s, compat_basestring):\n        return None\n    m = re.match(r'^(?P<age>\\d{1,2})\\+?$', s)\n    if m:\n        return int(m.group('age'))\n    if s in US_RATINGS:\n        return US_RATINGS[s]\n    m = re.match(r'^TV[_-]?(%s)$' % '|'.join(k[3:] for k in TV_PARENTAL_GUIDELINES), s)\n    if m:\n        return TV_PARENTAL_GUIDELINES['TV-' + m.group(1)]\n    return None", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "qualities", "parameters": ["quality_ids"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["quality_ids.index"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Get a numeric quality value out of a list of possible values", "source_code": "def qualities(quality_ids):\n    \"\"\" Get a numeric quality value out of a list of possible values \"\"\"\n    def q(qid):\n        try:\n            return quality_ids.index(qid)\n        except ValueError:\n            return -1\n    return q", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "limit_length", "parameters": ["s", "length"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add ellipses to overly long strings", "source_code": "def limit_length(s, length):\n    \"\"\" Add ellipses to overly long strings \"\"\"\n    if s is None:\n        return None\n    ELLIPSES = '...'\n    if len(s) > length:\n        return s[:length - len(ELLIPSES)] + ELLIPSES\n    return s", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "is_outdated_version", "parameters": ["version", "limit", "assume_new"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["version_tuple"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_outdated_version(version, limit, assume_new=True):\n    if not version:\n        return not assume_new\n    try:\n        return version_tuple(version) < version_tuple(limit)\n    except ValueError:\n        return not assume_new", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "ytdl_is_updateable", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["globals", "globals().get", "hasattr", "isinstance"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ytdl_is_updateable():\n    \"\"\" Returns if youtube-dl can be updated with -U \"\"\"\n    from zipimport import zipimporter\n\n    return isinstance(globals().get('__loader__'), zipimporter) or hasattr(sys, 'frozen')", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "mimetype2ext", "parameters": ["mt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mt.rpartition", "res.split", "res.split(';')[0].strip", "res.split(';')[0].strip().lower", "{'3gpp': '3gp', 'smptett+xml': 'tt', 'ttaf+xml': 'dfxp', 'ttml+xml': 'ttml', 'x-flv': 'flv', 'x-mp4-fragmented': 'mp4', 'x-ms-sami': 'sami', 'x-ms-wmv': 'wmv', 'mpegurl': 'm3u8', 'x-mpegurl': 'm3u8', 'vnd.apple.mpegurl': 'm3u8', 'dash+xml': 'mpd', 'f4m+xml': 'f4m', 'hds+xml': 'f4m', 'vnd.ms-sstr+xml': 'ism', 'quicktime': 'mov', 'mp2t': 'ts', 'x-wav': 'wav'}.get", "{'audio/mp4': 'm4a', 'audio/mpeg': 'mp3'}.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mimetype2ext(mt):\n    if mt is None:\n        return None\n\n    ext = {\n        'audio/mp4': 'm4a',\n        # Per RFC 3003, audio/mpeg can be .mp1, .mp2 or .mp3. Here use .mp3 as\n        # it's the most popular one\n        'audio/mpeg': 'mp3',\n    }.get(mt)\n    if ext is not None:\n        return ext\n\n    _, _, res = mt.rpartition('/')\n    res = res.split(';')[0].strip().lower()\n\n    return {\n        '3gpp': '3gp',\n        'smptett+xml': 'tt',\n        'ttaf+xml': 'dfxp',\n        'ttml+xml': 'ttml',\n        'x-flv': 'flv',\n        'x-mp4-fragmented': 'mp4',\n        'x-ms-sami': 'sami',\n        'x-ms-wmv': 'wmv',\n        'mpegurl': 'm3u8',\n        'x-mpegurl': 'm3u8',\n        'vnd.apple.mpegurl': 'm3u8',\n        'dash+xml': 'mpd',\n        'f4m+xml': 'f4m',\n        'hds+xml': 'f4m',\n        'vnd.ms-sstr+xml': 'ism',\n        'quicktime': 'mov',\n        'mp2t': 'ts',\n        'x-wav': 'wav',\n    }.get(res, res)", "loc": 36}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_codecs", "parameters": ["codecs_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["codecs_str.strip", "codecs_str.strip().strip", "codecs_str.strip().strip(',').split", "filter", "full_codec.split", "len", "list", "map", "str.strip", "write_string"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_codecs(codecs_str):\n    # http://tools.ietf.org/html/rfc6381\n    if not codecs_str:\n        return {}\n    split_codecs = list(filter(None, map(\n        lambda str: str.strip(), codecs_str.strip().strip(',').split(','))))\n    vcodec, acodec = None, None\n    for full_codec in split_codecs:\n        codec = full_codec.split('.')[0]\n        if codec in ('avc1', 'avc2', 'avc3', 'avc4', 'vp9', 'vp8', 'hev1', 'hev2', 'h263', 'h264', 'mp4v', 'hvc1', 'av01', 'theora'):\n            if not vcodec:\n                vcodec = full_codec\n        elif codec in ('mp4a', 'opus', 'vorbis', 'mp3', 'aac', 'ac-3', 'ec-3', 'eac3', 'dtsc', 'dtse', 'dtsh', 'dtsl'):\n            if not acodec:\n                acodec = full_codec\n        else:\n            write_string('WARNING: Unknown codec %s\\n' % full_codec, sys.stderr)\n    if not vcodec and not acodec:\n        if len(split_codecs) == 2:\n            return {\n                'vcodec': split_codecs[0],\n                'acodec': split_codecs[1],\n            }\n    else:\n        return {\n            'vcodec': vcodec or 'none',\n            'acodec': acodec or 'none',\n        }\n    return {}", "loc": 29}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "urlhandle_detect_ext", "parameters": ["url_handle"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "getheader", "m.group", "mimetype2ext", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def urlhandle_detect_ext(url_handle):\n    getheader = url_handle.headers.get\n\n    cd = getheader('Content-Disposition')\n    if cd:\n        m = re.match(r'attachment;\\s*filename=\"(?P<filename>[^\"]+)\"', cd)\n        if m:\n            e = determine_ext(m.group('filename'), default_ext=None)\n            if e:\n                return e\n\n    return mimetype2ext(getheader('Content-Type'))", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "age_restricted", "parameters": ["content_limit", "age_limit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def age_restricted(content_limit, age_limit):\n    \"\"\" Returns True iff the content should be blocked \"\"\"\n\n    if age_limit is None:  # No limit set\n        return False\n    if content_limit is None:\n        return False  # Content available for everyone\n    return age_limit < content_limit", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "is_html", "parameters": ["first_bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["first_bytes.decode", "first_bytes.startswith", "first_bytes[len(bom):].decode", "len", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Detect whether a file contains HTML by examining its first bytes.", "source_code": "def is_html(first_bytes):\n    \"\"\" Detect whether a file contains HTML by examining its first bytes. \"\"\"\n\n    BOMS = [\n        (b'\\xef\\xbb\\xbf', 'utf-8'),\n        (b'\\x00\\x00\\xfe\\xff', 'utf-32-be'),\n        (b'\\xff\\xfe\\x00\\x00', 'utf-32-le'),\n        (b'\\xff\\xfe', 'utf-16-le'),\n        (b'\\xfe\\xff', 'utf-16-be'),\n    ]\n    for bom, enc in BOMS:\n        if first_bytes.startswith(bom):\n            s = first_bytes[len(bom):].decode(enc, 'replace')\n            break\n    else:\n        s = first_bytes.decode('utf-8', 'replace')\n\n    return re.match(r'^\\s*<', s)", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "render_table", "parameters": ["header_row", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["' '.join", "'\\n'.join", "compat_str", "len", "max", "tuple", "zip"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Render a list of rows, each as a list of values", "source_code": "def render_table(header_row, data):\n    \"\"\" Render a list of rows, each as a list of values \"\"\"\n    table = [header_row] + data\n    max_lens = [max(len(compat_str(v)) for v in col) for col in zip(*table)]\n    format_str = ' '.join('%-' + compat_str(ml + 1) + 's' for ml in max_lens[:-1]) + '%s'\n    return '\\n'.join(format_str % tuple(row) for row in table)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "match_str", "parameters": ["filter_str", "dct"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_match_one", "all", "filter_str.split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false", "source_code": "def match_str(filter_str, dct):\n    \"\"\" Filter a dictionary with a simple string syntax. Returns True (=passes filter) or false \"\"\"\n\n    return all(\n        _match_one(filter_part, dct) for filter_part in filter_str.split('&'))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "match_filter_func", "parameters": ["filter_str"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info_dict.get", "match_str"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def match_filter_func(filter_str):\n    def _match_func(info_dict):\n        if match_str(filter_str, info_dict):\n            return None\n        else:\n            video_title = info_dict.get('title', info_dict.get('id', 'video'))\n            return '%s does not pass filter %s, skipping ..' % (video_title, filter_str)\n    return _match_func", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_dfxp_time_expr", "parameters": ["time_expr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "int", "mobj.group", "mobj.group(3).replace", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_dfxp_time_expr(time_expr):\n    if not time_expr:\n        return\n\n    mobj = re.match(r'^(?P<time_offset>\\d+(?:\\.\\d+)?)s?$', time_expr)\n    if mobj:\n        return float(mobj.group('time_offset'))\n\n    mobj = re.match(r'^(\\d+):(\\d\\d):(\\d\\d(?:(?:\\.|:)\\d+)?)$', time_expr)\n    if mobj:\n        return 3600 * int(mobj.group(1)) + 60 * int(mobj.group(2)) + float(mobj.group(3).replace(':', '.'))", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "cli_option", "parameters": ["params", "command_option", "param"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cli_option(params, command_option, param):\n    param = params.get(param)\n    if param:\n        param = compat_str(param)\n    return [command_option, param] if param is not None else []", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "cli_bool_option", "parameters": ["params", "command_option", "param", "true_value", "false_value", "separator"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cli_bool_option(params, command_option, param, true_value='true', false_value='false', separator=None):\n    param = params.get(param)\n    if param is None:\n        return []\n    assert isinstance(param, bool)\n    if separator:\n        return [command_option + separator + (true_value if param else false_value)]\n    return [command_option, true_value if param else false_value]", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "cli_configuration_args", "parameters": ["params", "param", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cli_configuration_args(params, param, default=[]):\n    ex_args = params.get(param)\n    if ex_args is None:\n        return default\n    assert isinstance(ex_args, list)\n    return ex_args", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "long_to_bytes", "parameters": ["n", "blocksize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_struct_pack", "int", "len", "range"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "long_to_bytes(n:long, blocksize:int) : string Convert a long integer to a byte string. If optional blocksize is given and greater than zero, pad the front of the", "source_code": "def long_to_bytes(n, blocksize=0):\n    \"\"\"long_to_bytes(n:long, blocksize:int) : string\n    Convert a long integer to a byte string.\n\n    If optional blocksize is given and greater than zero, pad the front of the\n    byte string with binary zeros so that the length is a multiple of\n    blocksize.\n    \"\"\"\n    # after much testing, this algorithm was deemed to be the fastest\n    s = b''\n    n = int(n)\n    while n > 0:\n        s = compat_struct_pack('>I', n & 0xffffffff) + s\n        n = n >> 32\n    # strip off leading zeros\n    for i in range(len(s)):\n        if s[i] != b'\\000'[0]:\n            break\n    else:\n        # only happens when n == 0\n        s = b'\\000'\n        i = 0\n    s = s[i:]\n    # add back some pad bytes.  this could be done more efficiently w.r.t. the\n    # de-padding being done above, but sigh...\n    if blocksize > 0 and len(s) % blocksize:\n        s = (blocksize - len(s) % blocksize) * b'\\000' + s\n    return s", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "bytes_to_long", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_struct_unpack", "len", "range"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "bytes_to_long(string) : long Convert a byte string to a long integer. This is (essentially) the inverse of long_to_bytes().", "source_code": "def bytes_to_long(s):\n    \"\"\"bytes_to_long(string) : long\n    Convert a byte string to a long integer.\n\n    This is (essentially) the inverse of long_to_bytes().\n    \"\"\"\n    acc = 0\n    length = len(s)\n    if length % 4:\n        extra = (4 - length % 4)\n        s = b'\\000' * extra + s\n        length = length + extra\n    for i in range(0, length, 4):\n        acc = (acc << 32) + compat_struct_unpack('>I', s[i:i + 4])[0]\n    return acc", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "ohdave_rsa_encrypt", "parameters": ["data", "exponent", "modulus"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["binascii.hexlify", "int", "pow"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/ Input: data: data to encrypt, bytes-like object", "source_code": "def ohdave_rsa_encrypt(data, exponent, modulus):\n    '''\n    Implement OHDave's RSA algorithm. See http://www.ohdave.com/rsa/\n\n    Input:\n        data: data to encrypt, bytes-like object\n        exponent, modulus: parameter e and N of RSA algorithm, both integer\n    Output: hex string of encrypted data\n\n    Limitation: supports one block encryption only\n    '''\n\n    payload = int(binascii.hexlify(data[::-1]), 16)\n    encrypted = pow(payload, exponent, modulus)\n    return '%x' % encrypted", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "pkcs1pad", "parameters": ["data", "length"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "len", "random.randint", "range"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Padding input data with PKCS#1 scheme @param {int[]} data        input data @param {int}   length      target length", "source_code": "def pkcs1pad(data, length):\n    \"\"\"\n    Padding input data with PKCS#1 scheme\n\n    @param {int[]} data        input data\n    @param {int}   length      target length\n    @returns {int[]}           padded data\n    \"\"\"\n    if len(data) > length - 11:\n        raise ValueError('Input data too long for PKCS#1 padding')\n\n    pseudo_random = [random.randint(0, 254) for _ in range(length - len(data) - 3)]\n    return [0, 2] + pseudo_random + [0] + data", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "encode_base_n", "parameters": ["num", "n", "table"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ValueError", "len"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encode_base_n(num, n, table=None):\n    FULL_TABLE = '0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n    if not table:\n        table = FULL_TABLE[:n]\n\n    if n > len(table):\n        raise ValueError('base %d exceeds table length %d' % (n, len(table)))\n\n    if num == 0:\n        return table[0]\n\n    ret = ''\n    while num:\n        ret = table[num % n] + ret\n        num = num // n\n    return ret", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "decode_packed_codes", "parameters": ["code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encode_base_n", "int", "mobj.group", "mobj.groups", "re.search", "re.sub", "symbols.split"], "control_structures": ["While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decode_packed_codes(code):\n    mobj = re.search(PACKED_CODES_RE, code)\n    obfuscated_code, base, count, symbols = mobj.groups()\n    base = int(base)\n    count = int(count)\n    symbols = symbols.split('|')\n    symbol_table = {}\n\n    while count:\n        count -= 1\n        base_n_count = encode_base_n(count, base)\n        symbol_table[base_n_count] = symbols[count] or base_n_count\n\n    return re.sub(\n        r'\\b(\\w+)\\b', lambda mobj: symbol_table[mobj.group(0)],\n        obfuscated_code)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "caesar", "parameters": ["s", "alphabet", "shift"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "alphabet.index", "len"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def caesar(s, alphabet, shift):\n    if shift == 0:\n        return s\n    l = len(alphabet)\n    return ''.join(\n        alphabet[(alphabet.index(c) + shift) % l] if c in alphabet else c\n        for c in s)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_m3u8_attributes", "parameters": ["attrib"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "val.startswith"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_m3u8_attributes(attrib):\n    info = {}\n    for (key, val) in re.findall(r'(?P<key>[A-Z0-9-]+)=(?P<val>\"[^\"]+\"|[^\",]+)(?:,|$)', attrib):\n        if val.startswith('\"'):\n            val = val[1:-1]\n        info[key] = val\n    return info", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "random_birthday", "parameters": ["year_field", "month_field", "day_field"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["datetime.date", "datetime.timedelta", "random.randint", "str"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def random_birthday(year_field, month_field, day_field):\n    start_date = datetime.date(1950, 1, 1)\n    end_date = datetime.date(1995, 12, 31)\n    offset = random.randint(0, (end_date - start_date).days)\n    random_date = start_date + datetime.timedelta(offset)\n    return {\n        year_field: str(random_date.year),\n        month_field: str(random_date.month),\n        day_field: str(random_date.day),\n    }", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "join_nonempty", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["delim.join", "filter", "kwargs.get", "map", "traverse_obj", "variadic"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def join_nonempty(*values, **kwargs):\n\n    # parameter defaults\n    delim = kwargs.get('delim', '-')\n    from_dict = kwargs.get('from_dict')\n\n    if from_dict is not None:\n        values = (traverse_obj(from_dict, variadic(v)) for v in values)\n    return delim.join(map(compat_str, filter(None, values)))", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "find_xpath_attr", "parameters": ["node", "xpath", "key", "val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["node.find", "re.match"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Find the xpath xpath[@key=val]", "source_code": "def find_xpath_attr(node, xpath, key, val=None):\n    \"\"\" Find the xpath xpath[@key=val] \"\"\"\n    assert re.match(r'^[a-zA-Z_-]+$', key)\n    expr = xpath + ('[@%s]' % key if val is None else \"[@%s='%s']\" % (key, val))\n    return node.find(expr)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "find_xpath_attr", "parameters": ["node", "xpath", "key", "val"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_xpath", "f.attrib.get", "node.findall"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_xpath_attr(node, xpath, key, val=None):\n    for f in node.findall(compat_xpath(xpath)):\n        if key not in f.attrib:\n            continue\n        if val is None or f.attrib.get(key) == val:\n            return f\n    return None", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "replace_insane", "parameters": ["char"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["char.isspace", "ord", "unicodedata.category"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def replace_insane(char):\n    if restricted and char in ACCENT_CHARS:\n        return ACCENT_CHARS[char]\n    if char == '?' or ord(char) < 32 or ord(char) == 127:\n        return ''\n    elif char == '\"':\n        return '' if restricted else '\\''\n    elif char == ':':\n        return '_-' if restricted else ' -'\n    elif char in '\\\\/|*<>':\n        return '_'\n    if restricted and (char in '!&\\'()[]{}$;`^,#' or char.isspace()):\n        return '_'\n    if restricted and ord(char) > 127:\n        return '' if unicodedata.category(char)[0] in 'CM' else '_'\n\n    return char", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "encodeFilename", "parameters": ["s", "for_subprocess"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_encode_compat_str", "get_subprocess_encoding", "isinstance", "sys.getwindowsversion"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "@param s The name of the file", "source_code": "def encodeFilename(s, for_subprocess=False):\n    \"\"\"\n    @param s The name of the file\n    \"\"\"\n\n    # Pass '' directly to use Unicode APIs on Windows 2000 and up\n    # (Detecting Windows NT 4 is tricky because 'major >= 4' would\n    # match Windows 9x series as well. Besides, NT 4 is obsolete.)\n    if (not for_subprocess\n            and sys.platform == 'win32'\n            and sys.getwindowsversion()[0] >= 5\n            and isinstance(s, compat_str)):\n        return s\n\n    return _encode_compat_str(s, get_subprocess_encoding(), 'ignore')", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "set_alpn_protocols", "parameters": ["ctx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_contextlib_suppress", "ctx.set_alpn_protocols"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_alpn_protocols(ctx):\n    # From https://github.com/yt-dlp/yt-dlp/commit/2c6dcb65fb612fc5bc5c61937bf438d3c473d8d0\n    # Thanks @coletdjnz\n    # Some servers may (wrongly) reject requests if ALPN extension is not sent. See:\n    # https://github.com/python/cpython/issues/85140\n    # https://github.com/yt-dlp/yt-dlp/issues/3878\n    with compat_contextlib_suppress(AttributeError, NotImplementedError):\n        # fails for Python < 2.7.10, not ssl.HAS_ALPN\n        ctx.set_alpn_protocols(ALPN_PROTOCOLS)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "YoutubeDLHandler", "function_name": "deflate_gz", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["zlib.decompress"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def deflate_gz(data):\n    try:\n        # format:zlib,gzip + windowsize:32768\n        return data and zlib.decompress(data, 32 + zlib.MAX_WBITS)\n    except zlib.error:\n        # raw zlib * windowsize:32768 (RFC 9110: \"non-conformant\")\n        return zlib.decompress(data, -zlib.MAX_WBITS)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "YoutubeDLHandler", "function_name": "gzip", "parameters": ["data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GzipFile", "_gzip", "gz.read", "io.BytesIO", "range"], "control_structures": ["For", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gzip(data):\n\n    from gzip import GzipFile\n\n    def _gzip(data):\n        with io.BytesIO(data) as data_buf:\n            gz = GzipFile(fileobj=data_buf, mode='rb')\n            return gz.read()\n\n    try:\n        return _gzip(data)\n    except IOError as original_ioerror:\n        # There may be junk at the end of the file\n        # See http://stackoverflow.com/q/4928560/35070 for details\n        for i in range(1, 1024):\n            try:\n                return _gzip(data[:-i])\n            except IOError:\n                continue\n        else:\n            raise original_ioerror", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "YoutubeDLCookieJar", "function_name": "get_cookie_header", "parameters": ["self", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cookie_req.get_header", "sanitized_Request", "self.add_cookie_header"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Generate a Cookie HTTP header for a given url", "source_code": "def get_cookie_header(self, url):\n    \"\"\"Generate a Cookie HTTP header for a given url\"\"\"\n    cookie_req = sanitized_Request(url)\n    self.add_cookie_header(cookie_req)\n    return cookie_req.get_header('Cookie')", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "YoutubeDLCookieJar", "function_name": "get_cookies_for_url", "parameters": ["self", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "sanitized_Request", "self._cookies_for_request", "time.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Generate a list of Cookie objects for a given url", "source_code": "def get_cookies_for_url(self, url):\n    \"\"\"Generate a list of Cookie objects for a given url\"\"\"\n    # Policy `_now` attribute must be set before calling `_cookies_for_request`\n    # Ref: https://github.com/python/cpython/blob/3.7/Lib/http/cookiejar.py#L1360\n    self._policy._now = self._now = int(time.time())\n    return self._cookies_for_request(sanitized_Request(url))", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "not_a_console", "parameters": ["handle"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["GetConsoleMode", "GetFileType", "ctypes.byref", "ctypes.wintypes.DWORD"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def not_a_console(handle):\n    if handle == INVALID_HANDLE_VALUE or handle is None:\n        return True\n    return ((GetFileType(handle) & ~FILE_TYPE_REMOTE) != FILE_TYPE_CHAR\n            or GetConsoleMode(handle, ctypes.byref(ctypes.wintypes.DWORD())) == 0)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "next_nonbmp_pos", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "len", "next", "ord"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def next_nonbmp_pos(s):\n    try:\n        return next(i for i, c in enumerate(s) if ord(c) > 0xffff)\n    except StopIteration:\n        return len(s)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "OnDemandPagedList", "function_name": "getslice", "parameters": ["self", "start", "end"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["itertools.count", "len", "list", "res.extend", "self._cache.get", "self._pagefunc"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getslice(self, start=0, end=None):\n    res = []\n    for pagenum in itertools.count(start // self._pagesize):\n        firstid = pagenum * self._pagesize\n        nextfirstid = pagenum * self._pagesize + self._pagesize\n        if start >= nextfirstid:\n            continue\n\n        page_results = None\n        if self._use_cache:\n            page_results = self._cache.get(pagenum)\n        if page_results is None:\n            page_results = list(self._pagefunc(pagenum))\n        if self._use_cache:\n            self._cache[pagenum] = page_results\n\n        startv = (\n            start % self._pagesize\n            if firstid <= start < nextfirstid\n            else 0)\n\n        endv = (\n            ((end - 1) % self._pagesize) + 1\n            if (end is not None and firstid <= end <= nextfirstid)\n            else None)\n\n        if startv != 0 or endv is not None:\n            page_results = page_results[startv:endv]\n        res.extend(page_results)\n\n        # A little optimization - if current page is not \"full\", ie. does\n        # not contain page_size videos then we can assume that this page\n        # is the last one - there are no more ids on further pages -\n        # i.e. no need to query again.\n        if len(page_results) + startv < self._pagesize:\n            break\n\n        # If we got the whole page, but the next page is not interesting,\n        # break out early as well\n        if end == nextfirstid:\n            break\n    return res", "loc": 42}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "InAdvancePagedList", "function_name": "getslice", "parameters": ["self", "start", "end"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "list", "range", "res.extend", "self._pagefunc"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getslice(self, start=0, end=None):\n    res = []\n    start_page = start // self._pagesize\n    end_page = (\n        self._pagecount if end is None else (end // self._pagesize + 1))\n    skip_elems = start - start_page * self._pagesize\n    only_more = None if end is None else end - start\n    for pagenum in range(start_page, end_page):\n        page = list(self._pagefunc(pagenum))\n        if skip_elems:\n            page = page[skip_elems:]\n            skip_elems = None\n        if only_more is not None:\n            if len(page) < only_more:\n                only_more -= len(page)\n            else:\n                page = page[:only_more]\n                res.extend(page)\n                break\n        res.extend(page)\n    return res", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "fixup", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_decode_compat_str", "len", "url.startswith", "url.strip"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fixup(url):\n    url = _decode_compat_str(url, 'utf-8', 'replace')\n    BOM_UTF8 = '\\xef\\xbb\\xbf'\n    if url.startswith(BOM_UTF8):\n        url = url[len(BOM_UTF8):]\n    url = url.strip()\n    if url.startswith(('#', ';', ']')):\n        return False\n    return url", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "template_substitute", "parameters": ["match"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["js_to_json", "json.loads", "match.group"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def template_substitute(match):\n    evaluated = js_to_json(match.group(1), vars, strict=strict)\n    if evaluated[0] == '\"':\n        return json.loads(evaluated)\n    return evaluated", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "fix_kv", "parameters": ["m"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\"{0}\"'.format", "ValueError", "any", "im.group", "im[-1].endswith", "int", "inv", "json.dumps", "json.loads", "len", "m.group", "re.match", "re.split", "re.sub", "try_call", "v.endswith", "v.startswith"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def fix_kv(m):\n    v = m.group(0)\n    if v in ('true', 'false', 'null'):\n        return v\n    elif v in ('undefined', 'void 0'):\n        return 'null'\n    elif v.startswith('/*') or v.startswith('//') or v == ',':\n        return ''\n\n    if v[0] in STRING_QUOTES:\n        v = re.sub(r'(?s)\\${([^}]+)}', template_substitute, v[1:-1]) if v[0] == '`' else v[1:-1]\n        escaped = re.sub(r'(?s)(\")|\\\\(.)', process_escape, v)\n        return '\"{0}\"'.format(escaped)\n\n    inv = IDENTITY\n    im = re.split(r'^!+', v)\n    if len(im) > 1 and not im[-1].endswith(':'):\n        if (len(v) - len(im[1])) % 2 == 1:\n            inv = lambda x: 'true' if x == 0 else 'false'\n        else:\n            inv = lambda x: 'false' if x == 0 else 'true'\n    if not any(x for x in im):\n        return\n    v = im[-1]\n\n    for regex, base in INTEGER_TABLE:\n        im = re.match(regex, v)\n        if im:\n            i = int(im.group(1), base)\n            return ('\"%s\":' if v.endswith(':') else '%s') % inv(i)\n\n    if v in vars:\n        try:\n            if not strict:\n                json.loads(vars[v])\n        except JSONDecodeError:\n            return inv(json.dumps(vars[v]))\n        else:\n            return inv(vars[v])\n\n    if not strict:\n        v = try_call(inv, args=(v,), default=v)\n        if v in ('true', 'false'):\n            return v\n        return '\"{0}\"'.format(v)\n\n    raise ValueError('Unknown value: ' + v)", "loc": 47}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "q", "parameters": ["qid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["quality_ids.index"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def q(qid):\n    try:\n        return quality_ids.index(qid)\n    except ValueError:\n        return -1", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "parse_node", "parameters": ["node"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["TTMLPElementParser", "parser.close", "parser.feed", "xml.etree.ElementTree.XMLParser", "xml.etree.ElementTree.tostring"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_node(node):\n    target = TTMLPElementParser()\n    parser = xml.etree.ElementTree.XMLParser(target=target)\n    parser.feed(xml.etree.ElementTree.tostring(node))\n    return parser.close()", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "ISO639Utils", "function_name": "long2short", "parameters": ["cls", "code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls._lang_map.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Convert language code from ISO 639-2/T to ISO 639-1", "source_code": "def long2short(cls, code):\n    \"\"\"Convert language code from ISO 639-2/T to ISO 639-1\"\"\"\n    for short_name, long_name in cls._lang_map.items():\n        if long_name == code:\n            return short_name", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "GeoUtils", "function_name": "random_ipv4", "parameters": ["cls", "code_or_block"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["block.split", "cls._country_ip_map.get", "code_or_block.upper", "compat_str", "compat_struct_pack", "compat_struct_unpack", "int", "len", "random.randint", "socket.inet_aton", "socket.inet_ntoa"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def random_ipv4(cls, code_or_block):\n    if len(code_or_block) == 2:\n        block = cls._country_ip_map.get(code_or_block.upper())\n        if not block:\n            return None\n    else:\n        block = code_or_block\n    addr, preflen = block.split('/')\n    addr_min = compat_struct_unpack('!L', socket.inet_aton(addr))[0]\n    addr_max = addr_min | (0xffffffff >> int(preflen))\n    return compat_str(socket.inet_ntoa(\n        compat_struct_pack('!L', random.randint(addr_min, addr_max))))", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "from_iterable", "parameters": ["iterables"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def from_iterable(iterables):\n    # chain.from_iterable(['ABC', 'DEF']) --> A B C D E F\n    for it in iterables:\n        for item in it:\n            yield item", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "lazy_last", "parameters": ["iterable"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["iter", "next"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def lazy_last(iterable):\n    iterator = iter(iterable)\n    prev = next(iterator, NO_DEFAULT)\n    if prev is NO_DEFAULT:\n        return\n\n    for item in iterator:\n        yield False, prev\n        prev = item\n\n    yield True, prev", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "apply_path", "parameters": ["start_obj", "path", "test_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_try_bind_args", "apply_key", "callable", "compat_casefold", "from_iterable", "isinstance", "lazy_last", "list", "map", "new_objs.append", "next", "variadic"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def apply_path(start_obj, path, test_type):\n    objs = (start_obj,)\n    has_branched = False\n\n    key = None\n    for last, key in lazy_last(variadic(path, (str, bytes, dict, set))):\n        if not casesense and isinstance(key, str):\n            key = compat_casefold(key)\n\n        if key in (any, all):\n            has_branched = False\n            filtered_objs = (obj for obj in objs if obj not in (None, {}))\n            if key is any:\n                objs = (next(filtered_objs, None),)\n            else:\n                objs = (list(filtered_objs),)\n            continue\n\n        if __debug__ and callable(key):\n            # Verify function signature\n            _try_bind_args(key, None, None)\n\n        new_objs = []\n        for obj in objs:\n            branching, results = apply_key(key, obj, last)\n            has_branched |= branching\n            new_objs.append(results)\n\n        objs = from_iterable(new_objs)\n\n    if test_type and not isinstance(key, (dict, list, tuple)):\n        objs = map(type_test, objs)\n\n    return objs, has_branched, isinstance(key, dict)", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "_UnsafeExtensionError", "function_name": "sanitize_extension", "parameters": ["cls", "extension"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls", "extension.rpartition", "kwargs.get", "last.lower"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sanitize_extension(cls, extension, **kwargs):\n    # ... /, *, prepend=False\n    prepend = kwargs.get('prepend', False)\n\n    if '/' in extension or '\\\\' in extension:\n        raise cls(extension)\n\n    if not prepend:\n        last = extension.rpartition('.')[-1]\n        if last == 'bin':\n            extension = last = 'unknown_video'\n        if not (cls.lenient or last.lower() in cls._ALLOWED_EXTENSIONS):\n            raise cls(extension)\n\n    return extension", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "encodings", "parameters": ["hdrs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["','.join", "','.join(hdrs).split", "e.strip", "reversed"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encodings(hdrs):\n    # A header field that allows multiple values can have multiple instances [2].\n    # [2]: https://datatracker.ietf.org/doc/html/rfc9110#name-fields\n    for e in reversed(','.join(hdrs).split(',')):\n        if e:\n            yield e.strip()", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "SocksConnection", "function_name": "connect", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "isinstance", "self._context.wrap_socket", "self.sock.connect", "self.sock.setproxy", "self.sock.settimeout", "sockssocket", "ssl.wrap_socket", "type"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def connect(self):\n    self.sock = sockssocket()\n    self.sock.setproxy(*proxy_args)\n    if type(self.timeout) in (int, float):\n        self.sock.settimeout(self.timeout)\n    self.sock.connect((self.host, self.port))\n\n    if isinstance(self, compat_http_client.HTTPSConnection):\n        if hasattr(self, '_context'):  # Python > 2.6\n            self.sock = self._context.wrap_socket(\n                self.sock, server_hostname=self.host)\n        else:\n            self.sock = ssl.wrap_socket(self.sock)", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "prepare_line", "parameters": ["line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_cookiejar.LoadError", "cookie.expires_at.isdigit", "len", "line.split", "line.startswith", "line.strip", "self._CookieFileEntry"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def prepare_line(line):\n    if line.startswith(self._HTTPONLY_PREFIX):\n        line = line[len(self._HTTPONLY_PREFIX):]\n    # comments and empty lines are fine\n    if line.startswith('#') or not line.strip():\n        return line\n    cookie_list = line.split('\\t')\n    if len(cookie_list) != self._ENTRY_LEN:\n        raise compat_cookiejar.LoadError('invalid length %d' % len(cookie_list))\n    cookie = self._CookieFileEntry(*cookie_list)\n    if cookie.expires_at and not cookie.expires_at.isdigit():\n        raise compat_cookiejar.LoadError('invalid expires at %s' % cookie.expires_at)\n    return line", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "TTMLPElementParser", "function_name": "start", "parameters": ["self", "tag", "attrib"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_x", "applied_style.update", "attrib.get", "self._applied_styles.append", "self._applied_styles[-1].get", "self._unclosed_elements.append", "sorted", "style.items", "style.update", "styles.get", "unclosed_elements.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def start(self, tag, attrib):\n    if tag in (_x('ttml:br'), 'br'):\n        self._out += '\\n'\n    else:\n        unclosed_elements = []\n        style = {}\n        element_style_id = attrib.get('style')\n        if default_style:\n            style.update(default_style)\n        if element_style_id:\n            style.update(styles.get(element_style_id, {}))\n        for prop in SUPPORTED_STYLING:\n            prop_val = attrib.get(_x('tts:' + prop))\n            if prop_val:\n                style[prop] = prop_val\n        if style:\n            font = ''\n            for k, v in sorted(style.items()):\n                if self._applied_styles and self._applied_styles[-1].get(k) == v:\n                    continue\n                if k == 'color':\n                    font += ' color=\"%s\"' % v\n                elif k == 'fontSize':\n                    font += ' size=\"%s\"' % v\n                elif k == 'fontFamily':\n                    font += ' face=\"%s\"' % v\n                elif k == 'fontWeight' and v == 'bold':\n                    self._out += '<b>'\n                    unclosed_elements.append('b')\n                elif k == 'fontStyle' and v == 'italic':\n                    self._out += '<i>'\n                    unclosed_elements.append('i')\n                elif k == 'textDecoration' and v == 'underline':\n                    self._out += '<u>'\n                    unclosed_elements.append('u')\n            if font:\n                self._out += '<font' + font + '>'\n                unclosed_elements.append('font')\n            applied_style = {}\n            if self._applied_styles:\n                applied_style.update(self._applied_styles[-1])\n            applied_style.update(style)\n            self._applied_styles.append(applied_style)\n        self._unclosed_elements.append(unclosed_elements)", "loc": 44}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": "TTMLPElementParser", "function_name": "end", "parameters": ["self", "tag"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_x", "reversed", "self._applied_styles.pop", "self._unclosed_elements.pop"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def end(self, tag):\n    if tag not in (_x('ttml:br'), 'br'):\n        unclosed_elements = self._unclosed_elements.pop()\n        for element in reversed(unclosed_elements):\n            self._out += '</%s>' % element\n        if unclosed_elements and self._applied_styles:\n            self._applied_styles.pop()", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\utils.py", "class_name": null, "function_name": "apply_specials", "parameters": ["element"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'apply_specials is missing case for {0!r}'.format", "SyntaxError", "special.startswith", "try_call"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def apply_specials(element):\n    if special is None:\n        return element\n    if special == '@':\n        return element.attrib\n    if special.startswith('@'):\n        return try_call(element.attrib.get, args=(special[1:],))\n    if special == 'text()':\n        return element.text\n    raise SyntaxError('apply_specials is missing case for {0!r}'.format(special))", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "wrapper", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0} found; to avoid damaging your system, this value is disallowed. If you believe this is an error{1}'.format", "bug_reports_message", "error_to_compat_str", "func", "functools.wraps", "self.report_error"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(self, *args, **kwargs):\n    try:\n        return func(self, *args, **kwargs)\n    except _UnsafeExtensionError as error:\n        self.report_error(\n            '{0} found; to avoid damaging your system, this value is disallowed.'\n            ' If you believe this is an error{1}'.format(\n                error_to_compat_str(error), bug_reports_message(',')))", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "warn_if_short_id", "parameters": ["self", "argv"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["args_to_str", "enumerate", "re.match", "self.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def warn_if_short_id(self, argv):\n    # short YouTube ID starting with dash?\n    idxs = [\n        i for i, a in enumerate(argv)\n        if re.match(r'^-[0-9A-Za-z_-]{10}$', a)]\n    if idxs:\n        correct_argv = (\n            ['youtube-dl']\n            + [a for i, a in enumerate(argv) if i not in idxs]\n            + ['--'] + [argv[i] for i in idxs]\n        )\n        self.report_warning(\n            'Long argument string detected. '\n            'Use -- to separate parameters and URLs, like this:\\n%s\\n' %\n            args_to_str(correct_argv))", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "add_info_extractor", "parameters": ["self", "ie"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ie.ie_key", "ie.set_downloader", "isinstance", "self._ies.append"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Add an InfoExtractor object to the end of the list.", "source_code": "def add_info_extractor(self, ie):\n    \"\"\"Add an InfoExtractor object to the end of the list.\"\"\"\n    self._ies.append(ie)\n    if not isinstance(ie, type):\n        self._ies_instances[ie.ie_key()] = ie\n        ie.set_downloader(self)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "get_info_extractor", "parameters": ["self", "ie_key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_info_extractor", "self._ies_instances.get", "self.add_info_extractor"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Get an instance of an IE with name ie_key, it will try to get one from the _ies list, if there's no instance it will create a new one and add it to the extractor list.", "source_code": "def get_info_extractor(self, ie_key):\n    \"\"\"\n    Get an instance of an IE with name ie_key, it will try to get one from\n    the _ies list, if there's no instance it will create a new one and add\n    it to the extractor list.\n    \"\"\"\n    ie = self._ies_instances.get(ie_key)\n    if ie is None:\n        ie = get_info_extractor(ie_key)()\n        self.add_info_extractor(ie)\n    return ie", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "add_default_info_extractors", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_extractor_classes", "self.add_info_extractor"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "Add the InfoExtractors returned by gen_extractors to the end of the list", "source_code": "def add_default_info_extractors(self):\n    \"\"\"\n    Add the InfoExtractors returned by gen_extractors to the end of the list\n    \"\"\"\n    for ie in gen_extractor_classes():\n        self.add_info_extractor(ie)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "to_stdout", "parameters": ["self", "message", "skip_eol", "check_quiet", "only_once"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._bidi_workaround", "self._write_string", "self.params.get", "self.params['logger'].debug"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Print message to stdout if not in quiet mode.", "source_code": "def to_stdout(self, message, skip_eol=False, check_quiet=False, only_once=False):\n    \"\"\"Print message to stdout if not in quiet mode.\"\"\"\n    if self.params.get('logger'):\n        self.params['logger'].debug(message)\n    elif not check_quiet or not self.params.get('quiet', False):\n        message = self._bidi_workaround(message)\n        terminator = ['\\n', ''][skip_eol]\n        output = message + terminator\n\n        self._write_string(output, self._screen_file, only_once=only_once)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "to_stderr", "parameters": ["self", "message", "only_once"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "self._bidi_workaround", "self._write_string", "self.params.get", "self.params['logger'].error"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Print message to stderr.", "source_code": "def to_stderr(self, message, only_once=False):\n    \"\"\"Print message to stderr.\"\"\"\n    assert isinstance(message, compat_str)\n    if self.params.get('logger'):\n        self.params['logger'].error(message)\n    else:\n        message = self._bidi_workaround(message)\n        output = message + '\\n'\n        self._write_string(output, self._err_file, only_once=only_once)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "to_console_title", "parameters": ["self", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ctypes.c_wchar_p", "ctypes.windll.kernel32.GetConsoleWindow", "ctypes.windll.kernel32.SetConsoleTitleW", "self._write_string", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def to_console_title(self, message):\n    if not self.params.get('consoletitle', False):\n        return\n    if compat_os_name == 'nt':\n        if ctypes.windll.kernel32.GetConsoleWindow():\n            # c_wchar_p() might not be necessary if `message` is\n            # already of type unicode()\n            ctypes.windll.kernel32.SetConsoleTitleW(ctypes.c_wchar_p(message))\n    elif 'TERM' in os.environ:\n        self._write_string('\\033]0;%s\\007' % message, self._screen_file)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "save_console_title", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._write_string", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def save_console_title(self):\n    if not self.params.get('consoletitle', False):\n        return\n    if self.params.get('simulate', False):\n        return\n    if compat_os_name != 'nt' and 'TERM' in os.environ:\n        # Save the title on stack\n        self._write_string('\\033[22;0t', self._screen_file)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "restore_console_title", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._write_string", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def restore_console_title(self):\n    if not self.params.get('consoletitle', False):\n        return\n    if self.params.get('simulate', False):\n        return\n    if compat_os_name != 'nt' and 'TERM' in os.environ:\n        # Restore the title from stack\n        self._write_string('\\033[23;0t', self._screen_file)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "trouble", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "DownloadError", "encode_compat_str", "hasattr", "kwargs.get", "len", "self.params.get", "self.to_stderr", "sys.exc_info", "traceback.extract_stack", "traceback.format_exc", "traceback.format_exception", "traceback.format_list"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Determine action to take when a download problem appears. Depending on if the downloader has been configured to ignore download errors or not, this method may throw an exception or", "source_code": "def trouble(self, *args, **kwargs):\n    \"\"\"Determine action to take when a download problem appears.\n\n    Depending on if the downloader has been configured to ignore\n    download errors or not, this method may throw an exception or\n    not when errors are found, after printing the message.\n\n    tb, if given, is additional traceback information.\n    \"\"\"\n    # message=None, tb=None, is_error=True\n    message = args[0] if len(args) > 0 else kwargs.get('message', None)\n    tb = args[1] if len(args) > 1 else kwargs.get('tb', None)\n    is_error = args[2] if len(args) > 2 else kwargs.get('is_error', True)\n\n    if message is not None:\n        self.to_stderr(message)\n    if self.params.get('verbose'):\n        if tb is None:\n            if sys.exc_info()[0]:  # if .trouble has been called from an except block\n                tb = ''\n                if hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n                    tb += ''.join(traceback.format_exception(*sys.exc_info()[1].exc_info))\n                tb += encode_compat_str(traceback.format_exc())\n            else:\n                tb_data = traceback.format_list(traceback.extract_stack())\n                tb = ''.join(tb_data)\n        if tb:\n            self.to_stderr(tb)\n    if not is_error:\n        return\n    if not self.params.get('ignoreerrors', False):\n        if sys.exc_info()[0] and hasattr(sys.exc_info()[1], 'exc_info') and sys.exc_info()[1].exc_info[0]:\n            exc_info = sys.exc_info()[1].exc_info\n        else:\n            exc_info = sys.exc_info()\n        raise DownloadError(message, exc_info)\n    self._download_retcode = 1", "loc": 37}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "report_warning", "parameters": ["self", "message", "only_once"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._err_file.isatty", "self.params.get", "self.params['logger'].warning", "self.to_stderr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Print the message to stderr, it will be prefixed with 'WARNING:' If stderr is a tty file the 'WARNING:' will be colored", "source_code": "def report_warning(self, message, only_once=False):\n    '''\n    Print the message to stderr, it will be prefixed with 'WARNING:'\n    If stderr is a tty file the 'WARNING:' will be colored\n    '''\n    if self.params.get('logger') is not None:\n        self.params['logger'].warning(message)\n    else:\n        if self.params.get('no_warnings'):\n            return\n        if not self.params.get('no_color') and self._err_file.isatty() and compat_os_name != 'nt':\n            _msg_header = '\\033[0;33mWARNING:\\033[0m'\n        else:\n            _msg_header = 'WARNING:'\n        warning_message = '%s %s' % (_msg_header, message)\n        self.to_stderr(warning_message, only_once=only_once)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "report_error", "parameters": ["self", "message"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._err_file.isatty", "self.params.get", "self.trouble"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Do the same as trouble, but prefixes the message with 'ERROR:', colored in red if stderr is a tty file.", "source_code": "def report_error(self, message, *args, **kwargs):\n    '''\n    Do the same as trouble, but prefixes the message with 'ERROR:', colored\n    in red if stderr is a tty file.\n    '''\n    if not self.params.get('no_color') and self._err_file.isatty() and compat_os_name != 'nt':\n        _msg_header = '\\033[0;31mERROR:\\033[0m'\n    else:\n        _msg_header = 'ERROR:'\n    kwargs['message'] = '%s %s' % (_msg_header, message)\n    self.trouble(*args, **kwargs)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "write_debug", "parameters": ["self", "message", "only_once"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'[debug] {0}'.format", "self.params.get", "self.params['logger'].debug", "self.to_stderr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Log debug message or Print message to stderr", "source_code": "def write_debug(self, message, only_once=False):\n    '''Log debug message or Print message to stderr'''\n    if not self.params.get('verbose', False):\n        return\n    message = '[debug] {0}'.format(message)\n    if self.params.get('logger'):\n        self.params['logger'].debug(message)\n    else:\n        self.to_stderr(message, only_once)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "report_unscoped_cookies", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.setdefault", "len", "self.report_error"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def report_unscoped_cookies(self, *args, **kwargs):\n    # message=None, tb=False, is_error=False\n    if len(args) <= 2:\n        kwargs.setdefault('is_error', False)\n        if len(args) <= 0:\n            kwargs.setdefault(\n                'message',\n                'Unscoped cookies are not allowed: please specify some sort of scoping')\n    self.report_error(*args, **kwargs)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "report_file_already_downloaded", "parameters": ["self", "file_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.to_screen"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Report file has already been fully downloaded.", "source_code": "def report_file_already_downloaded(self, file_name):\n    \"\"\"Report file has already been fully downloaded.\"\"\"\n    try:\n        self.to_screen('[download] %s has already been downloaded' % file_name)\n    except UnicodeEncodeError:\n        self.to_screen('[download] The file has already been downloaded')", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "prepare_filename", "parameters": ["self", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'${0}$'.format", "'%({0})s'.format", "'%{0}%'.format", "''.join", "FORMAT_RE.format", "collections.defaultdict", "compat_str", "dict", "encodeFilename", "encodeFilename(filename, True).decode", "error_to_compat_str", "expand_path", "expand_path(outtmpl).replace", "int", "isinstance", "k.endswith", "len", "mobj.group", "outtmpl.replace", "outtmpl.replace('%%', '%{0}%'.format(sep)).replace", "preferredencoding", "random.choice", "range", "re.search", "re.sub", "repr", "sanitize", "sanitize_filename", "sanitize_path", "self.params.get", "self.report_error", "str", "template_dict.get", "template_dict.items", "time.time"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Generate the output filename.", "source_code": "def prepare_filename(self, info_dict):\n    \"\"\"Generate the output filename.\"\"\"\n    try:\n        template_dict = dict(info_dict)\n\n        template_dict['epoch'] = int(time.time())\n        autonumber_size = self.params.get('autonumber_size')\n        if autonumber_size is None:\n            autonumber_size = 5\n        template_dict['autonumber'] = self.params.get('autonumber_start', 1) - 1 + self._num_downloads\n        if template_dict.get('resolution') is None:\n            if template_dict.get('width') and template_dict.get('height'):\n                template_dict['resolution'] = '%dx%d' % (template_dict['width'], template_dict['height'])\n            elif template_dict.get('height'):\n                template_dict['resolution'] = '%sp' % template_dict['height']\n            elif template_dict.get('width'):\n                template_dict['resolution'] = '%dx?' % template_dict['width']\n\n        sanitize = lambda k, v: sanitize_filename(\n            compat_str(v),\n            restricted=self.params.get('restrictfilenames'),\n            is_id=(k == 'id' or k.endswith('_id')))\n        template_dict = dict((k, v if isinstance(v, compat_numeric_types) else sanitize(k, v))\n                             for k, v in template_dict.items()\n                             if v is not None and not isinstance(v, (list, tuple, dict)))\n        template_dict = collections.defaultdict(lambda: self.params.get('outtmpl_na_placeholder', 'NA'), template_dict)\n\n        outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n\n        # For fields playlist_index and autonumber convert all occurrences\n        # of %(field)s to %(field)0Nd for backward compatibility\n        field_size_compat_map = {\n            'playlist_index': len(str(template_dict['n_entries'])),\n            'autonumber': autonumber_size,\n        }\n        FIELD_SIZE_COMPAT_RE = r'(?<!%)%\\((?P<field>autonumber|playlist_index)\\)s'\n        mobj = re.search(FIELD_SIZE_COMPAT_RE, outtmpl)\n        if mobj:\n            outtmpl = re.sub(\n                FIELD_SIZE_COMPAT_RE,\n                r'%%(\\1)0%dd' % field_size_compat_map[mobj.group('field')],\n                outtmpl)\n\n        # Missing numeric fields used together with integer presentation types\n        # in format specification will break the argument substitution since\n        # string NA placeholder is returned for missing fields. We will patch\n        # output template for missing fields to meet string presentation type.\n        for numeric_field in self._NUMERIC_FIELDS:\n            if numeric_field not in template_dict:\n                # As of [1] format syntax is:\n                #  %[mapping_key][conversion_flags][minimum_width][.precision][length_modifier]type\n                # 1. https://docs.python.org/2/library/stdtypes.html#string-formatting\n                FORMAT_RE = r'''(?x)\n                    (?<!%)\n                    %\n                    \\({0}\\)  # mapping key\n                    (?:[#0\\-+ ]+)?  # conversion flags (optional)\n                    (?:\\d+)?  # minimum field width (optional)\n                    (?:\\.\\d+)?  # precision (optional)\n                    [hlL]?  # length modifier (optional)\n                    [diouxXeEfFgGcrs%]  # conversion type\n                '''\n                outtmpl = re.sub(\n                    FORMAT_RE.format(numeric_field),\n                    r'%({0})s'.format(numeric_field), outtmpl)\n\n        # expand_path translates '%%' into '%' and '$$' into '$'\n        # correspondingly that is not what we want since we need to keep\n        # '%%' intact for template dict substitution step. Working around\n        # with boundary-alike separator hack.\n        sep = ''.join([random.choice(ascii_letters) for _ in range(32)])\n        outtmpl = outtmpl.replace('%%', '%{0}%'.format(sep)).replace('$$', '${0}$'.format(sep))\n\n        # outtmpl should be expand_path'ed before template dict substitution\n        # because meta fields may contain env variables we don't want to\n        # be expanded. For example, for outtmpl \"%(title)s.%(ext)s\" and\n        # title \"Hello $PATH\", we don't want `$PATH` to be expanded.\n        filename = expand_path(outtmpl).replace(sep, '') % template_dict\n\n        # Temporary fix for #4787\n        # 'Treat' all problem characters by passing filename through preferredencoding\n        # to workaround encoding issues with subprocess on python2 @ Windows\n        if sys.version_info < (3, 0) and sys.platform == 'win32':\n            filename = encodeFilename(filename, True).decode(preferredencoding())\n        return sanitize_path(filename)\n    except ValueError as err:\n        self.report_error('Error in output template: ' + error_to_compat_str(err) + ' (encoding: ' + repr(preferredencoding()) + ')')\n        return None", "loc": 88}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "extract_info", "parameters": ["self", "url", "download", "ie_key", "extra_info", "process", "force_generic_extractor"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ie.ie_key", "ie.suitable", "ie.working", "self.__extract_info", "self.get_info_extractor", "self.report_error", "self.report_warning"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a list with a dictionary for each video extracted. Arguments: url -- URL to extract", "source_code": "def extract_info(self, url, download=True, ie_key=None, extra_info={},\n                 process=True, force_generic_extractor=False):\n    \"\"\"\n    Return a list with a dictionary for each video extracted.\n\n    Arguments:\n    url -- URL to extract\n\n    Keyword arguments:\n    download -- whether to download videos during extraction\n    ie_key -- extractor key hint\n    extra_info -- dictionary containing the extra values to add to each result\n    process -- whether to resolve all unresolved references (URLs, playlist items),\n        must be True for download to work.\n    force_generic_extractor -- force using the generic extractor\n    \"\"\"\n\n    if not ie_key and force_generic_extractor:\n        ie_key = 'Generic'\n\n    if ie_key:\n        ies = [self.get_info_extractor(ie_key)]\n    else:\n        ies = self._ies\n\n    for ie in ies:\n        if not ie.suitable(url):\n            continue\n\n        ie = self.get_info_extractor(ie.ie_key())\n        if not ie.working():\n            self.report_warning('The program functionality for this site has been marked as broken, '\n                                'and will probably not work.')\n\n        return self.__extract_info(url, ie, download, extra_info, process)\n    else:\n        self.report_error('no suitable InfoExtractor for URL %s' % url)", "loc": 37}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "add_default_extra_info", "parameters": ["self", "ie_result", "ie", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ie.ie_key", "self.add_extra_info", "url_basename"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_default_extra_info(self, ie_result, ie, url):\n    self.add_extra_info(ie_result, {\n        'extractor': ie.IE_NAME,\n        'webpage_url': url,\n        'webpage_url_basename': url_basename(url),\n        'extractor_key': ie.ie_key(),\n    })", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "process_subtitles", "parameters": ["self", "video_id", "normal_subtitles", "automatic_captions"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["automatic_captions.items", "available_subs.get", "available_subs.keys", "available_subs.update", "filter", "formats_query.split", "list", "self.params.get", "self.report_warning"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Select the requested subtitles and their format", "source_code": "def process_subtitles(self, video_id, normal_subtitles, automatic_captions):\n    \"\"\"Select the requested subtitles and their format\"\"\"\n    available_subs = {}\n    if normal_subtitles and self.params.get('writesubtitles'):\n        available_subs.update(normal_subtitles)\n    if automatic_captions and self.params.get('writeautomaticsub'):\n        for lang, cap_info in automatic_captions.items():\n            if lang not in available_subs:\n                available_subs[lang] = cap_info\n\n    if (not self.params.get('writesubtitles') and not\n            self.params.get('writeautomaticsub') or not\n            available_subs):\n        return None\n\n    if self.params.get('allsubtitles', False):\n        requested_langs = available_subs.keys()\n    else:\n        if self.params.get('subtitleslangs', False):\n            requested_langs = self.params.get('subtitleslangs')\n        elif 'en' in available_subs:\n            requested_langs = ['en']\n        else:\n            requested_langs = [list(available_subs.keys())[0]]\n\n    formats_query = self.params.get('subtitlesformat', 'best')\n    formats_preference = formats_query.split('/') if formats_query else []\n    subs = {}\n    for lang in requested_langs:\n        formats = available_subs.get(lang)\n        if formats is None:\n            self.report_warning('%s subtitles not available for %s' % (lang, video_id))\n            continue\n        for ext in formats_preference:\n            if ext == 'best':\n                f = formats[-1]\n                break\n            matches = list(filter(lambda f: f['ext'] == ext, formats))\n            if matches:\n                f = matches[-1]\n                break\n        else:\n            f = formats[-1]\n            self.report_warning(\n                'No subtitle format found matching \"%s\" for language %s, '\n                'using %s' % (formats_query, lang, f['ext']))\n        subs[lang] = f\n    return subs", "loc": 48}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "download", "parameters": ["self", "url_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SameFileError", "json.dumps", "len", "self.extract_info", "self.params.get", "self.report_error", "self.sanitize_info", "self.to_screen", "self.to_stdout"], "control_structures": ["For", "If", "Try"], "behavior_type": ["serialization"], "doc_summary": "Download a given list of URLs.", "source_code": "def download(self, url_list):\n    \"\"\"Download a given list of URLs.\"\"\"\n    outtmpl = self.params.get('outtmpl', DEFAULT_OUTTMPL)\n    if (len(url_list) > 1\n            and outtmpl != '-'\n            and '%' not in outtmpl\n            and self.params.get('max_downloads') != 1):\n        raise SameFileError(outtmpl)\n\n    for url in url_list:\n        try:\n            # It also downloads the videos\n            res = self.extract_info(\n                url, force_generic_extractor=self.params.get('force_generic_extractor', False))\n        except UnavailableVideoError:\n            self.report_error('unable to download video')\n        except MaxDownloadsReached:\n            self.to_screen('[info] Maximum number of downloaded files reached.')\n            raise\n        else:\n            if self.params.get('dump_single_json', False):\n                self.to_stdout(json.dumps(self.sanitize_info(res)))\n\n    return self._download_retcode", "loc": 24}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "download_with_info_file", "parameters": ["self", "info_filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info.get", "json.load", "open", "self.download", "self.filter_requested_info", "self.process_ie_result", "self.report_warning"], "control_structures": ["If", "Try"], "behavior_type": ["file_io", "serialization"], "doc_summary": "", "source_code": "def download_with_info_file(self, info_filename):\n    with open(info_filename, encoding='utf-8') as f:\n        info = self.filter_requested_info(json.load(f))\n    try:\n        self.process_ie_result(info, download=True)\n    except DownloadError:\n        webpage_url = info.get('webpage_url')\n        if webpage_url is not None:\n            self.report_warning('The info failed to download, trying with \"%s\"' % webpage_url)\n            return self.download([webpage_url])\n        else:\n            raise\n    return self._download_retcode", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "sanitize_info", "parameters": ["info_dict", "remove_private_keys"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "dict", "filter_fn", "isinstance", "k.startswith", "list", "map", "obj.items", "reject", "repr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sanitize the infodict for converting to json", "source_code": "def sanitize_info(info_dict, remove_private_keys=False):\n    ''' Sanitize the infodict for converting to json '''\n    if info_dict is None:\n        return info_dict\n\n    if remove_private_keys:\n        reject = lambda k, v: (v is None\n                               or k.startswith('__')\n                               or k in ('requested_formats',\n                                        'requested_subtitles'))\n    else:\n        reject = lambda k, v: False\n\n    def filter_fn(obj):\n        if isinstance(obj, dict):\n            return dict((k, filter_fn(v)) for k, v in obj.items() if not reject(k, v))\n        elif isinstance(obj, (list, tuple, set, LazyList)):\n            return list(map(filter_fn, obj))\n        elif obj is None or any(isinstance(obj, c)\n                                for c in (compat_integer_types,\n                                          (compat_str, float, bool))):\n            return obj\n        else:\n            return repr(obj)\n\n    return filter_fn(info_dict)", "loc": 26}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "in_download_archive", "parameters": ["self", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["line.strip", "locked_file", "self._make_archive_id", "self.params.get"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def in_download_archive(self, info_dict):\n    fn = self.params.get('download_archive')\n    if fn is None:\n        return False\n\n    vid_id = self._make_archive_id(info_dict)\n    if not vid_id:\n        return False  # Incomplete video information\n\n    try:\n        with locked_file(fn, 'r', encoding='utf-8') as archive_file:\n            for line in archive_file:\n                if line.strip() == vid_id:\n                    return True\n    except IOError as ioe:\n        if ioe.errno != errno.ENOENT:\n            raise\n    return False", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "record_download_archive", "parameters": ["self", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["archive_file.write", "locked_file", "self._make_archive_id", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def record_download_archive(self, info_dict):\n    fn = self.params.get('download_archive')\n    if fn is None:\n        return\n    vid_id = self._make_archive_id(info_dict)\n    assert vid_id\n    with locked_file(fn, 'a', encoding='utf-8') as archive_file:\n        archive_file.write(vid_id + '\\n')", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "format_resolution", "parameters": ["format", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_resolution(format, default='unknown'):\n    if format.get('vcodec') == 'none':\n        return 'audio only'\n    if format.get('resolution') is not None:\n        return format['resolution']\n    if format.get('height') is not None:\n        if format.get('width') is not None:\n            res = '%sx%s' % (format['width'], format['height'])\n        else:\n            res = '%sp' % format['height']\n    elif format.get('width') is not None:\n        res = '%dx?' % format['width']\n    else:\n        res = default\n    return res", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "list_formats", "parameters": ["self", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.get", "info_dict.get", "len", "render_table", "self._format_note", "self.format_resolution", "self.to_screen"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_formats(self, info_dict):\n    formats = info_dict.get('formats', [info_dict])\n    table = [\n        [f['format_id'], f['ext'], self.format_resolution(f), self._format_note(f)]\n        for f in formats\n        if f.get('preference') is None or f['preference'] >= -1000]\n    if len(formats) > 1:\n        table[-1][-1] += (' ' if table[-1][-1] else '') + '(best)'\n\n    header_line = ['format code', 'extension', 'resolution', 'note']\n    self.to_screen(\n        '[info] Available formats for %s:\\n%s' %\n        (info_dict['id'], render_table(header_line, table)))", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "list_thumbnails", "parameters": ["self", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info_dict.get", "render_table", "self.to_screen", "t.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_thumbnails(self, info_dict):\n    thumbnails = info_dict.get('thumbnails')\n    if not thumbnails:\n        self.to_screen('[info] No thumbnails present for %s' % info_dict['id'])\n        return\n\n    self.to_screen(\n        '[info] Thumbnails for %s:' % info_dict['id'])\n    self.to_screen(render_table(\n        ['ID', 'width', 'height', 'URL'],\n        [[t['id'], t.get('width', 'unknown'), t.get('height', 'unknown'), t['url']] for t in thumbnails]))", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "list_subtitles", "parameters": ["self", "video_id", "subtitles", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "render_table", "reversed", "self.to_screen", "subtitles.items"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def list_subtitles(self, video_id, subtitles, name='subtitles'):\n    if not subtitles:\n        self.to_screen('%s has no %s' % (video_id, name))\n        return\n    self.to_screen(\n        'Available %s for %s:' % (name, video_id))\n    self.to_screen(render_table(\n        ['Language', 'formats'],\n        [[lang, ', '.join(f['ext'] for f in reversed(formats))]\n            for lang, formats in subtitles.items()]))", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "encode", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "s.encode", "self.get_encoding"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def encode(self, s):\n    if isinstance(s, bytes):\n        return s  # Already encoded\n\n    try:\n        return s.encode(self.get_encoding())\n    except UnicodeEncodeError as err:\n        err.reason = err.reason + '. Check your system encoding configuration or use the --encoding option.'\n        raise", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": "YoutubeDL", "function_name": "get_encoding", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["preferredencoding", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_encoding(self):\n    encoding = self.params.get('encoding')\n    if encoding is None:\n        encoding = preferredencoding()\n    return encoding", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "check_deprecated", "parameters": ["param", "option", "suggestion"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.params.get", "self.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_deprecated(param, option, suggestion):\n    if self.params.get(param) is not None:\n        self.report_warning(\n            '%s is deprecated. Use %s instead.' % (option, suggestion))\n        return True\n    return False", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "wrapper", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "compat_str", "e.format_traceback", "encode_compat_str", "error_to_compat_str", "func", "map", "self.params.get", "self.report_error", "traceback.format_exc"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def wrapper(self, *args, **kwargs):\n    try:\n        return func(self, *args, **kwargs)\n    except GeoRestrictedError as e:\n        msg = e.msg\n        if e.countries:\n            msg += '\\nThis video is available in %s.' % ', '.join(\n                map(ISO3166Utils.short2full, e.countries))\n        msg += '\\nYou might want to use a VPN or a proxy server (with --proxy) to workaround.'\n        self.report_error(msg)\n    except ExtractorError as e:  # An error we somewhat expected\n        self.report_error(compat_str(e), tb=e.format_traceback())\n    except MaxDownloadsReached:\n        raise\n    except Exception as e:\n        if self.params.get('ignoreerrors', False):\n            self.report_error(error_to_compat_str(e), tb=encode_compat_str(traceback.format_exc()))\n        else:\n            raise", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "prefer_best", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["can_merge", "info_dict.get", "self.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def prefer_best():\n    if self.params.get('simulate', False):\n        return False\n    if not download:\n        return False\n    if self.params.get('outtmpl', DEFAULT_OUTTMPL) == '-':\n        return True\n    if info_dict.get('is_live'):\n        return True\n    if not can_merge():\n        return True\n    return False", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "syntax_error", "parameters": ["note", "start"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'Invalid format specification: {0}\\n\\t{1}\\n\\t{2}^'.format", "SyntaxError"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def syntax_error(note, start):\n    message = (\n        'Invalid format specification: '\n        '{0}\\n\\t{1}\\n\\t{2}^'.format(note, format_spec, ' ' * start[1]))\n    return SyntaxError(message)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "sanitize_string_field", "parameters": ["info", "string_field"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "info.get", "isinstance", "report_force_conversion"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sanitize_string_field(info, string_field):\n    field = info.get(string_field)\n    if field is None or isinstance(field, compat_str):\n        return\n    report_force_conversion(string_field, 'a string', 'string')\n    info[string_field] = compat_str(field)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "sanitize_numeric_fields", "parameters": ["info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info.get", "int_or_none", "isinstance", "report_force_conversion"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sanitize_numeric_fields(info):\n    for numeric_field in self._NUMERIC_FIELDS:\n        field = info.get(numeric_field)\n        if field is None or isinstance(field, compat_numeric_types):\n            continue\n        report_force_conversion(numeric_field, 'numeric', 'int')\n        info[numeric_field] = int_or_none(field)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "is_wellformed", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.get", "isinstance", "sanitize_string_field", "self.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_wellformed(f):\n    url = f.get('url')\n    if not url:\n        self.report_warning(\n            '\"url\" field is missing or empty - skipping format, '\n            'there is an error in extractor')\n        return False\n    if isinstance(url, bytes):\n        sanitize_string_field(f, 'url')\n    return True", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "filter_fn", "parameters": ["obj"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "dict", "filter_fn", "isinstance", "list", "map", "obj.items", "reject", "repr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def filter_fn(obj):\n    if isinstance(obj, dict):\n        return dict((k, filter_fn(v)) for k, v in obj.items() if not reject(k, v))\n    elif isinstance(obj, (list, tuple, set, LazyList)):\n        return list(map(filter_fn, obj))\n    elif obj is None or any(isinstance(obj, c)\n                            for c in (compat_integer_types,\n                                      (compat_str, float, bool))):\n        return obj\n    else:\n        return repr(obj)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "python_implementation", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["hasattr", "platform.python_implementation"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def python_implementation():\n    impl_name = platform.python_implementation()\n    if impl_name == 'PyPy' and hasattr(sys, 'pypy_version_info'):\n        return impl_name + ' version %d.%d.%d' % sys.pypy_version_info[:3]\n    return impl_name", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "libc_ver", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["platform.libc_ver"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def libc_ver():\n    try:\n        return platform.libc_ver()\n    except OSError:  # We may not have access to the executable\n        return []", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "iter_playlistitems", "parameters": ["format"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format.split", "int", "range", "string_segment.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def iter_playlistitems(format):\n    for string_segment in format.split(','):\n        if '-' in string_segment:\n            start, end = string_segment.split('-')\n            for item in range(int(start), int(end) + 1):\n                yield int(item)\n        else:\n            yield int(string_segment)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "final_selector", "parameters": ["ctx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict", "filter", "list", "selector_function"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def final_selector(ctx):\n    ctx_copy = dict(ctx)\n    for _filter in filters:\n        ctx_copy['formats'] = list(filter(_filter, ctx_copy['formats']))\n    return selector_function(ctx_copy)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "checked_get_suitable_downloader", "parameters": ["info_dict", "params"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_suitable_downloader", "params.get", "self.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def checked_get_suitable_downloader(info_dict, params):\n    ed_args = params.get('external_downloader_args')\n    dler = get_suitable_downloader(info_dict, params)\n    if ed_args and not params.get('external_downloader_args'):\n        # external_downloader_args was cleared because external_downloader was rejected\n        self.report_warning('Requested external downloader cannot be used: '\n                            'ignoring --external-downloader-args.')\n    return dler", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "dl", "parameters": ["name", "info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["checked_get_suitable_downloader", "dict", "fd.add_progress_hook", "fd.download", "info.get", "info.items", "k.startswith", "self._calc_headers", "self.params.get", "self.to_screen"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def dl(name, info):\n    fd = checked_get_suitable_downloader(info, self.params)(self, self.params)\n    for ph in self._progress_hooks:\n        fd.add_progress_hook(ph)\n    if self.params.get('verbose'):\n        self.to_screen('[debug] Invoking downloader on %r' % info.get('url'))\n\n    new_info = dict((k, v) for k, v in info.items() if not k.startswith('__p'))\n    new_info['http_headers'] = self._calc_headers(new_info)\n\n    return fd.download(name, new_info)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "compatible_formats", "parameters": ["formats"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["audio.get", "video.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compatible_formats(formats):\n    video, audio = formats\n    # Check extension\n    video_ext, audio_ext = video.get('ext'), audio.get('ext')\n    if video_ext and audio_ext:\n        COMPATIBLE_EXTS = (\n            ('mp3', 'mp4', 'm4a', 'm4p', 'm4b', 'm4r', 'm4v', 'ismv', 'isma'),\n            ('webm')\n        )\n        for exts in COMPATIBLE_EXTS:\n            if video_ext in exts and audio_ext in exts:\n                return True\n    # TODO: Check acodec/vcodec\n    return False", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "selector_function", "parameters": ["ctx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f", "list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selector_function(ctx):\n    for f in fs:\n        picked_formats = list(f(ctx))\n        if picked_formats:\n            return picked_formats\n    return []", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\YoutubeDL.py", "class_name": null, "function_name": "selector_function", "parameters": ["ctx"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.get", "filter", "list"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def selector_function(ctx):\n    formats = list(ctx['formats'])\n    if not formats:\n        return\n    if format_spec == 'all':\n        for f in formats:\n            yield f\n    elif format_spec in ['best', 'worst', None]:\n        format_idx = 0 if format_spec == 'worst' else -1\n        audiovideo_formats = [\n            f for f in formats\n            if f.get('vcodec') != 'none' and f.get('acodec') != 'none']\n        if audiovideo_formats:\n            yield audiovideo_formats[format_idx]\n        # for extractors with incomplete formats (audio only (soundcloud)\n        # or video only (imgur)) we will fallback to best/worst\n        # {video,audio}-only format\n        elif ctx['incomplete_formats']:\n            yield formats[format_idx]\n    elif format_spec == 'bestaudio':\n        audio_formats = [\n            f for f in formats\n            if f.get('vcodec') == 'none']\n        if audio_formats:\n            yield audio_formats[-1]\n    elif format_spec == 'worstaudio':\n        audio_formats = [\n            f for f in formats\n            if f.get('vcodec') == 'none']\n        if audio_formats:\n            yield audio_formats[0]\n    elif format_spec == 'bestvideo':\n        video_formats = [\n            f for f in formats\n            if f.get('acodec') == 'none']\n        if video_formats:\n            yield video_formats[-1]\n    elif format_spec == 'worstvideo':\n        video_formats = [\n            f for f in formats\n            if f.get('acodec') == 'none']\n        if video_formats:\n            yield video_formats[0]\n    else:\n        extensions = ['mp4', 'flv', 'webm', '3gp', 'm4a', 'mp3', 'ogg', 'aac', 'wav']\n        if format_spec in extensions:\n            filter_f = lambda f: f['ext'] == format_spec\n        else:\n            filter_f = lambda f: f['format_id'] == format_spec\n        matches = list(filter(filter_f, formats))\n        if matches:\n            yield matches[-1]", "loc": 52}
{"file": "youtube-dl\\youtube_dl\\__init__.py", "class_name": null, "function_name": "main", "parameters": ["argv"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_real_main", "sys.exit"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def main(argv=None):\n    try:\n        _real_main(argv)\n    except DownloadError:\n        sys.exit(1)\n    except SameFileError:\n        sys.exit('ERROR: fixed output name but more than one file to download')\n    except KeyboardInterrupt:\n        sys.exit('\\nERROR: Interrupted by user')", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\__init__.py", "class_name": null, "function_name": "parse_retries", "parameters": ["retries"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "int", "parser.error"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_retries(retries):\n    if retries in ('inf', 'infinite'):\n        parsed_retries = float('inf')\n    else:\n        try:\n            parsed_retries = int(retries)\n        except (TypeError, ValueError):\n            parser.error('invalid retry count specified')\n    return parsed_retries", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "format_seconds", "parameters": ["seconds"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["divmod"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def format_seconds(seconds):\n    (mins, secs) = divmod(seconds, 60)\n    (hours, mins) = divmod(mins, 60)\n    if hours > 99:\n        return '--:--:--'\n    if hours == 0:\n        return '%02d:%02d' % (mins, secs)\n    else:\n        return '%02d:%02d:%02d' % (hours, mins, secs)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "calc_eta", "parameters": ["cls", "start_or_rate", "now_or_remaining"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cls.calc_speed", "float", "int", "len", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def calc_eta(cls, start_or_rate, now_or_remaining, *args):\n    if len(args) < 2:\n        rate, remaining = (start_or_rate, now_or_remaining)\n        if None in (rate, remaining):\n            return None\n        return int(float(remaining) / rate)\n    start, now = (start_or_rate, now_or_remaining)\n    total, current = args[:2]\n    if total is None:\n        return None\n    if now is None:\n        now = time.time()\n    rate = cls.calc_speed(start, now, current)\n    return rate and int((float(total) - float(current)) / rate)", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "calc_speed", "parameters": ["start", "now", "bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def calc_speed(start, now, bytes):\n    dif = now - start\n    if bytes == 0 or dif < 0.001:  # One millisecond\n        return None\n    return float(bytes) / dif", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "best_block_size", "parameters": ["elapsed_time", "bytes"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "max", "min"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def best_block_size(elapsed_time, bytes):\n    new_min = max(bytes / 2.0, 1.0)\n    new_max = min(max(bytes * 2.0, 1.0), 4194304)  # Do not surpass 4 MB\n    if elapsed_time < 0.001:\n        return int(new_max)\n    rate = bytes / elapsed_time\n    if rate > new_max:\n        return int(new_max)\n    if rate < new_min:\n        return int(new_min)\n    return int(rate)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "parse_bytes", "parameters": ["bytestr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'bkmgtpezy'.index", "float", "int", "matchobj.group", "matchobj.group(2).lower", "re.match", "round"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Parse a string indicating a byte quantity into an integer.", "source_code": "def parse_bytes(bytestr):\n    \"\"\"Parse a string indicating a byte quantity into an integer.\"\"\"\n    matchobj = re.match(r'(?i)^(\\d+(?:\\.\\d+)?)([kMGTPEZY]?)$', bytestr)\n    if matchobj is None:\n        return None\n    number = float(matchobj.group(1))\n    multiplier = 1024.0 ** 'bkmgtpezy'.index(matchobj.group(2).lower())\n    return int(round(number * multiplier))", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "slow_down", "parameters": ["self", "start_time", "now", "byte_counter"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float", "self.params.get", "time.sleep", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Sleep if the download speed is over the rate limit.", "source_code": "def slow_down(self, start_time, now, byte_counter):\n    \"\"\"Sleep if the download speed is over the rate limit.\"\"\"\n    rate_limit = self.params.get('ratelimit')\n    if rate_limit is None or byte_counter == 0:\n        return\n    if now is None:\n        now = time.time()\n    elapsed = now - start_time\n    if elapsed <= 0.0:\n        return\n    speed = float(byte_counter) / elapsed\n    if speed > rate_limit:\n        sleep_time = float(byte_counter) / rate_limit - elapsed\n        if sleep_time > 0:\n            time.sleep(sleep_time)", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "try_rename", "parameters": ["self", "old_filename", "new_filename"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encodeFilename", "error_to_compat_str", "os.rename", "self.report_error"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def try_rename(self, old_filename, new_filename):\n    try:\n        if old_filename == new_filename:\n            return\n        os.rename(encodeFilename(old_filename), encodeFilename(new_filename))\n    except (IOError, OSError) as err:\n        self.report_error('unable to rename file: %s' % error_to_compat_str(err))", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "report_progress", "parameters": ["self", "s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_bytes", "s.get", "self._report_progress_status", "self.format_eta", "self.format_percent", "self.format_seconds", "self.format_speed", "self.params.get", "self.to_screen"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def report_progress(self, s):\n    if s['status'] == 'finished':\n        if self.params.get('noprogress', False):\n            self.to_screen('[download] Download completed')\n        else:\n            msg_template = '100%%'\n            if s.get('total_bytes') is not None:\n                s['_total_bytes_str'] = format_bytes(s['total_bytes'])\n                msg_template += ' of %(_total_bytes_str)s'\n            if s.get('elapsed') is not None:\n                s['_elapsed_str'] = self.format_seconds(s['elapsed'])\n                msg_template += ' in %(_elapsed_str)s'\n            self._report_progress_status(\n                msg_template % s, is_last_line=True)\n\n    if self.params.get('noprogress'):\n        return\n\n    if s['status'] != 'downloading':\n        return\n\n    if s.get('eta') is not None:\n        s['_eta_str'] = self.format_eta(s['eta'])\n    else:\n        s['_eta_str'] = 'Unknown ETA'\n\n    if s.get('total_bytes') and s.get('downloaded_bytes') is not None:\n        s['_percent_str'] = self.format_percent(100 * s['downloaded_bytes'] / s['total_bytes'])\n    elif s.get('total_bytes_estimate') and s.get('downloaded_bytes') is not None:\n        s['_percent_str'] = self.format_percent(100 * s['downloaded_bytes'] / s['total_bytes_estimate'])\n    else:\n        if s.get('downloaded_bytes') == 0:\n            s['_percent_str'] = self.format_percent(0)\n        else:\n            s['_percent_str'] = 'Unknown %'\n\n    if s.get('speed') is not None:\n        s['_speed_str'] = self.format_speed(s['speed'])\n    else:\n        s['_speed_str'] = 'Unknown speed'\n\n    if s.get('total_bytes') is not None:\n        s['_total_bytes_str'] = format_bytes(s['total_bytes'])\n        msg_template = '%(_percent_str)s of %(_total_bytes_str)s at %(_speed_str)s ETA %(_eta_str)s'\n    elif s.get('total_bytes_estimate') is not None:\n        s['_total_bytes_estimate_str'] = format_bytes(s['total_bytes_estimate'])\n        msg_template = '%(_percent_str)s of ~%(_total_bytes_estimate_str)s at %(_speed_str)s ETA %(_eta_str)s'\n    else:\n        if s.get('downloaded_bytes') is not None:\n            s['_downloaded_bytes_str'] = format_bytes(s['downloaded_bytes'])\n            if s.get('elapsed'):\n                s['_elapsed_str'] = self.format_seconds(s['elapsed'])\n                msg_template = '%(_downloaded_bytes_str)s at %(_speed_str)s (%(_elapsed_str)s)'\n            else:\n                msg_template = '%(_downloaded_bytes_str)s at %(_speed_str)s'\n        else:\n            msg_template = '%(_percent_str)s % at %(_speed_str)s ETA %(_eta_str)s'\n\n    self._report_progress_status(msg_template % s)", "loc": 59}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "report_retry", "parameters": ["self", "err", "count", "retries"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["error_to_compat_str", "self.format_retries", "self.to_screen"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Report retry in case of HTTP error 5xx", "source_code": "def report_retry(self, err, count, retries):\n    \"\"\"Report retry in case of HTTP error 5xx\"\"\"\n    self.to_screen(\n        '[download] Got server HTTP error: %s. Retrying (attempt %d of %s)...'\n        % (error_to_compat_str(err), count, self.format_retries(retries)))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\downloader\\common.py", "class_name": "FileDownloader", "function_name": "report_file_already_downloaded", "parameters": ["self", "file_name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.to_screen"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "Report file has already been fully downloaded.", "source_code": "def report_file_already_downloaded(self, file_name):\n    \"\"\"Report file has already been fully downloaded.\"\"\"\n    try:\n        self.to_screen('[download] %s has already been downloaded' % file_name)\n    except UnicodeEncodeError:\n        self.to_screen('[download] The file has already been downloaded')", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\downloader\\dash.py", "class_name": "DashSegmentsFD", "function_name": "real_download", "parameters": ["self", "filename", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "fragment.get", "headers.copy", "info_dict.get", "itertools.count", "len", "self._append_fragment", "self._download_fragment", "self._finish_frag_download", "self._prepare_and_start_frag_download", "self.params.get", "self.report_error", "self.report_retry_fragment", "self.report_skip_fragment", "urljoin"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def real_download(self, filename, info_dict):\n    fragment_base_url = info_dict.get('fragment_base_url')\n    fragments = info_dict['fragments'][:1] if self.params.get(\n        'test', False) else info_dict['fragments']\n\n    ctx = {\n        'filename': filename,\n        'total_frags': len(fragments),\n    }\n\n    self._prepare_and_start_frag_download(ctx)\n\n    fragment_retries = self.params.get('fragment_retries', 0)\n    skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)\n\n    for frag_index, fragment in enumerate(fragments, 1):\n        if frag_index <= ctx['fragment_index']:\n            continue\n        success = False\n        # In DASH, the first segment contains necessary headers to\n        # generate a valid MP4 file, so always abort for the first segment\n        fatal = frag_index == 1 or not skip_unavailable_fragments\n        fragment_url = fragment.get('url')\n        if not fragment_url:\n            assert fragment_base_url\n            fragment_url = urljoin(fragment_base_url, fragment['path'])\n        headers = info_dict.get('http_headers')\n        fragment_range = fragment.get('range')\n        if fragment_range:\n            headers = headers.copy() if headers else {}\n            headers['Range'] = 'bytes=%s' % (fragment_range,)\n        for count in itertools.count():\n            try:\n                success, frag_content = self._download_fragment(ctx, fragment_url, info_dict, headers)\n                if not success:\n                    return False\n                self._append_fragment(ctx, frag_content)\n            except compat_urllib_error.HTTPError as err:\n                # YouTube may often return 404 HTTP error for a fragment causing the\n                # whole download to fail. However if the same fragment is immediately\n                # retried with the same request data this usually succeeds (1-2 attempts\n                # is usually enough) thus allowing to download the whole file successfully.\n                # To be future-proof we will retry all fragments that fail with any\n                # HTTP error.\n                if count < fragment_retries:\n                    self.report_retry_fragment(err, frag_index, count + 1, fragment_retries)\n                    continue\n            except DownloadError:\n                # Don't retry fragment if error occurred during HTTP downloading\n                # itself since it has its own retry settings\n                if fatal:\n                    raise\n            break\n\n        if not success:\n            if not fatal:\n                self.report_skip_fragment(frag_index)\n                continue\n            self.report_error('giving up after %s fragment retries' % count)\n            return False\n\n    self._finish_frag_download(ctx)\n\n    return True", "loc": 64}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": null, "function_name": "build_fragments_list", "parameters": ["boot_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["itertools.count", "next", "range", "res.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Return a list of (segment, fragment) for each fragment in the video", "source_code": "def build_fragments_list(boot_info):\n    \"\"\" Return a list of (segment, fragment) for each fragment in the video \"\"\"\n    res = []\n    segment_run_table = boot_info['segments'][0]\n    fragment_run_entry_table = boot_info['fragments'][0]['fragments']\n    first_frag_number = fragment_run_entry_table[0]['first']\n    fragments_counter = itertools.count(first_frag_number)\n    for segment, fragments_count in segment_run_table['segment_run']:\n        # In some live HDS streams (for example Rai), `fragments_count` is\n        # abnormal and causing out-of-memory errors. It's OK to change the\n        # number of fragments for live streams as they are updated periodically\n        if fragments_count == 4294967295 and boot_info['live']:\n            fragments_count = 2\n        for _ in range(fragments_count):\n            res.append((segment, next(fragments_counter)))\n\n    if boot_info['live']:\n        res = res[-2:]\n\n    return res", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": null, "function_name": "write_metadata_tag", "parameters": ["stream", "metadata"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "stream.write", "write_unsigned_int", "write_unsigned_int_24"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Writes optional metadata tag to stream", "source_code": "def write_metadata_tag(stream, metadata):\n    \"\"\"Writes optional metadata tag to stream\"\"\"\n    SCRIPT_TAG = b'\\x12'\n    FLV_TAG_HEADER_LEN = 11\n\n    if metadata:\n        stream.write(SCRIPT_TAG)\n        write_unsigned_int_24(stream, len(metadata))\n        stream.write(b'\\x00\\x00\\x00\\x00\\x00\\x00\\x00')\n        stream.write(metadata)\n        write_unsigned_int(stream, FLV_TAG_HEADER_LEN + len(metadata))", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": null, "function_name": "get_base_url", "parameters": ["manifest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_add_ns", "base_url.strip", "xpath_text"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_base_url(manifest):\n    base_url = xpath_text(\n        manifest, [_add_ns('baseURL'), _add_ns('baseURL', 2)],\n        'base URL', default=None)\n    if base_url:\n        base_url = base_url.strip()\n    return base_url", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_bytes", "parameters": ["self", "n"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["DataTruncatedError", "len", "self.read"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_bytes(self, n):\n    data = self.read(n)\n    if len(data) < n:\n        raise DataTruncatedError(\n            'FlvReader error: need %d bytes while only %d bytes got' % (\n                n, len(data)))\n    return data", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_string", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.read_bytes"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_string(self):\n    res = b''\n    while True:\n        char = self.read_bytes(1)\n        if char == b'\\x00':\n            break\n        res += char\n    return res", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_box_info", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.read_bytes", "self.read_unsigned_int", "self.read_unsigned_long_long"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Read a box and return the info as a tuple: (box_size, box_type, box_data)", "source_code": "def read_box_info(self):\n    \"\"\"\n    Read a box and return the info as a tuple: (box_size, box_type, box_data)\n    \"\"\"\n    real_size = size = self.read_unsigned_int()\n    box_type = self.read_bytes(4)\n    header_end = 8\n    if size == 1:\n        real_size = self.read_unsigned_long_long()\n        header_end = 16\n    return real_size, box_type, self.read_bytes(real_size - header_end)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_asrt", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["range", "segments.append", "self.read_bytes", "self.read_string", "self.read_unsigned_char", "self.read_unsigned_int"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_asrt(self):\n    # version\n    self.read_unsigned_char()\n    # flags\n    self.read_bytes(3)\n    quality_entry_count = self.read_unsigned_char()\n    # QualityEntryCount\n    for i in range(quality_entry_count):\n        self.read_string()\n\n    segment_run_count = self.read_unsigned_int()\n    segments = []\n    for i in range(segment_run_count):\n        first_segment = self.read_unsigned_int()\n        fragments_per_segment = self.read_unsigned_int()\n        segments.append((first_segment, fragments_per_segment))\n\n    return {\n        'segment_run': segments,\n    }", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_afrt", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fragments.append", "range", "self.read_bytes", "self.read_string", "self.read_unsigned_char", "self.read_unsigned_int", "self.read_unsigned_long_long"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_afrt(self):\n    # version\n    self.read_unsigned_char()\n    # flags\n    self.read_bytes(3)\n    # time scale\n    self.read_unsigned_int()\n\n    quality_entry_count = self.read_unsigned_char()\n    # QualitySegmentUrlModifiers\n    for i in range(quality_entry_count):\n        self.read_string()\n\n    fragments_count = self.read_unsigned_int()\n    fragments = []\n    for i in range(fragments_count):\n        first = self.read_unsigned_int()\n        first_ts = self.read_unsigned_long_long()\n        duration = self.read_unsigned_int()\n        if duration == 0:\n            discontinuity_indicator = self.read_unsigned_char()\n        else:\n            discontinuity_indicator = None\n        fragments.append({\n            'first': first,\n            'ts': first_ts,\n            'duration': duration,\n            'discontinuity_indicator': discontinuity_indicator,\n        })\n\n    return {\n        'fragments': fragments,\n    }", "loc": 33}
{"file": "youtube-dl\\youtube_dl\\downloader\\f4m.py", "class_name": "FlvReader", "function_name": "read_abst", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["FlvReader", "FlvReader(box_data).read_afrt", "FlvReader(box_data).read_asrt", "fragments.append", "range", "segments.append", "self.read_box_info", "self.read_bytes", "self.read_string", "self.read_unsigned_char", "self.read_unsigned_int", "self.read_unsigned_long_long"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def read_abst(self):\n    # version\n    self.read_unsigned_char()\n    # flags\n    self.read_bytes(3)\n\n    self.read_unsigned_int()  # BootstrapinfoVersion\n    # Profile,Live,Update,Reserved\n    flags = self.read_unsigned_char()\n    live = flags & 0x20 != 0\n    # time scale\n    self.read_unsigned_int()\n    # CurrentMediaTime\n    self.read_unsigned_long_long()\n    # SmpteTimeCodeOffset\n    self.read_unsigned_long_long()\n\n    self.read_string()  # MovieIdentifier\n    server_count = self.read_unsigned_char()\n    # ServerEntryTable\n    for i in range(server_count):\n        self.read_string()\n    quality_count = self.read_unsigned_char()\n    # QualityEntryTable\n    for i in range(quality_count):\n        self.read_string()\n    # DrmData\n    self.read_string()\n    # MetaData\n    self.read_string()\n\n    segments_count = self.read_unsigned_char()\n    segments = []\n    for i in range(segments_count):\n        box_size, box_type, box_data = self.read_box_info()\n        assert box_type == b'asrt'\n        segment = FlvReader(box_data).read_asrt()\n        segments.append(segment)\n    fragments_run_count = self.read_unsigned_char()\n    fragments = []\n    for i in range(fragments_run_count):\n        box_size, box_type, box_data = self.read_box_info()\n        assert box_type == b'afrt'\n        fragments.append(FlvReader(box_data).read_afrt())\n\n    return {\n        'segments': segments,\n        'fragments': fragments,\n        'live': live,\n    }", "loc": 50}
{"file": "youtube-dl\\youtube_dl\\downloader\\fragment.py", "class_name": null, "function_name": "frag_progress_hook", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ctx.get", "s.get", "self._hook_progress", "self.calc_eta", "self.calc_speed", "time.time"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def frag_progress_hook(s):\n    if s['status'] not in ('downloading', 'finished'):\n        return\n\n    if not total_frags and ctx.get('fragment_count'):\n        state['fragment_count'] = ctx['fragment_count']\n\n    time_now = time.time()\n    state['elapsed'] = time_now - start\n    frag_total_bytes = s.get('total_bytes') or 0\n    if not ctx['live']:\n        estimated_size = (\n            (ctx['complete_frags_downloaded_bytes'] + frag_total_bytes)\n            / (state['fragment_index'] + 1) * total_frags)\n        state['total_bytes_estimate'] = estimated_size\n\n    if s['status'] == 'finished':\n        state['fragment_index'] += 1\n        ctx['fragment_index'] = state['fragment_index']\n        state['downloaded_bytes'] += frag_total_bytes - ctx['prev_frag_downloaded_bytes']\n        ctx['complete_frags_downloaded_bytes'] = state['downloaded_bytes']\n        ctx['speed'] = state['speed'] = self.calc_speed(\n            ctx['fragment_started'], time_now, frag_total_bytes)\n        ctx['fragment_started'] = time.time()\n        ctx['prev_frag_downloaded_bytes'] = 0\n    else:\n        frag_downloaded_bytes = s['downloaded_bytes']\n        state['downloaded_bytes'] += frag_downloaded_bytes - ctx['prev_frag_downloaded_bytes']\n        ctx['speed'] = state['speed'] = self.calc_speed(\n            ctx['fragment_started'], time_now, frag_downloaded_bytes - ctx['frag_resume_len'])\n        if not ctx['live']:\n            state['eta'] = self.calc_eta(state['speed'], estimated_size - state['downloaded_bytes'])\n        ctx['prev_frag_downloaded_bytes'] = frag_downloaded_bytes\n    self._hook_progress(state)", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\downloader\\hls.py", "class_name": "HlsFD", "function_name": "can_download", "parameters": ["manifest", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["all", "check_results.append", "info_dict.get", "re.search"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def can_download(manifest, info_dict):\n    UNSUPPORTED_FEATURES = (\n        r'#EXT-X-KEY:METHOD=(?!NONE|AES-128)',  # encrypted streams [1]\n        # r'#EXT-X-BYTERANGE',  # playlists composed of byte ranges of media files [2]\n\n        # Live streams heuristic does not always work (e.g. geo restricted to Germany\n        # http://hls-geo.daserste.de/i/videoportal/Film/c_620000/622873/format,716451,716457,716450,716458,716459,.mp4.csmil/index_4_av.m3u8?null=0)\n        # r'#EXT-X-MEDIA-SEQUENCE:(?!0$)',  # live streams [3]\n\n        # This heuristic also is not correct since segments may not be appended as well.\n        # Twitch vods of finished streams have EXT-X-PLAYLIST-TYPE:EVENT despite\n        # no segments will definitely be appended to the end of the playlist.\n        # r'#EXT-X-PLAYLIST-TYPE:EVENT',  # media segments may be appended to the end of\n        #                                 # event media playlists [4]\n        r'#EXT-X-MAP:',  # media initialization [5]\n\n        # 1. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.2.4\n        # 2. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.2.2\n        # 3. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.3.2\n        # 4. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.3.5\n        # 5. https://tools.ietf.org/html/draft-pantos-http-live-streaming-17#section-4.3.2.5\n    )\n    check_results = [not re.search(feature, manifest) for feature in UNSUPPORTED_FEATURES]\n    is_aes128_enc = '#EXT-X-KEY:METHOD=AES-128' in manifest\n    check_results.append(can_decrypt_frag or not is_aes128_enc)\n    check_results.append(not (is_aes128_enc and r'#EXT-X-BYTERANGE' in manifest))\n    check_results.append(not info_dict.get('is_live'))\n    return all(check_results)", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\downloader\\http.py", "class_name": null, "function_name": "set_range", "parameters": ["req", "start", "end"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "req.add_header"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def set_range(req, start, end):\n    range_header = 'bytes=%d-' % start\n    if end:\n        range_header += compat_str(end)\n    req.add_header('Range', range_header)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\downloader\\ism.py", "class_name": null, "function_name": "extract_box_data", "parameters": ["data", "box_sequence"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data_reader.read", "data_reader.seek", "extract_box_data", "io.BytesIO", "len", "u32.unpack"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_box_data(data, box_sequence):\n    data_reader = io.BytesIO(data)\n    while True:\n        box_size = u32.unpack(data_reader.read(4))[0]\n        box_type = data_reader.read(4)\n        if box_type == box_sequence[0]:\n            box_data = data_reader.read(box_size - 8)\n            if len(box_sequence) == 1:\n                return box_data\n            return extract_box_data(box_data, box_sequence[1:])\n        data_reader.seek(box_size - 8, 1)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\downloader\\ism.py", "class_name": "IsmFD", "function_name": "real_download", "parameters": ["self", "filename", "info_dict"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "extract_box_data", "len", "self._append_fragment", "self._download_fragment", "self._finish_frag_download", "self._prepare_and_start_frag_download", "self.params.get", "self.report_error", "self.report_retry_fragment", "self.report_skip_fragment", "u32.unpack", "write_piff_header"], "control_structures": ["For", "If", "Try", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def real_download(self, filename, info_dict):\n    segments = info_dict['fragments'][:1] if self.params.get(\n        'test', False) else info_dict['fragments']\n\n    ctx = {\n        'filename': filename,\n        'total_frags': len(segments),\n    }\n\n    self._prepare_and_start_frag_download(ctx)\n\n    fragment_retries = self.params.get('fragment_retries', 0)\n    skip_unavailable_fragments = self.params.get('skip_unavailable_fragments', True)\n\n    track_written = False\n    frag_index = 0\n    for i, segment in enumerate(segments):\n        frag_index += 1\n        if frag_index <= ctx['fragment_index']:\n            continue\n        count = 0\n        while count <= fragment_retries:\n            try:\n                success, frag_content = self._download_fragment(ctx, segment['url'], info_dict)\n                if not success:\n                    return False\n                if not track_written:\n                    tfhd_data = extract_box_data(frag_content, [b'moof', b'traf', b'tfhd'])\n                    info_dict['_download_params']['track_id'] = u32.unpack(tfhd_data[4:8])[0]\n                    write_piff_header(ctx['dest_stream'], info_dict['_download_params'])\n                    track_written = True\n                self._append_fragment(ctx, frag_content)\n                break\n            except compat_urllib_error.HTTPError as err:\n                count += 1\n                if count <= fragment_retries:\n                    self.report_retry_fragment(err, frag_index, count, fragment_retries)\n        if count > fragment_retries:\n            if skip_unavailable_fragments:\n                self.report_skip_fragment(frag_index)\n                continue\n            self.report_error('giving up after %s fragment retries' % fragment_retries)\n            return False\n\n    self._finish_frag_download(ctx)\n\n    return True", "loc": 47}
{"file": "youtube-dl\\youtube_dl\\extractor\\abcnews.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AbcNewsVideoIE.ie_key", "article_contents.get", "featured_video.get", "inline.get", "parse_duration", "parse_iso8601", "self.url_result", "story.get", "try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    featured_video = story.get('featuredVideo') or {}\n    feed = try_get(featured_video, lambda x: x['video']['feed'])\n    if feed:\n        yield {\n            '_type': 'url',\n            'id': featured_video.get('id'),\n            'title': featured_video.get('name'),\n            'url': feed,\n            'thumbnail': featured_video.get('images'),\n            'description': featured_video.get('description'),\n            'timestamp': parse_iso8601(featured_video.get('uploadDate')),\n            'duration': parse_duration(featured_video.get('duration')),\n            'ie_key': AbcNewsVideoIE.ie_key(),\n        }\n\n    for inline in (article_contents.get('inlines') or []):\n        inline_type = inline.get('type')\n        if inline_type == 'iframe':\n            iframe_url = try_get(inline, lambda x: x['attrs']['src'])\n            if iframe_url:\n                yield self.url_result(iframe_url)\n        elif inline_type == 'video':\n            video_id = inline.get('id')\n            if video_id:\n                yield {\n                    '_type': 'url',\n                    'id': video_id,\n                    'url': 'http://abcnews.go.com/video/embed?id=' + video_id,\n                    'thumbnail': inline.get('imgSrc') or inline.get('imgDefault'),\n                    'description': inline.get('description'),\n                    'duration': parse_duration(inline.get('duration')),\n                    'ie_key': AbcNewsVideoIE.ie_key(),\n                }", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\extractor\\adobepass.py", "class_name": null, "function_name": "post_form", "parameters": ["form_page_res", "note", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_urlparse.urljoin", "form_data.update", "re.match", "self._download_webpage_handle", "self._hidden_inputs", "self._html_search_regex", "urlencode_postdata", "urlh.geturl"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def post_form(form_page_res, note, data={}):\n    form_page, urlh = form_page_res\n    post_url = self._html_search_regex(r'<form[^>]+action=([\"\\'])(?P<url>.+?)\\1', form_page, 'post url', group='url')\n    if not re.match(r'https?://', post_url):\n        post_url = compat_urlparse.urljoin(urlh.geturl(), post_url)\n    form_data = self._hidden_inputs(form_page)\n    form_data.update(data)\n    return self._download_webpage_handle(\n        post_url, video_id, note, data=urlencode_postdata(form_data), headers={\n            'Content-Type': 'application/x-www-form-urlencoded',\n        })", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\adobepass.py", "class_name": null, "function_name": "extract_redirect_url", "parameters": ["html", "url", "fatal"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_urlparse.urljoin", "self._search_regex", "unescapeHTML"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_redirect_url(html, url=None, fatal=False):\n    # TODO: eliminate code duplication with generic extractor and move\n    # redirection code into _download_webpage_handle\n    REDIRECT_REGEX = r'[0-9]{,2};\\s*(?:URL|url)=\\'?([^\\'\"]+)'\n    redirect_url = self._search_regex(\n        r'(?i)<meta\\s+(?=(?:[a-z-]+=\"[^\"]+\"\\s+)*http-equiv=\"refresh\")'\n        r'(?:[a-z-]+=\"[^\"]+\"\\s+)*?content=\"%s' % REDIRECT_REGEX,\n        html, 'meta refresh redirect',\n        default=NO_DEFAULT if fatal else None, fatal=fatal)\n    if not redirect_url:\n        return None\n    if url:\n        redirect_url = compat_urlparse.urljoin(url, unescapeHTML(redirect_url))\n    return redirect_url", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\extractor\\afreecatv.py", "class_name": "AfreecaTVIE", "function_name": "parse_video_key", "parameters": ["key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "m.group", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_video_key(key):\n    video_key = {}\n    m = re.match(r'^(?P<upload_date>\\d{8})_\\w+_(?P<part>\\d+)$', key)\n    if m:\n        video_key['upload_date'] = m.group('upload_date')\n        video_key['part'] = int(m.group('part'))\n    return video_key", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\animeondemand.py", "class_name": null, "function_name": "extract_entries", "parameters": ["html", "video_id", "common_info", "num"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["common_info.copy", "extract_info", "f.update", "m.group", "m.group('kind').lower", "re.search", "self._sort_formats", "urljoin"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_entries(html, video_id, common_info, num=None):\n    info = extract_info(html, video_id, num)\n\n    if info['formats']:\n        self._sort_formats(info['formats'])\n        f = common_info.copy()\n        f.update(info)\n        yield f\n\n    # Extract teaser/trailer only when full episode is not available\n    if not info['formats']:\n        m = re.search(\n            r'data-dialog-header=([\"\\'])(?P<title>.+?)\\1[^>]+href=([\"\\'])(?P<href>.+?)\\3[^>]*>(?P<kind>Teaser|Trailer)<',\n            html)\n        if m:\n            f = common_info.copy()\n            f.update({\n                'id': '%s-%s' % (f['id'], m.group('kind').lower()),\n                'title': m.group('title'),\n                'url': urljoin(url, m.group('href')),\n            })\n            yield f", "loc": 22}
{"file": "youtube-dl\\youtube_dl\\extractor\\animeondemand.py", "class_name": null, "function_name": "extract_episodes", "parameters": ["html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "extract_entries", "int", "re.findall", "self._search_regex"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_episodes(html):\n    for num, episode_html in enumerate(re.findall(\n            r'(?s)<h3[^>]+class=\"episodebox-title\".+?>Episodeninhalt<', html), 1):\n        episodebox_title = self._search_regex(\n            (r'class=\"episodebox-title\"[^>]+title=([\"\\'])(?P<title>.+?)\\1',\n             r'class=\"episodebox-title\"[^>]+>(?P<title>.+?)<'),\n            episode_html, 'episodebox title', default=None, group='title')\n        if not episodebox_title:\n            continue\n\n        episode_number = int(self._search_regex(\n            r'(?:Episode|Film)\\s*(\\d+)',\n            episodebox_title, 'episode number', default=num))\n        episode_title = self._search_regex(\n            r'(?:Episode|Film)\\s*\\d+\\s*-\\s*(.+)',\n            episodebox_title, 'episode title', default=None)\n\n        video_id = 'episode-%d' % episode_number\n\n        common_info = {\n            'id': video_id,\n            'series': anime_title,\n            'episode': episode_title,\n            'episode_number': episode_number,\n        }\n\n        for e in extract_entries(episode_html, video_id, common_info):\n            yield e", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\extractor\\animeondemand.py", "class_name": null, "function_name": "extract_film", "parameters": ["html", "video_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_entries"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_film(html, video_id):\n    common_info = {\n        'id': anime_id,\n        'title': anime_title,\n        'description': anime_description,\n    }\n    for e in extract_entries(html, video_id, common_info):\n        yield e", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\animeondemand.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_episodes", "extract_film"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    has_episodes = False\n    for e in extract_episodes(webpage):\n        has_episodes = True\n        yield e\n\n    if not has_episodes:\n        for e in extract_film(webpage, anime_id):\n            yield e", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\appletrailers.py", "class_name": null, "function_name": "fix_html", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["m.group", "m.group(1).replace", "re.sub"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fix_html(s):\n    s = re.sub(r'(?s)<script[^<]*?>.*?</script>', '', s)\n    s = re.sub(r'<img ([^<]*?)/?>', r'<img \\1/>', s)\n    # The ' in the onClick attributes are not escaped, it couldn't be parsed\n    # like: http://trailers.apple.com/trailers/wb/gravity/\n\n    def _clean_json(m):\n        return 'iTunes.playURL(%s);' % m.group(1).replace('\\'', '&#39;')\n    s = re.sub(self._JSON_RE, _clean_json, s)\n    s = '<html>%s</html>' % s\n    return s", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\bbc.py", "class_name": null, "function_name": "get_programme_id", "parameters": ["item"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_from_attributes", "item.find", "item.get", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_programme_id(item):\n    def get_from_attributes(item):\n        for p in ('identifier', 'group'):\n            value = item.get(p)\n            if value and re.match(r'^[pb][\\da-z]{7}$', value):\n                return value\n    get_from_attributes(item)\n    mediator = item.find('./{%s}mediator' % self._EMP_PLAYLIST_NS)\n    if mediator is not None:\n        return get_from_attributes(mediator)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\bbc.py", "class_name": null, "function_name": "parse_media", "parameters": ["media"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'\\n\\n'.join", "entries.append", "item.get", "meta.get", "self._download_media_selector", "self._sort_formats", "strip_or_none", "summary.append", "try_get", "unified_timestamp"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_media(media):\n    if not media:\n        return\n    for item in (try_get(media, lambda x: x['media']['items'], list) or []):\n        item_id = item.get('id')\n        item_title = item.get('title')\n        if not (item_id and item_title):\n            continue\n        formats, subtitles = self._download_media_selector(item_id)\n        self._sort_formats(formats)\n        item_desc = None\n        blocks = try_get(media, lambda x: x['summary']['blocks'], list)\n        if blocks:\n            summary = []\n            for block in blocks:\n                text = try_get(block, lambda x: x['model']['text'], compat_str)\n                if text:\n                    summary.append(text)\n            if summary:\n                item_desc = '\\n\\n'.join(summary)\n        item_time = None\n        for meta in try_get(media, lambda x: x['metadata']['items'], list) or []:\n            if try_get(meta, lambda x: x['label']) == 'Published':\n                item_time = unified_timestamp(meta.get('timestamp'))\n                break\n        entries.append({\n            'id': item_id,\n            'title': item_title,\n            'thumbnail': item.get('holdingImageUrl'),\n            'formats': formats,\n            'subtitles': subtitles,\n            'timestamp': item_time,\n            'description': strip_or_none(item_desc),\n        })", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\extractor\\bbc.py", "class_name": null, "function_name": "get_from_attributes", "parameters": ["item"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["item.get", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_from_attributes(item):\n    for p in ('identifier', 'group'):\n        value = item.get(p)\n        if value and re.match(r'^[pb][\\da-z]{7}$', value):\n            return value", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\brightcove.py", "class_name": null, "function_name": "find_param", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data_url_params.get", "find_xpath_attr"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_param(name):\n    if name in flashvars:\n        return flashvars[name]\n    node = find_xpath_attr(object_doc, './param', 'name', name)\n    if node is not None:\n        return node.attrib['value']\n    return data_url_params.get(name)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\brightcove.py", "class_name": null, "function_name": "extract_policy_key", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["catalog.get", "js_to_json", "self._download_json", "self._download_webpage", "self._parse_json", "self._search_regex", "store_pk", "try_get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_policy_key():\n    base_url = 'http://players.brightcove.net/%s/%s_%s/' % (account_id, player_id, embed)\n    config = self._download_json(\n        base_url + 'config.json', video_id, fatal=False) or {}\n    policy_key = try_get(\n        config, lambda x: x['video_cloud']['policy_key'])\n    if not policy_key:\n        webpage = self._download_webpage(\n            base_url + 'index.min.js', video_id)\n\n        catalog = self._search_regex(\n            r'catalog\\(({.+?})\\);', webpage, 'catalog', default=None)\n        if catalog:\n            catalog = self._parse_json(\n                js_to_json(catalog), video_id, fatal=False)\n            if catalog:\n                policy_key = catalog.get('policyKey')\n\n        if not policy_key:\n            policy_key = self._search_regex(\n                r'policyKey\\s*:\\s*([\"\\'])(?P<pk>.+?)\\1',\n                webpage, 'policy key', group='pk')\n\n    store_pk(policy_key)\n    return policy_key", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\extractor\\brightcove.py", "class_name": null, "function_name": "build_format_id", "parameters": ["kind"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_format_id(kind):\n    format_id = kind\n    if tbr:\n        format_id += '-%dk' % int(tbr)\n    if height:\n        format_id += '-%dp' % height\n    return format_id", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\cartoonnetwork.py", "class_name": null, "function_name": "find_field", "parameters": ["global_re", "name", "content_re", "value_re", "fatal"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_field(global_re, name, content_re=None, value_re='[^\"]+', fatal=False):\n    metadata_re = ''\n    if content_re:\n        metadata_re = r'|video_metadata\\.content_' + content_re\n    return self._search_regex(\n        r'(?:_cnglobal\\.currentVideo\\.%s%s)\\s*=\\s*\"(%s)\";' % (global_re, metadata_re, value_re),\n        webpage, name, fatal=fatal)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\cda.py", "class_name": null, "function_name": "extract_format", "parameters": ["page", "version"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["codecs.decode", "decrypt_file", "f.update", "info_dict['formats'].append", "int", "m.group", "parse_duration", "player_data.get", "re.search", "self._html_search_regex", "self._parse_json", "self.report_warning", "video.get", "video['file'].endswith", "video['file'].replace", "video['file'].startswith"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_format(page, version):\n    json_str = self._html_search_regex(\n        r'player_data=(\\\\?[\"\\'])(?P<player_data>.+?)\\1', page,\n        '%s player_json' % version, fatal=False, group='player_data')\n    if not json_str:\n        return\n    player_data = self._parse_json(\n        json_str, '%s player_data' % version, fatal=False)\n    if not player_data:\n        return\n    video = player_data.get('video')\n    if not video or 'file' not in video:\n        self.report_warning('Unable to extract %s version information' % version)\n        return\n    if video['file'].startswith('uggc'):\n        video['file'] = codecs.decode(video['file'], 'rot_13')\n        if video['file'].endswith('adc.mp4'):\n            video['file'] = video['file'].replace('adc.mp4', '.mp4')\n    elif not video['file'].startswith('http'):\n        video['file'] = decrypt_file(video['file'])\n    f = {\n        'url': video['file'],\n    }\n    m = re.search(\n        r'<a[^>]+data-quality=\"(?P<format_id>[^\"]+)\"[^>]+href=\"[^\"]+\"[^>]+class=\"[^\"]*quality-btn-active[^\"]*\">(?P<height>[0-9]+)p',\n        page)\n    if m:\n        f.update({\n            'format_id': m.group('format_id'),\n            'height': int(m.group('height')),\n        })\n    info_dict['formats'].append(f)\n    if not info_dict['duration']:\n        info_dict['duration'] = parse_duration(video.get('duration'))", "loc": 34}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "initialize", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._initialize_geo_bypass", "self._real_initialize"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "Initializes an instance (authentication, etc).", "source_code": "def initialize(self):\n    \"\"\"Initializes an instance (authentication, etc).\"\"\"\n    self._initialize_geo_bypass({\n        'countries': self._GEO_COUNTRIES,\n        'ip_blocks': self._GEO_IP_BLOCKS,\n    })\n    if not self._ready:\n        self._real_initialize()\n        self._ready = True", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "extract", "parameters": ["self", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "range", "self.__maybe_fake_ip_and_retry", "self._real_extract", "self.initialize"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Extracts URL information and returns it in list of dicts.", "source_code": "def extract(self, url):\n    \"\"\"Extracts URL information and returns it in list of dicts.\"\"\"\n    try:\n        for _ in range(2):\n            try:\n                self.initialize()\n                ie_result = self._real_extract(url)\n                if self._x_forwarded_for_ip:\n                    ie_result['__x_forwarded_for_ip'] = self._x_forwarded_for_ip\n                return ie_result\n            except GeoRestrictedError as e:\n                if self.__maybe_fake_ip_and_retry(e.countries):\n                    continue\n                raise\n    except ExtractorError:\n        raise\n    except compat_http_client.IncompleteRead as e:\n        raise ExtractorError('A network error has occurred.', cause=e, expected=True)\n    except (KeyError, StopIteration) as e:\n        raise ExtractorError('An extractor error has occurred.', cause=e)", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "report_warning", "parameters": ["self", "msg"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.pop", "len", "self.__ie_msg", "self._downloader.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def report_warning(self, msg, *args, **kwargs):\n    if len(args) > 0:\n        video_id = args[0]\n        args = args[1:]\n    else:\n        video_id = kwargs.pop('video_id', None)\n    idstr = '' if video_id is None else '%s: ' % video_id\n    self._downloader.report_warning(\n        self.__ie_msg(idstr, msg), *args, **kwargs)", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "get_param", "parameters": ["self", "name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kwargs.pop", "len", "self._downloader.params.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_param(self, name, *args, **kwargs):\n    default, args = (args[0], args[1:]) if len(args) > 0 else (kwargs.pop('default', None), args)\n    if self._downloader:\n        return self._downloader.params.get(name, default, *args, **kwargs)\n    return default", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "raise_no_formats", "parameters": ["self", "msg", "expected", "video_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "isinstance", "self.get_param", "self.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def raise_no_formats(self, msg, expected=False, video_id=None):\n    if expected and (\n            self.get_param('ignore_no_formats_error') or self.get_param('wait_for_video')):\n        self.report_warning(msg, video_id)\n    elif isinstance(msg, ExtractorError):\n        raise msg\n    else:\n        raise ExtractorError(msg, expected=expected, video_id=video_id)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "url_result", "parameters": ["url", "ie", "video_id", "video_title"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def url_result(url, ie=None, video_id=None, video_title=None):\n    \"\"\"Returns a URL that points to a page that should be processed\"\"\"\n    # TODO: ie should be the class used for getting the info\n    video_info = {'_type': 'url',\n                  'url': url,\n                  'ie_key': ie}\n    if video_id is not None:\n        video_info['id'] = video_id\n    if video_title is not None:\n        video_info['title'] = video_title\n    return video_info", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "playlist_from_matches", "parameters": ["self", "matches", "playlist_id", "playlist_title", "getter", "ie"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getter", "orderedSet", "self._proto_relative_url", "self.playlist_result", "self.url_result"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def playlist_from_matches(self, matches, playlist_id=None, playlist_title=None, getter=None, ie=None):\n    urls = orderedSet(\n        self.url_result(self._proto_relative_url(getter(m) if getter else m), ie)\n        for m in matches)\n    return self.playlist_result(\n        urls, playlist_id=playlist_id, playlist_title=playlist_title)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "playlist_result", "parameters": ["entries", "playlist_id", "playlist_title", "playlist_description"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def playlist_result(entries, playlist_id=None, playlist_title=None, playlist_description=None):\n    \"\"\"Returns a playlist\"\"\"\n    video_info = {'_type': 'playlist',\n                  'entries': entries}\n    if playlist_id:\n        video_info['id'] = playlist_id\n    if playlist_title:\n        video_info['title'] = playlist_title\n    if playlist_description:\n        video_info['description'] = playlist_description\n    return video_info", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "get_testcases", "parameters": ["self", "include_onlymatching"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["getattr", "hasattr", "len", "t.get", "type"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_testcases(self, include_onlymatching=False):\n    t = getattr(self, '_TEST', None)\n    if t:\n        assert not hasattr(self, '_TESTS'), \\\n            '%s has _TEST and _TESTS' % type(self).__name__\n        tests = [t]\n    else:\n        tests = getattr(self, '_TESTS', [])\n    for t in tests:\n        if not include_onlymatching and t.get('only_matching', False):\n            continue\n        t['name'] = type(self).__name__[:-len('IE')]\n        yield t", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "is_suitable", "parameters": ["self", "age_limit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["age_restricted", "self.get_testcases", "tc.get", "tc.get('info_dict', {}).get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Test whether the extractor is generally suitable for the given age limit (i.e. pornographic sites are not, all others usually are)", "source_code": "def is_suitable(self, age_limit):\n    \"\"\" Test whether the extractor is generally suitable for the given\n    age limit (i.e. pornographic sites are not, all others usually are) \"\"\"\n\n    any_restricted = False\n    for tc in self.get_testcases(include_onlymatching=False):\n        if tc.get('playlist', []):\n            tc = tc['playlist'][0]\n        is_restricted = age_restricted(\n            tc.get('info_dict', {}).get('age_limit'), age_limit)\n        if not is_restricted:\n            return True\n        any_restricted = any_restricted or is_restricted\n    return not any_restricted", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "extract_subtitles", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_subtitles", "self.get_param"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_subtitles(self, *args, **kwargs):\n    if (self.get_param('writesubtitles', False)\n            or self.get_param('listsubtitles')):\n        return self._get_subtitles(*args, **kwargs)\n    return {}", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "extract_automatic_captions", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_automatic_captions", "self.get_param"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_automatic_captions(self, *args, **kwargs):\n    if (self.get_param('writeautomaticsub', False)\n            or self.get_param('listsubtitles')):\n        return self._get_automatic_captions(*args, **kwargs)\n    return {}", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "mark_watched", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._get_login_info", "self._mark_watched", "self.get_param"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mark_watched(self, *args, **kwargs):\n    if (self.get_param('mark_watched', False)\n            and (self._get_login_info()[0] is not None\n                 or self.get_param('cookiefile') is not None)):\n        self._mark_watched(*args, **kwargs)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": "InfoExtractor", "function_name": "geo_verification_headers", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self.get_param"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def geo_verification_headers(self):\n    headers = {}\n    geo_verification_proxy = self.get_param('geo_verification_proxy')\n    if geo_verification_proxy:\n        headers['Ytdl-request-proxy'] = geo_verification_proxy\n    return headers", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_interaction_type", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["e.get", "interaction_type.get", "isinstance", "str_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_interaction_type(e):\n    interaction_type = e.get('interactionType')\n    if isinstance(interaction_type, dict):\n        interaction_type = interaction_type.get('@type')\n    return str_or_none(interaction_type)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_interaction_statistic", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["INTERACTION_TYPE_MAP.get", "e.get", "extract_interaction_type", "info.get", "interaction_type.split", "is_e.get", "isinstance", "str_to_int"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_interaction_statistic(e):\n    interaction_statistic = e.get('interactionStatistic')\n    if isinstance(interaction_statistic, dict):\n        interaction_statistic = [interaction_statistic]\n    if not isinstance(interaction_statistic, list):\n        return\n    for is_e in interaction_statistic:\n        if not isinstance(is_e, dict):\n            continue\n        if is_e.get('@type') != 'InteractionCounter':\n            continue\n        interaction_type = extract_interaction_type(is_e)\n        if not interaction_type:\n            continue\n        # For interaction count some sites provide string instead of\n        # an integer (as per spec) with non digit characters (e.g. \",\")\n        # so extracting count with more relaxed str_to_int\n        interaction_count = str_to_int(is_e.get('userInteractionCount'))\n        if interaction_count is None:\n            continue\n        count_kind = INTERACTION_TYPE_MAP.get(interaction_type.split('/')[-1])\n        if not count_kind:\n            continue\n        count_key = '%s_count' % count_kind\n        if info.get(count_key) is not None:\n            continue\n        info[count_key] = interaction_count", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_video_object", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["author.get", "e.get", "extract_interaction_statistic", "float_or_none", "info.update", "int_or_none", "isinstance", "parse_duration", "unescapeHTML", "unified_timestamp", "url_or_none"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_video_object(e):\n    assert e['@type'] == 'VideoObject'\n    author = e.get('author')\n    info.update({\n        'url': url_or_none(e.get('contentUrl')),\n        'title': unescapeHTML(e.get('name')),\n        'description': unescapeHTML(e.get('description')),\n        'thumbnail': url_or_none(e.get('thumbnailUrl') or e.get('thumbnailURL')),\n        'duration': parse_duration(e.get('duration')),\n        'timestamp': unified_timestamp(e.get('uploadDate')),\n        # author can be an instance of 'Organization' or 'Person' types.\n        # both types can have 'name' property(inherited from 'Thing' type). [1]\n        # however some websites are using 'Text' type instead.\n        # 1. https://schema.org/VideoObject\n        'uploader': author.get('name') if isinstance(author, dict) else author if isinstance(author, compat_str) else None,\n        'filesize': float_or_none(e.get('contentSize')),\n        'tbr': int_or_none(e.get('bitrate')),\n        'width': int_or_none(e.get('width')),\n        'height': int_or_none(e.get('height')),\n        'view_count': int_or_none(e.get('interactionCount')),\n    })\n    extract_interaction_statistic(e)", "loc": 22}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_media", "parameters": ["x_media_line"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'-'.join", "format_id.append", "format_url", "formats.append", "groups.setdefault", "groups.setdefault(group_id, []).append", "media.get", "parse_m3u8_attributes"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_media(x_media_line):\n    media = parse_m3u8_attributes(x_media_line)\n    # As per [1, 4.3.4.1] TYPE, GROUP-ID and NAME are REQUIRED\n    media_type, group_id, name = media.get('TYPE'), media.get('GROUP-ID'), media.get('NAME')\n    if not (media_type and group_id and name):\n        return\n    groups.setdefault(group_id, []).append(media)\n    if media_type not in ('VIDEO', 'AUDIO'):\n        return\n    media_url = media.get('URI')\n    if media_url:\n        format_id = []\n        for v in (m3u8_id, group_id, name):\n            if v:\n                format_id.append(v)\n        f = {\n            'format_id': '-'.join(format_id),\n            'url': format_url(media_url),\n            'manifest_url': m3u8_url,\n            'language': media.get('LANGUAGE'),\n            'ext': ext,\n            'protocol': entry_protocol,\n            'preference': preference,\n        }\n        if media_type == 'AUDIO':\n            f['vcodec'] = 'none'\n        formats.append(f)", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "build_stream_name", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["groups.get", "last_stream_inf.get", "rendition.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_stream_name():\n    # Despite specification does not mention NAME attribute for\n    # EXT-X-STREAM-INF tag it still sometimes may be present (see [1]\n    # or vidio test in TestInfoExtractor.test_parse_m3u8_formats)\n    # 1. http://www.vidio.com/watch/165683-dj_ambred-booyah-live-2015\n    stream_name = last_stream_inf.get('NAME')\n    if stream_name:\n        return stream_name\n    # If there is no NAME in EXT-X-STREAM-INF it will be obtained\n    # from corresponding rendition group\n    stream_group_id = last_stream_inf.get('VIDEO')\n    if not stream_group_id:\n        return\n    stream_group = groups.get(stream_group_id)\n    if not stream_group:\n        return stream_group_id\n    rendition = stream_group[0]\n    return rendition.get('NAME') or stream_group_id", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "resolve_base_url", "parameters": ["element", "parent_base_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["T", "_add_ns", "compat_urlparse.urljoin", "e.find", "fix_path", "traverse_obj"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve_base_url(element, parent_base_url=None):\n    # TODO: use native XML traversal when ready\n    b_url = traverse_obj(element, (\n        T(lambda e: e.find(_add_ns('BaseURL')).text)))\n    if parent_base_url and b_url:\n        if not parent_base_url[-1] in ('/', ':'):\n            parent_base_url += '/'\n        b_url = compat_urlparse.urljoin(parent_base_url, b_url)\n    if b_url:\n        b_url = fix_path(b_url)\n    return b_url or parent_base_url", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_multisegment_info", "parameters": ["element", "ms_parent_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["T", "_add_ns", "element.find", "extract_Initialization", "extract_common", "float", "initialization.get", "int", "ms_info.get", "ms_info['s'].append", "ms_parent_info.copy", "re.findall", "resolve_base_url", "s.get", "segment_list.findall", "segment_template.get", "segment_timeline.findall", "source.find", "source.get", "traverse_obj"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_multisegment_info(element, ms_parent_info):\n    ms_info = ms_parent_info.copy()\n    base_url = ms_info['base_url'] = resolve_base_url(element, ms_info.get('base_url'))\n\n    # As per [1, 5.3.9.2.2] SegmentList and SegmentTemplate share some\n    # common attributes and elements.  We will only extract relevant\n    # for us.\n    def extract_common(source):\n        segment_timeline = source.find(_add_ns('SegmentTimeline'))\n        if segment_timeline is not None:\n            s_e = segment_timeline.findall(_add_ns('S'))\n            if s_e:\n                ms_info['total_number'] = 0\n                ms_info['s'] = []\n                for s in s_e:\n                    r = int(s.get('r', 0))\n                    ms_info['total_number'] += 1 + r\n                    ms_info['s'].append({\n                        't': int(s.get('t', 0)),\n                        # @d is mandatory (see [1, 5.3.9.6.2, Table 17, page 60])\n                        'd': int(s.attrib['d']),\n                        'r': r,\n                    })\n        start_number = source.get('startNumber')\n        if start_number:\n            ms_info['start_number'] = int(start_number)\n        timescale = source.get('timescale')\n        if timescale:\n            ms_info['timescale'] = int(timescale)\n        segment_duration = source.get('duration')\n        if segment_duration:\n            ms_info['segment_duration'] = float(segment_duration)\n\n    def extract_Initialization(source):\n        initialization = source.find(_add_ns('Initialization'))\n        if initialization is not None:\n            ms_info['initialization_url'] = initialization.get('sourceURL') or base_url\n            initialization_url_range = initialization.get('range')\n            if initialization_url_range:\n                ms_info['initialization_url_range'] = initialization_url_range\n\n    segment_list = element.find(_add_ns('SegmentList'))\n    if segment_list is not None:\n        extract_common(segment_list)\n        extract_Initialization(segment_list)\n        segment_urls_e = segment_list.findall(_add_ns('SegmentURL'))\n        segment_urls = traverse_obj(segment_urls_e, (\n            Ellipsis, T(lambda e: e.attrib), 'media'))\n        if segment_urls:\n            ms_info['segment_urls'] = segment_urls\n        segment_urls_range = traverse_obj(segment_urls_e, (\n            Ellipsis, T(lambda e: e.attrib), 'mediaRange',\n            T(lambda r: re.findall(r'^\\d+-\\d+$', r)), 0))\n        if segment_urls_range:\n            ms_info['segment_urls_range'] = segment_urls_range\n            if not segment_urls:\n                ms_info['segment_urls'] = [base_url for _ in segment_urls_range]\n    else:\n        segment_template = element.find(_add_ns('SegmentTemplate'))\n        if segment_template is not None:\n            extract_common(segment_template)\n            media = segment_template.get('media')\n            if media:\n                ms_info['media'] = media\n            initialization = segment_template.get('initialization')\n            if initialization:\n                ms_info['initialization'] = initialization\n            else:\n                extract_Initialization(segment_template)\n    return ms_info", "loc": 70}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "parse_content_type", "parameters": ["content_type"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ctr.groups", "mimetype2ext", "parse_codecs", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_content_type(content_type):\n    if not content_type:\n        return {}\n    ctr = re.search(r'(?P<mimetype>[^/]+/[^;]+)(?:;\\s*codecs=\"?(?P<codecs>[^\"]+))?', content_type)\n    if ctr:\n        mimetype, codecs = ctr.groups()\n        f = parse_codecs(codecs)\n        f['ext'] = mimetype2ext(mimetype)\n        return f\n    return {}", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "manifest_url", "parameters": ["manifest"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def manifest_url(manifest):\n    m_url = '%s/%s' % (http_base_url, manifest)\n    if query:\n        m_url += '?%s' % query\n    return m_url", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_common", "parameters": ["source"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_add_ns", "float", "int", "ms_info['s'].append", "s.get", "segment_timeline.findall", "source.find", "source.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_common(source):\n    segment_timeline = source.find(_add_ns('SegmentTimeline'))\n    if segment_timeline is not None:\n        s_e = segment_timeline.findall(_add_ns('S'))\n        if s_e:\n            ms_info['total_number'] = 0\n            ms_info['s'] = []\n            for s in s_e:\n                r = int(s.get('r', 0))\n                ms_info['total_number'] += 1 + r\n                ms_info['s'].append({\n                    't': int(s.get('t', 0)),\n                    # @d is mandatory (see [1, 5.3.9.6.2, Table 17, page 60])\n                    'd': int(s.attrib['d']),\n                    'r': r,\n                })\n    start_number = source.get('startNumber')\n    if start_number:\n        ms_info['start_number'] = int(start_number)\n    timescale = source.get('timescale')\n    if timescale:\n        ms_info['timescale'] = int(timescale)\n    segment_duration = source.get('duration')\n    if segment_duration:\n        ms_info['segment_duration'] = float(segment_duration)", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "extract_Initialization", "parameters": ["source"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["_add_ns", "initialization.get", "source.find"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_Initialization(source):\n    initialization = source.find(_add_ns('Initialization'))\n    if initialization is not None:\n        ms_info['initialization_url'] = initialization.get('sourceURL') or base_url\n        initialization_url_range = initialization.get('range')\n        if initialization_url_range:\n            ms_info['initialization_url_range'] = initialization_url_range", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "prepare_template", "parameters": ["template_name", "identifiers"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'|'.join", "re.sub", "t.replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def prepare_template(template_name, identifiers):\n    tmpl = representation_ms_info[template_name]\n    # First of, % characters outside $...$ templates\n    # must be escaped by doubling for proper processing\n    # by % operator string formatting used further (see\n    # https://github.com/ytdl-org/youtube-dl/issues/16867).\n    t = ''\n    in_template = False\n    for c in tmpl:\n        t += c\n        if c == '$':\n            in_template = not in_template\n        elif c == '%' and not in_template:\n            t += c\n    # Next, $...$ templates are translated to their\n    # %(...) counterparts to be used with % operator\n    t = t.replace('$RepresentationID$', representation_id)\n    t = re.sub(r'\\$(%s)\\$' % '|'.join(identifiers), r'%(\\1)d', t)\n    t = re.sub(r'\\$(%s)%%([^$]+)\\$' % '|'.join(identifiers), r'%(\\1)\\2', t)\n    t.replace('$$', '$')\n    return t", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\extractor\\common.py", "class_name": null, "function_name": "add_segment_url", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float_or_none", "representation_ms_info['fragments'].append"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_segment_url():\n    segment_url = media_template % {\n        'Time': segment_time,\n        'Bandwidth': bandwidth,\n        'Number': segment_number,\n    }\n    representation_ms_info['fragments'].append({\n        media_location_key: segment_url,\n        'duration': float_or_none(segment_d, representation_ms_info['timescale']),\n    })", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\crunchyroll.py", "class_name": null, "function_name": "obfuscate_key_aux", "parameters": ["count", "modulo", "start"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["list", "map", "output.append", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def obfuscate_key_aux(count, modulo, start):\n    output = list(start)\n    for _ in range(count):\n        output.append(output[-1] + output[-2])\n    # cut off start values\n    output = output[2:]\n    output = list(map(lambda x: x % modulo + 33, output))\n    return output", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\crunchyroll.py", "class_name": null, "function_name": "obfuscate_key", "parameters": ["key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bytes_to_intlist", "floor", "int", "intlist_to_bytes", "obfuscate_key_aux", "pow", "sha1", "sha1(prefix + str(num4).encode('ascii')).digest", "sqrt", "str", "str(num4).encode"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def obfuscate_key(key):\n    num1 = int(floor(pow(2, 25) * sqrt(6.9)))\n    num2 = (num1 ^ key) << 5\n    num3 = key ^ num1\n    num4 = num3 ^ (num3 >> 3) ^ num2\n    prefix = intlist_to_bytes(obfuscate_key_aux(20, 97, (1, 2)))\n    shaHash = bytes_to_intlist(sha1(prefix + str(num4).encode('ascii')).digest())\n    # Extend 160 Bit hash to 256 Bit\n    return shaHash + [0] * 12", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\crunchyroll.py", "class_name": null, "function_name": "ass_bool", "parameters": ["strvalue"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ass_bool(strvalue):\n    assvalue = '0'\n    if strvalue == '1':\n        assvalue = '-1'\n    return assvalue", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\drtv.py", "class_name": null, "function_name": "decrypt_uri", "parameters": ["e"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'%s:sRBzYNXBzkKgnjj8pGtkACch' % a.encode", "aes_cbc_decrypt", "bytes_to_intlist", "hashlib.sha256", "hashlib.sha256(('%s:sRBzYNXBzkKgnjj8pGtkACch' % a).encode('utf-8')).digest", "hex_to_bytes", "int", "intlist_to_bytes", "intlist_to_bytes(decrypted[:-decrypted[-1]]).decode", "intlist_to_bytes(decrypted[:-decrypted[-1]]).decode('utf-8').split"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decrypt_uri(e):\n    n = int(e[2:10], 16)\n    a = e[10 + n:]\n    data = bytes_to_intlist(hex_to_bytes(e[10:10 + n]))\n    key = bytes_to_intlist(hashlib.sha256(\n        ('%s:sRBzYNXBzkKgnjj8pGtkACch' % a).encode('utf-8')).digest())\n    iv = bytes_to_intlist(hex_to_bytes(a))\n    decrypted = aes_cbc_decrypt(data, key, iv)\n    return intlist_to_bytes(\n        decrypted[:-decrypted[-1]]).decode('utf-8').split('?')[0]", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\epidemicsound.py", "class_name": null, "function_name": "fmt_or_none", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fmt_or_none(f):\n    if not f.get('format'):\n        f['format'] = f.get('format_id')\n    elif not f.get('format_id'):\n        f['format_id'] = f['format']\n    if not (f['url'] and f['format']):\n        return\n    if f.get('format_note'):\n        f['format_note'] = 'track ID ' + f['format_note']\n    f['preference'] = -1 if f['format'] == 'full' else -2\n    return f", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\espn.py", "class_name": null, "function_name": "traverse_source", "parameters": ["source", "base_source_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_source", "isinstance", "source.items", "traverse_source"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def traverse_source(source, base_source_id=None):\n    for source_id, source in source.items():\n        if source_id == 'alert':\n            continue\n        elif isinstance(source, compat_str):\n            extract_source(source, base_source_id)\n        elif isinstance(source, dict):\n            traverse_source(\n                source,\n                '%s-%s' % (base_source_id, source_id)\n                if base_source_id else source_id)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\espn.py", "class_name": null, "function_name": "extract_source", "parameters": ["source_url", "source_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["OnceIE.suitable", "determine_ext", "f.update", "format_urls.add", "formats.append", "formats.extend", "int", "mobj.group", "re.search", "self._extract_f4m_formats", "self._extract_m3u8_formats", "self._extract_once_formats", "self._extract_smil_formats"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_source(source_url, source_id=None):\n    if source_url in format_urls:\n        return\n    format_urls.add(source_url)\n    ext = determine_ext(source_url)\n    if OnceIE.suitable(source_url):\n        formats.extend(self._extract_once_formats(source_url))\n    elif ext == 'smil':\n        formats.extend(self._extract_smil_formats(\n            source_url, video_id, fatal=False))\n    elif ext == 'f4m':\n        formats.extend(self._extract_f4m_formats(\n            source_url, video_id, f4m_id=source_id, fatal=False))\n    elif ext == 'm3u8':\n        formats.extend(self._extract_m3u8_formats(\n            source_url, video_id, 'mp4', entry_protocol='m3u8_native',\n            m3u8_id=source_id, fatal=False))\n    else:\n        f = {\n            'url': source_url,\n            'format_id': source_id,\n        }\n        mobj = re.search(r'(\\d+)p(\\d+)_(\\d+)k\\.', source_url)\n        if mobj:\n            f.update({\n                'height': int(mobj.group(1)),\n                'fps': int(mobj.group(2)),\n                'tbr': int(mobj.group(3)),\n            })\n        if source_id == 'mezzanine':\n            f['preference'] = 1\n        formats.append(f)", "loc": 32}
{"file": "youtube-dl\\youtube_dl\\extractor\\europa.py", "class_name": null, "function_name": "get_item", "parameters": ["type_", "preference"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["items.get", "label.strip", "playlist.findall", "xpath_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_item(type_, preference):\n    items = {}\n    for item in playlist.findall('./info/%s/item' % type_):\n        lang, label = xpath_text(item, 'lg', default=None), xpath_text(item, 'label', default=None)\n        if lang and label:\n            items[lang] = label.strip()\n    for p in preference:\n        if items.get(p):\n            return items[p]", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\expressen.py", "class_name": null, "function_name": "extract_data", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parse_json", "self._search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_data(name):\n    return self._parse_json(\n        self._search_regex(\n            r'data-%s=([\"\\'])(?P<value>(?:(?!\\1).)+)\\1' % name,\n            webpage, 'info', group='value'),\n        display_id, transform_source=unescapeHTML)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\facebook.py", "class_name": null, "function_name": "extract_video_data", "parameters": ["instances"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["try_get", "video_data.append", "video_item.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_video_data(instances):\n    video_data = []\n    for item in instances:\n        if try_get(item, lambda x: x[1][0]) == 'VideoConfig':\n            video_item = item[2][0]\n            if video_item.get('video_id'):\n                video_data.append(video_item['videoData'])\n    return video_data", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\facebook.py", "class_name": null, "function_name": "process_formats", "parameters": ["formats"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f.setdefault", "self._sort_formats"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_formats(formats):\n    # Downloads with browser's User-Agent are rate limited. Working around\n    # with non-browser User-Agent.\n    for f in formats:\n        f.setdefault('http_headers', {})['User-Agent'] = 'facebookexternalhit/1.1'\n\n    self._sort_formats(formats)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\facebook.py", "class_name": null, "function_name": "extract_relay_prefetched_data", "parameters": ["_filter"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_relay_data", "replay_data.get", "try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_relay_prefetched_data(_filter):\n    replay_data = extract_relay_data(_filter)\n    for require in (replay_data.get('require') or []):\n        if require[0] == 'RelayPrefetchedStreamCache':\n            return try_get(require, lambda x: x[3][1]['__bbox']['result']['data'], dict) or {}", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\facebook.py", "class_name": null, "function_name": "parse_graphql_video", "parameters": ["video"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["entries.append", "extract_dash_manifest", "float_or_none", "formats.append", "info.update", "int_or_none", "process_formats", "q", "qualities", "try_get", "video.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_graphql_video(video):\n    formats = []\n    q = qualities(['sd', 'hd'])\n    for (suffix, format_id) in [('', 'sd'), ('_quality_hd', 'hd')]:\n        playable_url = video.get('playable_url' + suffix)\n        if not playable_url:\n            continue\n        formats.append({\n            'format_id': format_id,\n            'quality': q(format_id),\n            'url': playable_url,\n        })\n    extract_dash_manifest(video, formats)\n    process_formats(formats)\n    v_id = video.get('videoId') or video.get('id') or video_id\n    info = {\n        'id': v_id,\n        'formats': formats,\n        'thumbnail': try_get(video, lambda x: x['thumbnailImage']['uri']),\n        'uploader_id': try_get(video, lambda x: x['owner']['id']),\n        'timestamp': int_or_none(video.get('publish_time')),\n        'duration': float_or_none(video.get('playable_duration_in_ms'), 1000),\n    }\n    description = try_get(video, lambda x: x['savable_description']['text'])\n    title = video.get('name')\n    if title:\n        info.update({\n            'title': title,\n            'description': description,\n        })\n    else:\n        info['title'] = description or 'Facebook video #%s' % v_id\n    entries.append(info)", "loc": 33}
{"file": "youtube-dl\\youtube_dl\\extractor\\francetv.py", "class_name": null, "function_name": "sign", "parameters": ["manifest_url", "manifest_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._download_webpage", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sign(manifest_url, manifest_id):\n    for host in ('hdfauthftv-a.akamaihd.net', 'hdfauth.francetv.fr'):\n        signed_url = url_or_none(self._download_webpage(\n            'https://%s/esi/TA' % host, video_id,\n            'Downloading signed %s manifest URL' % manifest_id,\n            fatal=False, query={\n                'url': manifest_url,\n            }))\n        if signed_url:\n            return signed_url\n    return manifest_url", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\generic.py", "class_name": null, "function_name": "getlicensetoken", "parameters": ["license"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "abs", "compat_str", "int", "len", "license.replace", "license.replace('$', '').replace", "parts", "range"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getlicensetoken(license):\n    modlicense = license.replace('$', '').replace('0', '1')\n    center = int(len(modlicense) / 2)\n    fronthalf = int(modlicense[:center + 1])\n    backhalf = int(modlicense[center:])\n\n    modlicense = compat_str(4 * abs(fronthalf - backhalf))\n\n    def parts():\n        for o in range(0, center + 1):\n            for i in range(1, 5):\n                yield compat_str((int(license[o + i]) + int(modlicense[o])) % 10)\n\n    return ''.join(parts())", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\extractor\\generic.py", "class_name": null, "function_name": "getrealurl", "parameters": ["video_url", "license_code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "'/'.join", "getlicensetoken", "int", "len", "range", "spells", "sum", "url_path.split", "video_url.partition", "video_url.startswith", "{l: x[o], o: x[l]}.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def getrealurl(video_url, license_code):\n    if not video_url.startswith('function/0/'):\n        return video_url  # not obfuscated\n\n    url_path, _, url_query = video_url.partition('?')\n    urlparts = url_path.split('/')[2:]\n    license = getlicensetoken(license_code)\n    newmagic = urlparts[5][:32]\n\n    def spells(x, o):\n        l = (o + sum(int(n) for n in license[o:])) % 32\n        for i in range(0, len(x)):\n            yield {l: x[o], o: x[l]}.get(i, x[i])\n\n    for o in range(len(newmagic) - 1, -1, -1):\n        newmagic = ''.join(spells(newmagic, o))\n\n    urlparts[5] = newmagic + urlparts[5][32:]\n    return '/'.join(urlparts) + '?' + url_query", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\extractor\\generic.py", "class_name": null, "function_name": "check_video", "parameters": ["vurl"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["RtmpIE.suitable", "YoutubeIE.suitable", "compat_urlparse.urlparse", "determine_ext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_video(vurl):\n    if YoutubeIE.suitable(vurl):\n        return True\n    if RtmpIE.suitable(vurl):\n        return True\n    vpath = compat_urlparse.urlparse(vurl).path\n    vext = determine_ext(vpath)\n    return '.' in vpath and vext not in ('swf', 'png', 'jpg', 'srt', 'sbv', 'sub', 'vtt', 'ttml', 'js', 'xml')", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\googledrive.py", "class_name": null, "function_name": "add_source_format", "parameters": ["urlh"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "determine_ext(title, 'mp4').lower", "formats.append", "urlh.geturl"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_source_format(urlh):\n    formats.append({\n        # Use redirect URLs as download URLs in order to calculate\n        # correct cookies in _calc_cookies.\n        # Using original URLs may result in redirect loop due to\n        # google.com's cookies mistakenly used for googleusercontent.com\n        # redirect URLs (see #23919).\n        'url': urlh.geturl(),\n        'ext': determine_ext(title, 'mp4').lower(),\n        'format_id': 'source',\n        'quality': 1,\n    })", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\heise.py", "class_name": null, "function_name": "extract_title", "parameters": ["default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._html_search_meta", "self._html_search_regex", "self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_title(default=NO_DEFAULT):\n    title = self._html_search_meta(\n        ('fulltitle', 'title'), webpage, default=None)\n    if not title or title == \"c't\":\n        title = self._search_regex(\n            r'<div[^>]+class=\"videoplayerjw\"[^>]+data-title=\"([^\"]+)\"',\n            webpage, 'title', default=None)\n    if not title:\n        title = self._html_search_regex(\n            r'<h1[^>]+\\bclass=[\"\\']article_page_title[^>]+>(.+?)<',\n            webpage, 'title', default=default)\n    return title", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\hrfernsehen.py", "class_name": "HRFernsehenIE", "function_name": "extract_airdate", "parameters": ["self", "loader_data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["loader_data.get", "loader_data.get('mediaMetadata', {}).get", "loader_data.get('mediaMetadata', {}).get('agf', {}).get", "unified_timestamp"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_airdate(self, loader_data):\n    airdate_str = loader_data.get('mediaMetadata', {}).get('agf', {}).get('airdate')\n\n    if airdate_str is None:\n        return None\n\n    return unified_timestamp(airdate_str)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\hrfernsehen.py", "class_name": "HRFernsehenIE", "function_name": "extract_formats", "parameters": ["self", "loader_data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "quality_information.group", "re.search", "self._sort_formats", "str", "stream_formats.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(self, loader_data):\n    stream_formats = []\n    for stream_obj in loader_data[\"videoResolutionLevels\"]:\n        stream_format = {\n            'format_id': str(stream_obj['verticalResolution']) + \"p\",\n            'height': stream_obj['verticalResolution'],\n            'url': stream_obj['url'],\n        }\n\n        quality_information = re.search(r'([0-9]{3,4})x([0-9]{3,4})-([0-9]{2})p-([0-9]{3,4})kbit',\n                                        stream_obj['url'])\n        if quality_information:\n            stream_format['width'] = int_or_none(quality_information.group(1))\n            stream_format['height'] = int_or_none(quality_information.group(2))\n            stream_format['fps'] = int_or_none(quality_information.group(3))\n            stream_format['tbr'] = int_or_none(quality_information.group(4))\n\n        stream_formats.append(stream_format)\n\n    self._sort_formats(stream_formats)\n    return stream_formats", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\extractor\\ign.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["IGNIE.ie_key", "article.get", "re.findall", "self.url_result", "traverse_obj", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    media_url = traverse_obj(\n        article, ('mediaRelations', 0, 'media', 'metadata', 'url'),\n        expected_type=url_or_none)\n    if media_url:\n        yield self.url_result(media_url, IGNIE.ie_key())\n    for content in (article.get('content') or []):\n        for video_url in re.findall(r'(?:\\[(?:ignvideo\\s+url|youtube\\s+clip_id)|<iframe[^>]+src)=\"([^\"]+)\"', content):\n            if url_or_none(video_url):\n                yield self.url_result(video_url)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\ign.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_parse_qs", "extract_attributes", "extract_attributes(flashvars).get", "flashvars.get", "m.group", "re.finditer", "self._search_regex", "self.url_result", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    for m in re.finditer(\n            r'''(?s)<object\\b[^>]+\\bclass\\s*=\\s*(\"|')ign-videoplayer\\1[^>]*>(?P<params>.+?)</object''',\n            webpage):\n        flashvars = self._search_regex(\n            r'''(<param\\b[^>]+\\bname\\s*=\\s*(\"|')flashvars\\2[^>]*>)''',\n            m.group('params'), 'flashvars', default='')\n        flashvars = compat_parse_qs(extract_attributes(flashvars).get('value') or '')\n        v_url = url_or_none((flashvars.get('url') or [None])[-1])\n        if v_url:\n            yield self.url_result(v_url)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\ign.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["k.startswith", "merge_dicts", "player.replace", "self._extract_video_info", "traverse_obj"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    for player in traverse_obj(\n            nextjs_data,\n            ('props', 'apolloState', 'ROOT_QUERY', lambda k, _: k.startswith('videoPlayerProps('), '__ref')):\n        # skip promo links (which may not always be served, eg GH CI servers)\n        if traverse_obj(nextjs_data,\n                        ('props', 'apolloState', player.replace('PlayerProps', 'ModernContent')),\n                        expected_type=dict):\n            continue\n        video = traverse_obj(nextjs_data, ('props', 'apolloState', player), expected_type=dict) or {}\n        info = self._extract_video_info(video, fatal=False)\n        if info:\n            yield merge_dicts({\n                'display_id': display_id,\n            }, info)", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\extractor\\imgur.py", "class_name": null, "function_name": "mung_format", "parameters": ["fmt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["fmt.update"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def mung_format(fmt, *extra):\n    fmt.update({\n        'http_headers': {\n            'User-Agent': 'youtube-dl (like wget)',\n        },\n    })\n    for d in extra:\n        fmt.update(d)\n    return fmt", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\imgur.py", "class_name": null, "function_name": "yield_media_ids", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["T", "traverse_obj", "v.get"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yield_media_ids():\n    for m_id in traverse_obj(data, (\n            'media', lambda _, v: v.get('type') == 'video' or v['metadata']['is_animated'],\n            'id', T(txt_or_none))):\n        yield m_id", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\instagram.py", "class_name": null, "function_name": "get_count", "parameters": ["keys", "kind"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "isinstance", "try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_count(keys, kind):\n    if not isinstance(keys, (list, tuple)):\n        keys = [keys]\n    for key in keys:\n        count = int_or_none(try_get(\n            media, (lambda x: x['edge_media_%s' % key]['count'],\n                    lambda x: x['%ss' % kind]['count'])))\n        if count is not None:\n            return count", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\iprima.py", "class_name": null, "function_name": "extract_formats", "parameters": ["format_url", "format_key", "lang"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "f.get", "formats.extend", "self._extract_m3u8_formats", "self._extract_mpd_formats"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(format_url, format_key=None, lang=None):\n    ext = determine_ext(format_url)\n    new_formats = []\n    if format_key == 'hls' or ext == 'm3u8':\n        new_formats = self._extract_m3u8_formats(\n            format_url, video_id, 'mp4', entry_protocol='m3u8_native',\n            m3u8_id='hls', fatal=False)\n    elif format_key == 'dash' or ext == 'mpd':\n        return\n        new_formats = self._extract_mpd_formats(\n            format_url, video_id, mpd_id='dash', fatal=False)\n    if lang:\n        for f in new_formats:\n            if not f.get('language'):\n                f['language'] = lang\n    formats.extend(new_formats)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiSDK", "function_name": "preprocess", "parameters": ["self", "chunksize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["chunks.append", "list", "map", "md5_text", "range", "self.ip.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def preprocess(self, chunksize):\n    self.target = md5_text(self.target)\n    chunks = []\n    for i in range(32 // chunksize):\n        chunks.append(self.target[chunksize * i:chunksize * (i + 1)])\n    if 32 % chunksize:\n        chunks.append(self.target[32 - 32 % chunksize:])\n    return chunks, list(map(int, self.ip.split('.')))", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiSDK", "function_name": "split", "parameters": ["self", "chunksize"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "len", "range", "self.preprocess"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def split(self, chunksize):\n    modulus_map = {\n        4: 256,\n        5: 10,\n        8: 100,\n    }\n\n    chunks, ip = self.preprocess(chunksize)\n    ret = ''\n    for i in range(len(chunks)):\n        ip_part = compat_str(ip[i] % modulus_map[chunksize]) if i < 4 else ''\n        if chunksize == 8:\n            ret += ip_part + chunks[i]\n        else:\n            ret += chunks[i] + ip_part\n    self.target = ret", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiSDK", "function_name": "handle_input8", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["md5_text", "range", "self.split_sum"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def handle_input8(self):\n    self.target = md5_text(self.target)\n    ret = ''\n    for i in range(4):\n        part = self.target[8 * i:8 * (i + 1)]\n        ret += self.split_sum(part) + part\n    self.target = ret", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiSDK", "function_name": "date", "parameters": ["self", "scheme"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "compat_str", "list", "map", "md5_text", "time.localtime"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def date(self, scheme):\n    self.target = md5_text(self.target)\n    d = time.localtime(self.timestamp)\n    strings = {\n        'y': compat_str(d.tm_year),\n        'm': '%02d' % d.tm_mon,\n        'd': '%02d' % d.tm_mday,\n    }\n    self.target += ''.join(map(lambda c: strings[c], list(scheme)))", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiSDKInterpreter", "function_name": "run", "parameters": ["self", "target", "ip", "timestamp"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "IqiyiSDK", "decode_packed_codes", "int", "re.findall", "re.match", "sdk.date", "sdk.mod", "sdk.split"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, target, ip, timestamp):\n    self.sdk_code = decode_packed_codes(self.sdk_code)\n\n    functions = re.findall(r'input=([a-zA-Z0-9]+)\\(input', self.sdk_code)\n\n    sdk = IqiyiSDK(target, ip, timestamp)\n\n    other_functions = {\n        'handleSum': sdk.handleSum,\n        'handleInput8': sdk.handle_input8,\n        'handleInput16': sdk.handle_input16,\n        'splitTimeEvenOdd': sdk.split_time_even_odd,\n        'splitTimeOddEven': sdk.split_time_odd_even,\n        'splitIpTimeSum': sdk.split_ip_time_sum,\n        'splitTimeIpSum': sdk.split_time_ip_sum,\n    }\n    for function in functions:\n        if re.match(r'mod\\d+', function):\n            sdk.mod(int(function[3:]))\n        elif re.match(r'date[ymd]{3}', function):\n            sdk.date(function[4:])\n        elif re.match(r'split\\d+', function):\n            sdk.split(int(function[5:]))\n        elif function in other_functions:\n            other_functions[function]()\n        else:\n            raise ExtractorError('Unknown function %s' % function)\n\n    return sdk.target", "loc": 29}
{"file": "youtube-dl\\youtube_dl\\extractor\\iqiyi.py", "class_name": "IqiyiIE", "function_name": "get_raw_data", "parameters": ["self", "tvid", "video_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "int", "md5_text", "remove_start", "self._download_json", "self.geo_verification_headers", "time.time"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_raw_data(self, tvid, video_id):\n    tm = int(time.time() * 1000)\n\n    key = 'd5fb4bd9d50c4be6948c97edd7254b0e'\n    sc = md5_text(compat_str(tm) + key + tvid)\n    params = {\n        'tvid': tvid,\n        'vid': video_id,\n        'src': '76f90cbd92f94a2e925d83e8ccd22cb7',\n        'sc': sc,\n        't': tm,\n    }\n\n    return self._download_json(\n        'http://cache.m.iqiyi.com/jp/tmts/%s/%s/' % (tvid, video_id),\n        video_id, transform_source=lambda s: remove_start(s, 'var tvInfoJs='),\n        query=params, headers=self.geo_verification_headers())", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\extractor\\itv.py", "class_name": null, "function_name": "get_thumbnails", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "tns.append", "traverse_obj", "traverse_obj(video_data, 'imagePresets', expected_type=dict) or {}.items", "x.items"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_thumbnails():\n    tns = []\n    for w, x in (traverse_obj(video_data, ('imagePresets'), expected_type=dict) or {}).items():\n        if isinstance(x, dict):\n            for y, z in x.items():\n                tns.append({'id': w + '_' + y, 'url': z})\n    return tns or None", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\itv.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["BrightcoveNewIE.ie_key", "data.get", "re.findall", "self.url_result", "smuggle_url"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n\n    for data in content or []:\n        video_id = data.get('id')\n        if not video_id:\n            continue\n        account = data.get('accountId') or self.BRIGHTCOVE_ACCOUNT\n        player = data.get('playerId') or self.BRIGHTCOVE_PLAYER\n        yield self.url_result(\n            smuggle_url(self.BRIGHTCOVE_URL_TEMPLATE % (account, player, video_id), contraband),\n            ie=BrightcoveNewIE.ie_key(), video_id=video_id)\n\n    # obsolete ?\n    for video_id in re.findall(r'''data-video-id=[\"'](\\d+)''', webpage):\n        yield self.url_result(\n            smuggle_url(self.BRIGHTCOVE_URL_TEMPLATE % (self.BRIGHTCOVE_ACCOUNT, self.BRIGHTCOVE_PLAYER, video_id), contraband),\n            ie=BrightcoveNewIE.ie_key(), video_id=video_id)", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\extractor\\kaltura.py", "class_name": null, "function_name": "sign_url", "parameters": ["unsigned_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": [], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sign_url(unsigned_url):\n    if ks:\n        unsigned_url += '/ks/%s' % ks\n    if referrer:\n        unsigned_url += '?referrer=%s' % referrer\n    return unsigned_url", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\keezmovies.py", "class_name": null, "function_name": "extract_format", "parameters": ["format_url", "height"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["aes_decrypt_text", "aes_decrypt_text(video_url, title, 32).decode", "format_url.startswith", "format_urls.add", "formats.append", "int_or_none", "self._search_regex", "url_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_format(format_url, height=None):\n    format_url = url_or_none(format_url)\n    if not format_url or not format_url.startswith(('http', '//')):\n        return\n    if format_url in format_urls:\n        return\n    format_urls.add(format_url)\n    tbr = int_or_none(self._search_regex(\n        r'[/_](\\d+)[kK][/_]', format_url, 'tbr', default=None))\n    if not height:\n        height = int_or_none(self._search_regex(\n            r'[/_](\\d+)[pP][/_]', format_url, 'height', default=None))\n    if encrypted:\n        format_url = aes_decrypt_text(\n            video_url, title, 32).decode('utf-8')\n    formats.append({\n        'url': format_url,\n        'format_id': '%dp' % height if height else None,\n        'height': height,\n        'tbr': tbr,\n    })", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\extractor\\kuwo.py", "class_name": null, "function_name": "page_func", "parameters": ["page_num"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_urlparse.urljoin", "re.findall", "self._download_webpage", "self.url_result"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def page_func(page_num):\n    webpage = self._download_webpage(\n        'http://www.kuwo.cn/artist/contentMusicsAjax',\n        singer_id, note='Download song list page #%d' % (page_num + 1),\n        errnote='Unable to get song list page #%d' % (page_num + 1),\n        query={'artistId': artist_id, 'pn': page_num, 'rn': self.PAGE_SIZE})\n\n    return [\n        self.url_result(compat_urlparse.urljoin(url, song_url), 'Kuwo')\n        for song_url in re.findall(\n            r'<div[^>]+class=\"name\"><a[^>]+href=\"(/yinyue/\\d+)',\n            webpage)\n    ]", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\extractor\\laola1tv.py", "class_name": null, "function_name": "get_flashvar", "parameters": ["x"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_flashvar(x, *args, **kwargs):\n    flash_var = self._search_regex(\n        r'%s\\s*:\\s*\"([^\"]+)\"' % x,\n        flash_vars, x, default=None)\n    if not flash_var:\n        flash_var = self._search_regex([\n            r'flashvars\\.%s\\s*=\\s*\"([^\"]+)\"' % x,\n            r'%s\\s*=\\s*\"([^\"]+)\"' % x],\n            webpage, x, *args, **kwargs)\n    return flash_var", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\leeco.py", "class_name": "LeIE", "function_name": "ror", "parameters": ["self", "param1", "param2"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["urshift"], "control_structures": ["While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ror(self, param1, param2):\n    _loc3_ = 0\n    while _loc3_ < param2:\n        param1 = urshift(param1, 1) + ((param1 & 1) << 31)\n        _loc3_ += 1\n    return param1", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\leeco.py", "class_name": "LeIE", "function_name": "decrypt_m3u8", "parameters": ["encrypted_data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bytearray", "bytes", "compat_ord", "encrypted_data[:5].decode", "encrypted_data[:5].decode('utf-8').lower", "enumerate", "len", "range"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def decrypt_m3u8(encrypted_data):\n    if encrypted_data[:5].decode('utf-8').lower() != 'vc_01':\n        return encrypted_data\n    encrypted_data = encrypted_data[5:]\n\n    _loc4_ = bytearray(2 * len(encrypted_data))\n    for idx, val in enumerate(encrypted_data):\n        b = compat_ord(val)\n        _loc4_[2 * idx] = b // 16\n        _loc4_[2 * idx + 1] = b % 16\n    idx = len(_loc4_) - 11\n    _loc4_ = _loc4_[idx:] + _loc4_[:idx]\n    _loc7_ = bytearray(len(encrypted_data))\n    for i in range(len(encrypted_data)):\n        _loc7_[i] = _loc4_[2 * i] * 16 + _loc4_[2 * i + 1]\n\n    return bytes(_loc7_)", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\extractor\\leeco.py", "class_name": "LetvCloudIE", "function_name": "sign_data", "parameters": ["obj"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "hashlib.md5", "hashlib.md5(input_data.encode('utf-8')).hexdigest", "input_data.encode"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def sign_data(obj):\n    if obj['cf'] == 'flash':\n        salt = '2f9d6924b33a165a6d8b5d3d42f4f987'\n        items = ['cf', 'format', 'ran', 'uu', 'ver', 'vu']\n    elif obj['cf'] == 'html5':\n        salt = 'fbeh5player12c43eccf2bec3300344'\n        items = ['cf', 'ran', 'uu', 'bver', 'vu']\n    input_data = ''.join([item + obj[item] for item in items]) + salt\n    obj['sign'] = hashlib.md5(input_data.encode('utf-8')).hexdigest()", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\leeco.py", "class_name": null, "function_name": "get_flash_urls", "parameters": ["media_url", "format_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encode_data_uri", "req.read", "self._download_json", "self._request_webpage", "self.decrypt_m3u8"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_flash_urls(media_url, format_id):\n    nodes_data = self._download_json(\n        media_url, media_id,\n        'Download JSON metadata for format %s' % format_id,\n        query={\n            'm3v': 1,\n            'format': 1,\n            'expect': 3,\n            'tss': 'ios',\n        })\n\n    req = self._request_webpage(\n        nodes_data['nodelist'][0]['location'], media_id,\n        note='Downloading m3u8 information for format %s' % format_id)\n\n    m3u8_data = self.decrypt_m3u8(req.read())\n\n    return {\n        'hls': encode_data_uri(m3u8_data, 'application/vnd.apple.mpegurl'),\n    }", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\extractor\\lifenews.py", "class_name": null, "function_name": "make_entry", "parameters": ["video_id", "video_url", "index"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cur_info.update", "dict"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_entry(video_id, video_url, index=None):\n    cur_info = dict(common_info)\n    cur_info.update({\n        'id': video_id if not index else '%s-video%s' % (video_id, index),\n        'url': video_url,\n        'title': title if not index else '%s (Видео %s)' % (title, index),\n    })\n    return cur_info", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\lifenews.py", "class_name": null, "function_name": "make_iframe_entry", "parameters": ["video_id", "video_url", "index"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["make_entry", "self._proto_relative_url"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_iframe_entry(video_id, video_url, index=None):\n    video_url = self._proto_relative_url(video_url, 'http:')\n    cur_info = make_entry(video_id, video_url, index)\n    cur_info['_type'] = 'url_transparent'\n    return cur_info", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\lifenews.py", "class_name": null, "function_name": "extract_original", "parameters": ["original_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "formats.append"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_original(original_url):\n    formats.append({\n        'url': original_url,\n        'format_id': determine_ext(original_url, None),\n        'preference': 1,\n    })", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\manyvids.py", "class_name": null, "function_name": "get_likes", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_attributes", "int_or_none", "likes.get", "self._search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_likes():\n    likes = self._search_regex(\n        r'''(<a\\b[^>]*\\bdata-id\\s*=\\s*(['\"])%s\\2[^>]*>)''' % (video_id, ),\n        webpage, 'likes', default='')\n    likes = extract_attributes(likes)\n    return int_or_none(likes.get('data-likes'))", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\medaltv.py", "class_name": null, "function_name": "add_item", "parameters": ["container", "item_url", "height", "id_key", "item_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["container.append", "int", "round"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_item(container, item_url, height, id_key='format_id', item_id=None):\n    item_id = item_id or '%dp' % height\n    if item_id not in item_url:\n        return\n    width = int(round(aspect_ratio * height))\n    container.append({\n        'url': item_url,\n        id_key: item_id,\n        'width': width,\n        'height': height\n    })", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\myspace.py", "class_name": null, "function_name": "formats_from_stream_urls", "parameters": ["stream_url", "hls_stream_url", "http_stream_url", "width", "height"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["formats.append", "stream_url.split"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def formats_from_stream_urls(stream_url, hls_stream_url, http_stream_url, width=None, height=None):\n    formats = []\n    vcodec = 'none' if is_song else None\n    if hls_stream_url:\n        formats.append({\n            'format_id': 'hls',\n            'url': hls_stream_url,\n            'protocol': 'm3u8_native',\n            'ext': 'm4a' if is_song else 'mp4',\n            'vcodec': vcodec,\n        })\n    if stream_url and player_url:\n        rtmp_url, play_path = stream_url.split(';', 1)\n        formats.append({\n            'format_id': 'rtmp',\n            'url': rtmp_url,\n            'play_path': play_path,\n            'player_url': player_url,\n            'protocol': 'rtmp',\n            'ext': 'flv',\n            'width': width,\n            'height': height,\n            'vcodec': vcodec,\n        })\n    if http_stream_url:\n        formats.append({\n            'format_id': 'http',\n            'url': http_stream_url,\n            'width': width,\n            'height': height,\n            'vcodec': vcodec,\n        })\n    return formats", "loc": 33}
{"file": "youtube-dl\\youtube_dl\\extractor\\naver.py", "class_name": null, "function_name": "extract_formats", "parameters": ["streams", "stream_type", "query"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["bitrate.get", "dict_get", "encoding_option.get", "formats.append", "int_or_none", "stream.get", "update_url_query"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(streams, stream_type, query={}):\n    for stream in streams:\n        stream_url = stream.get('source')\n        if not stream_url:\n            continue\n        stream_url = update_url_query(stream_url, query)\n        encoding_option = stream.get('encodingOption', {})\n        bitrate = stream.get('bitrate', {})\n        formats.append({\n            'format_id': '%s_%s' % (stream.get('type') or stream_type, dict_get(encoding_option, ('name', 'id'))),\n            'url': stream_url,\n            'width': int_or_none(encoding_option.get('width')),\n            'height': int_or_none(encoding_option.get('height')),\n            'vbr': int_or_none(bitrate.get('video')),\n            'abr': int_or_none(bitrate.get('audio')),\n            'filesize': int_or_none(stream.get('size')),\n            'protocol': 'm3u8_native' if stream_type == 'HLS' else None,\n        })", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\extractor\\naver.py", "class_name": null, "function_name": "get_subs", "parameters": ["caption_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.search", "replace_ext"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_subs(caption_url):\n    if re.search(self._CAPTION_EXT_RE, caption_url):\n        return [{\n            'url': replace_ext(caption_url, 'ttml'),\n        }, {\n            'url': replace_ext(caption_url, 'vtt'),\n        }]\n    else:\n        return [{'url': caption_url}]", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\neteasemusic.py", "class_name": "NetEaseMusicBaseIE", "function_name": "make_player_api_request_data_and_headers", "parameters": ["cls", "song_id", "bitrate"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'; '.join", "'[{0}]'.format", "'nobody{0}use{1}md5forencrypt'.format", "'nobody{0}use{1}md5forencrypt'.format(URL, request_text).encode", "'params={0}'.format", "'{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format", "'{0}={1}'.format", "'{0}_{1:04}'.format", "aes_ecb_encrypt", "bytes_to_intlist", "cookie.items", "hexlify", "hexlify(encrypted).decode", "hexlify(encrypted).decode('ascii').upper", "int", "intlist_to_bytes", "json.dumps", "md5", "md5(message).hexdigest", "pkcs7_padding", "randint", "time.time"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def make_player_api_request_data_and_headers(cls, song_id, bitrate):\n    KEY = b'e82ckenh8dichen8'\n    URL = '/api/song/enhance/player/url'\n    now = int(time.time() * 1000)\n    rand = randint(0, 1000)\n    cookie = {\n        'osver': None,\n        'deviceId': None,\n        'appver': '8.0.0',\n        'versioncode': '140',\n        'mobilename': None,\n        'buildver': '1623435496',\n        'resolution': '1920x1080',\n        '__csrf': '',\n        'os': 'pc',\n        'channel': None,\n        'requestId': '{0}_{1:04}'.format(now, rand),\n    }\n    request_text = json.dumps(\n        {'ids': '[{0}]'.format(song_id), 'br': bitrate, 'header': cookie},\n        separators=(',', ':'))\n    message = 'nobody{0}use{1}md5forencrypt'.format(\n        URL, request_text).encode('latin1')\n    msg_digest = md5(message).hexdigest()\n\n    data = '{0}-36cd479b6b5-{1}-36cd479b6b5-{2}'.format(\n        URL, request_text, msg_digest)\n    data = pkcs7_padding(bytes_to_intlist(data))\n    encrypted = intlist_to_bytes(aes_ecb_encrypt(data, bytes_to_intlist(KEY)))\n    encrypted_params = hexlify(encrypted).decode('ascii').upper()\n\n    cookie = '; '.join(\n        ['{0}={1}'.format(k, v if v is not None else 'undefined')\n         for [k, v] in cookie.items()])\n\n    headers = {\n        'User-Agent': std_headers['User-Agent'],\n        'Content-Type': 'application/x-www-form-urlencoded',\n        'Referer': 'https://music.163.com',\n        'Cookie': cookie,\n    }\n    return ('params={0}'.format(encrypted_params), headers)", "loc": 42}
{"file": "youtube-dl\\youtube_dl\\extractor\\neteasemusic.py", "class_name": "NetEaseMusicBaseIE", "function_name": "extract_formats", "parameters": ["self", "info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "details.get", "float_or_none", "formats.append", "info.get", "int_or_none", "self._call_player_api", "self._is_valid_url", "self.raise_geo_restricted", "song.get", "try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(self, info):\n    err = 0\n    formats = []\n    song_id = info['id']\n    for song_format in self._FORMATS:\n        details = info.get(song_format)\n        if not details:\n            continue\n\n        bitrate = int_or_none(details.get('bitrate')) or 999000\n        data = self._call_player_api(song_id, bitrate)\n        for song in try_get(data, lambda x: x['data'], list) or []:\n            song_url = try_get(song, lambda x: x['url'])\n            if not song_url:\n                continue\n            if self._is_valid_url(song_url, info['id'], 'song'):\n                formats.append({\n                    'url': song_url,\n                    'ext': details.get('extension'),\n                    'abr': float_or_none(song.get('br'), scale=1000),\n                    'format_id': song_format,\n                    'filesize': int_or_none(song.get('size')),\n                    'asr': int_or_none(details.get('sr')),\n                })\n            elif err == 0:\n                err = try_get(song, lambda x: x['code'], int)\n\n    if not formats:\n        msg = 'No media links found'\n        if err != 0 and (err < 200 or err >= 400):\n            raise ExtractorError(\n                '%s (site code %d)' % (msg, err, ), expected=True)\n        else:\n            self.raise_geo_restricted(\n                msg + ': probably this video is not available from your location due to geo restriction.',\n                countries=['CN'])\n\n    return formats", "loc": 38}
{"file": "youtube-dl\\youtube_dl\\extractor\\nexx.py", "class_name": null, "function_name": "get_cdn_shield_base", "parameters": ["shield_type", "static"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "secure.upper", "stream_data.get", "stream_data['azureAccount'].replace", "stream_data['azureAccount'].replace('nexxplayplus', '').replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_cdn_shield_base(shield_type='', static=False):\n    for secure in ('', 's'):\n        cdn_shield = stream_data.get('cdnShield%sHTTP%s' % (shield_type, secure.upper()))\n        if cdn_shield:\n            return 'http%s://%s' % (secure, cdn_shield)\n    else:\n        if 'fb' in stream_data['azureAccount']:\n            prefix = 'df' if static else 'f'\n        else:\n            prefix = 'd' if static else 'p'\n        account = int(stream_data['azureAccount'].replace('nexxplayplus', '').replace('nexxplayfb', ''))\n        return 'http://nx-%s%02d.akamaized.net/' % (prefix, account)", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\nexx.py", "class_name": null, "function_name": "find_video", "parameters": ["result"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "isinstance", "try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def find_video(result):\n    if isinstance(result, dict):\n        return result\n    elif isinstance(result, list):\n        vid = int(video_id)\n        for v in result:\n            if try_get(v, lambda x: x['general']['ID'], int) == vid:\n                return v\n    return None", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\niconico.py", "class_name": null, "function_name": "ping", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._download_json", "self.report_warning", "try_get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ping():\n    status = try_get(\n        self._download_json(\n            'https://nvapi.nicovideo.jp/v1/2ab0cbaa/watch', video_id,\n            query={'t': try_get(api_data, lambda x: x['media']['delivery']['trackingId'])},\n            note='Acquiring permission for downloading video',\n            headers=self._API_HEADERS),\n        lambda x: x['meta']['status'])\n    if status != 200:\n        self.report_warning('Failed to acquire permission for playing video. The video may not download.')", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\niconico.py", "class_name": null, "function_name": "parse_format_id", "parameters": ["id_code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["mobj.groupdict", "re.match"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_format_id(id_code):\n    mobj = re.match(r'''(?x)\n            (?:archive_)?\n            (?:(?P<codec>[^_]+)_)?\n            (?:(?P<br>[\\d]+)kbps_)?\n            (?:(?P<res>[\\d+]+)p_)?\n        ''', '%s_' % id_code)\n    return mobj.groupdict() if mobj else {}", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\niconico.py", "class_name": null, "function_name": "get_video_info_xml", "parameters": ["items"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "xpath_text"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_video_info_xml(items):\n    if not isinstance(items, list):\n        items = [items]\n    for item in items:\n        ret = xpath_text(video_info_xml, './/' + item)\n        if ret:\n            return ret", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\niconico.py", "class_name": null, "function_name": "pagefunc", "parameters": ["pagenum"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.get", "get_page_data", "item.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pagefunc(pagenum):\n    data = get_page_data(pagenum, 25)\n    return ({\n        '_type': 'url',\n        'url': 'http://www.nicovideo.jp/watch/' + item.get('watchId'),\n    } for item in data.get('items'))", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\npo.py", "class_name": null, "function_name": "add_format_url", "parameters": ["format_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["formats.append", "int_or_none", "quality_from_format_id", "quality_from_label", "self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_format_url(format_url):\n    width = int_or_none(self._search_regex(\n        r'(\\d+)[xX]\\d+', format_url, 'width', default=None))\n    height = int_or_none(self._search_regex(\n        r'\\d+[xX](\\d+)', format_url, 'height', default=None))\n    if item_label in QUALITY_LABELS:\n        quality = quality_from_label(item_label)\n        f_id = item_label\n    elif item_label in QUALITY_FORMATS:\n        quality = quality_from_format_id(format_id)\n        f_id = format_id\n    else:\n        quality, f_id = [None] * 2\n    formats.append({\n        'url': format_url,\n        'format_id': f_id,\n        'width': width,\n        'height': height,\n        'quality': quality,\n    })", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\extractor\\nrk.py", "class_name": "NRKTVSeriesIE", "function_name": "suitable", "parameters": ["cls", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "ie.suitable", "super", "super(NRKTVSeriesIE, cls).suitable"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def suitable(cls, url):\n    return (\n        False if any(ie.suitable(url)\n                     for ie in (NRKTVIE, NRKTVEpisodeIE, NRKRadioPodkastIE, NRKTVSeasonIE))\n        else super(NRKTVSeriesIE, cls).suitable(url))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\nytimes.py", "class_name": null, "function_name": "get_file_size", "parameters": ["file_size"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["file_size.get", "int", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_file_size(file_size):\n    if isinstance(file_size, int):\n        return file_size\n    elif isinstance(file_size, dict):\n        return int(file_size.get('value', 0))\n    else:\n        return None", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\openload.py", "class_name": null, "function_name": "cookie_to_dict", "parameters": ["cookie"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["cookie.has_nonstandard_attr"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cookie_to_dict(cookie):\n    cookie_dict = {\n        'name': cookie.name,\n        'value': cookie.value,\n    }\n    if cookie.port_specified:\n        cookie_dict['port'] = cookie.port\n    if cookie.domain_specified:\n        cookie_dict['domain'] = cookie.domain\n    if cookie.path_specified:\n        cookie_dict['path'] = cookie.path\n    if cookie.expires is not None:\n        cookie_dict['expires'] = cookie.expires\n    if cookie.secure is not None:\n        cookie_dict['secure'] = cookie.secure\n    if cookie.discard is not None:\n        cookie_dict['discard'] = cookie.discard\n    try:\n        if (cookie.has_nonstandard_attr('httpOnly')\n                or cookie.has_nonstandard_attr('httponly')\n                or cookie.has_nonstandard_attr('HttpOnly')):\n            cookie_dict['httponly'] = True\n    except TypeError:\n        pass\n    return cookie_dict", "loc": 25}
{"file": "youtube-dl\\youtube_dl\\extractor\\orf.py", "class_name": null, "function_name": "yield_items", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "any", "item.get", "self._get_item_id", "target.get", "target['params'].get", "traverse_obj", "txt_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yield_items():\n    for item in traverse_obj(data, (\n            'content', 'items', lambda _, v: any(k in v['target']['params'] for k in self._ID_NAMES))):\n        if item_id is None or item_id == txt_or_none(item.get('id')):\n            target = item['target']\n            typed_item_id = self._get_item_id(target['params'])\n            station = target['params'].get('station')\n            item_type = target.get('type')\n            if typed_item_id and (station or item_type):\n                yield station, typed_item_id, item_type\n            if item_id is not None:\n                break\n    else:\n        if item_id is not None:\n            raise ExtractorError('Item not found in collection',\n                                 video_id=coll_id, expected=True)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\extractor\\orf.py", "class_name": null, "function_name": "extract_sources", "parameters": ["src_json", "video_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["T", "formats.extend", "self._extract_m3u8_formats", "self._extract_mpd_formats_and_subtitles", "self._merge_subtitles", "traverse_obj"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_sources(src_json, video_id):\n    for manifest_type in traverse_obj(src_json, ('sources', T(dict.keys), Ellipsis)):\n        for manifest_url in traverse_obj(src_json, ('sources', manifest_type, Ellipsis, 'src', T(url_or_none))):\n            if manifest_type == 'hls':\n                fmts, subs = self._extract_m3u8_formats(\n                    manifest_url, video_id, fatal=False, m3u8_id='hls',\n                    ext='mp4', entry_protocol='m3u8_native'), {}\n                for f in fmts:\n                    if '_vo.' in f['url']:\n                        f['acodec'] = 'none'\n            elif manifest_type == 'dash':\n                fmts, subs = self._extract_mpd_formats_and_subtitles(\n                    manifest_url, video_id, fatal=False, mpd_id='dash')\n            else:\n                continue\n            formats.extend(fmts)\n            self._merge_subtitles(subs, target=subtitles)", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\extractor\\pbs.py", "class_name": null, "function_name": "extract_redirect_urls", "parameters": ["info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info.get", "isinstance", "redirect.get", "redirect_urls.add", "redirects.append", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_redirect_urls(info):\n    for encoding_name in ('recommended_encoding', 'alternate_encoding'):\n        redirect = info.get(encoding_name)\n        if not redirect:\n            continue\n        redirect_url = redirect.get('url')\n        if redirect_url and redirect_url not in redirect_urls:\n            redirects.append(redirect)\n            redirect_urls.add(redirect_url)\n    encodings = info.get('encodings')\n    if isinstance(encodings, list):\n        for encoding in encodings:\n            encoding_url = url_or_none(encoding)\n            if encoding_url and encoding_url not in redirect_urls:\n                redirects.append({'url': encoding_url})\n                redirect_urls.add(encoding_url)", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\extractor\\peekvids.py", "class_name": null, "function_name": "cat_tags", "parameters": ["name", "html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.escape", "re.split", "self._html_search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def cat_tags(name, html):\n    l = self._html_search_regex(\n        r'(?s)<span\\b[^>]*>\\s*%s\\s*:\\s*</span>(.+?)</li>' % (re.escape(name), ),\n        html, name, default='')\n    return [x for x in re.split(r'\\s+', l) if x]", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\philharmoniedeparis.py", "class_name": null, "function_name": "extract_entry", "parameters": ["source"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["format_urls.add", "formats.extend", "isinstance", "self._extract_m3u8_formats", "self._sort_formats", "set", "source.get", "try_get", "urljoin"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_entry(source):\n    if not isinstance(source, dict):\n        return\n    title = source.get('title')\n    if not title:\n        return\n    files = source.get('files')\n    if not isinstance(files, dict):\n        return\n    format_urls = set()\n    formats = []\n    for format_id in ('mobile', 'desktop'):\n        format_url = try_get(\n            files, lambda x: x[format_id]['file'], compat_str)\n        if not format_url or format_url in format_urls:\n            continue\n        format_urls.add(format_url)\n        m3u8_url = urljoin(self._LIVE_URL, format_url)\n        formats.extend(self._extract_m3u8_formats(\n            m3u8_url, video_id, 'mp4', entry_protocol='m3u8_native',\n            m3u8_id='hls', fatal=False))\n    if not formats:\n        return\n    self._sort_formats(formats)\n    return {\n        'title': title,\n        'formats': formats,\n    }", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\extractor\\piksel.py", "class_name": null, "function_name": "process_asset_file", "parameters": ["asset_file"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'-'.join", "asset_file.get", "compat_str", "format_id.append", "formats.append", "int_or_none", "unescapeHTML"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_asset_file(asset_file):\n    if not asset_file:\n        return\n    # TODO: extract rtmp formats\n    http_url = asset_file.get('http_url')\n    if not http_url:\n        return\n    tbr = None\n    vbr = int_or_none(asset_file.get('videoBitrate'), 1024)\n    abr = int_or_none(asset_file.get('audioBitrate'), 1024)\n    if asset_type == 'video':\n        tbr = vbr + abr\n    elif asset_type == 'audio':\n        tbr = abr\n\n    format_id = ['http']\n    if tbr:\n        format_id.append(compat_str(tbr))\n\n    formats.append({\n        'format_id': '-'.join(format_id),\n        'url': unescapeHTML(http_url),\n        'vbr': vbr,\n        'abr': abr,\n        'width': int_or_none(asset_file.get('videoWidth')),\n        'height': int_or_none(asset_file.get('videoHeight')),\n        'filesize': int_or_none(asset_file.get('filesize')),\n        'tbr': tbr,\n    })", "loc": 29}
{"file": "youtube-dl\\youtube_dl\\extractor\\pluralsight.py", "class_name": null, "function_name": "guess_allowed_qualities", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'-'.join", "AllowedQuality", "len", "req_format.split", "req_quality.split", "self._downloader.params.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def guess_allowed_qualities():\n    req_format = self._downloader.params.get('format') or 'best'\n    req_format_split = req_format.split('-', 1)\n    if len(req_format_split) > 1:\n        req_ext, req_quality = req_format_split\n        req_quality = '-'.join(req_quality.split('-')[:2])\n        for allowed_quality in ALLOWED_QUALITIES:\n            if req_ext == allowed_quality.ext and req_quality in allowed_quality.qualities:\n                return (AllowedQuality(req_ext, (req_quality, )), )\n    req_ext = 'webm' if self._downloader.params.get('prefer_free_formats') else 'mp4'\n    return (AllowedQuality(req_ext, (best_quality, )), )", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\porncom.py", "class_name": null, "function_name": "extract_list", "parameters": ["kind"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["kind.capitalize", "re.findall", "self._search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_list(kind):\n    s = self._search_regex(\n        (r'(?s)%s:\\s*</span>\\s*<span>(.+?)</span>' % kind.capitalize(),\n         r'(?s)<p[^>]*>%s:(.+?)</p>' % kind.capitalize()),\n        webpage, kind, fatal=False)\n    return re.findall(r'<a[^>]+>([^<]+)</a>', s or '')", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "dl_webpage", "parameters": ["platform"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._download_webpage", "self._set_cookie"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def dl_webpage(platform):\n    self._set_cookie(host, 'platform', platform)\n    return self._download_webpage(\n        'https://www.%s/view_video.php?viewkey=%s' % (host, video_id),\n        video_id, 'Downloading %s webpage' % platform)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "extract_js_vars", "parameters": ["webpage", "pattern", "default"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["assignments.split", "assn.split", "assn.strip", "functools.reduce", "inp.split", "inp.strip", "map", "parse_js_value", "re.sub", "remove_quotes", "self._search_regex"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_js_vars(webpage, pattern, default=NO_DEFAULT):\n    assignments = self._search_regex(\n        pattern, webpage, 'encoded url', default=default)\n    if not assignments:\n        return {}\n\n    assignments = assignments.split(';')\n\n    js_vars = {}\n\n    def parse_js_value(inp):\n        inp = re.sub(r'/\\*(?:(?!\\*/).)*?\\*/', '', inp)\n        if '+' in inp:\n            inps = inp.split('+')\n            return functools.reduce(\n                operator.concat, map(parse_js_value, inps))\n        inp = inp.strip()\n        if inp in js_vars:\n            return js_vars[inp]\n        return remove_quotes(inp)\n\n    for assn in assignments:\n        assn = assn.strip()\n        if not assn:\n            continue\n        assn = re.sub(r'var\\s+', '', assn)\n        vname, value = assn.split('=', 1)\n        js_vars[vname] = parse_js_value(value)\n    return js_vars", "loc": 29}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "add_video_url", "parameters": ["video_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["url_or_none", "video_urls.append", "video_urls_set.add"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_video_url(video_url):\n    v_url = url_or_none(video_url)\n    if not v_url:\n        return\n    if v_url in video_urls_set:\n        return\n    video_urls.append((v_url, None))\n    video_urls_set.add(v_url)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "parse_quality_items", "parameters": ["quality_items"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_video_url", "isinstance", "item.get", "self._parse_json"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_quality_items(quality_items):\n    q_items = self._parse_json(quality_items, video_id, fatal=False)\n    if not isinstance(q_items, list):\n        return\n    for item in q_items:\n        if isinstance(item, dict):\n            add_video_url(item.get('url'))", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "add_format", "parameters": ["format_url", "height"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "formats.append", "formats.extend", "int_or_none", "self._extract_m3u8_formats", "self._extract_mpd_formats", "self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_format(format_url, height=None):\n    ext = determine_ext(format_url)\n    if ext == 'mpd':\n        formats.extend(self._extract_mpd_formats(\n            format_url, video_id, mpd_id='dash', fatal=False))\n        return\n    if ext == 'm3u8':\n        formats.extend(self._extract_m3u8_formats(\n            format_url, video_id, 'mp4', entry_protocol='m3u8_native',\n            m3u8_id='hls', fatal=False))\n        return\n    if not height:\n        height = int_or_none(self._search_regex(\n            r'(?P<height>\\d+)[pP]?_\\d+[kK]', format_url, 'height',\n            default=None))\n    formats.append({\n        'url': format_url,\n        'format_id': '%dp' % height if height else None,\n        'height': height,\n    })", "loc": 20}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "extract_list", "parameters": ["meta_key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_list(meta_key):\n    div = self._search_regex(\n        r'(?s)<div[^>]+\\bclass=[\"\\'].*?\\b%sWrapper[^>]*>(.+?)</div>'\n        % meta_key, webpage, meta_key, default=None)\n    if div:\n        return re.findall(r'<a[^>]+\\bhref=[^>]+>([^<]+)', div)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\pornhub.py", "class_name": null, "function_name": "parse_js_value", "parameters": ["inp"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["functools.reduce", "inp.split", "inp.strip", "map", "re.sub", "remove_quotes"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def parse_js_value(inp):\n    inp = re.sub(r'/\\*(?:(?!\\*/).)*?\\*/', '', inp)\n    if '+' in inp:\n        inps = inp.split('+')\n        return functools.reduce(\n            operator.concat, map(parse_js_value, inps))\n    inp = inp.strip()\n    if inp in js_vars:\n        return js_vars[inp]\n    return remove_quotes(inp)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\prosiebensat1.py", "class_name": null, "function_name": "fix_bitrate", "parameters": ["bitrate"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def fix_bitrate(bitrate):\n    bitrate = int_or_none(bitrate)\n    if not bitrate:\n        return None\n    return (bitrate // 1000) if bitrate % 1000 == 0 else bitrate", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\qqmusic.py", "class_name": "QQPlaylistBaseIE", "function_name": "get_entries_from_page", "parameters": ["self", "singmid"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["entries.append", "item['musicData'].get", "self._parse_json", "self.get_singer_all_songs", "self.url_result"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_entries_from_page(self, singmid):\n    entries = []\n\n    default_num = 1\n    json_text = self.get_singer_all_songs(singmid, default_num)\n    json_obj_all_songs = self._parse_json(json_text, singmid)\n\n    if json_obj_all_songs['code'] == 0:\n        total = json_obj_all_songs['data']['total']\n        json_text = self.get_singer_all_songs(singmid, total)\n        json_obj_all_songs = self._parse_json(json_text, singmid)\n\n    for item in json_obj_all_songs['data']['list']:\n        if item['musicData'].get('songmid') is not None:\n            songmid = item['musicData']['songmid']\n            entries.append(self.url_result(\n                r'https://y.qq.com/n/yqq/song/%s.html' % songmid, 'QQMusic', songmid))\n\n    return entries", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\extractor\\radiocanada.py", "class_name": null, "function_name": "get_meta", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["meta.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_meta(name):\n    for meta in metas:\n        if meta.get('name') == name:\n            text = meta.get('text')\n            if text:\n                return text", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\rai.py", "class_name": null, "function_name": "test_url", "parameters": ["url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["HEADRequest", "self._request_webpage"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def test_url(url):\n    resp = self._request_webpage(\n        HEADRequest(url), None, headers={'User-Agent': 'Rai'},\n        fatal=False, errnote=False, note=False)\n\n    if resp is False:\n        return False\n\n    if resp.code == 200:\n        return False if resp.url == url else resp.url\n    return None", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\rai.py", "class_name": null, "function_name": "get_format_info", "parameters": ["tbr"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_str", "f.copy", "f.get", "fmts[0].get", "format_copy.get", "int", "int_or_none", "len", "math.floor"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_format_info(tbr):\n    import math\n    br = int_or_none(tbr)\n    if len(fmts) == 1 and not br:\n        br = fmts[0].get('tbr')\n    if br > 300:\n        tbr = compat_str(math.floor(br / 100) * 100)\n    else:\n        tbr = '250'\n\n    # try extracting info from available m3u8 formats\n    format_copy = None\n    for f in fmts:\n        if f.get('tbr'):\n            br_limit = math.floor(br / 100)\n            if br_limit - 1 <= math.floor(f['tbr'] / 100) <= br_limit + 1:\n                format_copy = f.copy()\n    return {\n        'width': format_copy.get('width'),\n        'height': format_copy.get('height'),\n        'tbr': format_copy.get('tbr'),\n        'vcodec': format_copy.get('vcodec'),\n        'acodec': format_copy.get('acodec'),\n        'fps': format_copy.get('fps'),\n        'format_id': 'https-%s' % tbr,\n    } if format_copy else {\n        'width': _QUALITY[tbr][0],\n        'height': _QUALITY[tbr][1],\n        'format_id': 'https-%s' % tbr,\n        'tbr': int(tbr),\n    }", "loc": 31}
{"file": "youtube-dl\\youtube_dl\\extractor\\redbulltv.py", "class_name": "RedBullTVIE", "function_name": "extract_info", "parameters": ["self", "video_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "e.cause.read", "e.cause.read().decode", "float_or_none", "isinstance", "resource.split", "resource.startswith", "self._download_json", "self._extract_m3u8_formats", "self._parse_json", "self._sort_formats", "session.get", "subtitles.setdefault", "subtitles.setdefault('en', []).append", "video.get", "video['title'].strip"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_info(self, video_id):\n    session = self._download_json(\n        'https://api.redbull.tv/v3/session', video_id,\n        note='Downloading access token', query={\n            'category': 'personal_computer',\n            'os_family': 'http',\n        })\n    if session.get('code') == 'error':\n        raise ExtractorError('%s said: %s' % (\n            self.IE_NAME, session['message']))\n    token = session['token']\n\n    try:\n        video = self._download_json(\n            'https://api.redbull.tv/v3/products/' + video_id,\n            video_id, note='Downloading video information',\n            headers={'Authorization': token}\n        )\n    except ExtractorError as e:\n        if isinstance(e.cause, compat_HTTPError) and e.cause.code == 404:\n            error_message = self._parse_json(\n                e.cause.read().decode(), video_id)['error']\n            raise ExtractorError('%s said: %s' % (\n                self.IE_NAME, error_message), expected=True)\n        raise\n\n    title = video['title'].strip()\n\n    formats = self._extract_m3u8_formats(\n        'https://dms.redbull.tv/v3/%s/%s/playlist.m3u8' % (video_id, token),\n        video_id, 'mp4', entry_protocol='m3u8_native', m3u8_id='hls')\n    self._sort_formats(formats)\n\n    subtitles = {}\n    for resource in video.get('resources', []):\n        if resource.startswith('closed_caption_'):\n            splitted_resource = resource.split('_')\n            if splitted_resource[2]:\n                subtitles.setdefault('en', []).append({\n                    'url': 'https://resources.redbull.tv/%s/%s' % (video_id, resource),\n                    'ext': splitted_resource[2],\n                })\n\n    subheading = video.get('subheading')\n    if subheading:\n        title += ' - %s' % subheading\n\n    return {\n        'id': video_id,\n        'title': title,\n        'description': video.get('long_description') or video.get(\n            'short_description'),\n        'duration': float_or_none(video.get('duration'), scale=1000),\n        'formats': formats,\n        'subtitles': subtitles,\n    }", "loc": 56}
{"file": "youtube-dl\\youtube_dl\\extractor\\reddit.py", "class_name": null, "function_name": "add_thumbnail", "parameters": ["src"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "isinstance", "src.get", "thumbnails.append", "unescapeHTML", "url_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_thumbnail(src):\n    if not isinstance(src, dict):\n        return\n    thumbnail_url = url_or_none(src.get('url'))\n    if not thumbnail_url:\n        return\n    thumbnails.append({\n        'url': unescapeHTML(thumbnail_url),\n        'width': int_or_none(src.get('width')),\n        'height': int_or_none(src.get('height')),\n    })", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\ruutu.py", "class_name": null, "function_name": "pv", "parameters": ["name"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["find_xpath_attr", "node.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def pv(name):\n    node = find_xpath_attr(\n        video_xml, './Clip/PassthroughVariables/variable', 'name', name)\n    if node is not None:\n        return node.get('value')", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\shahid.py", "class_name": null, "function_name": "page_func", "parameters": ["page_num"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["playlist.get", "playlist.get('productList', {}).get", "product.get", "product.get('productUrl', []).get", "self._call_api", "self.url_result", "str_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def page_func(page_num):\n    playlist = self._call_api(\n        'product/playlist', show_id, {\n            'playListId': playlist_id,\n            'pageNumber': page_num,\n            'pageSize': 30,\n            'sorts': [{\n                'order': 'DESC',\n                'type': 'SORTDATE'\n            }],\n        })\n    for product in playlist.get('productList', {}).get('products', []):\n        product_url = product.get('productUrl', []).get('url')\n        if not product_url:\n            continue\n        yield self.url_result(\n            product_url, 'Shahid',\n            str_or_none(product.get('id')),\n            product.get('title'))", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\extractor\\simplecast.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["episodes.get", "self._call_api", "self._parse_episode"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    episodes = self._call_api('podcasts/%s/episodes', podcast_id)\n    for episode in (episodes.get('collection') or []):\n        info = self._parse_episode(episode)\n        info['series'] = podcast_title\n        yield info", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\sixplay.py", "class_name": null, "function_name": "get", "parameters": ["getter"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["try_get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get(getter):\n    for src in (data, clip_data):\n        v = try_get(src, getter, compat_str)\n        if v:\n            return v", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\soundcloud.py", "class_name": null, "function_name": "add_format", "parameters": ["f", "protocol", "is_preview"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'_'.join", "f.get", "f.update", "format_id_list.append", "formats.append", "int", "mobj.groupdict", "mobj.groupdict().items", "re.search"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_format(f, protocol, is_preview=False):\n    mobj = re.search(r'\\.(?P<abr>\\d+)\\.(?P<ext>[0-9a-z]{3,4})(?=[/?])', stream_url)\n    if mobj:\n        for k, v in mobj.groupdict().items():\n            if not f.get(k):\n                f[k] = v\n    format_id_list = []\n    if protocol:\n        format_id_list.append(protocol)\n    ext = f.get('ext')\n    if ext == 'aac':\n        f['abr'] = '256'\n    for k in ('ext', 'abr'):\n        v = f.get(k)\n        if v:\n            format_id_list.append(v)\n    preview = is_preview or re.search(r'/(?:preview|playlist)/0/30/', f['url'])\n    if preview:\n        format_id_list.append('preview')\n    abr = f.get('abr')\n    if abr:\n        f['abr'] = int(abr)\n    if protocol == 'hls':\n        protocol = 'm3u8' if ext == 'aac' else 'm3u8_native'\n    else:\n        protocol = 'http'\n    f.update({\n        'format_id': '_'.join(format_id_list),\n        'protocol': protocol,\n        'preference': -10 if preview else None,\n    })\n    formats.append(f)", "loc": 32}
{"file": "youtube-dl\\youtube_dl\\extractor\\soundcloud.py", "class_name": null, "function_name": "resolve_entry", "parameters": ["candidates"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["SoundcloudIE.ie_key", "SoundcloudIE.suitable", "cand.get", "isinstance", "self.url_result", "str_or_none", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def resolve_entry(candidates):\n    for cand in candidates:\n        if not isinstance(cand, dict):\n            continue\n        permalink_url = url_or_none(cand.get('permalink_url'))\n        if not permalink_url:\n            continue\n        return self.url_result(\n            permalink_url,\n            SoundcloudIE.ie_key() if SoundcloudIE.suitable(permalink_url) else None,\n            str_or_none(cand.get('id')), cand.get('title'))", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\spankbang.py", "class_name": null, "function_name": "extract_format", "parameters": ["format_id", "format_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["determine_ext", "f.get", "f.update", "format_id.startswith", "formats.append", "formats.extend", "parse_resolution", "self._extract_m3u8_formats", "self._extract_mpd_formats", "url_or_none"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_format(format_id, format_url):\n    f_url = url_or_none(format_url)\n    if not f_url:\n        return\n    f = parse_resolution(format_id)\n    ext = determine_ext(f_url)\n    if format_id.startswith('m3u8') or ext == 'm3u8':\n        formats.extend(self._extract_m3u8_formats(\n            f_url, video_id, 'mp4', entry_protocol='m3u8_native',\n            m3u8_id='hls', fatal=False))\n    elif format_id.startswith('mpd') or ext == 'mpd':\n        formats.extend(self._extract_mpd_formats(\n            f_url, video_id, mpd_id='dash', fatal=False))\n    elif ext == 'mp4' or f.get('width') or f.get('height'):\n        f.update({\n            'url': f_url,\n            'format_id': format_id,\n        })\n        formats.append(f)", "loc": 19}
{"file": "youtube-dl\\youtube_dl\\extractor\\spankwire.py", "class_name": null, "function_name": "extract_names", "parameters": ["key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["entries.append", "entry.get", "isinstance", "str_or_none", "video.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_names(key):\n    entries_list = video.get(key)\n    if not isinstance(entries_list, list):\n        return\n    entries = []\n    for entry in entries_list:\n        name = str_or_none(entry.get('name'))\n        if name:\n            entries.append(name)\n    return entries", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\sportdeutschland.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["enumerate", "float_or_none", "self._extract_m3u8_formats", "video.get", "video_url.replace"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    for i, video in enumerate(videos, 1):\n        video_id = video.get('uuid')\n        video_url = video.get('url')\n        if not (video_id and video_url):\n            continue\n        formats = self._extract_m3u8_formats(\n            video_url.replace('.smil', '.m3u8'), video_id, 'mp4', fatal=False)\n        if not formats:\n            continue\n        yield {\n            'id': video_id,\n            'formats': formats,\n            'title': title + ' - ' + (video.get('label') or 'Teil %d' % i),\n            'duration': float_or_none(video.get('duration')),\n        }", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\extractor\\spreaker.py", "class_name": null, "function_name": "stats", "parameters": ["key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "try_get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def stats(key):\n    return int_or_none(try_get(\n        data,\n        (lambda x: x['%ss_count' % key],\n         lambda x: x['stats']['%ss' % key])))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\teachable.py", "class_name": null, "function_name": "is_logged", "parameters": ["webpage"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "re.search"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_logged(webpage):\n    return any(re.search(p, webpage) for p in (\n        r'class=[\"\\']user-signout',\n        r'<a[^>]+\\bhref=[\"\\']/sign_out',\n        r'Log\\s+[Oo]ut\\s*<'))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\teamtreehouse.py", "class_name": null, "function_name": "extract_urls", "parameters": ["html", "extract_info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["entries.append", "entry.update", "re.findall", "self._match_id", "self.ie_key", "urljoin"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_urls(html, extract_info=None):\n    for path in re.findall(r'<a[^>]+href=\"([^\"]+)\"', html):\n        page_url = urljoin(url, path)\n        entry = {\n            '_type': 'url_transparent',\n            'id': self._match_id(page_url),\n            'url': page_url,\n            'id_key': self.ie_key(),\n        }\n        if extract_info:\n            entry.update(extract_info)\n        entries.append(entry)", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\thisvid.py", "class_name": null, "function_name": "entries", "parameters": ["page_url", "html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_urlparse.urlparse", "compat_urlparse.urlunparse", "get_element_by_class", "int_or_none", "itertools.count", "parsed_url._replace", "parsed_url.path.rsplit", "parsed_url.path.rstrip", "self._download_webpage", "self._search_regex", "self._urls", "urljoin"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries(page_url, html=None):\n    for page in itertools.count(1):\n        if not html:\n            html = self._download_webpage(\n                page_url, pl_id, note='Downloading page %d' % (page, ),\n                fatal=False) or ''\n        for u in self._urls(html):\n            yield u\n        next_page = get_element_by_class('pagination-next', html) or ''\n        if next_page:\n            # member list page\n            next_page = urljoin(url, self._search_regex(\n                r'''<a\\b[^>]+\\bhref\\s*=\\s*(\"|')(?P<url>(?!#)(?:(?!\\1).)+)''',\n                next_page, 'next page link', group='url', default=None))\n        # in case a member page should have pagination-next with empty link, not just `else:`\n        if next_page is None:\n            # playlist page\n            parsed_url = compat_urlparse.urlparse(page_url)\n            base_path, num = parsed_url.path.rsplit('/', 1)\n            num = int_or_none(num)\n            if num is None:\n                base_path, num = parsed_url.path.rstrip('/'), 1\n            parsed_url = parsed_url._replace(path=base_path + ('/%d' % (num + 1, )))\n            next_page = compat_urlparse.urlunparse(parsed_url)\n            if page_url == next_page:\n                next_page = None\n        if not next_page:\n            break\n        page_url, html = next_page, None", "loc": 29}
{"file": "youtube-dl\\youtube_dl\\extractor\\tnaflix.py", "class_name": null, "function_name": "get_child", "parameters": ["elem", "names"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["elem.find"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_child(elem, names):\n    for name in names:\n        child = elem.find(name)\n        if child is not None:\n            return child", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\tv2.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["article.get", "self.url_result", "video.get", "video_type.capitalize"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    for video in (article.get('videos') or []):\n        video_type = video.get('videotype')\n        video_url = video.get('url')\n        if not (video_url and video_type in ('katsomo', 'youtube')):\n            continue\n        yield self.url_result(\n            video_url, video_type.capitalize(), video.get('video_id'))", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\tvn24.py", "class_name": null, "function_name": "extract_json", "parameters": ["attr", "name", "default", "fatal"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._parse_json", "self._search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_json(attr, name, default=NO_DEFAULT, fatal=True):\n    return self._parse_json(\n        self._search_regex(\n            r'\\b%s=([\"\\'])(?P<json>(?!\\1).+?)\\1' % attr, webpage,\n            name, group='json', default=default, fatal=fatal) or '{}',\n        display_id, transform_source=unescapeHTML, fatal=fatal)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\tvnow.py", "class_name": null, "function_name": "make_urls", "parameters": ["proto", "suffix"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["url_repl", "urls.append", "urls[0].replace"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def make_urls(proto, suffix):\n    urls = [url_repl(proto, suffix)]\n    hd_url = urls[0].replace('/manifest/', '/ngvod/')\n    if hd_url != urls[0]:\n        urls.append(hd_url)\n    return urls", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\twentythreevideo.py", "class_name": null, "function_name": "add_common_info_to_list", "parameters": ["l", "template", "id_field", "id_value"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "l.append", "photo_data.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_common_info_to_list(l, template, id_field, id_value):\n    f_base = template % id_value\n    f_path = photo_data.get(f_base + 'download')\n    if not f_path:\n        return\n    l.append({\n        id_field: id_value,\n        'url': base_url + f_path,\n        'width': int_or_none(photo_data.get(f_base + 'width')),\n        'height': int_or_none(photo_data.get(f_base + 'height')),\n        'filesize': int_or_none(photo_data.get(f_base + 'size')),\n    })", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\twitch.py", "class_name": "TwitchVideosIE", "function_name": "suitable", "parameters": ["cls", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "ie.suitable", "super", "super(TwitchVideosIE, cls).suitable"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def suitable(cls, url):\n    return (False\n            if any(ie.suitable(url) for ie in (\n                TwitchVideosClipsIE,\n                TwitchVideosCollectionsIE))\n            else super(TwitchVideosIE, cls).suitable(url))", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\twitch.py", "class_name": "TwitchStreamIE", "function_name": "suitable", "parameters": ["cls", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "ie.suitable", "super", "super(TwitchStreamIE, cls).suitable"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def suitable(cls, url):\n    return (False\n            if any(ie.suitable(url) for ie in (\n                TwitchVodIE,\n                TwitchCollectionIE,\n                TwitchVideosIE,\n                TwitchVideosClipsIE,\n                TwitchVideosCollectionsIE,\n                TwitchClipsIE))\n            else super(TwitchStreamIE, cls).suitable(url))", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\twitch.py", "class_name": null, "function_name": "login_step", "parameters": ["page", "urlh", "note", "data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["dict_get", "fail", "form.update", "json.dumps", "json.dumps(form).encode", "response.get", "self._download_json", "self._download_webpage_handle", "self._hidden_inputs", "self._search_regex", "urlh.geturl", "urljoin"], "control_structures": ["If"], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def login_step(page, urlh, note, data):\n    form = self._hidden_inputs(page)\n    form.update(data)\n\n    page_url = urlh.geturl()\n    post_url = self._search_regex(\n        r'<form[^>]+action=([\"\\'])(?P<url>.+?)\\1', page,\n        'post url', default=self._LOGIN_POST_URL, group='url')\n    post_url = urljoin(page_url, post_url)\n\n    headers = {\n        'Referer': page_url,\n        'Origin': 'https://www.twitch.tv',\n        'Content-Type': 'text/plain;charset=UTF-8',\n    }\n\n    response = self._download_json(\n        post_url, None, note, data=json.dumps(form).encode(),\n        headers=headers, expected_status=400)\n    error = dict_get(response, ('error', 'error_description', 'error_code'))\n    if error:\n        fail(error)\n\n    if 'Authenticated successfully' in response.get('message', ''):\n        return None, None\n\n    redirect_url = urljoin(\n        post_url,\n        response.get('redirect') or response['redirect_path'])\n    return self._download_webpage_handle(\n        redirect_url, None, 'Downloading login redirect page',\n        headers=headers)", "loc": 32}
{"file": "youtube-dl\\youtube_dl\\extractor\\twitter.py", "class_name": null, "function_name": "extract_from_video_info", "parameters": ["media"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_thumbnail", "float_or_none", "formats.extend", "info.update", "int_or_none", "media.get", "media.get('sizes', {}).items", "self._extract_variant_formats", "self._sort_formats", "size.get", "thumbnails.append", "update_url_query", "video_info.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_from_video_info(media):\n    video_info = media.get('video_info') or {}\n\n    formats = []\n    for variant in video_info.get('variants', []):\n        formats.extend(self._extract_variant_formats(variant, twid))\n    self._sort_formats(formats)\n\n    thumbnails = []\n    media_url = media.get('media_url_https') or media.get('media_url')\n    if media_url:\n        def add_thumbnail(name, size):\n            thumbnails.append({\n                'id': name,\n                'url': update_url_query(media_url, {'name': name}),\n                'width': int_or_none(size.get('w') or size.get('width')),\n                'height': int_or_none(size.get('h') or size.get('height')),\n            })\n        for name, size in media.get('sizes', {}).items():\n            add_thumbnail(name, size)\n        add_thumbnail('orig', media.get('original_info') or {})\n\n    info.update({\n        'formats': formats,\n        'thumbnails': thumbnails,\n        'duration': float_or_none(video_info.get('duration_millis'), 1000),\n    })", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\extractor\\twitter.py", "class_name": null, "function_name": "add_thumbnail", "parameters": ["name", "size"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "size.get", "thumbnails.append", "update_url_query"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_thumbnail(name, size):\n    thumbnails.append({\n        'id': name,\n        'url': update_url_query(media_url, {'name': name}),\n        'width': int_or_none(size.get('w') or size.get('width')),\n        'height': int_or_none(size.get('h') or size.get('height')),\n    })", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\udemy.py", "class_name": null, "function_name": "extract_output_format", "parameters": ["src", "f_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int_or_none", "src.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_output_format(src, f_id):\n    return {\n        'url': src.get('url'),\n        'format_id': '%sp' % (src.get('height') or f_id),\n        'width': int_or_none(src.get('width')),\n        'height': int_or_none(src.get('height')),\n        'vbr': int_or_none(src.get('video_bitrate_in_kbps')),\n        'vcodec': src.get('video_codec'),\n        'fps': int_or_none(src.get('frame_rate')),\n        'abr': int_or_none(src.get('audio_bitrate_in_kbps')),\n        'acodec': src.get('audio_codec'),\n        'asr': int_or_none(src.get('audio_sample_rate')),\n        'tbr': int_or_none(src.get('total_bitrate_in_kbps')),\n        'filesize': int_or_none(src.get('file_size_in_bytes')),\n    }", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\extractor\\udemy.py", "class_name": null, "function_name": "add_output_format_meta", "parameters": ["f", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_output_format", "isinstance", "output_format.update", "outputs.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_output_format_meta(f, key):\n    output = outputs.get(key)\n    if isinstance(output, dict):\n        output_format = extract_output_format(output, key)\n        output_format.update(f)\n        return output_format\n    return f", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\udemy.py", "class_name": null, "function_name": "extract_formats", "parameters": ["source_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["add_output_format_meta", "determine_ext", "formats.append", "formats.extend", "int_or_none", "isinstance", "self._extract_m3u8_formats", "source.get", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(source_list):\n    if not isinstance(source_list, list):\n        return\n    for source in source_list:\n        video_url = url_or_none(source.get('file') or source.get('src'))\n        if not video_url:\n            continue\n        if source.get('type') == 'application/x-mpegURL' or determine_ext(video_url) == 'm3u8':\n            formats.extend(self._extract_m3u8_formats(\n                video_url, video_id, 'mp4', entry_protocol='m3u8_native',\n                m3u8_id='hls', fatal=False))\n            continue\n        format_id = source.get('label')\n        f = {\n            'url': video_url,\n            'format_id': '%sp' % format_id,\n            'height': int_or_none(format_id),\n        }\n        if format_id:\n            # Some videos contain additional metadata (e.g.\n            # https://www.udemy.com/ios9-swift/learn/#/lecture/3383208)\n            f = add_output_format_meta(f, format_id)\n        formats.append(f)", "loc": 23}
{"file": "youtube-dl\\youtube_dl\\extractor\\udemy.py", "class_name": null, "function_name": "extract_subtitles", "parameters": ["track_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["isinstance", "sub_dict.setdefault", "sub_dict.setdefault(lang, []).append", "track.get", "url_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_subtitles(track_list):\n    if not isinstance(track_list, list):\n        return\n    for track in track_list:\n        if not isinstance(track, dict):\n            continue\n        if track.get('kind') != 'captions':\n            continue\n        src = url_or_none(track.get('src'))\n        if not src:\n            continue\n        lang = track.get('language') or track.get(\n            'srclang') or track.get('label')\n        sub_dict = automatic_captions if track.get(\n            'autogenerated') is True else subtitles\n        sub_dict.setdefault(lang, []).append({\n            'url': src,\n        })", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\extractor\\urplay.py", "class_name": null, "function_name": "parse_lang_code", "parameters": ["code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ISO639Utils.long2short", "ISO639Utils.short2long", "code.lower"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "3-character language code or None (utils candidate)", "source_code": "def parse_lang_code(code):\n    \"3-character language code or None (utils candidate)\"\n    if code is None:\n        return\n    lang = code.lower()\n    if not ISO639Utils.long2short(lang):\n        lang = ISO639Utils.short2long(lang)\n    return lang or None", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\ustudio.py", "class_name": null, "function_name": "extract", "parameters": ["kind"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["config.findall", "int_or_none", "item.get", "unescapeHTML"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract(kind):\n    return [{\n        'url': unescapeHTML(item.attrib['url']),\n        'width': int_or_none(item.get('width')),\n        'height': int_or_none(item.get('height')),\n    } for item in config.findall('./qualities/quality/%s' % kind) if item.get('url')]", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\videa.py", "class_name": null, "function_name": "compat_random_choices", "parameters": ["population"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "all", "kwargs.get", "random.choice", "range"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def compat_random_choices(population, *args, **kwargs):\n    # weights=None, *, cum_weights=None, k=1\n    # limited implementation needed here\n    weights = args[0] if args else kwargs.get('weights')\n    assert all(w is None for w in (weights, kwargs.get('cum_weights')))\n    k = kwargs.get('k', 1)\n    return ''.join(random.choice(population) for _ in range(k))", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\videa.py", "class_name": "VideaIE", "function_name": "rc4", "parameters": ["cipher_text", "key"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["compat_ord", "compat_struct_pack", "len", "list", "ord", "range", "res.decode"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def rc4(cipher_text, key):\n    res = b''\n\n    key_len = len(key)\n    S = list(range(256))\n\n    j = 0\n    for i in range(256):\n        j = (j + S[i] + ord(key[i % key_len])) % 256\n        S[i], S[j] = S[j], S[i]\n\n    i = 0\n    j = 0\n    for m in range(len(cipher_text)):\n        i = (i + 1) % 256\n        j = (j + S[i]) % 256\n        S[i], S[j] = S[j], S[i]\n        k = S[(S[i] + S[j]) % 256]\n        res += compat_struct_pack('B', k ^ compat_ord(cipher_text[m]))\n\n    return res.decode('utf-8')", "loc": 21}
{"file": "youtube-dl\\youtube_dl\\extractor\\vidlii.py", "class_name": null, "function_name": "add_format", "parameters": ["format_url", "height"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["formats.append", "int", "self._search_regex"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add_format(format_url, height=None):\n    height = int(self._search_regex(r'(\\d+)\\.mp4',\n                 format_url, 'height', default=360))\n\n    formats.append({\n        'url': format_url,\n        'format_id': '%dp' % height if height else None,\n        'height': height,\n    })", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\extractor\\viidea.py", "class_name": null, "function_name": "extract_part", "parameters": ["part_id"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["item_info.update", "lecture_info.copy", "parse_duration", "self._download_smil", "self._parse_smil", "self._sort_formats", "smil.find", "switch.attrib.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_part(part_id):\n    smil_url = '%s/%s/video/%s/smil.xml' % (base_url, lecture_slug, part_id)\n    smil = self._download_smil(smil_url, lecture_id)\n    info = self._parse_smil(smil, smil_url, lecture_id)\n    self._sort_formats(info['formats'])\n    info['id'] = lecture_id if not multipart else '%s_part%s' % (lecture_id, part_id)\n    info['display_id'] = lecture_slug if not multipart else '%s_part%s' % (lecture_slug, part_id)\n    if multipart:\n        info['title'] += ' (Part %s)' % part_id\n    switch = smil.find('.//switch')\n    if switch is not None:\n        info['duration'] = parse_duration(switch.attrib.get('dur'))\n    item_info = lecture_info.copy()\n    item_info.update(info)\n    return item_info", "loc": 15}
{"file": "youtube-dl\\youtube_dl\\extractor\\vimeo.py", "class_name": null, "function_name": "is_rented", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["label.endswith", "label.startswith", "purchase_option.get", "try_get", "vod.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_rented():\n    if '>You rented this title.<' in webpage:\n        return True\n    if try_get(config, lambda x: x['user']['purchased']):\n        return True\n    for purchase_option in (vod.get('purchase_options') or []):\n        if purchase_option.get('purchased'):\n            return True\n        label = purchase_option.get('label_string')\n        if label and (label.startswith('You rented this') or label.endswith(' remaining')):\n            return True\n    return False", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\vine.py", "class_name": null, "function_name": "video_url", "parameters": ["kind"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def video_url(kind):\n    for url_suffix in ('Url', 'URL'):\n        format_url = data.get('video%s%s' % (kind, url_suffix))\n        if format_url:\n            return format_url", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\vlive.py", "class_name": null, "function_name": "is_logged_in", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._download_json", "try_get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_logged_in():\n    login_info = self._download_json(\n        'https://www.vlive.tv/auth/loginInfo', None,\n        note='Downloading login info',\n        headers={'Referer': 'https://www.vlive.tv/home'})\n    return try_get(\n        login_info, lambda x: x['message']['login'], bool) or False", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\vlive.py", "class_name": null, "function_name": "get_common_fields", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["channel.get", "int_or_none", "post.get", "post.get('author', {}).get", "video.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_common_fields():\n    channel = post.get('channel') or {}\n    return {\n        'title': video.get('title'),\n        'creator': post.get('author', {}).get('nickname'),\n        'channel': channel.get('channelName'),\n        'channel_id': channel.get('channelCode'),\n        'duration': int_or_none(video.get('playTime')),\n        'view_count': int_or_none(video.get('playCount')),\n        'like_count': int_or_none(video.get('likeCount')),\n        'comment_count': int_or_none(video.get('commentCount')),\n    }", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\voxmedia.py", "class_name": null, "function_name": "create_entry", "parameters": ["provider_video_id", "provider_video_type", "title", "description"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["self._og_search_description", "self._og_search_title"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def create_entry(provider_video_id, provider_video_type, title=None, description=None):\n    video_url = {\n        'youtube': '%s',\n        'ooyala': 'ooyala:%s',\n        'volume': 'http://volume.vox-cdn.com/embed/%s',\n    }[provider_video_type] % provider_video_id\n    return {\n        '_type': 'url_transparent',\n        'url': video_url,\n        'title': title or self._og_search_title(webpage),\n        'description': description or self._og_search_description(webpage),\n    }", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\vvvvid.py", "class_name": null, "function_name": "ds", "parameters": ["h"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["c.append", "chr", "f", "g.index", "l.append", "len", "range"], "control_structures": ["For", "If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def ds(h):\n    g = \"MNOPIJKL89+/4567UVWXQRSTEFGHABCDcdefYZabstuvopqr0123wxyzklmnghij\"\n\n    def f(m):\n        l = []\n        o = 0\n        b = False\n        m_len = len(m)\n        while ((not b) and o < m_len):\n            n = m[o] << 2\n            o += 1\n            k = -1\n            j = -1\n            if o < m_len:\n                n += m[o] >> 4\n                o += 1\n                if o < m_len:\n                    k = (m[o - 1] << 4) & 255\n                    k += m[o] >> 2\n                    o += 1\n                    if o < m_len:\n                        j = (m[o - 1] << 6) & 255\n                        j += m[o]\n                        o += 1\n                    else:\n                        b = True\n                else:\n                    b = True\n            else:\n                b = True\n            l.append(n)\n            if k != -1:\n                l.append(k)\n            if j != -1:\n                l.append(j)\n        return l\n\n    c = []\n    for e in h:\n        c.append(g.index(e))\n\n    c_len = len(c)\n    for e in range(c_len * 2 - 1, -1, -1):\n        a = c[e % c_len] ^ c[(e + 1) % c_len]\n        c[e % c_len] = a\n\n    c = f(c)\n    d = ''\n    for e in c:\n        d += chr(e)\n\n    return d", "loc": 52}
{"file": "youtube-dl\\youtube_dl\\extractor\\vvvvid.py", "class_name": null, "function_name": "metadata_from_url", "parameters": ["r_url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["int", "mobj.group", "re.search"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def metadata_from_url(r_url):\n    if not info and r_url:\n        mobj = re.search(r'_(?:S(\\d+))?Ep(\\d+)', r_url)\n        if mobj:\n            info['episode_number'] = int(mobj.group(2))\n            season_number = mobj.group(1)\n            if season_number:\n                info['season_number'] = int(season_number)", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\vvvvid.py", "class_name": null, "function_name": "f", "parameters": ["m"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["l.append", "len"], "control_structures": ["If", "While"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def f(m):\n    l = []\n    o = 0\n    b = False\n    m_len = len(m)\n    while ((not b) and o < m_len):\n        n = m[o] << 2\n        o += 1\n        k = -1\n        j = -1\n        if o < m_len:\n            n += m[o] >> 4\n            o += 1\n            if o < m_len:\n                k = (m[o - 1] << 4) & 255\n                k += m[o] >> 2\n                o += 1\n                if o < m_len:\n                    j = (m[o - 1] << 6) & 255\n                    j += m[o]\n                    o += 1\n                else:\n                    b = True\n            else:\n                b = True\n        else:\n            b = True\n        l.append(n)\n        if k != -1:\n            l.append(k)\n        if j != -1:\n            l.append(j)\n    return l", "loc": 33}
{"file": "youtube-dl\\youtube_dl\\extractor\\wat.py", "class_name": null, "function_name": "extract_formats", "parameters": ["manifest_urls"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["f_url.replace", "formats.extend", "manifest_urls.items", "self._extract_m3u8_formats", "self._extract_mpd_formats"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_formats(manifest_urls):\n    for f, f_url in manifest_urls.items():\n        if not f_url:\n            continue\n        if f in ('dash', 'mpd'):\n            formats.extend(self._extract_mpd_formats(\n                f_url.replace('://das-q1.tf1.fr/', '://das-q1-ssl.tf1.fr/'),\n                video_id, mpd_id='dash', fatal=False))\n        elif f == 'hls':\n            formats.extend(self._extract_m3u8_formats(\n                f_url, video_id, 'mp4',\n                'm3u8_native', m3u8_id='hls', fatal=False))", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\extractor\\xfileshare.py", "class_name": null, "function_name": "aa_decode", "parameters": ["aa_code"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "aa_code.split", "c.replace", "c.startswith", "chr_from_code", "compat_chr", "int_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def aa_decode(aa_code):\n    symbol_table = (\n        ('7', '((ﾟｰﾟ) + (o^_^o))'),\n        ('6', '((o^_^o) +(o^_^o))'),\n        ('5', '((ﾟｰﾟ) + (ﾟΘﾟ))'),\n        ('2', '((o^_^o) - (ﾟΘﾟ))'),\n        ('4', '(ﾟｰﾟ)'),\n        ('3', '(o^_^o)'),\n        ('1', '(ﾟΘﾟ)'),\n        ('0', '(c^_^o)'),\n        ('+', ''),\n    )\n    delim = '(ﾟДﾟ)[ﾟεﾟ]+'\n\n    def chr_from_code(c):\n        for val, pat in symbol_table:\n            c = c.replace(pat, val)\n        if c.startswith(('u', 'U')):\n            base = 16\n            c = c[1:]\n        else:\n            base = 10\n        c = int_or_none(c, base=base)\n        return '' if c is None else compat_chr(c)\n\n    return ''.join(\n        chr_from_code(aa_char)\n        for aa_char in aa_code.split(delim))", "loc": 28}
{"file": "youtube-dl\\youtube_dl\\extractor\\xfileshare.py", "class_name": null, "function_name": "chr_from_code", "parameters": ["c"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["c.replace", "c.startswith", "compat_chr", "int_or_none"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def chr_from_code(c):\n    for val, pat in symbol_table:\n        c = c.replace(pat, val)\n    if c.startswith(('u', 'U')):\n        base = 16\n        c = c[1:]\n    else:\n        base = 10\n    c = int_or_none(c, base=base)\n    return '' if c is None else compat_chr(c)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\extractor\\yandexdisk.py", "class_name": null, "function_name": "call_api", "parameters": ["action"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["json.dumps", "json.dumps({'hash': video_hash, 'sk': sk}).encode", "self._download_json", "self._download_json(urljoin(url, '/public/api/') + action, video_id, data=json.dumps({'hash': video_hash, 'sk': sk}).encode(), headers={'Content-Type': 'text/plain'}, fatal=False) or {}.get", "urljoin"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def call_api(action):\n    return (self._download_json(\n        urljoin(url, '/public/api/') + action, video_id, data=json.dumps({\n            'hash': video_hash,\n            'sk': sk,\n        }).encode(), headers={\n            'Content-Type': 'text/plain',\n        }, fatal=False) or {}).get('data') or {}", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\extractor\\yandexmusic.py", "class_name": null, "function_name": "extract_artist_name", "parameters": ["artist"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["''.join", "artist.get", "element.get", "isinstance", "parts.append"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_artist_name(artist):\n    decomposed = artist.get('decomposed')\n    if not isinstance(decomposed, list):\n        return artist['name']\n    parts = [artist['name']]\n    for element in decomposed:\n        if isinstance(element, dict) and element.get('name'):\n            parts.append(element['name'])\n        elif isinstance(element, compat_str):\n            parts.append(element)\n    return ''.join(parts)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\yandexmusic.py", "class_name": null, "function_name": "extract_artist", "parameters": ["artist_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["', '.join", "a.get", "extract_artist_name", "isinstance"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_artist(artist_list):\n    if artist_list and isinstance(artist_list, list):\n        artists_names = [extract_artist_name(a) for a in artist_list if a.get('name')]\n        if artists_names:\n            return ', '.join(artists_names)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "get_fmt", "parameters": ["x"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["url_or_none", "x.get"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_fmt(x):\n    v_url = url_or_none(x.get('videoUrl'))\n    if v_url:\n        x['videoUrl'] = v_url\n        return (x['format'], x)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "get_format_data", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}-formats'.format", "self._download_json"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_format_data(f):\n    if f not in defs_by_format:\n        return []\n    return self._download_json(\n        defs_by_format[f]['videoUrl'], video_id, '{0}-formats'.format(f))", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "extract_tag_box", "parameters": ["regex", "title"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["re.findall", "self._search_regex"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_tag_box(regex, title):\n    tag_box = self._search_regex(regex, webpage, title, default=None)\n    if not tag_box:\n        return []\n    return re.findall(r'<a[^>]+href=[^>]+>([^<]+)', tag_box)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "yield_pages", "parameters": ["url", "html", "page_num"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["itertools.count", "self._download_webpage", "self._get_next_url"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yield_pages(url, html=html, page_num=page_num):\n    fatal = not html\n    for pnum in itertools.count(start=page_num or 1):\n        if not html:\n            html = self._download_webpage(\n                url, pl_id, note='Downloading page %d' % pnum,\n                fatal=fatal)\n        if not html:\n            break\n        fatal = False\n        yield (url, html, pnum)\n        # explicit page: extract just that page\n        if page_num is not None:\n            break\n        next_url = self._get_next_url(url, pl_id, html)\n        if not next_url or next_url == url:\n            break\n        url, html = next_url, None", "loc": 18}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "retry_page", "parameters": ["msg", "tries_left", "page_data"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["next", "self.report_warning", "sleep", "yield_pages"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def retry_page(msg, tries_left, page_data):\n    if tries_left <= 0:\n        return\n    self.report_warning(msg, pl_id)\n    sleep(self._PAGE_RETRY_DELAY)\n    return next(\n        yield_pages(page_data[0], page_num=page_data[2]), None)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\youporn.py", "class_name": null, "function_name": "yield_entries", "parameters": ["html"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_element_by_class", "m.group", "re.finditer", "re.search", "re.split", "self.url_result", "urljoin"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def yield_entries(html):\n    for frag in re.split(PLAYLIST_SECTION_RE, html):\n        if not frag:\n            continue\n        t_text = get_element_by_class('title-text', frag or '')\n        if not (t_text and re.search(self._PLAYLIST_TITLEBAR_RE, t_text)):\n            continue\n        for m in re.finditer(VIDEO_URL_RE, frag):\n            video_url = urljoin(url, m.group('url'))\n            if video_url:\n                yield self.url_result(video_url)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": "YoutubeIE", "function_name": "extract_id", "parameters": ["cls", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "mobj.group", "re.match"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_id(cls, url):\n    mobj = re.match(cls._VALID_URL, url, re.VERBOSE)\n    if mobj is None:\n        raise ExtractorError('Invalid URL: %s' % url)\n    return mobj.group(2)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": "YoutubePlaylistIE", "function_name": "suitable", "parameters": ["cls", "url"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["YoutubeTabIE.suitable", "parse_qs", "parse_qs(url).get", "super", "super(YoutubePlaylistIE, cls).suitable"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def suitable(cls, url):\n    if YoutubeTabIE.suitable(url):\n        return False\n    if parse_qs(url).get('v', [None])[0]:\n        return False\n    return super(YoutubePlaylistIE, cls).suitable(url)", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "req", "parameters": ["url", "f_req", "note", "errnote"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.update", "json.dumps", "login_form.copy", "re.sub", "self._download_json", "urlencode_postdata"], "control_structures": [], "behavior_type": ["serialization"], "doc_summary": "", "source_code": "def req(url, f_req, note, errnote):\n    data = login_form.copy()\n    data.update({\n        'pstMsg': 1,\n        'checkConnection': 'youtube',\n        'checkedDomains': 'youtube',\n        'hl': 'en',\n        'deviceinfo': '[null,null,null,[],null,\"US\",null,null,[],\"GlifWebSignIn\",null,[null,null,[]]]',\n        'f.req': json.dumps(f_req),\n        'flowName': 'GlifWebSignIn',\n        'flowEntry': 'ServiceLogin',\n        # TODO: reverse actual botguard identifier generation algo\n        'bgRequest': '[\"identifier\",\"\"]',\n    })\n    return self._download_json(\n        url, None, note=note, errnote=errnote,\n        transform_source=lambda s: re.sub(r'^[^[]*', '', s),\n        fatal=False,\n        data=urlencode_postdata(data), headers={\n            'Content-Type': 'application/x-www-form-urlencoded;charset=utf-8',\n            'Google-Accounts-XSRF': 1,\n        })", "loc": 22}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "gen_sig_code", "parameters": ["idxs"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'s[{0}{1}{2}]'.format", "_genslice", "str", "zip"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def gen_sig_code(idxs):\n    def _genslice(start, end, step):\n        starts = '' if start == 0 else str(start)\n        ends = (':%d' % (end + step)) if end + step >= 0 else ':'\n        steps = '' if step == 1 else (':%d' % step)\n        return 's[{0}{1}{2}]'.format(starts, ends, steps)\n\n    step = None\n    # Quelch pyflakes warnings - start will be set when step is set\n    start = '(Never used)'\n    for i, prev in zip(idxs[1:], idxs[:-1]):\n        if step is not None:\n            if i - prev == step:\n                continue\n            yield _genslice(start, prev, step)\n            step = None\n            continue\n        if i - prev in [-1, 1]:\n            step = i - prev\n            start = prev\n            continue\n        else:\n            yield 's[%d]' % prev\n    if step is None:\n        yield 's[%d]' % i\n    else:\n        yield _genslice(start, i, step)", "loc": 27}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "inner", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["ExtractorError", "func", "isinstance", "traceback.format_exc"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def inner(*args, **kwargs):\n    if cache_id not in self._player_cache:\n        try:\n            self._player_cache[cache_id] = func(*args, **kwargs)\n        except ExtractorError as e:\n            self._player_cache[cache_id] = e\n        except Exception as e:\n            self._player_cache[cache_id] = ExtractorError(traceback.format_exc(), cause=e)\n\n    ret = self._player_cache[cache_id]\n    if isinstance(ret, Exception):\n        raise ret\n    return ret", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "extract_nsig", "parameters": ["s"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["JSInterpreter.Exception", "func", "ret.endswith", "ret.startswith", "traceback.format_exc"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def extract_nsig(s):\n    try:\n        ret = func([s], kwargs={'_ytdl_do_not_return': s})\n    except JSInterpreter.Exception:\n        raise\n    except Exception as e:\n        raise JSInterpreter.Exception(traceback.format_exc(), cause=e)\n\n    if ret.startswith('enhanced_except_') or ret.endswith(s):\n        raise JSInterpreter.Exception('Signature function returned an exception')\n    return ret", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "chapter_time", "parameters": ["chapter"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["float_or_none", "try_get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def chapter_time(chapter):\n    return float_or_none(\n        try_get(\n            chapter,\n            lambda x: x['chapterRenderer']['timeRangeStartMillis'],\n            int),\n        scale=1000)", "loc": 7}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "is_agegated", "parameters": ["playability"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["any", "playability.get", "traverse_obj"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def is_agegated(playability):\n    # playability: dict\n    if not playability:\n        return False\n\n    if playability.get('desktopLegacyAgeGateReason'):\n        return True\n\n    reasons = traverse_obj(playability, (('status', 'reason'),))\n    AGE_GATE_REASONS = (\n        'confirm your age', 'age-restricted', 'inappropriate',  # reason\n        'age_verification_required', 'age_check_required',  # status\n    )\n    return any(expected in reason for expected in AGE_GATE_REASONS for reason in reasons)", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "build_fragments", "parameters": ["f"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}-{1}'.format", "LazyList", "min", "range", "update_url_query"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def build_fragments(f):\n    return LazyList({\n        'url': update_url_query(f['url'], {\n            'range': '{0}-{1}'.format(range_start, min(range_start + CHUNK_SIZE - 1, f['filesize'])),\n        })\n    } for range_start in range(0, f['filesize'], CHUNK_SIZE))", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "process_manifest_format", "parameters": ["f", "proto", "client_name", "itag", "all_formats"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["'{0}-{1}'.format", "T", "any", "f.get", "itags[itag].add", "join_nonempty", "q", "s.split", "traverse_obj", "try_call"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_manifest_format(f, proto, client_name, itag, all_formats=False):\n    key = (proto, f.get('language'))\n    if not all_formats and key in itags[itag]:\n        return False\n    itags[itag].add(key)\n\n    if itag:\n        f['format_id'] = (\n            '{0}-{1}'.format(itag, proto)\n            if all_formats or any(p != proto for p, _ in itags[itag])\n            else itag)\n\n    if f.get('source_preference') is None:\n        f['source_preference'] = -1\n\n    # Deprioritize since its pre-merged m3u8 formats may have lower quality audio streams\n    if client_name == 'web_safari' and proto == 'hls' and not is_live:\n        f['source_preference'] -= 1\n\n    if itag in ('616', '235'):\n        f['format_note'] = join_nonempty(f.get('format_note'), 'Premium', delim=' ')\n        f['source_preference'] += 100\n\n    f['quality'] = q(traverse_obj(f, (\n        'format_id', T(lambda s: itag_qualities[s.split('-')[0]])), default=-1))\n    if try_call(lambda: f['fps'] <= 1):\n        del f['fps']\n\n    if proto == 'hls' and f.get('has_drm'):\n        f['has_drm'] = 'maybe'\n        f['source_preference'] -= 5\n    return True", "loc": 32}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "process_language", "parameters": ["container", "base_url", "lang_code", "query"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["lang_subs.append", "query.update", "update_url_query"], "control_structures": ["For"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_language(container, base_url, lang_code, query):\n    lang_subs = []\n    for fmt in self._SUBTITLE_FORMATS:\n        query.update({\n            'fmt': fmt,\n            # xosf=1 causes undesirable text position data for vtt, json3 & srv* subtitles\n            # See: https://github.com/yt-dlp/yt-dlp/issues/13654\n            'xosf': []\n        })\n        lang_subs.append({\n            'ext': fmt,\n            'url': update_url_query(base_url, query),\n        })\n    container[lang_code] = lang_subs", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\extractor\\youtube.py", "class_name": null, "function_name": "process_subtitles", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["caption_track.get", "process_language", "self._yt_urljoin", "traverse_obj", "v.get"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def process_subtitles():\n    subtitles = {}\n    for caption_track in traverse_obj(pctr, (\n            Ellipsis, 'captionTracks', lambda _, v: (\n                v.get('baseUrl') and v.get('languageCode')))):\n        base_url = self._yt_urljoin(caption_track['baseUrl'])\n        if not base_url:\n            continue\n        lang_code = caption_track['languageCode']\n        if caption_track.get('kind') != 'asr':\n            process_language(\n                subtitles, base_url, lang_code, {})\n            continue\n        automatic_captions = {}\n        process_language(\n            automatic_captions, base_url, lang_code, {})\n        for translation_language in traverse_obj(pctr, (\n                Ellipsis, 'translationLanguages', lambda _, v: v.get('languageCode'))):\n            translation_language_code = translation_language['languageCode']\n            process_language(\n                automatic_captions, base_url, translation_language_code,\n                {'tlang': translation_language_code})\n        info['automatic_captions'] = automatic_captions\n    info['subtitles'] = subtitles", "loc": 24}
{"file": "youtube-dl\\youtube_dl\\extractor\\zdf.py", "class_name": null, "function_name": "check_video", "parameters": ["m"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["extract_attributes", "m.group", "self._search_regex", "v_ref.get"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_video(m):\n    v_ref = self._search_regex(\n        r'''(<a\\b[^>]*?\\shref\\s*=[^>]+?\\sdata-target-id\\s*=\\s*([\"'])%s\\2[^>]*>)''' % (m.group('p_id'), ),\n        webpage, 'check id', default='')\n    v_ref = extract_attributes(v_ref)\n    return v_ref.get('data-target-video-type') != 'novideo'", "loc": 6}
{"file": "youtube-dl\\youtube_dl\\extractor\\zingmp3.py", "class_name": null, "function_name": "entries", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["data.get", "self._extract_item"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def entries():\n    for item in (data.get('items') or []):\n        entry = self._extract_item(item, False)\n        if entry:\n            yield entry", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\__init__.py", "class_name": null, "function_name": "gen_extractors", "parameters": [], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["gen_extractor_classes", "klass"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a list of an instance of every supported extractor. The order does matter; the first extractor matched is the one handling the URL.", "source_code": "def gen_extractors():\n    \"\"\" Return a list of an instance of every supported extractor.\n    The order does matter; the first extractor matched is the one handling the URL.\n    \"\"\"\n    return [klass() for klass in gen_extractor_classes()]", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\extractor\\__init__.py", "class_name": null, "function_name": "list_extractors", "parameters": ["age_limit"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["filter", "gen_extractors", "ie.IE_NAME.lower", "ie.is_suitable", "sorted"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "Return a list of extractors that are suitable for the given age, sorted by extractor ID.", "source_code": "def list_extractors(age_limit):\n    \"\"\"\n    Return a list of extractors that are suitable for the given age,\n    sorted by extractor ID.\n    \"\"\"\n\n    return sorted(\n        filter(lambda ie: ie.is_suitable(age_limit), gen_extractors()),\n        key=lambda ie: ie.IE_NAME.lower())", "loc": 9}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\common.py", "class_name": "PostProcessor", "function_name": "try_utime", "parameters": ["self", "path", "atime", "mtime", "errnote"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encodeFilename", "os.utime", "self._downloader.report_warning"], "control_structures": ["Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def try_utime(self, path, atime, mtime, errnote='Cannot update utime of file'):\n    try:\n        os.utime(encodeFilename(path), (atime, mtime))\n    except Exception:\n        self._downloader.report_warning(errnote)", "loc": 5}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\execafterdownload.py", "class_name": "ExecAfterDownloadPP", "function_name": "run", "parameters": ["self", "information"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["PostProcessingError", "cmd.replace", "compat_shlex_quote", "encodeArgument", "self._downloader.to_screen", "subprocess.call"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, information):\n    cmd = self.exec_cmd\n    if '{}' not in cmd:\n        cmd += ' {}'\n\n    cmd = cmd.replace('{}', compat_shlex_quote(information['filepath']))\n\n    self._downloader.to_screen('[exec] Executing command: %s' % cmd)\n    retCode = subprocess.call(encodeArgument(cmd), shell=True)\n    if retCode != 0:\n        raise PostProcessingError(\n            'Command returned error code %d' % retCode)\n\n    return [], information", "loc": 14}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": "FFmpegPostProcessor", "function_name": "check_version", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["FFmpegPostProcessorError", "is_outdated_version", "self._downloader.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def check_version(self):\n    if not self.available:\n        raise FFmpegPostProcessorError('ffmpeg or avconv not found. Please install one.')\n\n    required_version = '10-0' if self.basename == 'avconv' else '1.0'\n    if is_outdated_version(\n            self._versions[self.basename], required_version):\n        warning = 'Your copy of %s is outdated, update %s to version %s or newer if you encounter any errors.' % (\n            self.basename, self.basename, required_version)\n        if self._downloader:\n            self._downloader.report_warning(warning)", "loc": 11}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": "FFmpegExtractAudioPP", "function_name": "run_ffmpeg", "parameters": ["self", "path", "out_path", "codec", "more_opts"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["AudioConversionError", "FFmpegPostProcessor.run_ffmpeg"], "control_structures": ["If", "Try"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run_ffmpeg(self, path, out_path, codec, more_opts):\n    if codec is None:\n        acodec_opts = []\n    else:\n        acodec_opts = ['-acodec', codec]\n    opts = ['-vn'] + acodec_opts + more_opts\n    try:\n        FFmpegPostProcessor.run_ffmpeg(self, path, out_path, opts)\n    except FFmpegPostProcessorError as err:\n        raise AudioConversionError(err.msg)", "loc": 10}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": "FFmpegVideoConvertorPP", "function_name": "run", "parameters": ["self", "information"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["options.extend", "path.rpartition", "self._downloader.to_screen", "self.run_ffmpeg"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, information):\n    path = information['filepath']\n    if information['ext'] == self._preferedformat:\n        self._downloader.to_screen('[ffmpeg] Not converting video file %s - already is in target format %s' % (path, self._preferedformat))\n        return [], information\n    options = []\n    if self._preferedformat == 'avi':\n        options.extend(['-c:v', 'libxvid', '-vtag', 'XVID'])\n    prefix, sep, ext = path.rpartition('.')\n    outpath = prefix + sep + self._preferedformat\n    self._downloader.to_screen('[' + 'ffmpeg' + '] Converting video from %s to %s, Destination: ' % (information['ext'], self._preferedformat) + outpath)\n    self.run_ffmpeg(path, outpath, options)\n    information['filepath'] = outpath\n    information['format'] = self._preferedformat\n    information['ext'] = self._preferedformat\n    return [path], information", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": "FFmpegMergerPP", "function_name": "run", "parameters": ["self", "info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["encodeFilename", "os.rename", "prepend_extension", "self._downloader.to_screen", "self.run_ffmpeg_multiple_files"], "control_structures": [], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, info):\n    filename = info['filepath']\n    temp_filename = prepend_extension(filename, 'temp')\n    args = ['-c', 'copy', '-map', '0:v:0', '-map', '1:a:0']\n    self._downloader.to_screen('[ffmpeg] Merging formats into \"%s\"' % filename)\n    self.run_ffmpeg_multiple_files(info['__files_to_merge'], temp_filename, args)\n    os.rename(encodeFilename(temp_filename), encodeFilename(filename))\n    return info['__files_to_merge'], info", "loc": 8}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": "FFmpegMergerPP", "function_name": "can_merge", "parameters": ["self"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["is_outdated_version", "self._downloader.report_warning"], "control_structures": ["If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def can_merge(self):\n    # TODO: figure out merge-capable ffmpeg version\n    if self.basename != 'avconv':\n        return True\n\n    required_version = '10-0'\n    if is_outdated_version(\n            self._versions[self.basename], required_version):\n        warning = ('Your copy of %s is outdated and unable to properly mux separate video and audio files, '\n                   'youtube-dl will download single file media. '\n                   'Update %s to version %s or newer to fix this.') % (\n                       self.basename, self.basename, required_version)\n        if self._downloader:\n            self._downloader.report_warning(warning)\n        return False\n    return True", "loc": 16}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": null, "function_name": "get_ffmpeg_version", "parameters": ["path"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["get_exe_version", "mobj.group", "re.match"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def get_ffmpeg_version(path):\n    ver = get_exe_version(path, args=['-version'])\n    if ver:\n        regexs = [\n            r'(?:\\d+:)?([0-9.]+)-[0-9]+ubuntu[0-9.]+$',  # Ubuntu, see [1]\n            r'n([0-9.]+)$',  # Arch Linux\n            # 1. http://www.ducea.com/2006/06/17/ubuntu-package-version-naming-explanation/\n        ]\n        for regex in regexs:\n            mobj = re.match(regex, ver)\n            if mobj:\n                ver = mobj.group(1)\n    return ver", "loc": 13}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\ffmpeg.py", "class_name": null, "function_name": "add", "parameters": ["meta_list", "info_list"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["info.get", "isinstance"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def add(meta_list, info_list=None):\n    if not info_list:\n        info_list = meta_list\n    if not isinstance(meta_list, (list, tuple)):\n        meta_list = (meta_list,)\n    if not isinstance(info_list, (list, tuple)):\n        info_list = (info_list,)\n    for info_f in info_list:\n        if info.get(info_f) is not None:\n            for meta_f in meta_list:\n                metadata[meta_f] = info[info_f]\n            break", "loc": 12}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\metadatafromtitle.py", "class_name": "MetadataFromTitlePP", "function_name": "format_to_regex", "parameters": ["self", "fmt"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["len", "match.end", "match.group", "match.start", "re.escape", "re.finditer"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "Converts a string like '%(title)s - %(artist)s' to a regex like", "source_code": "def format_to_regex(self, fmt):\n    r\"\"\"\n    Converts a string like\n       '%(title)s - %(artist)s'\n    to a regex like\n       '(?P<title>.+)\\ \\-\\ (?P<artist>.+)'\n    \"\"\"\n    lastpos = 0\n    regex = ''\n    # replace %(..)s with regex group and escape other string parts\n    for match in re.finditer(r'%\\((\\w+)\\)s', fmt):\n        regex += re.escape(fmt[lastpos:match.start()])\n        regex += r'(?P<' + match.group(1) + '>.+)'\n        lastpos = match.end()\n    if lastpos < len(fmt):\n        regex += re.escape(fmt[lastpos:])\n    return regex", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\metadatafromtitle.py", "class_name": "MetadataFromTitlePP", "function_name": "run", "parameters": ["self", "info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["match.groupdict", "match.groupdict().items", "re.match", "self._downloader.to_screen"], "control_structures": ["For", "If"], "behavior_type": ["logic"], "doc_summary": "", "source_code": "def run(self, info):\n    title = info['title']\n    match = re.match(self._titleregex, title)\n    if match is None:\n        self._downloader.to_screen(\n            '[fromtitle] Could not interpret title of video as \"%s\"'\n            % self._titleformat)\n        return [], info\n    for attribute, value in match.groupdict().items():\n        if value is None:\n            continue\n        info[attribute] = value\n        self._downloader.to_screen(\n            '[fromtitle] parsed %s: %s'\n            % (attribute, value if value is not None else 'NA'))\n\n    return [], info", "loc": 17}
{"file": "youtube-dl\\youtube_dl\\postprocessor\\xattrpp.py", "class_name": "XAttrMetadataPP", "function_name": "run", "parameters": ["self", "info"], "param_types": {}, "return_type": null, "param_doc": {}, "return_doc": "", "raises_doc": [], "called_functions": ["('Some ' if num_written else '') + 'extended attributes are not written.'.capitalize", "hyphenate_date", "info.get", "self._downloader.report_error", "self._downloader.report_warning", "self._downloader.to_screen", "str", "value.encode", "write_xattr", "xattr_mapping.items"], "control_structures": ["For", "If", "Try"], "behavior_type": ["logic"], "doc_summary": "Set extended attributes on downloaded file (if xattr support is found).", "source_code": "def run(self, info):\n    \"\"\" Set extended attributes on downloaded file (if xattr support is found). \"\"\"\n\n    # Write the metadata to the file's xattrs\n    self._downloader.to_screen('[metadata] Writing metadata to file\\'s xattrs')\n\n    filename = info['filepath']\n\n    try:\n        xattr_mapping = {\n            'user.xdg.referrer.url': 'webpage_url',\n            # 'user.xdg.comment':            'description',\n            'user.dublincore.title': 'title',\n            'user.dublincore.date': 'upload_date',\n            'user.dublincore.description': 'description',\n            'user.dublincore.contributor': 'uploader',\n            'user.dublincore.format': 'format',\n        }\n\n        num_written = 0\n        for xattrname, infoname in xattr_mapping.items():\n\n            value = info.get(infoname)\n\n            if value:\n                if infoname == 'upload_date':\n                    value = hyphenate_date(value)\n\n                byte_value = value.encode('utf-8')\n                write_xattr(filename, xattrname, byte_value)\n                num_written += 1\n\n        return [], info\n\n    except XAttrUnavailableError as e:\n        self._downloader.report_error(str(e))\n        return [], info\n\n    except XAttrMetadataError as e:\n        if e.reason == 'NO_SPACE':\n            self._downloader.report_warning(\n                'There\\'s no disk space left, disk quota exceeded or filesystem xattr limit exceeded. '\n                + (('Some ' if num_written else '') + 'extended attributes are not written.').capitalize())\n        elif e.reason == 'VALUE_TOO_LONG':\n            self._downloader.report_warning(\n                'Unable to write extended attributes due to too long values.')\n        else:\n            msg = 'This filesystem doesn\\'t support extended attributes. '\n            if compat_os_name == 'nt':\n                msg += 'You need to use NTFS.'\n            else:\n                msg += '(You may have to enable them in your /etc/fstab)'\n            self._downloader.report_error(msg)\n        return [], info", "loc": 54}
